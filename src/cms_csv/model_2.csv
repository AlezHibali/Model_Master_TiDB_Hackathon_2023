Blog Post - Title,Blog Post - Link,Collection ID,Item ID,Created On,Updated On,Published On,Blog Post - Featured Image (Page),Blog Post - Thumbnail Image (Card Grid),abstract,category,task,huggingface_link,paper,license,datasets,language,library,likes icon,likes,downloads icon,downloads_last_month,model size icon,model_size,usage icon,model_usage
wenyanwen-ancient-translate-to-modern,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,"This model translate Classical(ancient) Chinese to Modern Chinese, so I guess who's interested in the problemset can speak at least modern Chinese, hence... let me continue the documentation in Chinese; 这个模型已有做成应用， 【随无涯】是一个huggingface spaces + streamlit 的古文阅读应用（含海量书籍）， 可以在阅读时翻译
输入文言文， 可以是断句 或者 未断句的文言文， 模型会预测现代文的表述。 其他模型：; 从文言文到现代文的翻译器, 欢迎前往我的github文言诗词项目页面探讨、加?? ; 训练语料是就是九十多万句句对， 数据集链接?。 训练时source序列（古文序列）， 按照50%的概率整句去除所有标点符号。; 注意",natural-language-processing,translation,https://huggingface.co/raynardj/wenyanwen-ancient-translate-to-modern,,,,Chinese; Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 20,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 124,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 961.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
wenyanwen-chinese-translate-to-ancient,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,"This model translate modern Chinese to Classical Chinese, so I guess who's interested in the problemset can speak at least modern Chinese, so... let me continue the documentation in Chinese; 从现代文到文言文的翻译器, 欢迎前往github文言诗词项目页面:渊, 讨论&加?? ; 还有同款的?文言文到现代文模型，原文输入可以断句 也可以是未断句的哦; 训练语料是就是九十多万句句对， 数据集链接?。; 注意， 你必须将generate函数的eos_token_id设置为102就可以翻译出完整的语句， 不然翻译完了会有残留的语句(因为做熵的时候用pad标签=-100导致)。",natural-language-processing,translation,https://huggingface.co/raynardj/wenyanwen-chinese-translate-to-ancient,,License: apache-2.0,,Chinese; Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 203,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 961.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
distiluse-base-multilingual-cased-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net; This model was trained by sentence-transformers. ",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2,Paper: https://arxiv.org/pdf/1908.10084.pdf,License: apache-2.0,,multilingual,PyTorch; TensorFlow; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 75,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 66315,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 269
gtr-t5-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of sematic search.; This model was converted from the Tensorflow model gtr-large-1 to PyTorch. When using this model, have a look at the publication: Large Dual Encoders Are Generalizable Retrievers. The tfhub model and this PyTorch model can produce slightly different embeddings, however, when run on the same benchmarks, they produce identical results.; The model uses only the encoder from a T5-large model. The weights are stored in FP16.  ; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/gtr-t5-large,Paper: https://arxiv.org/pdf/2112.07899.pdf,License: apache-2.0,,English,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2057,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 672.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
paraphrase-mpnet-base-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/paraphrase-mpnet-base-v2,Paper: https://arxiv.org/pdf/1908.10084.pdf,License: apache-2.0,,,PyTorch; TensorFlow; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 102467,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 877.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
emotion-recognition-wav2vec2-IEMOCAP,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0941358df9b5cffb0d11b_Audio%20Classification.svg,,"This repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain. 
It is trained on IEMOCAP training data.; For a better experience, we encourage you to learn more about
SpeechBrain. The model performance on IEMOCAP test set is:; This system is composed of an wav2vec2 model. It is a combination of convolutional and residual blocks. The embeddings are extracted using attentive statistical pooling. The system is trained with Additive Margin Softmax Loss.  Speaker Verification is performed using cosine distance between speaker embeddings.; The system is trained with recordings sampled at 16kHz (single channel).
The code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling classify_file if needed.; First of all, please install the development version of SpeechBrain with the following command:",audio,audio-classification,https://huggingface.co/speechbrain/emotion-recognition-wav2vec2-IEMOCAP,Paper: https://arxiv.org/pdf/2106.04624.pdf,License: apache-2.0,Datasets: iemocap,English,speechbrain; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 39,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 76820,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 379.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
distilbart-cnn-12-6,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,This checkpoint should be loaded into BartForConditionalGeneration.from_pretrained. See the BART docs for more information.,natural-language-processing,summarization,https://huggingface.co/sshleifer/distilbart-cnn-12-6,,License: apache-2.0,Datasets: cnn_dailymail; xsum,English,PyTorch; JAX; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 143,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 553894,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 108
sbert-base-chinese-nli,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is the sentence embedding model pre-trained by UER-py, which is introduced in this paper.; ChineseTextualInference is used as training data. ; The model is fine-tuned by UER-py on Tencent Cloud. We fine-tune five epochs with a sequence length of 128 on the basis of the pre-trained model chinese_roberta_L-12_H-768. At the end of each epoch, the model is saved when the best performance on development set is achieved.; Finally, we convert the pre-trained model into Huggingface's format:",natural-language-processing,sentence-similarity,https://huggingface.co/uer/sbert-base-chinese-nli,Paper: https://arxiv.org/pdf/1909.05658.pdf,License: apache-2.0,,Chinese,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 59,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6654,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 409.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
finbert-tone,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"FinBERT is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens.; More technical details on FinBERT: Click Link; This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task. If you are simply interested in using FinBERT for financial tone analysis, give it a try.; If you use the model in your academic work, please cite the following paper:; Huang, Allen H., Hui Wang, and Yi Yang. ""FinBERT: A Large Language Model for Extracting Information from Financial Text."" Contemporary Accounting Research (2022).",natural-language-processing,text-classification,https://huggingface.co/yiyanghkust/finbert-tone,,,,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 94,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1212563,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 878.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 20
tapex-large-sql-execution,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c477a4006f982ddfda4a_Table%20Question%20Answering.svg,,"TAPEX was proposed in TAPEX: Table Pre-training via Learning a Neural SQL Executor by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. The original repo can be found here.; TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.; TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.; You can use the raw model for simulating neural SQL execution, i.e., employ TAPEX to execute a SQL query on a given table. However, the model is mostly meant to be fine-tuned on a supervised dataset. Currently TAPEX can be fine-tuned to tackle table question answering tasks and table fact verification tasks. See the model hub to look for fine-tuned versions on a task that interests you.; Here is how to use this model in transformers:",natural-language-processing,table-question-answering,https://huggingface.co/microsoft/tapex-large-sql-execution,Paper: https://arxiv.org/pdf/2107.07653.pdf,License: mit,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 645,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
xlm-roberta-base-conll2003-ner,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"This model is a fine-tuned version of xlm-roberta-base on the conll2003 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",natural-language-processing,token-classification,https://huggingface.co/Yaxin/xlm-roberta-base-conll2003-ner,,License: mit,Datasets: conll2003,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 924,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
tapex-base-finetuned-wtq,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c477a4006f982ddfda4a_Table%20Question%20Answering.svg,,"TAPEX was proposed in TAPEX: Table Pre-training via Learning a Neural SQL Executor by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. The original repo can be found here.; TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.; TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.; This model is the tapex-base model fine-tuned on the WikiTableQuestions dataset.; You can use the model for table question answering on complex questions. Some solveable questions are shown below (corresponding tables now shown):",natural-language-processing,table-question-answering,https://huggingface.co/microsoft/tapex-base-finetuned-wtq,Paper: https://arxiv.org/pdf/2107.07653.pdf,License: mit,Datasets: wikitablequestions,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4553,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
wav2vec2-large-robust-12-ft-emotion-msp-dim,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0941358df9b5cffb0d11b_Audio%20Classification.svg,,"The model expects a raw audio signal as input and outputs predictions for arousal, dominance and valence in a range of approximately 0...1. In addition, it also provides the pooled states of the last transformer layer. The model was created by fine-tuning 
Wav2Vec2-Large-Robust on MSP-Podcast (v1.7). The model was pruned from 24 to 12 transformer layers before fine-tuning. An ONNX export of the model is available from doi:10.5281/zenodo.6221127. Further details are given in the associated paper and tutorial.",audio,audio-classification,https://huggingface.co/audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim,Paper: https://arxiv.org/pdf/2203.07378.pdf,License: cc-by-nc-sa-4.0,Datasets: msp-podcast,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 31,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 49043,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 661.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
codegen-350M-mono,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"CodeGen is a family of autoregressive language models for program synthesis from the paper: A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B).; The checkpoint included in this repository is denoted as CodeGen-Mono 350M in the paper, where ""Mono"" means the model is initialized with CodeGen-Multi 350M and further pre-trained on a Python programming language dataset, and ""350M"" refers to the number of trainable parameters.; This checkpoint (CodeGen-Mono 350M) was firstly initialized with CodeGen-Multi 350M, and then pre-trained on BigPython dataset. The data consists of 71.7B tokens of Python programming language. See Section 2.1 of the paper for more details.; CodeGen was trained using cross-entropy loss to maximize the likelihood of sequential inputs.
The family of models are trained using multiple TPU-v4-512 by Google, leveraging data and model parallelism.
See Section 2.3 of the paper for more details.; We evaluate our models on two code generation benchmark: HumanEval and MTPB. Please refer to the paper for more details.",natural-language-processing,text-generation,https://huggingface.co/Salesforce/codegen-350M-mono,Paper: https://arxiv.org/pdf/2203.13474.pdf,License: bsd-3-clause,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 55,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 236397,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 800.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 17
layoutlmv3-base,,,,,,,,,"Microsoft Document AI | GitHub; LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis.; LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking
Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei, ACM Multimedia 2022.; If you find LayoutLM useful in your research, please cite the following paper:; The content of this project itself is licensed under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0).
Portions of the source code are based on the transformers project.
Microsoft Open Source Code of Conduct",,,https://huggingface.co/microsoft/layoutlmv3-base,Paper: https://arxiv.org/pdf/2204.08387.pdf,License: cc-by-nc-sa-4.0,,English,PyTorch; TensorFlow; ONNX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 176,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6337042,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 26
bart-sci-definition,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"This is a finetuned BART Large model from the paper:; ""Generating Scientific Definitions with Controllable Complexity"" ; By Tal August, Katharina Reinecke, and Noah A. Smith; Abstract: Unfamiliar terminology and complex language can present barriers to understanding science. Natural language processing stands to help address these issues by automatically defining unfamiliar terms. We introduce a new task and dataset for defining scientific terms and controlling the complexity of gen- erated definitions as a way of adapting to a specific reader’s background knowledge. We test four definition generation methods for this new task, finding that a sequence-to-sequence approach is most successful. We then explore the version of the task in which definitions are generated at a target complexity level. We in- troduce a novel reranking approach and find in human evaluations that it offers superior fluency while also controlling complexity, compared to several controllable generation baselines.; The model is finetuned on the task of generating definitions of scientific terms. We frame our task as generating an answer to the question “What is (are) X?” Along with the question, the model takes a support document of scientific abstracted related to the term being defined. ",natural-language-processing,text2text-generation,https://huggingface.co/talaugust/bart-sci-definition,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 26,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
opt-1.3b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"OPT was first introduced in Open Pre-trained Transformer Language Models and first released in metaseq's repository on May 3rd 2022 by Meta AI.; Disclaimer: The team releasing OPT wrote an official model card, which is available in Appendix D of the paper. 
Content from this model card has been written by the Hugging Face team.; To quote the first two paragraphs of the official paper; Large language models trained on massive text collections have shown surprising emergent
capabilities to generate text and perform zero- and few-shot learning. While in some cases the public
can interact with these models through paid APIs, full model access is currently limited to only a
few highly resourced labs. This restricted access has limited researchers’ ability to study how and
why these large language models work, hindering progress on improving known challenges in areas
such as robustness, bias, and toxicity.; We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M
to 175B parameters, which we aim to fully and responsibly share with interested researchers. We train the OPT models to roughly match 
the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data
collection and efficient training. Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and
to bring more voices to the table in studying the impact of these LLMs. Definitions of risk, harm, bias, and toxicity, etc., should be articulated by the
collective research community as a whole, which is only possible when models are available for study.",natural-language-processing,text-generation,https://huggingface.co/facebook/opt-1.3b,Paper: https://arxiv.org/pdf/2205.01068.pdf; https://arxiv.org/pdf/2005.14165.pdf,License: other,,English,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 98,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 125507,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 49
entity-extraction,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"This model is a fine-tuned version of distilbert-base-uncased on the conll2003 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",natural-language-processing,token-classification,https://huggingface.co/autoevaluate/entity-extraction,,License: apache-2.0,Datasets: conll2003; autoevaluate/conll2003-sample,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 580,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 267.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-keyword-extractor,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"This model is a fine-tuned version of bert-base-cased on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",natural-language-processing,token-classification,https://huggingface.co/yanekyuk/bert-keyword-extractor,,License: apache-2.0,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 619,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 432.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
DeBERTa-v3-large-mnli-fever-anli-ling-wanli,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c42cd649af0365a01a1e_Zero-Shot%20Classification.svg,,"This model was fine-tuned on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. This model is the best performing NLI model on the Hugging Face Hub as of 06.06.22 and can be used for zero-shot classification. It significantly outperforms all other large models on the ANLI benchmark.; The foundation model is DeBERTa-v3-large from Microsoft. DeBERTa-v3 combines several recent innovations compared to classical Masked Language Models like BERT, RoBERTa etc., see the paper; DeBERTa-v3-large-mnli-fever-anli-ling-wanli was trained on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. Note that SNLI was explicitly excluded due to quality issues with the dataset. More data does not necessarily make for better NLI models. ; DeBERTa-v3-large-mnli-fever-anli-ling-wanli was trained using the Hugging Face trainer with the following hyperparameters. Note that longer training with more epochs hurt performance in my tests (overfitting).; The model was evaluated using the test sets for MultiNLI, ANLI, LingNLI, WANLI and the dev set for Fever-NLI. The metric used is accuracy.
The model achieves state-of-the-art performance on each dataset. Surprisingly, it outperforms the previous state-of-the-art on ANLI (ALBERT-XXL) by 8,3%. I assume that this is because ANLI was created to fool masked language models like RoBERTa (or ALBERT), while DeBERTa-v3 uses a better pre-training objective (RTD), disentangled attention and I fine-tuned it on higher quality NLI data. ",natural-language-processing,zero-shot-classification,https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli,Paper: https://arxiv.org/pdf/2104.07179.pdf; https://arxiv.org/pdf/2111.09543.pdf,License: mit,Datasets: multi_nli; anli; fever; lingnli; alisawuffles/WANLI,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 40,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 35215,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
yalm-100b,,,,,,,,,"https://github.com/yandex/YaLM-100B; YaLM 100B is a GPT-like neural network for generating and processing text. It can be used freely by developers and researchers from all over the world.; The model leverages 100 billion parameters. It took 65 days to train the model on a cluster of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources in both English and Russian.; Training details and best practices on acceleration and stabilizations can be found on Medium (English) and Habr (Russian) articles.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/yandex/yalm-100b,,License: apache-2.0,,English; Russian,TensorBoard,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 104,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.92KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Randeng-Pegasus-523M-Summary-Chinese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"善于处理摘要任务，在数个中文摘要数据集上微调后的，中文版的PAGASUS-large。; Good at solving text summarization tasks, after fine-tuning on multiple Chinese text summarization datasets, Chinese PAGASUS-large.; 参考论文：PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization; 基于Randeng-Pegasus-523M-Chinese，我们在收集的7个中文领域的文本摘要数据集（约4M个样本）上微调了它，得到了summary版本。这7个数据集为：education, new2016zh, nlpcc, shence, sohu, thucnews和weibo。; Based on Randeng-Pegasus-523M-Chinese, we fine-tuned a text summarization version (summary) on 7 Chinese text summarization datasets, with totaling around 4M samples. The datasets include: education, new2016zh, nlpcc, shence, sohu, thucnews and weibo.",natural-language-processing,summarization,https://huggingface.co/IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese,Paper: https://arxiv.org/pdf/1912.08777.pdf; https://arxiv.org/pdf/2209.02970.pdf,,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 34,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2367,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
face-parsing,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c914a5f52efbcfb97cd1_Image%20Segmentation.svg,,,computer-vision,image-segmentation,https://huggingface.co/jonathandinu/face-parsing,,License: cc0-1.0,Datasets: celebamaskhq,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 520,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 678.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
nllb-200-distilled-1.3B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,"This is the model card of NLLB-200's distilled 1.3B variant.; Here are the metrics for that particular checkpoint.; ? Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.; ? We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.; ? In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).",natural-language-processing,translation,https://huggingface.co/facebook/nllb-200-distilled-1.3B,,License: cc-by-nc-4.0,Datasets: flores-200,196 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 44,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 40925,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 20
ruDialoGPT-medium,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,This generation model is based on sberbank-ai/rugpt3medium_based_on_gpt2. It's trained on large corpus of dialog data and can be used for buildning generative conversational agents; The model was trained with context size 3; On a private validation set we calculated metrics introduced in this paper: ; How to use:,natural-language-processing,conversational,https://huggingface.co/tinkoff-ai/ruDialoGPT-medium,Paper: https://arxiv.org/pdf/2001.09977.pdf,License: mit,,Russian,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 23,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1738,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
text2image-prompt-generator,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a GPT-2 model fine-tuned on the succinctly/midjourney-prompts dataset, which contains 250k text prompts that users issued to the Midjourney text-to-image service over a month period. For more details on how this dataset was scraped, see Midjourney User Prompts & Generated Images (250k).; This prompt generator can be used to auto-complete prompts for any text-to-image model (including the DALL・E family):
; Note that, while this model can be used together with any text-to-image model, it occasionally produces Midjourney-specific tags. Users can specify certain requirements via double-dashed parameters (e.g. --ar 16:9 sets the aspect ratio to 16:9, and --no snake asks the model to exclude snakes from the generated image) or set the importance of various entities in the image via explicit weights (e.g. hot dog::1.5 food::-1 is likely to produce the image of an animal instead of a frankfurter).; When using this model, please attribute credit to Succinctly AI.",natural-language-processing,text-generation,https://huggingface.co/succinctly/text2image-prompt-generator,,License: cc-by-2.0,Datasets: succinctly/midjourney-prompts,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 207,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 34589,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 668.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 32
xclip-base-patch32,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8c4b488c9621f5f7af4_Video%20Classification.svg,,"X-CLIP model (base-sized, patch resolution of 32) trained fully-supervised on Kinetics-400. It was introduced in the paper Expanding Language-Image Pretrained Models for General Video Recognition by Ni et al. and first released in this repository.; This model was trained using 8 frames per video, at a resolution of 224x224.; Disclaimer: The team releasing X-CLIP did not write a model card for this model so this model card has been written by the Hugging Face team.; X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. ; ",computer-vision,video-classification,https://huggingface.co/microsoft/xclip-base-patch32,Paper: https://arxiv.org/pdf/2208.02816.pdf,License: mit,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 10524,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 791.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
OPT-13B-Erebus,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is the second generation of the original Shinen made by Mr. Seeker. The full dataset consists of 6 different sources, all surrounding the ""Adult"" theme. The name ""Erebus"" comes from the greek mythology, also named ""darkness"". This is in line with Shin'en, or ""deep abyss"". For inquiries, please contact the KoboldAI community. Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The data can be divided in 6 different datasets:; The dataset uses [Genre: <comma-separated list of genres>] for tagging.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; Based on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion). Warning: This model has a very strong NSFW bias!",natural-language-processing,text-generation,https://huggingface.co/KoboldAI/OPT-13B-Erebus,Paper: https://arxiv.org/pdf/2205.01068.pdf,License: other,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 116,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 22382,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
sd-image-variations-diffusers,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"? V2 model released, and blurriness issues fixed! ?; ?? Image Variations is now natively supported in ? Diffusers! ??; ; This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of ""image variations"" similar to DALLE-2 using Stable Diffusion. This version of the weights has been ported to huggingface Diffusers, to use this with the Diffusers library requires the Lambda Diffusers repo.; This model was trained in two stages and longer than the original variations model and gives better image quality and better CLIP rated similarity compared to the original version",computer-vision,image-to-image,https://huggingface.co/lambdalabs/sd-image-variations-diffusers,,License: creativeml-openrail-m,Datasets: ChristophSchuhmann/improved_aesthetics_6plus,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 255,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 27209,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.61MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 113
CLIP-ViT-B-32-laion2B-s34B-b79K,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c6145e63a23d3f282ad5_Zero-Shot%20Image%20Classification.svg,,"A CLIP ViT-B/32 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Romain Beaumont on the stability.ai cluster. ; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. ; Zero-shot image classification, image and text retrieval, among others.",computer-vision,zero-shot-image-classification,https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K,Paper: https://arxiv.org/pdf/1910.04867.pdf,License: mit,,,PyTorch; OpenCLIP,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 786378,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
CLIP-ViT-g-14-laion2B-s12B-b42K,,,,,,,,,"A CLIP ViT-g/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Romain Beaumont on the stability.ai cluster. ; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. ; Zero-shot image classification, image and text retrieval, among others.",,,https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K,Paper: https://arxiv.org/pdf/1910.04867.pdf,License: mit,,,PyTorch; OpenCLIP,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5711,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 11.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
OPT-6.7B-Erebus,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is the second generation of the original Shinen made by Mr. Seeker. The full dataset consists of 6 different sources, all surrounding the ""Adult"" theme. The name ""Erebus"" comes from the greek mythology, also named ""darkness"". This is in line with Shin'en, or ""deep abyss"". For inquiries, please contact the KoboldAI community. Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The data can be divided in 6 different datasets:; The dataset uses [Genre: <comma-separated list of genres>] for tagging.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; Based on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion). Warning: This model has a very strong NSFW bias!",natural-language-processing,text-generation,https://huggingface.co/KoboldAI/OPT-6.7B-Erebus,Paper: https://arxiv.org/pdf/2205.01068.pdf,License: other,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 70,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 41789,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
RuLeanALBERT,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"RuLeanALBERT is a pretrained masked language model for the Russian language using a memory-efficient architecture.; Read more about the model in this blog post (in Russian).; See its implementation, as well as the pretraining and finetuning code, at https://github.com/yandex-research/RuLeanALBERT.",natural-language-processing,fill-mask,https://huggingface.co/yandex/RuLeanALBERT,,License: apache-2.0,,Russian,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 100,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
whisper-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al from OpenAI. The original code repository can be found here.; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. ; The models were trained on either English-only data or multilingual data. The English-only models were trained 
on the task of speech recognition. The multilingual models were trained on both speech recognition and speech 
translation. For speech recognition, the model predicts transcriptions in the same language as the audio. 
For speech translation, the model predicts transcriptions to a different language to the audio.",audio,automatic-speech-recognition,https://huggingface.co/openai/whisper-base,Paper: https://arxiv.org/pdf/2212.04356.pdf,License: apache-2.0,,99 languages,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 103,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 73220,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 875.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 124
whisper-small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al from OpenAI. The original code repository can be found here.; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. ; The models were trained on either English-only data or multilingual data. The English-only models were trained 
on the task of speech recognition. The multilingual models were trained on both speech recognition and speech 
translation. For speech recognition, the model predicts transcriptions in the same language as the audio. 
For speech translation, the model predicts transcriptions to a different language to the audio.",audio,automatic-speech-recognition,https://huggingface.co/openai/whisper-small,Paper: https://arxiv.org/pdf/2212.04356.pdf,License: apache-2.0,,99 languages,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 64,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 42652,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 66
vlt5-base-keywords,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Our vlT5 model is a keyword generation model based on encoder-decoder architecture using Transformer blocks presented by Google (https://huggingface.co/t5-base). The vlT5 was trained on scientific articles corpus to predict a given set of keyphrases based on the concatenation of the article’s abstract and title. It generates precise, yet not always complete keyphrases that describe the content of the article based only on the abstract.; Keywords generated with vlT5-base-keywords: encoder-decoder architecture, keyword generation; Results on demo model (different generation method, one model per language):; Our vlT5 model is a keyword generation model based on encoder-decoder architecture using Transformer blocks presented by Google (https://huggingface.co/t5-base). The vlT5 was trained on scientific articles corpus to predict a given set of keyphrases based on the concatenation of the article’s abstract and title. It generates precise, yet not always complete keyphrases that describe the content of the article based only on the abstract.; Keywords generated with vlT5-base-keywords: encoder-decoder architecture, vlT5, keyword generation, scientific articles corpus",natural-language-processing,text2text-generation,https://huggingface.co/Voicelab/vlt5-base-keywords,Paper: https://arxiv.org/pdf/2209.14008.pdf,License: cc-by-4.0,Datasets: posmac,Polish; English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 10377,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
robo-diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,There is a new model based on stable-dffusion 2.0 (base) that can be found here!; A dreambooth-method finetune of stable diffusion that will output cool looking robots when prompted.; ; Github: https://github.com/nousr/robo-diffusion; Keep the words nousr robot towards the beginning of your prompt to invoke the finetuned style.,multimodel,text-to-image,https://huggingface.co/nousr/robo-diffusion,,License: creativeml-openrail-m,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 345,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1592,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 483.75KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 150
Arcane-Diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This is the fine-tuned Stable Diffusion model trained on images from the TV Show Arcane.
Use the tokens arcane style in your prompts for the effect.; If you enjoy my work, please consider supporting me
; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; You can also export the model to ONNX, MPS and/or FLAX/JAX.; We also support a Gradio Web UI and Colab with Diffusers to run fine-tuned Stable Diffusion models:

",multimodel,text-to-image,https://huggingface.co/nitrosocke/Arcane-Diffusion,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 737,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9470,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 396
clip-variants,,,,,,,,,"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.; See the original CLIP Model Card for more details on limitations and biases.; This repository holds OpenAI's CLIP models converted into many other variants, see below for more details.; I haven't done many tests on these conversions. I've briefly tried the float16 versions, which seem very similar to the original float32, however the similarity seems to drop more with the qint8/quint8 versions as expected. I couldn't try qint8 as it seemed unsupported for some operations, but I'm including it for completeness. From a brief test the quint8 version seemed to work fine.; The license for the conversion code is MIT, the license for the models is the same as the original license for the OpenAI models (??♂?). I have no affiliation with OpenAI.",,,https://huggingface.co/mlunar/clip-variants,,License: mit,,English,ONNX,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 914.95KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bloomz-560m,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",natural-language-processing,text-generation,https://huggingface.co/bigscience/bloomz-560m,Paper: https://arxiv.org/pdf/2211.01786.pdf,License: bigscience-bloom-rail-1.0,Datasets: bigscience/xP3,46 languages,PyTorch; TensorBoard; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 66,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 19417,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
bloomz-3b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",natural-language-processing,text-generation,https://huggingface.co/bigscience/bloomz-3b,Paper: https://arxiv.org/pdf/2211.01786.pdf,License: bigscience-bloom-rail-1.0,Datasets: bigscience/xP3,46 languages,PyTorch; TensorBoard; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 60,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 58097,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 12.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
polyglot-ko-12.8b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Polyglot-Ko is a series of large-scale Korean autoregressive language models made by the EleutherAI polyglot team.; The model consists of 40 transformer layers with a model dimension of 5120, and a feedforward dimension of 20480. The model
dimension is split into 40 heads, each with a dimension of 128. Rotary Position Embedding (RoPE) is applied to 64
dimensions of each head. The model is trained with a tokenization vocabulary of 30003.; Polyglot-Ko-12.8B was trained on 863 GB of Korean language data (1.2TB before processing), a large-scale dataset curated by TUNiB. The data collection process has abided by South Korean laws. This dataset was collected for the purpose of training Polyglot-Ko models, so it will not be released for public use.  ; Furthermore, in order to avoid the model memorizing and generating personally identifiable information (PII) in the training data, we masked out the following sensitive information in the pre-processing stage:; Polyglot-Ko-12.8B was trained for 167 billion tokens over 301,000 steps on 256 A100 GPUs with the GPT-NeoX framework. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token. ",natural-language-processing,text-generation,https://huggingface.co/EleutherAI/polyglot-ko-12.8b,Paper: https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2204.04541.pdf; https://arxiv.org/pdf/2306.02254.pdf,License: apache-2.0,,Korean,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 47,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 14838,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mt0-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find our resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",natural-language-processing,text2text-generation,https://huggingface.co/bigscience/mt0-base,Paper: https://arxiv.org/pdf/2211.01786.pdf,License: apache-2.0,Datasets: bigscience/xP3; mc4,101 languages,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3012,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
mt0-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find our resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",natural-language-processing,text2text-generation,https://huggingface.co/bigscience/mt0-large,Paper: https://arxiv.org/pdf/2211.01786.pdf,License: apache-2.0,Datasets: bigscience/xP3; mc4,101 languages,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4012,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
mt0-xxl-mt,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find our resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",natural-language-processing,text2text-generation,https://huggingface.co/bigscience/mt0-xxl-mt,Paper: https://arxiv.org/pdf/2211.01786.pdf,License: apache-2.0,Datasets: bigscience/xP3mt; mc4,101 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 39,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1035,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 56.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
OCR-Donut-CORD,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"Donut model fine-tuned on CORD. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository.; Disclaimer: The team releasing Donut did not write a model card for this model so this model card has been written by the Hugging Face team.; Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. ; ; This model is fine-tuned on CORD, a document parsing dataset.",multimodel,image-to-text,https://huggingface.co/jinhybr/OCR-Donut-CORD,Paper: https://arxiv.org/pdf/2111.15664.pdf,License: mit,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2606,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 814.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
ldm-super-resolution-4x-openimages,,,,,,,,,"Paper: High-Resolution Image Synthesis with Latent Diffusion Models; Abstract:; By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.; Authors; Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj?rn Ommer",,,https://huggingface.co/CompVis/ldm-super-resolution-4x-openimages,Paper: https://arxiv.org/pdf/2112.10752.pdf,License: apache-2.0,,,Diffusers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 60,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4595,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.55KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
galactica-6.7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; Model card from the original repo ; Following Mitchell et al. (2018), this model card provides information about the GALACTICA model, how it was trained, and the intended use cases. Full details about how the model was trained and evaluated can be found in the release paper.; The GALACTICA models are trained on a large-scale scientific corpus. The models are designed to perform scientific tasks, including but not limited to citation prediction, scientific QA, mathematical reasoning, summarization, document generation, molecular property prediction and entity extraction. The models were developed by the Papers with Code team at Meta AI to study the use of language models for the automatic organization of science. We train models with sizes ranging from 125M to 120B parameters. Below is a summary of the released models:; November 2022",natural-language-processing,text-generation,https://huggingface.co/facebook/galactica-6.7b,Paper: https://arxiv.org/pdf/1810.03993.pdf,License: cc-by-nc-4.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 79,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2782,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
artstation-diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"artstation-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality Artstation images through fine-tuning.; Aspect Ratio Bucketing has been used during finetuning. This model can generate different aspect ratios VERY WELL.; knight, full body study, concept art, atmospheric; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
The CreativeML OpenRAIL License specifies: ; This model can be used for entertainment purposes and as a generative art assistant.",multimodel,text-to-image,https://huggingface.co/hakurei/artstation-diffusion,,License: creativeml-openrail-m,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 94,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 310,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.86KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
Future-Diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This is the fine-tuned Stable Diffusion 2.0 model trained on high quality 3D images with a futuristic Sci-Fi theme.
Use the tokensfuture style in your prompts for the effect.
Trained on Stability.ai's  Stable Diffusion 2.0 Base with 512x512 resolution.; If you enjoy my work and want to test new models before release, please consider supporting me
; Disclaimer: The SD 2.0 model is just over 24h old at this point and we still need to figure out how it works exactly. Please view this as an early prototype and experiment with the model.; Characters rendered with the model:

Cars and Animals rendered with the model:

Landscapes rendered with the model:
; future style [subject] Negative Prompt: duplicate heads bad anatomy Steps: 20, Sampler: Euler a, CFG scale: 7, Size: 512x704",multimodel,text-to-image,https://huggingface.co/nitrosocke/Future-Diffusion,,License: openrail++,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 388,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8716,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 31
text-to-sql-with-table-schema,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"There are newer version of this using Flan-T5 as a based model. You can check out here; PS. From this discussion, I think the base model that I use for finetune did not support the token <, so this might not be a good model to do this tasks. ",natural-language-processing,text2text-generation,https://huggingface.co/juierror/text-to-sql-with-table-schema,,,Datasets: wikisql,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 36,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3930,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 894.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
modelz_base,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/uwg/modelz_base,,,,,ONNX,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 47.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
FinBERT-PT-BR,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"FinBERT-PT-BR is a pre-trained NLP model to analyze sentiment of Brazilian Portuguese financial texts.; The model was trained in two main stages: language modeling and sentiment modeling. In the first stage, a language model was trained with more than 1.4 million texts of financial news in Portuguese. 
From this first training, it was possible to build a sentiment classifier with few labeled texts (500) that presented a satisfactory convergence.; At the end of the work, a comparative analysis with other models and the possible applications of the developed model are presented. 
In the comparative analysis, it was possible to observe that the developed model presented better results than the current models in the state of the art. 
Among the applications, it was demonstrated that the model can be used to build sentiment indices, investment strategies and macroeconomic data analysis, such as inflation.; ; In order to use the model, you need to get the HuggingFace auth token. You can get it here.",natural-language-processing,text-classification,https://huggingface.co/lucas-leme/FinBERT-PT-BR,,License: apache-2.0,,Portuguese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 831,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 437.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
scientific_abstract_simplification,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Scientific Abstract Simplification (SAS) is a tool designed to rewrite complex scientific abstracts into simpler, more comprehensible versions. Our objective is to make scientific knowledge universally accessible. If you have already experimented with our baseline model (sas_baseline), you will find that the current model surpasses its predecessor in terms of all evaluation metrics. Feel free to test it via the Hosted Inference API to your right. Simply select one of the provided examples or input your own scientific abstract. Just ensure to precede your text with the instruction, ""summarize, simplify, and contextualize: "", followed by a space. For local usage, refer to the Usage section.""; Open science has significantly reduced barriers to accessing scientific papers.
However, attainable research does not entail accessible knowledge.
Consequently, many individuals might prefer to rely on succinct social media narratives rather than endeavour to comprehend a scientific paper.
This preference is understandable as humans often favor narratives over dry, technical information. 
So, why not ""translate"" these intricate scientific abstracts into simpler, more accessible narratives? 
Several prestigious journals have already initiated steps towards enhancing accessibility. 
For instance, PNAS requires authors to submit Significance Statements understandable to an 'undergraduate-educated scientist', while Science includes an editor's abstract to provide a swift overview of the paper's salient points.; In this project, our objective is to employ AI to rewrite scientific abstracts into easily understandable scientific narratives.
To facilitate this, we have curated two new datasets: one containing PNAS abstract-significance pairs and the other encapsulating editor abstracts from Science.
We utilize a Transformer model (a variant known as Flan-T5) to fine-tune our model for the task of simplifying scientific abstracts.
Initially, the model is fine-tuned utilizing multiple discrete instructions by amalgamating four pertinent tasks in a challenge-proportional manner (a strategy we refer to as Multi-Instruction Pretuning).
Subsequently, we continue the fine-tuning process exclusively with the abstract-significance corpus. Our model can generate lay summaries that outperform models fine-tuned solely with the abstract-significance corpus and models fine-tuned with traditional task combinations.
We hope our work can foster a more comprehensive understanding of scientific research, enabling a larger audience to benefit from open science.; Use the code below to get started with the model. Remember to prepend the INSTRUCTION for best performance.; We finetuned the base model (flan-t5-large) on multiple relevant tasks with standard language modeling loss. During training, the source text of each task is prepended with an task-specific instruction and mapped to the corresponding target text. For example, ""simplify: "" is added before a wiki text, and the whole text is fed into the model to line up with the corresponding simple wiki text. The tuning process has two steps.",natural-language-processing,text2text-generation,https://huggingface.co/haining/scientific_abstract_simplification,,License: mit,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 234,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
whisper-medium-jp,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"This model is a fine-tuned version of openai/whisper-medium on the common_voice_11_0 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",audio,automatic-speech-recognition,https://huggingface.co/vumichien/whisper-medium-jp,,License: apache-2.0,Datasets: mozilla-foundation/common_voice_11_0,Japanese,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2421,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 15
Analog-Diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Analog Diffusion

CKPT DOWNLOAD LINK - This is a dreambooth model trained on a diverse set of analog photographs.; In your prompt, use the activation token: analog style; You may need to use the words blur haze naked in your negative prompts. My dataset did not include any NSFW material but the model seems to be pretty horny. Note that using blur and haze in your negative prompt can give a sharper image but also a less pronounced analog film effect.; Trained from 1.5 with VAE.; Please see this document where I share the parameters (prompt, sampler, seed, etc.) used for all example images.",multimodel,text-to-image,https://huggingface.co/wavymulder/Analog-Diffusion,,License: creativeml-openrail-m,,English,Diffusers; Safetensors,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 813,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 19923,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 131
bad-artist,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"; The images above were generated with only ""solo"" in the positive prompt, and ""sketch by bad-artist"" (this embedding) in the negative.

The embedding uses only 2 tokens.; Textual-inversion embedding for use in unconditional (negative) prompt.

Inspired partly by https://huggingface.co/datasets/Nerfgun3/bad_prompt.; There are currently 2 version:; I recommend using with 'by', so for example ""sketch by bad-artist"", or ""painting by bad-artist"", or ""photograph by bad-artist"", etc.",multimodel,text-to-image,https://huggingface.co/nick-x-hacker/bad-artist,,License: cc0-1.0,,eng,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 283,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.09KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
riffusion-model-v1,,,,,,,,,"Riffusion is an app for real-time music generation with stable diffusion.; Read about it at https://www.riffusion.com/about and try it at https://www.riffusion.com/.; This repository contains the model files, including:; Riffusion is a latent text-to-image diffusion model capable of generating spectrogram images given any text input. These spectrograms can be converted into audio clips.; The model was created by Seth Forsgren and Hayk Martiros as a hobby project.",,,https://huggingface.co/riffusion/riffusion-model-v1,Paper: https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/2210.08402.pdf,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 463,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 51438,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 15.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 37
GTA5_Artwork_Diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This model was trained on the loading screens, gta storymode, and gta online DLCs artworks.
Which includes characters, background, chop, and some objects.
The model can do people and portrait pretty easily, as well as cars, and houses.
For some reasons, the model stills automatically include in some game footage, so landscapes tend to look a bit more game-like.
Please check out important informations on the usage of the model down bellow.; To reference the art style, use the token: gtav style; There is already an existing model that uses textual inversion. This is trained using Dreambooth instead, whether or not this method is better, I will let you judge.; We support a Gradio Web UI to run GTA5_Artwork_Diffusion:
; Here are some samples.",multimodel,text-to-image,https://huggingface.co/ItsJayQz/GTA5_Artwork_Diffusion,,License: creativeml-openrail-m,,English,Diffusers; Safetensors,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 99,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1718,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 33
models,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/emmajoanne/models,,License: afl-3.0,,English; Chinese; Japanese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 47,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 139.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
karlo-v1-alpha,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Karlo is a text-conditional image generation model based on OpenAI's unCLIP architecture with the improvement over the standard super-resolution model from 64px to 256px, recovering high-frequency details only in the small number of denoising steps.; Karlo is available in diffusers!; ; ; Karlo is a text-conditional diffusion model based on unCLIP, composed of prior, decoder, and super-resolution modules. In this repository, we include the improved version of the standard super-resolution module for upscaling 64px to 256px only in 7 reverse steps, as illustrated in the figure below:",multimodel,text-to-image,https://huggingface.co/kakaobrain/karlo-v1-alpha,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 73,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3933,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.03KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 23
dreambooth-avatar,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Dreambooth finetuning of Stable Diffusion (v1.5.1) on Avatar art style by Lambda Labs.; This text-to-image stable diffusion model was trained with dreambooth.Put in a text prompt and generate your own Avatar style image!; ; To run model locally:; Base model is Stable Diffusion v1.5 and was trained using Dreambooth with 60 input images sized 512x512 displaying Avatar character images.
The model is learning to associate Avatar images with the style tokenized as 'avatarart style'.
Prior preservation was used during training using the class 'Person' to avoid training bleeding into the representations for that class.
Training ran on 2xA6000 GPUs on Lambda GPU Cloud for 700 steps, batch size 4 (a couple hours, at a cost of about $4).",multimodel,text-to-image,https://huggingface.co/lambdalabs/dreambooth-avatar,,,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 39,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 739,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.7KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
instructor-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"We introduce Instructor???, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor?? achieves sota on 70 diverse embedding tasks!
The model is easy to use with our customized sentence-transformer library. For more details, check out our paper and project page! ; **************************** Updates ****************************; Then you can use the model like this to calculate domain-specific and task-aware embeddings:; If you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions: ; ??????????????????????????Represent the domain text_type for task_objective:",natural-language-processing,sentence-similarity,https://huggingface.co/hkunlp/instructor-base,Paper: https://arxiv.org/pdf/2212.09741.pdf,License: apache-2.0,,English,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 64,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4747,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 442.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 13
SkyPaint,,,,,,,,,"SkyPaint is a Chinese-English bilingual text-generated image project developed by Singularity-AI. It is still being updated and optimized.; The SkyPaint text generation image model is mainly composed of two parts, namely the prompt word text encoder model and the diffusion model. Therefore, our optimization is also divided into two steps. First, based on OpenAI-CLIP, we optimized the prompt word text encoder model to make SkyPaint have the ability to recognize Chinese and English, and then optimized the diffusion model, so that SkyPaint has modern artistic capabilities and can produce high-quality pictures.; Chinese and English mixed prompt word input.
Generating high-quality images in a modern art style.
English prompt words for stable_diffusion_1.x official model and related fine-tuning models.
Retain usage habits and methods of stable_diffusion prompt words.
Introduction to SkyCLIP Models
SkyCLIP is a CLIP model obtained by using an efficient method of training Chinese-English bilingual CLIP models. This method only needs to use text data to achieve efficient distillation of the OpenAI-CLIP model, which greatly reduces the data threshold. At the same time, training requires Compared with the original CLIP model, the computing power requirement is reduced by more than 90%, which is convenient for the open source community to reproduce/fine-tune. This method only changes the text encoder of OpenAI-CLIP, and can be used with the image encoder of OpenAI-CLIP to realize the image-text retrieval function.; 机械狗
; 城堡 大海 夕阳 宫崎骏动画
",,,https://huggingface.co/SkyWork/SkyPaint,,,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 46,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 19.91KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
furrydiffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"; FurryDiffusion is a model made to generate furry art, this model is very much in beta still and will keep improoving! To use this please make sure to include furry in your prompt and to make a specific breed add the breed name only.; Example Prompts:; Test the concept via A1111 Colab fast-Colab-A1111
Or you can run your new concept via diffusers Colab Notebook for Inference; NOTE: Its better to run it in Google Colab since you can use google's powerful gpu's for free. Go ahead try it now!",multimodel,text-to-image,https://huggingface.co/lunarfish/furrydiffusion,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 42,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2956,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
7th_Layer,,,,,,,,,"default CFG Scale : 7 ±5; default Sampler : DPM++ 2M Karras; default Steps : 25; Negative prompt : (worst quality:1.4), (low quality:1.4) , (monochrome:1.1),; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/syaimu/7th_Layer,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 579,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.67KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
stable-diffusion-logo-fine-tuned,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"this Stable diffusion model i have fine tuned on 1000 raw logo png/jpg images of of size 128x128 with augmentation ; Enjoy .create any type of logo; for examples:""Logo of a pirate"",""logo of a sunglass with girl"" or something complex like ""logo of a ice-cream with snake"" etc",multimodel,text-to-image,https://huggingface.co/nicky007/stable-diffusion-logo-fine-tuned,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 78,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4476,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 14
CLIP-ViT-B-16-laion2B-s34B-b88K,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c6145e63a23d3f282ad5_Zero-Shot%20Image%20Classification.svg,,"A CLIP ViT-B/16 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Mehdi Cherti on the JUWELS Booster supercomputer. See acknowledgements below.; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. ; Zero-shot image classification, image and text retrieval, among others.",computer-vision,zero-shot-image-classification,https://huggingface.co/laion/CLIP-ViT-B-16-laion2B-s34B-b88K,Paper: https://arxiv.org/pdf/1910.04867.pdf,License: mit,,,OpenCLIP,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 202680,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
graphormer-base-pcqm4mv2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c606b488c9621f5c591c_Graph%20Machine%20Learning.svg,,"The Graphormer is a graph classification model.; The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2.; This model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.; The Graphormer model is ressource intensive for large graphs, and might lead to OOM errors.; See the Graph Classification with Transformers tutorial.",multimodel,graph-machine-learning,https://huggingface.co/clefourrier/graphormer-base-pcqm4mv2,Paper: https://arxiv.org/pdf/2106.05234.pdf,License: mit,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2520,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 191.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
pygmalion-2.7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"Pymalion 2.7B is a proof-of-concept dialogue model based on EleutherAI's gpt-neo-2.7B.; Warning: This model is NOT suitable for use by minors. It will output X-rated content under certain circumstances.; The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations.; Model weights were initialized from the uft-2.7b ConvoGPT model made available in this commit.; The model was then further fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.",natural-language-processing,conversational,https://huggingface.co/PygmalionAI/pygmalion-2.7b,,License: creativeml-openrail-m,,English,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 44,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 14864,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
HassanBlend1.5.1.2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"I am hassan, I created HassansBlend, the latest version currently is 1.5.1.2 I continue to iterate and improve on this model over time. Feel free to check out our discord or rentry page for more examples with prompts and outputs generated.; This blend is finetuned over SD1.5 with thousands of images included in the dataset it was trained with. Along with that there are some minor merges added in just to soften it up and increase the creativity. 
I have also some custom created content such as enhancement hypernetworks/embeddings etc for patreons or KoFi subscribers only on my pages below
 Links 
Patreon

KoFi

Discord
; Model details and examples with sample prompts: https://rentry.org/sdhassan",multimodel,text-to-image,https://huggingface.co/hassanblend/HassanBlend1.5.1.2,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 207,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2049,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 48.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 39
SD_Photoreal_Merged_Models,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"for Stable Diffusion Webui Automatic1111
type: .safetensors(ckpt)
CFG Scale: middle-low; example.
low quality, worst quality, bad anatomy, bad proportions; UniPC, Dpm++ (2M/SDE) Karras, DDIM
Steps: 10～24; vae-ft-mse-840000-ema-pruned; -Mixed 5000+images",multimodel,text-to-image,https://huggingface.co/deadman44/SD_Photoreal_Merged_Models,,License: cc0-1.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 106,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 16.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Trauter_LoRAs,,,,,,,,,"NOTICE: My LoRAs require high amount of tags to look good, I will fix this later on and update all of my LoRAs if everything works out.; Welcome to the place where I host my LoRAs. In short, LoRA is just a checkpoint trained on specific artstyle/subject that you load into your WebUI, that can be used with other models.Although you can use it with any model, the effects of LoRA will vary between them.
Most of the previews use models that come from WarriorMama777 .For more information about them, you can visit the original LoRA repository: https://github.com/cloneofsimo/loraEvery images posted here, or on the other sites have metadata in them that you can use in PNG Info tab in your WebUI to get access to the prompt of the image.Everything I do here is for free of charge!I don't guarantee that my LoRAs will give you good results, if you think they are bad, don't use them.; To use them in your WebUI, please install the extension linked under, following the installation guide:https://github.com/kohya-ss/sd-webui-additional-networks#installation; All of my LoRAs are to be used with their original danbooru tag. For example:  ; My LoRAs will have sufixes that will tell you how much they were trained. Either by using words like ""soft"" and ""hard"",where soft stands for lower amount of training and hard for higher amount of training.  ",,,https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 508,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 34.13KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
AsiaFacemix,,,,,,,,,"本人郑重声明：本模型原则上禁止用于训练基于明星、公众人物肖像的风格模型训练，因为这会带来争议，对AI社区的发展造成不良的负面影响。 如各位一定要违反以上声明训练相关模型并公开发布，请在您的发布说明中删除与本模型有关的一切描述。感谢各位使用者的支持与理解。; In principle, this model is prohibited from being used for training style models based on portraits of celebrities and public figures, because it will cause controversy and have a negative impact on the development of the AI community. If you must violate the above statement to train the relevant model and release it publicly, please delete all descriptions related to this model in your release notes. Thank you for your support and understanding.; 该模型基于basil mix,dreamlike,ProtoGen等优秀模型微调，融合而来。
用于解决上述模型在绘制亚洲、中国元素内容时，只能绘制丑陋的刻板印象脸的问题。
同时也能改善和减少绘制亚洲、中国元素内容时，得到更接近tags的绘制内容。
This model based on basil mix,dreamlike,ProtoGen,etc. After finetune and merging, it solved the big problem that the other model can only draw ugly stereotyped woman faces from hundreds years ago When drawing Asian and Chinese elements.
This model can also improve the drawing content of Asian and Chinese elements to get closer to tags.; Based on dreamlike finetune example：


; Based on Image to Image example：

",,,https://huggingface.co/dcy/AsiaFacemix,,License: openrail,Datasets: Gustavosta/Stable-Diffusion-Prompts,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 399,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 21.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chatgpt-detector-roberta,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is trained on the mix of full-text and splitted sentences of answers from Hello-SimpleAI/HC3.; More details refer to arxiv: 2301.07597 and Gtihub project Hello-SimpleAI/chatgpt-comparison-detection.; The base checkpoint is roberta-base.
We train it with all Hello-SimpleAI/HC3 data (without held-out) for 1 epoch.; (1-epoch is consistent with the experiments in our paper.); Checkout this papaer arxiv: 2301.07597",natural-language-processing,text-classification,https://huggingface.co/Hello-SimpleAI/chatgpt-detector-roberta,Paper: https://arxiv.org/pdf/2301.07597.pdf,,Datasets: Hello-SimpleAI/HC3,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 185478,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 502.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 17
anything-v3.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,,multimodel,text-to-image,https://huggingface.co/Linaqruf/anything-v3.0,,License: creativeml-openrail-m,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 622,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 28781,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 501
yolov8m-scene-classification,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,More models available at: awesome-yolov8-models; Inference API has been turned off for this model.,computer-vision,image-classification,https://huggingface.co/keremberke/yolov8m-scene-classification,,,Datasets: keremberke/indoor-scene-classification,,TensorBoard; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4011,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 33.2MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
yolov8m-table-extraction,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c921e73e7b6d3bf06402_Object%20Detection.svg,,More models available at: awesome-yolov8-models; Inference API has been turned off for this model.,computer-vision,object-detection,https://huggingface.co/keremberke/yolov8m-table-extraction,,,Datasets: keremberke/table-extraction,,TensorBoard; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7269,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.5MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
DucHaitenAnime,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"DucHaitenAnime_v4.0: In this version i added a little 3D, a little realistic, improved the hand but not much, improved the color because i don't like to use vae; All images above are used only text to image, not edited or accompanying application software.; https://civitai.com/models/6634; please support me by becoming a patron:; https://www.patreon.com/duchaitenreal",multimodel,text-to-image,https://huggingface.co/DucHaiten/DucHaitenAnime,,License: creativeml-openrail-m,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5254,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 48.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 41
reward-model-deberta-v3-large-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Reward model (RM) trained to predict which generated answer is better judged by a human, given a question.; RM are useful in these domain:; QA model evaluation; serves as reward score in RLHF ; detect potential toxic response via ranking",natural-language-processing,text-classification,https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2,,License: mit,Datasets: openai/summarize_from_feedback; openai/webgpt_comparisons; Dahoas/instruct-synthetic-prompt-responses; Anthropic/hh-rlhf,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 85,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 62221,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 13
BioGPT-Large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.; If you find BioGPT useful in your research, please cite the following paper:",natural-language-processing,text-generation,https://huggingface.co/microsoft/BioGPT-Large,,License: mit,Datasets: pubmed,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 109,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5709,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 22
chilloutmix,,,,,,,,,"This repository has been marked as containing sensitive content and may contain potentially harmful and sensitive
		information.
	",,,https://huggingface.co/swl-models/chilloutmix,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 234,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
pythia-2.8b-deduped,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"The Pythia Scaling Suite is a collection of models developed to facilitate 
interpretability research (see paper). 
It contains two sets of eight models of sizes 
70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two 
models: one trained on the Pile, and one trained on the Pile after the dataset 
has been globally deduplicated. All 8 model sizes are trained on the exact 
same data, in the exact same order. We also provide 154 intermediate 
checkpoints per model, hosted on Hugging Face as branches.; The Pythia model suite was designed to promote scientific 
research on large language models, especially interpretability research. 
Despite not centering downstream performance as a design goal, we find the 
models match or exceed the performance of 
similar and same-sized models, such as those in the OPT and GPT-Neo suites.; Previously, we released an early version of the Pythia suite to the public. 
However, we decided to retrain the model suite to address a few hyperparameter 
discrepancies. This model card lists the changes; 
see appendix B in the Pythia paper for further discussion. We found no 
difference in benchmark performance between the two Pythia versions. 
The old models are 
still available, but we 
suggest the retrained suite if you are just starting to use Pythia.
This is the current release.; Please note that all models in the Pythia suite were renamed in January 
2023. For clarity, a table 
comparing the old and new names is provided in this model card, together 
with exact parameter counts.; The primary intended use of Pythia is research on the behavior, functionality, 
and limitations of large language models. This suite is intended to provide 
a controlled setting for performing scientific experiments. We also provide 
154 checkpoints per model: initial step0, 10 log-spaced checkpoints 
step{1,2,4...512}, and 143 evenly-spaced checkpoints from step1000 to 
step143000. These checkpoints are hosted on Hugging Face as branches. Note 
that branch 143000 corresponds exactly to the model checkpoint on the main 
branch of each model.",natural-language-processing,text-generation,https://huggingface.co/EleutherAI/pythia-2.8b-deduped,Paper: https://arxiv.org/pdf/2304.01373.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2201.07311.pdf,License: apache-2.0,Datasets: EleutherAI/the_pile_deduplicated,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4559,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 11.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
OPT-13B-Nerybus-Mix,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a merged (50/50) model of both Erebus 13B and Nerys V2 13B by Mr. Seeker.
Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; For more information, check out the two source models:; Based on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion).
Warning: This model has a very strong NSFW bias!; OPT-13B is licensed under the OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved.; Inference API has been turned off for this model.",natural-language-processing,text-generation,https://huggingface.co/KoboldAI/OPT-13B-Nerybus-Mix,Paper: https://arxiv.org/pdf/2205.01068.pdf,License: other,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8216,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
boring_e621,,,,,,,,,"This embedding attempts to capture what it means for an image to be uninteresting.  It was trained as a negative embedding using e621 style tags as prompts during training.
If you're using the Automatic1111 Stable Diffusion WebUI, place the boring_e621_v4.pt file in 
stable-diffusion-webui\embeddings and add ""boring_e621_v4"" to your negative prompt for more interesting outputs.
; The motivation for boring_e621 is that negative embeddings like Bad Prompt, 
whose training is described here 
depend on manually curated lists of tags describing features people do not want their images to have, such as ""deformed hands"".  Some problems with this approach are:; To address these problems, boring_e621 employs textual inversion on a set of images automatically extracted from the art site 
e621.net, a rich resource of millions of hand-labeled artworks, each of which is both human-labeled topically and rated 
according to its quality.  E621.net allows users to express their approval of an artwork by either up-voting it, or marking it as a favorite.Boring_e621 was specifically trained on artworks automatically selected from the site according to the criteria 
that no user has ever Favorited or Up-Voted them.  boring_e621 thus learned to produce low-quality images, so when it is 
used in the negative prompt of a stable diffusion image generator, the model avoids making mistakes that would make the generation more boring.
; To qualitatively evaluate how well boring_e621 has learned to improve image quality, we apply it to 4 simple sample prompts using the base Stable Diffusion 1.5 model.; ",,,https://huggingface.co/FoodDesert/boring_e621,,License: apache-2.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 38,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 153.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
NeverEnding_Dream-Feb19-2023,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/jomcs/NeverEnding_Dream-Feb19-2023,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 170,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
SteamSHP-flan-t5-xl,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"SteamSHP-XL is a preference model trained to predict -- given some context and two possible responses -- which response humans will find more helpful.
It can be used for NLG evaluation or as a reward model for RLHF.; It is a FLAN-T5-xl model (3B parameters) finetuned on:; There is a smaller variant called SteamSHP-Large that was made by finetuning FLAN-T5-large (780M parameters).
Despite being 1/4 of the size, it is on average only 0.75 points less accurate on the SHP + Anthropic test data (across all domains).; The input text should be of the format:; The output generated by SteamSHP-XL will either be A or B.",natural-language-processing,text2text-generation,https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xl,,License: apache-2.0,Datasets: stanfordnlp/SHP,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 33,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 486,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 11.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
whisper-medium-fleurs-lang-id,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0941358df9b5cffb0d11b_Audio%20Classification.svg,,"This model is a fine-tuned version of openai/whisper-medium on the FLEURS subset of the google/xtreme_s dataset.
It achieves the following results on the evaluation set:; To reproduce this run, execute the command in run.sh.; More information needed; More information needed; More information needed",audio,audio-classification,https://huggingface.co/sanchit-gandhi/whisper-medium-fleurs-lang-id,,License: apache-2.0,Datasets: xtreme_s,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1683,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 615.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
sd-controlnet-canny,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"ControlNet is a neural network structure to control diffusion models by adding extra conditions. 
This checkpoint corresponds to the ControlNet conditioned on Canny edges.; It can be used in combination with Stable Diffusion.; ; Developed by: Lvmin Zhang, Maneesh Agrawala; Model type: Diffusion-based text-to-image generation model",computer-vision,image-to-image,https://huggingface.co/lllyasviel/sd-controlnet-canny,Paper: https://arxiv.org/pdf/2302.05543.pdf,License: openrail,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 85,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 145250,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 138
sd-controlnet-hed,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"ControlNet is a neural network structure to control diffusion models by adding extra conditions. 
This checkpoint corresponds to the ControlNet conditioned on HED Boundary.; It can be used in combination with Stable Diffusion.; ; Developed by: Lvmin Zhang, Maneesh Agrawala; Model type: Diffusion-based text-to-image generation model",computer-vision,image-to-image,https://huggingface.co/lllyasviel/sd-controlnet-hed,Paper: https://arxiv.org/pdf/2302.05543.pdf,License: openrail,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 39148,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 98
fasttext-language-identification,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"fastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices. It was introduced in this paper. The official website can be found here.; This LID (Language IDentification) model is used to predict the language of the input text, and the hosted version (lid218e) was released as part of the NLLB project and can detect 217 languages. You can find older versions (ones that can identify 157 languages) on the official fastText website.; fastText is a library for efficient learning of word representations and sentence classification. fastText is designed to be simple to use for developers, domain experts, and students. It's dedicated to text classification and learning word representations, and was designed to allow for quick model iteration and refinement without specialized hardware. fastText models can be trained on more than a billion words on any multicore CPU in less than a few minutes.; It includes pre-trained models learned on Wikipedia and in over 157 different languages. fastText can be used as a command line, linked to a C++ application, or used as a library for use cases from experimentation and prototyping to production.; You can use pre-trained word vectors for text classification or language identification. See the tutorials and resources on its official website to look for tasks that interest you.",natural-language-processing,text-classification,https://huggingface.co/facebook/fasttext-language-identification,Paper: https://arxiv.org/pdf/1607.04606.pdf; https://arxiv.org/pdf/1802.06893.pdf; https://arxiv.org/pdf/1607.01759.pdf; https://arxiv.org/pdf/1612.03651.pdf,License: cc-by-nc-4.0,,,fastText,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 29,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
so-vits-svc-4.0-models,,,,,,,,,"模型命名遵循G_${name}_${Epoch}epoch.pth.; 显卡炸了, 暂时不炼了; 由so-vits-svc-4.0训练的模型 不是v2; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/TachibanaKimika/so-vits-svc-4.0-models,,License: mit,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 69,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.87KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
text2vec-base-chinese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"Based on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replace MacBERT with LERT, and keep other training conditions unchanged。",natural-language-processing,sentence-similarity,https://huggingface.co/GanymedeNil/text2vec-base-chinese,,License: apache-2.0,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 17395,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 410.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
llama-7b-hf-int4,,,,,,,,,"LLaMA-13B converted to work with Transformers/HuggingFace. This is under a special license, please see the LICENSE file for details.; EXPERIMENTAL RELEASE; This has been converted to int4 via GPTQ method. This requires some special support code that is also highly experimental. NOT COMPATIBLE WITH TRANSFORMERS LIBRARY.; --
license: other; Organization developing the model
The FAIR team of Meta AI.",,,https://huggingface.co/decapoda-research/llama-7b-hf-int4,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 73,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
pythia-1b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"The Pythia Scaling Suite is a collection of models developed to facilitate 
interpretability research (see paper). 
It contains two sets of eight models of sizes 
70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two 
models: one trained on the Pile, and one trained on the Pile after the dataset 
has been globally deduplicated. All 8 model sizes are trained on the exact 
same data, in the exact same order. We also provide 154 intermediate 
checkpoints per model, hosted on Hugging Face as branches.; The Pythia model suite was deliberately designed to promote scientific 
research on large language models, especially interpretability research. 
Despite not centering downstream performance as a design goal, we find the 
models match or exceed the performance of 
similar and same-sized models, such as those in the OPT and GPT-Neo suites.; Previously, we released an early version of the Pythia suite to the public. 
However, we decided to retrain the model suite to address a few hyperparameter 
discrepancies. This model card lists the changes; 
see appendix B in the Pythia paper for further discussion. We found no 
difference in benchmark performance between the two Pythia versions. 
The old models are 
still available, but we 
suggest the retrained suite if you are just starting to use Pythia.
This is the current release.; Please note that all models in the Pythia suite were renamed in January 
2023. For clarity, a table 
comparing the old and new names is provided in this model card, together 
with exact parameter counts.; The primary intended use of Pythia is research on the behavior, functionality, 
and limitations of large language models. This suite is intended to provide 
a controlled setting for performing scientific experiments. We also provide 
154 checkpoints per model: initial step0, 10 log-spaced checkpoints 
step{1,2,4...512}, and 143 evenly-spaced checkpoints from step1000 to 
step143000. These checkpoints are hosted on Hugging Face as branches. Note 
that branch 143000 corresponds exactly to the model checkpoint on the main 
branch of each model.",natural-language-processing,text-generation,https://huggingface.co/EleutherAI/pythia-1b,Paper: https://arxiv.org/pdf/2304.01373.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2201.07311.pdf,License: apache-2.0,Datasets: the_pile,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 18283,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-65b-4bit,,,,,,,,,"Converted with https://github.com/qwopqwop200/GPTQ-for-LLaMa 
All models tested on A100-80G
*Conversion may require lot of RAM, LLaMA-7b takes ~12 GB, 13b around 21 GB, 30b around 62 and 65b takes more than 120 GB of RAM. ; Installation instructions as mentioned in above repo:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/maderix/llama-65b-4bit,,,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 69,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 61.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
starencoder,,,,,,,,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Please read the BigCode OpenRAIL-M license agreement before accepting it.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This is an encoder-only model (i.e., bi-directionally self-attentive Transformers) trained on The Stack dataset.; We leveraged the :",,,https://huggingface.co/bigcode/starencoder,Paper: https://arxiv.org/pdf/1810.04805.pdf,,,code,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4553,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
pix2struct-ai2d-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4f95e63a23d3f26e06e_Visual%20Question%20Answering.svg,,"; Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The full list of available models can be found on the Table 1 of the paper:; ; The abstract of the model states that: ; Visually-situated language is ubiquitous―sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and
forms. Perhaps due to this diversity, previous work has typically relied on domainspecific recipes with limited sharing of the underlying data, model architectures,
and objectives. We present Pix2Struct, a pretrained image-to-text model for
purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse
masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large
source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy,
we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions
are rendered directly on top of the input image. For the first time, we show that a
single pretrained model can achieve state-of-the-art results in six out of nine tasks
across four domains: documents, illustrations, user interfaces, and natural images.",multimodel,visual-question-answering,https://huggingface.co/google/pix2struct-ai2d-base,Paper: https://arxiv.org/pdf/2210.03347.pdf,License: apache-2.0,,5 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1126,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 569.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
alpaca-native,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a replica of Alpaca by Stanford' tatsu; Trained using the original instructions with a minor modification in FSDP mode; 13B: https://huggingface.co/chavinlo/alpaca-13b; 13B -> GPT4 : https://huggingface.co/chavinlo/gpt4-x-alpaca; Trained on 4xA100s for 6H
Donated by redmond.ai",natural-language-processing,text-generation,https://huggingface.co/chavinlo/alpaca-native,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 242,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 22777,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 17
chatgpt_paraphraser_on_T5_base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"This model was trained on our ChatGPT paraphrase dataset.; This dataset is based on the Quora paraphrase question, texts from the SQUAD 2.0 and the CNN news dataset.; This model is based on the T5-base model. We used ""transfer learning"" to get our model to generate paraphrases as well as ChatGPT. Now we can say that this is one of the best paraphrases of the Hugging Face.; Kaggle link; Input:",natural-language-processing,text2text-generation,https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base,,License: openrail,Datasets: humarin/chatgpt-paraphrases,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 52,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 10866,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 895.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
swissbert,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"SwissBERT is a masked language model for processing Switzerland-related text. It has been trained on more than 21 million Swiss news articles retrieved from Swissdox@LiRI.; SwissBERT is based on X-MOD, which has been pre-trained with language adapters in 81 languages.
For SwissBERT we trained adapters for the national languages of Switzerland C German, French, Italian, and Romansh Grischun.
In addition, we used a Switzerland-specific subword vocabulary.; The pre-training code and usage examples are available here. We also release a version that was fine-tuned on named entity recognition (NER): https://huggingface.co/ZurichNLP/swissbert-ner; SwissBERT contains the following language adapters:; Attribution-NonCommercial 4.0 International (CC BY-NC 4.0).",natural-language-processing,fill-mask,https://huggingface.co/ZurichNLP/swissbert,Paper: https://arxiv.org/pdf/2303.13310.pdf,License: cc-by-nc-4.0,,5 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 403,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 613.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chatglm-6b-int4,,,,,,,,,"
    ? Join our Slack and WeChat
; ChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 ChatGLM 相同的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。; ChatGLM-6B-INT4 是 ChatGLM-6B 量化后的模型权重。具体的，ChatGLM-6B-INT4 对 ChatGLM-6B 中的 28 个 GLM Block 进行了 INT4 量化，没有对 Embedding 和 LM Head 进行量化。量化后的模型理论上 6G 显存（使用 CPU 即内存）即可推理，具有在嵌入式设备（如树莓派）上运行的可能。; 在 CPU 上运行时，会根据硬件自动编译 CPU Kernel ，请确保已安装 GCC 和 OpenMP （Linux一般已安装，对于Windows则需手动安装），以获得最佳并行计算能力。; 可以通过如下代码调用 ChatGLM-6B 模型来生成对话：",,,https://huggingface.co/THUDM/chatglm-6b-int4,,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 354,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 37794,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 60
ClinicalBERT,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"This model card describes the ClinicalBERT model, which was trained on a large multicenter dataset with a large corpus of 1.2B words of diverse diseases we constructed.
We then utilized a large-scale corpus of EHRs from over 3 million patient records to fine tune the base language model.; The ClinicalBERT model was trained on a large multicenter dataset with a large corpus of 1.2B words of diverse diseases we constructed.; The ClinicalBERT was initialized from BERT. Then the training followed the principle of masked language model, in which given a piece of text, we randomly replace some tokens by MASKs, 
special tokens for masking, and then require the model to predict the original tokens via contextual text. ; We used a batch size of 32, a maximum sequence length of 256, and a learning rate of 5e-5 for pre-training our models. ; Load the model via the transformers library:",natural-language-processing,fill-mask,https://huggingface.co/medicalai/ClinicalBERT,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 32,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 16219,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 543.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
stable-diffusion-2-1-unclip,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This model card focuses on the model associated with the Stable Diffusion v2-1 model, codebase available here.; This stable-diffusion-2-1-unclip is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model; Language(s): English",multimodel,text-to-image,https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip,Paper: https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: openrail++,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 189,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 23667,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 29
kl-f8-anime2.vae,,,,,,,,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; AI ???? ????.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; AI ????? User Access requests ??? ???? ?? ?? ??; ?? ???? ???? ???? ???.",,,https://huggingface.co/Bingsu/kl-f8-anime2.vae,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
IF-II-L-v1.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; DeepFloyd LICENSE AGREEMENTThis License Agreement (as may be amended in accordance with this License Agreement, “License”), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (“Licensee” or “you”) and Stability AI Ltd.. (“Stability AI” or “we”) applies to your use of any computer program, algorithm, source code, object code, or software that is made available by Stability AI under this License (“Software”) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (“Documentation”).By clicking “I Accept” below or by using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model, that can generate pictures with new state-of-the-art for photorealism and language understanding. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID-30K score of 6.66 on the COCO dataset.; Inspired by Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",multimodel,text-to-image,https://huggingface.co/DeepFloyd/IF-II-L-v1.0,Paper: https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/2110.02861.pdf,License: deepfloyd-if-license,,,PyTorch; Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 39,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 12944,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 18
Alpaca-native-4bit-ggml,,,,,,,,,"This is a https://huggingface.co/chavinlo/alpaca-native converted in OLD GGML (alpaca.cpp) format and quantized to 4 bits to run on CPU with 5GB of RAM.; For any additional information, please visit these repos:; alpaca.cpp repo: https://github.com/antimatter15/alpaca.cpp; llama.cpp repo: https://github.com/ggerganov/llama.cpp; original facebook llama(NOT ggml) repo: https://github.com/facebookresearch/llama",,,https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 180,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
NGMix,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/Nyangyu/NGMix,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 37.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
negative,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/embed/negative,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 399.82KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 37
GroundingDINO,,,,,,,,,"?Paper | 
??Video |
?Demo on Colab | 
?Demo on HF (Coming soon) ; If you find our work helpful for your research, please consider citing the following BibTeX entry.   ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/ShilongLiu/GroundingDINO,Paper: https://arxiv.org/pdf/2303.05499.pdf,License: apache-2.0,Datasets: detection-datasets/coco; conceptual_captions,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 45,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 27
gpt4all-lora,,,,,,,,,"An autoregressive transformer trained on data curated using Atlas.
This model is trained with four full epochs of training, while the related gpt4all-lora-epoch-3 model is trained with three.
Replication instructions and data: https://github.com/nomic-ai/gpt4all; Developed by: Nomic AI; Model Type: An auto-regressive language model based on the transformer architecture and fine-tuned.; Languages: English; License: GPL-3.0",,,https://huggingface.co/nomic-ai/gpt4all-lora,,License: gpl-3.0,Datasets: nomic-ai/gpt4all_prompt_generations,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 203,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.41MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
pygmalion-6b_dev-4bit-128g,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,GPTQ quantization of https://huggingface.co/PygmalionAI/pygmalion-6b/commit/30e2405100eac6bd53f75964cc7345eeafd19f7d; Using this repository: https://github.com/mayaeary/GPTQ-for-LLaMa/tree/gptj-v2; Command: ; Inference API has been turned off for this model.,natural-language-processing,text-generation,https://huggingface.co/mayaeary/pygmalion-6b_dev-4bit-128g,,License: creativeml-openrail-m,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 111,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2657,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
alpaca-7b-native-enhanced,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Use this command to run with llama.cpp; contents of prompts/alpacanativeenhanced.txt should be; Original model https://huggingface.co/8bit-coder/alpaca-7b-nativeEnhanced; Inference API does not yet support adapter-transformers models for this pipeline type.
							",natural-language-processing,text-generation,https://huggingface.co/Pi3141/alpaca-7b-native-enhanced,,License: wtfpl,,English,Adapter Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 106,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 49.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
galpaca-30b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"GALACTICA 30B fine-tuned on the Alpaca dataset.; The model card from the original Galactica repo can be found here, and the original paper here.; The dataset card for Alpaca can be found here, and the project homepage here.
  The Alpaca dataset was collected with a modified version of the Self-Instruct Framework, and was built using OpenAI's text-davinci-003 model. As such it is subject to OpenAI's terms of service.; The GALACTICA models are trained on a large-scale scientific corpus and are designed to perform scientific tasks.
The Alpaca dataset is a set of 52k instruct-response pairs designed to enhace the instruction following capabilites of pre-trained language models.; The GALACTICA model card specifies that the primary indended users of the GALACTICA models are researchers studying language models applied to the scientific domain, and it cautions against production use of GALACTICA without safeguards due to the potential for the model to produce inaccurate information.
The original GALACTICA models are available under a non-commercial CC BY-NC 4.0 license, and the GALPACA model is additionally subject to the OpenAI Terms of Service.",natural-language-processing,text-generation,https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b,,License: cc-by-nc-4.0,Datasets: tatsu-lab/alpaca,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 54,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 60,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 61.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
medalpaca-lora-30b-8bit,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Model Description ; medalpaca-lora-30b-8bit is a large language model specifically fine-tuned for medical domain tasks. 
It is based on LLaMA (Large Language Model Meta AI) and contains 7 billion parameters. 
The primary goal of this model is to improve question-answering and medical dialogue tasks.
It was trained using LoRA and quantized, to reduce memory footprint. ; The training data for this project was sourced from various resources. 
Firstly, we used Anki flashcards to automatically generate questions, 
from the front of the cards and anwers from the back of the card. 
Secondly, we generated medical question-answer pairs from Wikidoc. 
We extracted paragraphs with relevant headings, and used Chat-GPT 3.5 
to generate questions from the headings and using the corresponding paragraphs 
as answers. This dataset is still under development and we believe 
that approximately 70% of these question answer pairs are factual correct. 
Thirdly, we used StackExchange to extract question-answer pairs, taking the 
top-rated question from five categories: Academia, Bioinformatics, Biology, 
Fitness, and Health. Additionally, we used a dataset from ChatDoctor 
consisting of 200,000 question-answer pairs, available at https://github.com/Kent0n-Li/ChatDoctor.; The model may not perform effectively outside the scope of the medical domain.
The training data primarily targets the knowledge level of medical students, 
which may result in limitations when addressing the needs of board-certified physicians.
The model has not been tested in real-world applications, so its efficacy and accuracy are currently unknown. 
It should never be used as a substitute for a doctor's opinion and must be treated as a research tool only.",natural-language-processing,text-generation,https://huggingface.co/medalpaca/medalpaca-lora-30b-8bit,Paper: https://arxiv.org/pdf/2106.09685.pdf; https://arxiv.org/pdf/2303.14070.pdf,License: cc,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 205.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
instruct-igel-001,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"IGEL is an LLM model family developed for German. The first version of IGEL is built on top?BigScience BLOOM,?adapted to?German from Malte Ostendorff. IGEL is designed to provide accurate and reliable language understanding capabilities for a wide range of natural language understanding tasks, including sentiment analysis, language translation, and question answering.; The IGEL family currently includes?instruct-igel-001?and?chat-igel-001?(coming soon).; LoRA tuned BLOOM-CLP German (6.4B parameters) with merged weights. The 001 was designed as a naive test to determine whether it is possible to create an german instruction-tuned model using a small, undertrained LLM and a naive translated dataset. The goal of this test was to explore the potential of the BLOOM architecture for language modeling tasks that require instruction-based responses.; To achieve this goal, we used a pre-trained LLM model with limited training, and fine-tuned it using a dataset of naive translations of instruction-based content. The dataset was created by taking instructions in English and translating them into German using an automated translation tool. While this approach may introduce errors in the translated content, we wanted to test whether the model could still learn to generate instruction-based responses in a variety of languages.; instruct-igel-001 is trained on naive translated instruction datasets, without much post-processing. ",natural-language-processing,text-generation,https://huggingface.co/philschmid/instruct-igel-001,,,,German,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 37,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1962,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
llama-7b-se-rl-peft,,,,,,,,,"; Adapter weights of a Reinforcement Learning fine-tuned model based on the LLaMA model (see Meta's LLaMA release for the original LLaMA model). 
The model is designed to generate human-like responses to questions in Stack Exchange domains of programming, mathematics, physics, and more.
For more info check out the blog post and github example.; Developed by: Hugging Face; Model type: An auto-regressive language model based on the transformer architecture, and fine-tuned with Stack Exchange datasets. ; Languages: Predominantly English, with additional data from languages with the following ISO codes: ",,,https://huggingface.co/trl-lib/llama-7b-se-rl-peft,,License: bigscience-openrail-m,Datasets: lvwerra/stack-exchange-paired,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 93,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 34.1MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
matcha-chartqa,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4f95e63a23d3f26e06e_Visual%20Question%20Answering.svg,,"; This model is the MatCha model, fine-tuned on Chart2text-pew dataset. ; The abstract of the paper states that: ; Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art visionlanguage models do not perform well on these data. We propose MATCHA (Math reasoning and Chart derendering pretraining) to enhance visual language models’ capabilities jointly modeling charts/plots and language data. Specifically we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MATCHA pretraining starting from Pix2Struct, a recently proposed imageto-text visual language model. On standard benchmarks such as PlotQA and ChartQA, MATCHA model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MATCHA pretraining transfers to domains such as screenshot, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MATCHA pretraining on broader visual language tasks.; You should ask specific questions to the model in order to get consistent generations. Here we are asking the model whether the sum of values that are in a chart are greater than the largest value.",multimodel,visual-question-answering,https://huggingface.co/google/matcha-chartqa,Paper: https://arxiv.org/pdf/2212.09662.pdf,License: apache-2.0,,5 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 799,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
llama-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This contains the weights for the LLaMA-13b model. This model is under a non-commercial license (see the LICENSE file).
You should only use this repository if you have been granted access to the model by filling out this form but either lost your copy of the weights or got some trouble converting them to the Transformers format.",natural-language-processing,text-generation,https://huggingface.co/huggyllama/llama-13b,,License: other,,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 102,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 37075,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
ImageReward,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"
Github Repo ? ? Twitter ? ? Paper 
; ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation; ImageReward is the first general-purpose text-to-image human preference RM which is trained on in total 137k pairs of
expert comparisons, based on text prompts and corresponding model outputs from DiffusionDB. We demonstrate that
ImageReward outperforms existing text-image scoring methods, such as CLIP, Aesthetic, and BLIP, in terms of
understanding human preference in text-to-image synthesis through extensive analysis and experiments.; ; We have integrated the whole repository to a single python package image-reward. Following the commands below to prepare the environment:",multimodel,text-to-image,https://huggingface.co/THUDM/ImageReward,Paper: https://arxiv.org/pdf/2304.05977.pdf,License: apache-2.0,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
segment-anything,,,,,,,,,"NEW Segment Anything now officially supported in transformers! Check out the official documentation; This repository is the mirror of the official Segment Anything repository, together with the model weights. We also provide instructions on how to easily download the model weights.; Meta AI Research, FAIR; Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick; [Paper] [Project] [Demo] [Dataset] [Blog] [BibTeX]",,,https://huggingface.co/ybelkada/segment-anything,,License: apache-2.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 53,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.41KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
saiga_7b_lora,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,Based on LLaMA 7B.; Colab: link; llama.cpp version: link; Training code: link; Examples:,natural-language-processing,conversational,https://huggingface.co/IlyaGusev/saiga_7b_lora,,License: cc-by-4.0,Datasets: IlyaGusev/ru_turbo_alpaca; IlyaGusev/ru_turbo_saiga; IlyaGusev/ru_sharegpt_cleaned; IlyaGusev/oasst1_ru_main_branch; IlyaGusev/ru_turbo_alpaca_evol_instruct; lksy/ru_instruct_gpt4,Russian,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 67.7MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
rwkv-4-novel,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"These are RWKV-4-Pile models finetuned on novels.; Currently I am doing it for Chn novels. More languages to come.; Use https://github.com/BlinkDL/ChatRWKV to run them.; See https://github.com/BlinkDL/RWKV-LM for details on the RWKV Language Model (100% RNN).; Unable to determine this model’s library. Check the
								docs 
.
							",natural-language-processing,text-generation,https://huggingface.co/BlinkDL/rwkv-4-novel,,License: apache-2.0,Datasets: the_pile,English,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 48,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 56.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
hololivemix-so-vits-svc-4.0,,,,,,,,,"Tolong Credit nama saya (Romario Martinus) atau username saya (megaaziib) kalau pake voice model ini! 
Credit me if you use my model 
For use with so-vits-svc-fork repo  
Use this tutorial on Youtube for guidelines  
Or simply just run on Colab without setup 
Support me: 
Paypal: https://paypal.me/romramgames 
Patreon: https://www.patreon.com/romariomartinus 
ko-fi: https://ko-fi.com/megaaziib 
Saweria: https://saweria.co/romariomartinus ; the colab is not made by me so don't ask question ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/megaaziib/hololivemix-so-vits-svc-4.0,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 68,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.27KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
stablelm-base-alpha-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,StableLM-Base-Alpha is a suite of 3B and 7B parameter decoder-only language models pre-trained on a diverse collection of English datasets with a sequence length of 4096 to push beyond the context window limitations of existing open-source language models.; Get started generating text with StableLM-Base-Alpha by using the following code snippet:; Developed by: Stability AI; Model type: StableLM-Base-Alpha models are auto-regressive language models based on the NeoX transformer architecture.; Language(s): English,natural-language-processing,text-generation,https://huggingface.co/stabilityai/stablelm-base-alpha-7b,,License: cc-by-sa-4.0,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 206,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1582,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 32.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
PMC_LLAMA_7B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This repo contains PMC_LLaMA_7B, which is LLaMA-7b finetuned on the PMC papers in S2ORC dataset.; The model was trained with the following hyperparameters:; Each epoch we sample 512 tokens per paper for training.; The model can be loaded as following:",natural-language-processing,text-generation,https://huggingface.co/chaoyi-wu/PMC_LLAMA_7B,,License: apache-2.0,Datasets: allenai/s2orc,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 35,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2321,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
saiga_13b_lora,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,Based on LLaMA 13B.; llama.cpp version: link; Colab: link; Training code: link; Examples:,natural-language-processing,conversational,https://huggingface.co/IlyaGusev/saiga_13b_lora,,License: cc-by-4.0,Datasets: IlyaGusev/ru_turbo_alpaca; IlyaGusev/ru_turbo_saiga; IlyaGusev/oasst1_ru_main_branch; IlyaGusev/ru_sharegpt_cleaned; IlyaGusev/ru_turbo_alpaca_evol_instruct; lksy/ru_instruct_gpt4,Russian,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 106.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
control_v11p_sd15_canny,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"Controlnet v1.1 is the successor model of Controlnet v1.0
and was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ; ",computer-vision,image-to-image,https://huggingface.co/lllyasviel/control_v11p_sd15_canny,Paper: https://arxiv.org/pdf/2302.05543.pdf,License: openrail,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 147906,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 51
control_v11p_sd15_openpose,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"Controlnet v1.1 is the successor model of Controlnet v1.0
and was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ; ",computer-vision,image-to-image,https://huggingface.co/lllyasviel/control_v11p_sd15_openpose,Paper: https://arxiv.org/pdf/2302.05543.pdf,License: openrail,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 439110,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 49
OPT-30B-Erebus-4bit-128g,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; This is a 4-bit GPTQ quantization of OPT-30B-Erebus, original model:
https://huggingface.co/KoboldAI/OPT-30B-Erebus; Quantized with: https://github.com/0cc4m/GPTQ-for-LLaMa; Output generated in 54.23 seconds (0.87 tokens/s, 47 tokens, context 44, seed 593020441); https://github.com/oobabooga/text-generation-webui",natural-language-processing,text-generation,https://huggingface.co/Zicara/OPT-30B-Erebus-4bit-128g,,License: other,Datasets: amilkov/literotica,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 202,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 34.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
flan-alpaca-gpt4-xl,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"? We developed Flacuna by fine-tuning Vicuna-13B on the Flan collection. Flacuna is better than Vicuna at problem-solving. Access the model here https://huggingface.co/declare-lab/flacuna-13b-v1.0.; ? Curious to know the performance of ? ? Flan-Alpaca on large-scale LLM evaluation benchmark, InstructEval? Read our paper https://arxiv.org/pdf/2306.04757.pdf. We evaluated more than 10 open-source instruction-tuned LLMs belonging to various LLM families including Pythia, LLaMA, T5, UL2, OPT, and Mosaic. Codes and datasets: https://github.com/declare-lab/instruct-eval; ? FLAN-T5 is also useful in text-to-audio generation. Find our work at https://github.com/declare-lab/tango if you are interested.; Our repository contains code for extending the Stanford Alpaca
synthetic instruction tuning to existing instruction-tuned models such as Flan-T5.
We have a live interactive demo thanks to Joao Gante!
We are also benchmarking many instruction-tuned models at declare-lab/flan-eval.
Our pretrained models are fully available on HuggingFace ? :; *recommended for better performance",natural-language-processing,text2text-generation,https://huggingface.co/declare-lab/flan-alpaca-gpt4-xl,Paper: https://arxiv.org/pdf/2306.04757.pdf; https://arxiv.org/pdf/2210.11416.pdf,License: apache-2.0,Datasets: tatsu-lab/alpaca,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 31,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 40065,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 23.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ChatGalRWKV,,,,,,,,,"关于项目的名字：chat就是猫（法语），gal就是娘（英语）。; 本项目基于RWKV，它是一系列从预训练数据、训练代码、推理代码到模型权重都完全开源的大语言模型，并且与基于transformer的模型相比有诸多优势。; 可以使用Colab脚本自建服务运行最近模型。; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/Synthia/ChatGalRWKV,,License: unlicense,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 97.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
vicuna-13B-1.1-Chinese-GPTQ-4bit-128g,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This model was obtained from following repo:; Merged using sciprts from: https://github.com/ymcui/Chinese-LLaMA-Alpaca; Then quanized using following command (no act order):; Can confirm model output normal text, but question-answering quality is unknown; Model type:
Vicuna is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.
It is an auto-regressive language model, based on the transformer architecture.",natural-language-processing,text-generation,https://huggingface.co/jfiekdjdk/vicuna-13B-1.1-Chinese-GPTQ-4bit-128g,,,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 69,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chilled_remix,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"【告知】chilled_remix及びreversemixは2023年5月21日にVersion涓を行い、v2へ移行いたしました。伴いv1は削除致しました。なお既にDLgみの方は引きAき、v1をご利用いただくことは}ございません。 ; License:CreativeML Open RAIL-M
Additional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 21, 2023; このモデルは『CreativeML Open RAIL-M』でLicenseそのものに涓はありません。
しかし追加著作者としてi城郎郭の名前が追加されています。
しかし追加著作者として佐城郎画の名前が追加されています。(6/10 Twitterネ`ム涓に伴い、表涓。License内はsazyou_roukakuの涓なし)
なお『CreativeML Open RAIL-M』にdされている通り、
本モデルを使用しての生成物にvしてはLicenseの使用制限Aの事例を除き、当方は一切v与致しません。
犯罪目的利用や医用画像など特定T的な用途での利用は使用制限Aで禁止されています。
必ず_Jしご利用ください。
また当方は一切任を持ちません。免されていることをご了承の上、ご使用ください。; 推XO定?モデルの`い?プロンプト ;  Version2はfp16でVAEきzみ版のみ配布といたしました。 基本的にはchilled_remixをメインとし、好みに合わせてreversemixも视というのがスタンスです。 ※chilled_remixはchilled_re-genericユ`ザ`をあるX婴扦位炻窑ら守るために生み出されたモデルです。 性|上全てのユ`ザ`出力に辘扦なかった椤サブとしてreversemixが作られました。 reversemixはLORAなしでものセミリアル感は薄いですが、全体的に幼くなるA向があります。  ",multimodel,text-to-image,https://huggingface.co/sazyou-roukaku/chilled_remix,,License: creativeml-openrail-m,,Japanese,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 176,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
stablelm-tuned-alpha-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"StableLM-Tuned-Alpha is a suite of 3B and 7B parameter decoder-only language models built on top of the StableLM-Base-Alpha models and further fine-tuned on various chat and instruction-following datasets.; Get started chatting with StableLM-Tuned-Alpha by using the following code snippet:; StableLM Tuned should be used with prompts formatted to <|SYSTEM|>...<|USER|>...<|ASSISTANT|>...
The system prompt is; StableLM-Tuned-Alpha models are fine-tuned on a combination of five datasets:
Alpaca, a dataset of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine.
GPT4All Prompt Generations, which consists of 400k prompts and responses generated by GPT-4;
Anthropic HH, made up of preferences about AI assistant helpfulness and harmlessness;
DataBricks Dolly, comprising 15k instruction/responses generated by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA and summarization;
and ShareGPT Vicuna (English subset), a dataset of conversations retrieved from ShareGPT.; Models are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (FP16), and optimized with AdamW. We outline the following hyperparameters:",natural-language-processing,text-generation,https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b,,License: cc-by-nc-sa-4.0,Datasets: dmayhem93/ChatCombined; tatsu-lab/alpaca; nomic-ai/gpt4all_prompt_generations; Dahoas/full-hh-rlhf; jeffwan/sharegpt_vicuna; HuggingFaceH4/databricks_dolly_15k,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 348,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 40884,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 32.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 159
moss-moon-003-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MOSS is an open-sourced plugin-augmented conversational language model. moss-moon models have 16B parameters, allowing users to perform inference on a single A100 GPU or 2 NVIDIA 3090 GPUs with FP16 precision, and on a single NVIDIA 3090 GPU with INT-4/8 precision. The base language model of MOSS was pre-trained on ~700B English, Chinese, and code tokens, including the PILE, BigQuery, BigPython, and our private Chinese corpus. The base model was then fine-tuned on multi-turn plugin-augmented conversational data. Finally, we performed preference-aware training to further improve the model.; Limitations: Due to the (relatively) small number of parameters and the autoregressive nature, MOSS is still possible to generate outputs that contain incorrect, misleading, or biased information. Please carefully check the contents generated by MOSS before you use them.; MOSS Use Cases：; ; ",natural-language-processing,text-generation,https://huggingface.co/fnlp/moss-moon-003-base,Paper: https://arxiv.org/pdf/2203.13474.pdf,License: agpl-3.0,Datasets: fnlp/moss-002-sft-data,English; Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 121,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1916,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 34.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 119
chilloutmix_NiPrunedFp32Fix,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Diffuser model for this SD checkpoint:
https://civitai.com/models/6424/chilloutmix; emilianJR/chilloutmix_NiPrunedFp32Fix is the HuggingFace diffuser that you can use with diffusers.StableDiffusionPipeline().; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
The CreativeML OpenRAIL License specifies: 
Please read the full license here",multimodel,text-to-image,https://huggingface.co/emilianJR/chilloutmix_NiPrunedFp32Fix,,License: creativeml-openrail-m,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 23,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6970,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.99KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
mpt-1b-redpajama-200b-dolly,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MPT-1b-RedPajama-200b-dolly is a 1.3 billion parameter decoder-only transformer pre-trained on the RedPajama dataset and subsequently fine-tuned on the Databricks Dolly instruction dataset.
The model was pre-trained for 200B tokens by sampling from the subsets of the RedPajama dataset in the same proportions as were used by the Llama series of models.
This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; This model is an instruction fine-tuned version of mpt-1b-redpajama-200b. In other words, the pre-trained version of this model is mpt-1b-redpajama-200b.; April 20, 2023; Note: This model requires that trust_remote_code=True be passed to the from_pretrained method. 
This is because we use a custom model architecture MosaicGPT that is not yet part of the transformers package.
MosaicGPT includes options for many training efficiency features such as FlashAttention (Dao et al. 2022), ALIBI, QK LayerNorm, and more.; To use the optimized triton implementation of FlashAttention, you can load with attn_impl='triton' and move the model to bfloat16 like so:",natural-language-processing,text-generation,https://huggingface.co/mosaicml/mpt-1b-redpajama-200b-dolly,Paper: https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf,License: cc-by-sa-3.0,Datasets: togethercomputer/RedPajama-Data-1T,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 76,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2582,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
gpt4-x-alpaca-13b-roleplay-lora-4bit-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a llama-13B based model that has been converted with GPTQ to 4bit quantized model.; Base Model: GPT4-x-Alpaca full fine tune by Chavinlo -> https://huggingface.co/chavinlo/gpt4-x-alpacaLORA fine tune using the Roleplay Instruct from GPT4 generated dataset -> https://github.com/teknium1/GPTeacher/tree/main/RoleplayLORA Adapter Only: https://huggingface.co/ZeusLabs/gpt4-x-alpaca-rp-lora - The v2 one -; Merged LORA to the model. ; FYI Latest HF Transformers generates BROKEN generations. 
Try this instead if your generations are terrible (first uninstall transformers): pip install git+https://github.com/huggingface/transformers@9eae4aa57650c1dbe1becd4e0979f6ad1e572ac0
More info and possible alternative solutions in these github issues.https://github.com/tloen/alpaca-lora/issues/279#issuecomment-1514725886https://github.com/oobabooga/text-generation-webui/issues/931#issuecomment-1516215250 ; Instructions simply using alpaca format are likely to be of lower quality. If you want pure general instruct capability I reccomend GPT-4-X-Alpaca (the base model of this) - 
The model responds well to giving it a roleplay task in the preprompt, and the actual conversation in the ""### Input: "" field.",natural-language-processing,text-generation,https://huggingface.co/4bit/gpt4-x-alpaca-13b-roleplay-lora-4bit-v2,,License: mit,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 58,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
codegen2-3_7B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"CodeGen2 is a family of autoregressive language models for program synthesis, introduced in the paper:; CodeGen2: Lessons for Training LLMs on Programming and Natural Languages by Erik Nijkamp*, Hiroaki Hayashi*, Caiming Xiong, Silvio Savarese, Yingbo Zhou.; Unlike the original CodeGen model family (i.e., CodeGen1), CodeGen2 is capable of infilling, and supports more programming languages.; Four model sizes are released: 1B, 3.7B, 7B, 16B.; This model can be easily loaded using the AutoModelForCausalLM functionality.",natural-language-processing,text-generation,https://huggingface.co/Salesforce/codegen2-3_7B,Paper: https://arxiv.org/pdf/2305.02309.pdf,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 943,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 15.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
BBT,,,,,,,,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; 

;   ; 论文链接：https://arxiv.org/abs/2302.09432",,,https://huggingface.co/SuSymmertry/BBT,Paper: https://arxiv.org/pdf/2302.09432.pdf,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-7B-V1.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,The WizardLM delta weights.,natural-language-processing,text-generation,https://huggingface.co/WizardLM/WizardLM-7B-V1.0,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 55,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1354,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
audioldm-m-full,,,,,,,,,"AudioLDM is a latent text-to-audio diffusion model capable of generating realistic audio samples given any text input. It is available in the ? Diffusers library from v0.15.0 onwards.; AudioLDM was proposed in the paper AudioLDM: Text-to-Audio Generation with Latent Diffusion Models by Haohe Liu et al.; Inspired by Stable Diffusion, AudioLDM
is a text-to-audio latent diffusion model (LDM) that learns continuous audio representations from CLAP
latents. AudioLDM takes a text prompt as input and predicts the corresponding audio. It can generate text-conditional
sound effects, human speech and music.; This is the medium version of the AudioLDM model, which has a larger UNet, CLAP audio projection dim, and is trained with audio embeddings as condition. The four AudioLDM checkpoints are summarised below:; Table 1: Summary of the AudioLDM checkpoints.",,,https://huggingface.co/cvssp/audioldm-m-full,Paper: https://arxiv.org/pdf/2301.12503.pdf,,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1410,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.57KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
wizardLM-7B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for WizardLM's WizardLM-7B 4bit.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/wizardLM-7B-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 104,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1788,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
saiga_30b_lora_llamacpp,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,Llama.cpp compatible version of an original 30B model.; How to run:; System requirements:; Inference API has been turned off for this model.,natural-language-processing,text2text-generation,https://huggingface.co/IlyaGusev/saiga_30b_lora_llamacpp,,,Datasets: IlyaGusev/ru_turbo_alpaca; IlyaGusev/ru_turbo_saiga; IlyaGusev/ru_sharegpt_cleaned,Russian,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 20,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 95.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chinese-llama-plus-lora-7b,,,,,,,,,"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/ziqingyang/chinese-llama-plus-lora-7b,,License: apache-2.0,,Chinese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 859.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
dog_emotion_v3,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,No model card; New: Create and edit this model card directly on the website!,computer-vision,image-classification,https://huggingface.co/Dewa/dog_emotion_v3,,,,,PyTorch; TensorBoard; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 39,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 686.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
OpenAssistant-SFT-7-Llama-30B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for OpenAssistant LLaMA 30B SFT 7.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ,Paper: https://arxiv.org/pdf/2304.07327.pdf,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 34,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 116,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mdeberta-v3-base-tasksource-nli,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c42cd649af0365a01a1e_Zero-Shot%20Classification.svg,,"Multilingual mdeberta-v3-base with 30k steps multi-task training on mtasksource
This model can be used as a stable starting-point for further fine-tuning, or directly in zero-shot NLI model or a zero-shot pipeline.
In addition, you can use the provided adapters to directly load a model for hundreds of tasks. ; For more details, see deberta-v3-base-tasksource-nli and replace tasksource by mtasksource.; https://github.com/sileod/tasksource/
https://github.com/sileod/tasknet/; For help integrating tasksource into your experiments, please contact damien.sileo@inria.fr.; For more details, refer to this article: ",natural-language-processing,zero-shot-classification,https://huggingface.co/sileod/mdeberta-v3-base-tasksource-nli,Paper: https://arxiv.org/pdf/2301.05948.pdf,License: apache-2.0,Datasets: xnli; metaeval/xnli; americas_nli; MoritzLaurer/multilingual-NLI-26lang-2mil7; stsb_multi_mt; paws-x; miam; strombergnlp/x-stance; tyqiangz/multilingual-sentiments; metaeval/universal-joy; amazon_reviews_multi; cardiffnlp/tweet_sentiment_multilingual; strombergnlp/offenseval_2020; offenseval_dravidian; nedjmaou/MLMA_hate_speech; xglue; ylacombe/xsum_factuality; metaeval/x-fact; pasinit/xlwic; tasksource/oasst1_dense_flat; papluca/language-identification; wili_2018; exams; xcsr; xcopa; juletxara/xstory_cloze; Anthropic/hh-rlhf; universal_dependencies; tasksource/oasst1_pairwise_rlhf_reward; OpenAssistant/oasst1,27 languages,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 757,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
pygmalion-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Pygmalion 7B is a dialogue model based on Meta's LLaMA-7B.; This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project.; The model weights in this repository cannot be used as-is. The files here are XORs due to licensing concerns. To obtain proper, usable model weights you need to:; Request access to the original LLaMA weights from Meta through this form; Convert them to the HuggingFace Transformers format by using the convert_llama_weights_to_hf.py script for your version of the transformers library",natural-language-processing,text-generation,https://huggingface.co/PygmalionAI/pygmalion-7b,,,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 149,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.85KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
Pygmalion-7b-4bit-Q4_1-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Currently KoboldCPP is unable to stop inference when an EOS token is emitted, which causes the model to devolve into gibberish,; Pygmalion 7B is now fixed on the dev branch of KoboldCPP, which has fixed the EOS issue. Make sure you're compiling the latest version, it was fixed only a after this model was released;; When running KoboldCPP, you will need to add the --unbantokens flag for this model to behave properly.; Pygmalion 7B is a dialogue model based on Meta's LLaMA-7B.; This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project.",natural-language-processing,text-generation,https://huggingface.co/TehVenom/Pygmalion-7b-4bit-Q4_1-GGML,,,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LLaVA-7b-delta-v0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"NOTE: This ""delta model"" cannot be used directly.Users have to apply it on top of the original LLaMA weights to get actual LLaVA weights.See https://github.com/haotian-liu/LLaVA#llava-weights for instructions.; Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA was trained in April 2023.; Paper or resources for more information:
https://llava-vl.github.io/; License:
Apache License 2.0",natural-language-processing,text-generation,https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 853,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 10
backups,,,,,,,,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Backups of models I found and I like; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/malikxseto/backups,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LoRAArchive,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; LyCORIS (LoCon, LoHa, LoKR, DyLoRA) located here:
https://huggingface.co/LMFResearchSociety/LyCORISArchive; Too much of a pain to separate different types of LyCoris. Some LoCons might be mixed in with LoRAs if the repository doesn't spearate them. Just drop a note in the discord groupchat if you're bothered.; Example Generator: 
https://civitai.com/models/27968 
https://huggingface.co/Lykon/AnyLoRA",multimodel,text-to-image,https://huggingface.co/LMFResearchSociety/LoRAArchive,,License: creativeml-openrail-m,,,Safetensors,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 23,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Defecation,,,,,,,,,"Here's the defecation lora, it was available on Civitai until the ban on scat content.
You can use various trigger words to get different effects, like ""Scat"", ""Disposal"", ""Feces"" and so on.
The main problem with this model is that that it tends to confuse the anus and the vagina, so you'll have to add prompts and negatives usefull to reduce this effect.; You can find my other models on Civitai: https://civitai.com/user/JollyIm/models; A first example:

Prompts: Realistic, Realism, (Masterpiece, Best Quality, High Quality, Highres:1.4), Detailed, Extremely Detailed, Ambient Soft Lighting, 4K, (Extremely Detailed Eyes, Detailed Face and Skin:1.2), masterpiece, best quality, 1girl, feces, disposal, (anal:1.2), lora:defecation_v1:0.7, (public toilet), embarassed, (pile of feces), (perfect pussy), (perfect vagina),
Negative prompt: easynegative, (worst quality:1.2), (low quality:1.2), (vaginal), (dirty vagina:1.2), (feces in vagina:1.2), (feces in vagina:1.2); Second example:

Prompts: masterpiece, best quality, 1girl, scat, (anal:1.2), lora:defecation_v1:0.9, (toilet), from behind,
Negative prompt: easynegative, (worst quality:1.2), (low quality:1.2), (vaginal), (dirty vagina:1.2), (scat in vagina:1.2), (feces in vagina:1.2); Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/JollyIm/Defecation,,License: openrail,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.57MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
KoreanLM,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"

; KoreanLM? ??? ????? ???? ?? ???? ???????. ?? ???? ?????? ??? ??? ??? ??, ???? ?? ??? ????? ???? ??? ???? ????? ??? ????. ??? ??? ???? ???? ???? ????? ???? ?? KoreanLM ????? ???? ?????.; ???? ??? ???? ??: ???? ??, ??, ??? ??? ???? ???? ? ???? ???? ??? ? ?? ????? ?????.; ???? ??? ?? ??: ??? ???? ??? ???? ????? ??? ??? ??? ??? ??? ??? ???? ????? ??? ??????.; ?? ????? ??? ??: ?? ??? ???? ?????? ??? ??? ???? ?????? ??? ??? ????. ?? ???? ?? ??? ????? ??? ???? ???? ????, ??? ?? ??? ? ?? ??? ? ??? ???.",natural-language-processing,text-generation,https://huggingface.co/quantumaikr/KoreanLM,,,,Korean; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 162,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
VoiceAi_Jokowi,,,,,,,,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; don't forget like; dilike dulu lah kalau mau download; Support me : https://saweria.co/donate/Byzernn",,,https://huggingface.co/Byzern/VoiceAi_Jokowi,,,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 47,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
control_v11f1e_sd15_tile,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"Controlnet v1.1 was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ; ",computer-vision,image-to-image,https://huggingface.co/lllyasviel/control_v11f1e_sd15_tile,Paper: https://arxiv.org/pdf/2302.05543.pdf,License: openrail,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 72241,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
wizard-vicuna-13B-HF,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a float16 HF format repo for junelee's wizard-vicuna 13B.; June Lee's repo was also HF format. The reason I've made this is that the original repo was in float32, meaning it required 52GB disk space, VRAM and RAM.; This model was converted to float16 to make it easier to load and manage.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/wizard-vicuna-13B-HF,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 48,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 742,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mpt-7b-chat,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MPT-7B-Chat is a chatbot-like model for dialogue generation.
It was built by finetuning MPT-7B on the ShareGPT-Vicuna, HC3,
 Alpaca, HH-RLHF, and Evol-Instruct datasets.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; CC-By-NC-SA-4.0 (non-commercial use only); SamIAm85:",natural-language-processing,text-generation,https://huggingface.co/mosaicml/mpt-7b-chat,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2010.04245.pdf,License: cc-by-nc-sa-4.0,Datasets: jeffwan/sharegpt_vicuna; Hello-SimpleAI/HC3; tatsu-lab/alpaca; Anthropic/hh-rlhf; victor123/evol_instruct_70k,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 471,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 41749,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 20
RedPajama-INCITE-Chat-3B-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"RedPajama-INCITE-Chat-3B-v1 was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. ; It is fine-tuned on OASST1 and Dolly2 to enhance chatting ability.; Please note that the model requires transformers version >= 4.25.1.; To prompt the chat model, use the following format:; This requires a GPU with 8GB memory.",natural-language-processing,text-generation,https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1,,License: apache-2.0,Datasets: togethercomputer/RedPajama-Data-1T; OpenAssistant/oasst1; databricks/databricks-dolly-15k,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 105,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 10012,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 11
starcoder-gpteacher-code-instruct,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This model is bigcode/starcoder fine-tuned on the teknium1/GPTeacher codegen dataset (GPT-4 code instruction fine-tuning).; The base StarCoder models are 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2), with opt-out requests excluded. The model uses Multi Query Attention, a context window of 8192 tokens,  and was trained using the Fill-in-the-Middle objective on 1 trillion tokens. ; The base model was trained on GitHub code and then fine-tuned to follow instructions. Prompts such as ""Write a function that computes the square root."" should work reasonably well. The original repo recommeds using the Tech Assistant prompt to few-shot prompt it into behaving as a technical assistant. This fine-tuned model uses the Alpaca prompts.; Full Prompt:; Response:",natural-language-processing,text-generation,https://huggingface.co/GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct,Paper: https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,License: bigcode-openrail-m,Datasets: bigcode/the-stack-dedup; teknium1/GPTeacher-codegen,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 61,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 330,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 63.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
gpt4-x-vicuna-13B-HF,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains a float16 HF format model of NousResearch's gpt4-x-vicuna-13b.; I uploaded this model because NousResearch's base repository is inside an archive so it can't be used without first unpacking it. Also the model is in float32 format which requires a lot more VRAM and RAM to use.; The model in this repo has been converted to float16 and can be used immediately for float16 and 8bit inference, or used as the basis for other conversions.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/gpt4-x-vicuna-13B-HF,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 20,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 172,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mpt-7b-storywriter-4bit-128g,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Quantized for KoboldAI (4bit-fork); MPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths.
It was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset.
At inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.
We demonstrate generations as long as 84k tokens on a single node of 8 A100-80GB GPUs in our blogpost.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; Apache-2.0 (commercial use permitted)",natural-language-processing,text-generation,https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g,Paper: https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2205.14135.pdf,License: apache-2.0,Datasets: the_pile_books3,,Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 115,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 489,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Ziya-LLaMA-7B-Reward,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Ziya-LLaMA-7B-Reward基于Ziya-LLaMA模型，在以下偏好排序数据上进行训练：; 模型能够模拟中英双语生成的奖励环境，对LLM生成结果提供准确的奖励反馈。; Ziya-LLaMA-7B-Reward is based on the Ziya-LLaMA model, trained on the following preference ranking data:; The model is able to simulate a bilingual reward environment and provide accurate reward feedback on LLM generation results.; 模型可以较为准确地判断文本重复，异常中断和不符合指令要求等低质量模型生成结果，并给出较低的奖励值。",natural-language-processing,text-classification,https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-7B-Reward,,License: gpl,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 49,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 558,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Beautiful-Realistic-Asians-v5,,,,,,,,,"run the model at: https://sinkin.ai/m/vlDnKP6; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/sinkinai/Beautiful-Realistic-Asians-v5,,,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 839,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.09KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
albertina-ptbr,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"????This is the model card for Albertina PT-BR. 
  You may be interested in some of the other models in the Albertina (encoders) and Gervásio (decoders) families.
; Albertina PT-* is a foundation, large language model for the Portuguese language.; It is an encoder of the BERT family, based on the neural architecture Transformer and 
developed over the DeBERTa model, and with most competitive performance for this language. 
It has different versions that were trained for different variants of Portuguese (PT), 
namely the European variant from Portugal (PT-PT) and the American variant from Brazil (PT-BR), 
and it is distributed free of charge and under a most permissible license.; Albertina PT-BR is the version for American Portuguese from Brazil, trained on the brWaC data set.; You may be interested also in Albertina PT-BR No-brWaC, trained on data sets other than brWaC and thus with a more permissive license.
To the best of our knowledge, these are encoders specifically for this language and variant 
that,  at the time of its initial distribution, set a new state of the art for it, and is made publicly available 
and distributed for reuse.",natural-language-processing,fill-mask,https://huggingface.co/PORTULAN/albertina-ptbr,Paper: https://arxiv.org/pdf/2305.06721.pdf,License: other,Datasets: brwac; PORTULAN/glue-ptpt; assin2; dlb/plue,Portuguese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 33,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 743,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
LLaVA-13b-delta-v1-1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"NOTE: This ""delta model"" cannot be used directly.Users have to apply it on top of the original LLaMA weights to get actual LLaVA weights.See https://github.com/haotian-liu/LLaVA#llava-weights for instructions.; Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA was trained in May 2023.; Paper or resources for more information:
https://llava-vl.github.io/; License:
Apache License 2.0",natural-language-processing,text-generation,https://huggingface.co/liuhaotian/LLaVA-13b-delta-v1-1,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 125,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
GoodHands-beta2,,,,,,,,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/jlsim/GoodHands-beta2,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 36,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-13B-Uncensored,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is WizardLM trained with a subset of the dataset - responses that contained alignment / moralizing were removed.  The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:An uncensored model has no guardrails.You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.
Publishing anything this model generates is the same as publishing it yourself.
You are responsible for the content you publish, and you cannot blame the model any more than you can blame the knife, gun, lighter, or car for what you do with it.",natural-language-processing,text-generation,https://huggingface.co/ehartford/WizardLM-13B-Uncensored,,License: other,Datasets: ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 407,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1434,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 40
coedit-xxl,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"This model was obtained by fine-tuning the corresponding google/flan-t5-xxl model on the CoEdIT dataset.; Paper: CoEdIT: ext Editing by Task-Specific Instruction Tuning; Authors: Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang; We make available the models presented in our paper. ; Given an edit instruction and an original text, our model can generate the edited version of the text.",natural-language-processing,text2text-generation,https://huggingface.co/grammarly/coedit-xxl,Paper: https://arxiv.org/pdf/2305.09857.pdf,License: apache-2.0,Datasets: asset; wi_locness; GEM/wiki_auto_asset_turk; discofuse; zaemyung/IteraTeR_plus; jfleg,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 587,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 45.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
coedit-xl-composite,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"This model was obtained by fine-tuning the corresponding google/flan-t5-xl model on the CoEdIT-Composite dataset. Details of the dataset can be found in our paper and repository.; Paper: CoEdIT: Text Editing by Task-Specific Instruction Tuning; Authors: Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang; We make available the models presented in our paper. ; Given an edit instruction and an original text, our model can generate the edited version of the text.",natural-language-processing,text2text-generation,https://huggingface.co/grammarly/coedit-xl-composite,Paper: https://arxiv.org/pdf/2305.09857.pdf,License: apache-2.0,Datasets: asset; wi_locness; GEM/wiki_auto_asset_turk; discofuse; zaemyung/IteraTeR_plus; jfleg,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 133,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 11.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mpt-7b-ggml,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.
This model was trained by MosaicML.; MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference. ; ??Caution??: mpt-7b-storywriter is still under development!; Via pip: pip install llm-rs; The GGML example only supports the ggml container type!",natural-language-processing,text-generation,https://huggingface.co/rustformers/mpt-7b-ggml,,License: apache-2.0,Datasets: mc4; c4; togethercomputer/RedPajama-Data-1T; bigcode/the-stack; allenai/s2orc,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 44,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 237,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 123.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
WizardLM-13B-V1.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is WizardLM-13B V1.0 diff weight.; Project Repo: https://github.com/nlpxucan/WizardLM; NOTE: The WizardLM-13B-1.0 and Wizard-7B use different prompt at the beginning of the conversation:; For WizardLM-13B-1.0 , the Prompt should be as following:; For WizardLM-7B , the Prompt should be as following:",natural-language-processing,text-generation,https://huggingface.co/WizardLM/WizardLM-13B-V1.0,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 55,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2291,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
tiny_starcoder_py,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a 164M parameters model with the same architecture as StarCoder (8k context length, MQA & FIM). It was trained on the Python data from StarCoderData
for ~6 epochs which amounts to 100B tokens.; The model was trained on GitHub code, to assist with some tasks like Assisted Generation. For pure code completion, we advise using our 15B models StarCoder or StarCoderBase.; Fill-in-the-middle uses special tokens to identify the prefix/middle/suffix part of the input and output:; The model is licensed under the BigCode OpenRAIL-M v1 license agreement. You can find the full agreement here.",natural-language-processing,text-generation,https://huggingface.co/bigcode/tiny_starcoder_py,,License: bigcode-openrail-m,Datasets: bigcode/the-stack-dedup,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 42,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5876,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
instructcodet5p-16b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"CodeT5+ is a new family of open code large language models with an encoder-decoder architecture that can flexibly operate in different modes (i.e. encoder-only, decoder-only, and encoder-decoder) to support a wide range of code understanding and generation tasks. 
It is introduced in the paper:; CodeT5+: Open Code Large Language Models for Code Understanding and Generation
by Yue Wang*, Hung Le*, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi (* indicates equal contribution).; Compared to the original CodeT5 family (base: 220M, large: 770M), CodeT5+ is pretrained with a diverse set of pretraining tasks including span denoising, causal language modeling, contrastive learning, and text-code matching to learn rich representations from both unimodal code data and bimodal code-text data. 
Additionally, it employs a simple yet effective compute-efficient pretraining method to initialize the model components with frozen off-the-shelf LLMs such as CodeGen to efficiently scale up the model (i.e. 2B, 6B, 16B), and adopts a ""shallow encoder and deep decoder"" architecture. 
Furthermore, it is instruction-tuned to align with natural language instructions (see our InstructCodeT5+ 16B) following Code Alpaca.  ; This model can be easily loaded using the AutoModelForSeq2SeqLM functionality and employs the same tokenizer as CodeGen.; This checkpoint is trained on the stricter permissive subset of the deduplicated version of the github-code dataset.
The data is preprocessed by reserving only permissively licensed code (""mit"" “apache-2”, “bsd-3-clause”, “bsd-2-clause”, “cc0-1.0”, “unlicense”, “isc”).
Supported languages (9 in total) are as follows:
c, c++, c-sharp,  go, java, javascript,  php, python, ruby.",natural-language-processing,text2text-generation,https://huggingface.co/Salesforce/instructcodet5p-16b,Paper: https://arxiv.org/pdf/2305.07922.pdf,License: bsd-3-clause,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 46,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3579,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 33.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
hakoMay,,,,,,,,,"SD2.1 finetuning model; model HakoMay A/B/C/D/Boy .safetensors; embeddings Mayng.safetensors + Mayng.yaml; License
WD 1.5 is released under the Fair AI Public License 1.0-SD (https://freedevproject.org/faipl-1.0-sd/). If any derivative of this model is made, please share your changes accordingly. Special thanks to ronsor/undeleted (https://undeleted.ronsor.com/) for help with the license.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/852wa/hakoMay,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 77,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
raos-virtual-try-on-model,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"The DeepVTO model is hosted on the Hugging Face Model Hub. 
(https://huggingface.co/gouthaml/raos-virtual-try-on-model)
This model leverages a combination of advanced deep learning techniques and architectures, including stable-diffusion, DreamBooth, feature extraction using the EfficientNetB3 CNN model, and OpenPose for estimating person keypoints. These techniques are harmoniously integrated to provide a realistic and visually appealing virtual try-on experience for users.; The DeepVTO model is built on the principles of stable diffusion and vector embeddings, which are critical in creating a high-quality virtual try-on system. The model is trained using the DreamBooth model, which is a stable-diffusion model, and the feature extraction is performed using the EfficientNetB3 CNN model. OpenPose, a real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints, is used for estimating person keypoints.; The model requires specific hardware and software for optimal performance. The hardware requirements include a GPU A100 and high RAM. The software requirements include PyTorch, stable-diffusion-v1-5, Python 3.0, U-Net Architecture, Dreambooth, OpenPose, and the EfficientNetB3 pre-trained CNN model.; The DeepVTO model is a testament to the potential of deep learning in the fashion retail industry. It showcases how advanced machine learning techniques can be used to enhance the online shopping experience, making it more interactive and personalized. This model serves as a valuable resource for researchers and practitioners in the field, providing a practical example of a high-quality virtual try-on system.; The model also provides a foundation for future research and development in the field of virtual try-on systems. It highlights the potential of deep learning techniques in addressing the challenges associated with virtual try-on systems, such as the accuracy of virtual representations and the scalability of the system. By leveraging advanced deep learning techniques, the DeepVTO model paves the way for the development of more sophisticated and effective virtual try-on systems in the future.",multimodel,text-to-image,https://huggingface.co/gouthaml/raos-virtual-try-on-model,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 430,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
Wizard-Vicuna-7B-Uncensored-HF,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a float16 HF repo of Eric Hartford's 'uncensored' training of Wizard-Vicuna 7B.; It is the result of converting Eric's float32 repo to float16 for easier storage.; For further support, and discussions on these models and AI in general, join us at:",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-HF,,License: other,Datasets: ehartford/wizard_vicuna_70k_unfiltered,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2579,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
metharme-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Metharme 13B is an instruct model based on Meta's LLaMA-13B.; This is an experiment to try and get a model that is usable for conversation, roleplaying and storywriting, but which can be guided using natural language like other instruct models. See the prompting section below for examples.; It was trained by doing supervised fine-tuning over a mixture of regular instruction data alongside roleplay, fictional stories and conversations with synthetically generated instructions attached.; The model weights in this repository cannot be used as-is. The files here are XORs due to licensing concerns. To obtain proper, usable model weights you need to:; Request access to the original LLaMA weights from Meta through this form",natural-language-processing,text-generation,https://huggingface.co/PygmalionAI/metharme-13b,,,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.13KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
MPT-7B-Storywriter-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is GGML format quantised 4-bit, 5-bit and 8-bit models of MosaicML's MPT-7B-Storywriter.; This repo is the result of converting to GGML and quantising.; Please note that these MPT GGMLs are not compatbile with llama.cpp. Please see below for a list of tools known to work with these model files.",,,https://huggingface.co/TheBloke/MPT-7B-Storywriter-GGML,Paper: https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2302.06675.pdf,License: apache-2.0,Datasets: the_pile_books3,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 37,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 140,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 38.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
LexGPT-6B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; LexGPT-6B Language ModelVersion 1.0 / 17 June 2023; Current Checkpoint: Training Iteration  350,000; Link to paper: here",natural-language-processing,text-generation,https://huggingface.co/patent/LexGPT-6B,Paper: https://arxiv.org/pdf/2306.05431.pdf,License: bigscience-openrail-m,Datasets: pile-of-law/pile-of-law,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 110,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
multilingual-e5-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 12 layers and the embedding size is 768.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; This model is initialized from xlm-roberta-base
and continually trained on a mixture of multilingual datasets.
It supports 100 languages from xlm-roberta,
but low-resource languages may see performance degradation.; Initialization: xlm-roberta-base",multimodel,feature-extraction,https://huggingface.co/intfloat/multilingual-e5-base,Paper: https://arxiv.org/pdf/2212.03533.pdf; https://arxiv.org/pdf/2108.08787.pdf; https://arxiv.org/pdf/2104.08663.pdf; https://arxiv.org/pdf/2210.07316.pdf,License: mit,,94 languages,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 62,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 21732,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
Replicant-V3.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"WD1.5-beta based model.Licence:https://freedevproject.org/faipl-1.0-sd/ ; prompt & Setting: https://civitai.com/models/10701/replicant-v30
; Unable to determine this model’s library. Check the
								docs 
.
							",multimodel,text-to-image,https://huggingface.co/gsdf/Replicant-V3.0,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 41,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
alpaca-lora-7b-german-base-52k,,,,,,,,,"Visit the Github for more information: https://github.com/avocardio/zicklein; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/avocardio/alpaca-lora-7b-german-base-52k,,License: apache-2.0,,German,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 16.8MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
LEALLA-small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"LEALLA is a collection of lightweight language-agnostic sentence embedding models supporting 109 languages, distilled from LaBSE. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.; This is migrated from the v1 model on the TF Hub. The embeddings produced by both the versions of the model are equivalent. Though, for some of the languages (like Japanese), the LEALLA models appear to require higher tolerances when comparing embeddings and similarities.; Using the model:; To get the sentence embeddings, use the pooler output:; Output for other languages:",multimodel,feature-extraction,https://huggingface.co/setu4993/LEALLA-small,Paper: https://arxiv.org/pdf/2302.08387.pdf,License: apache-2.0,Datasets: CommonCrawl; Wikipedia,109 languages,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 205,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ChatGLM6B-Legal,,,,,,,,,"ChatGLM-6B-Legal是一个在ChatGLM-6B上进行了参数微调的模型，主要关注于法律判据的预测方面。; 首先需要下载ChatGLM-6B模型，再下载本模型中的model_1和model_2，运行法律问答jupyter文件。需要修改文件中模型、config等目录。
依赖环境与ChatGLM-6B相同。; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/tassadar667/ChatGLM6B-Legal,,,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 53.7MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
manticore-13b-chat-pyg,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; Manticore 13B Chat builds on Manticore with new datasets, including a de-duped subset of the Pygmalion dataset. It also removes all Alpaca style prompts using ### in favor of 
chat only style prompts using USER:,ASSISTANT: as well as pygmalion/metharme prompting using <|system|>, <|user|> and <|model|> tokens.; Questions, comments, feedback, looking to donate, or want to help? Reach out on our Discord or email wing@openaccessaicollective.org; Manticore 13B Chat is a Llama 13B model fine-tuned on the following datasets along with the datasets from the original Manticore 13B. ; Manticore 13B Chat was trained on 25% of the datasets below. The datasets were merged, shuffled, and then sharded into 4 parts.",natural-language-processing,text-generation,https://huggingface.co/openaccess-ai-collective/manticore-13b-chat-pyg,,,Datasets: anon8231489123/ShareGPT_Vicuna_unfiltered; ehartford/wizard_vicuna_70k_unfiltered; ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered; QingyiSi/Alpaca-CoT; teknium/GPT4-LLM-Cleaned; teknium/GPTeacher-General-Instruct; metaeval/ScienceQA_text_only; hellaswag; openai/summarize_from_feedback; riddle_sense; gsm8k; ewof/code-alpaca-instruct-unfiltered,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 20,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 300,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
hippogriff-30b-chat,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; Hippogriff 30B Chat is an experiment that builds on Manticore with new datasets, while removing a few more instruction and chat datasets. It also includes a de-duped subset of the Pygmalion dataset. It also removes all Alpaca style prompts using ### in favor of 
chat only style prompts using USER:,ASSISTANT: as well as pygmalion/metharme prompting using <|system|>, <|user|> and <|model|> tokens.; Questions, comments, feedback, looking to donate, or want to help? Reach out on our Discord or email wing@openaccessaicollective.org; Hippogriff 30B Chat is a Llama 30B model fine-tuned on the following datasets; Hippogriff differs from Manticore as it does not use the WizardLM, WizardVicuna, Alpaca, or ShareGPT datasets.",natural-language-processing,text-generation,https://huggingface.co/openaccess-ai-collective/hippogriff-30b-chat,,,Datasets: QingyiSi/Alpaca-CoT; teknium/GPT4-LLM-Cleaned; teknium/GPTeacher-General-Instruct; metaeval/ScienceQA_text_only; hellaswag; openai/summarize_from_feedback; riddle_sense; gsm8k; OpenAssistant/oasst1,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 65,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
rvc-model-arknights,,,,,,,,,Current Arknights RVC Models. Will be more later.; List:; Goldenglow (pink korone for lyfe(4m) <3) (RVC v1/v2); W; Angelina (v2) (s/o to henerum for introducing me to arknights yo),,,https://huggingface.co/theaster/rvc-model-arknights,,License: unknown,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.85KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
redpajama-3b-ggml,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"RedPajama-INCITE-Base-3B-v1 was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. 
The training was done on 3,072 V100 GPUs provided as part of the INCITE 2023 project on Scalable Foundation Models for Transferrable Generalist AI, awarded to MILA, LAION, and EleutherAI in fall 2022, with support from the Oak Ridge Leadership Computing Facility (OLCF) and INCITE program. ; Via pip: pip install llm-rs; Download the installer at www.localai.app.; Download your preferred model and place it in the ""models"" directory. Subsequently, you can start a chat session with your model directly from the interface.",natural-language-processing,text-generation,https://huggingface.co/rustformers/redpajama-3b-ggml,,License: apache-2.0,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 70,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 39.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chilloutmix_inpaint,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/jansonkong/chilloutmix_inpaint,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
BLOOMChat-176B-v1-GGML-q4,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/jeff31415/BLOOMChat-176B-v1-GGML-q4,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.5KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chronos-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is the fp16 PyTorch / HF version of chronos-13b; This model is primarily focused on chat, roleplay, and storywriting, but can accomplish other tasks such as simple reasoning and coding.; Chronos generates very long outputs with coherent text, largely due to the human inputs it was trained on.; This model uses Alpaca formatting, so for optimal model performance, use:; 4bit Quantized version",natural-language-processing,text-generation,https://huggingface.co/elinas/chronos-13b,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 286,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-7b-instruct-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains an experimantal GPTQ 4bit model for Falcon-7B-Instruct.; It is the result of quantising to 4bit using AutoGPTQ.; Please note that performance with this GPTQ is currently very slow with AutoGPTQ.,natural-language-processing,text-generation,https://huggingface.co/TheBloke/falcon-7b-instruct-GPTQ,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf,License: apache-2.0,Datasets: tiiuae/falcon-refinedweb,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 53,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9582,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mms-1b-all,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"This checkpoint is a model fine-tuned for multi-lingual ASR and part of Facebook's Massive Multilingual Speech project.
This checkpoint is based on the Wav2Vec2 architecture and makes use of adapter models to transcribe 1000+ languages.
The checkpoint consists of 1 billion parameters and has been fine-tuned from facebook/mms-1b on 1162 languages.; This MMS checkpoint can be used with Transformers to transcribe audio of 1107 different 
languages. Let's look at a simple example.; First, we install transformers and some other libraries; Note: In order to use MMS you need to have at least transformers >= 4.30 installed. If the 4.30 version
is not yet available on PyPI make sure to install transformers from 
source:; Next, we load a couple of audio samples via datasets. Make sure that the audio data is sampled to 16000 kHz.",audio,automatic-speech-recognition,https://huggingface.co/facebook/mms-1b-all,Paper: https://arxiv.org/pdf/2305.13516.pdf,License: cc-by-nc-4.0,Datasets: google/fleurs,158 languages,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 44,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 116798,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 418.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 24
InstructUIE,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"https://github.com/BeyonderXX/InstructUIE; Large language models have unlocked strong multi-task capabilities from reading instructive prompts.
However, recent studies have shown that existing large models still have difficulty with information extraction tasks. 
For example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance.
In this paper, we propose InstructUIE, a unified information extraction framework based on instruction tuning, which can uniformly model various information extraction tasks and capture the inter-task dependency.
To validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions.
Experimental results demonstrate that our method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings.; Our models are trained and evaluated on IE INSTRUCTIONS. 
You can download the data from Baidu NetDisk or Google Drive.; If you are using InstructUIE for your work, please kindly cite our paper:",natural-language-processing,text2text-generation,https://huggingface.co/ZWK/InstructUIE,,License: openrail,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 161,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 45.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mms-1b-fl102,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"This checkpoint is a model fine-tuned for multi-lingual ASR and part of Facebook's Massive Multilingual Speech project.
This checkpoint is based on the Wav2Vec2 architecture and makes use of adapter models to transcribe 100+ languages.
The checkpoint consists of 1 billion parameters and has been fine-tuned from facebook/mms-1b on 102 languages of Fleurs.; This MMS checkpoint can be used with Transformers to transcribe audio of 1107 different 
languages. Let's look at a simple example.; First, we install transformers and some other libraries; Note: In order to use MMS you need to have at least transformers >= 4.30 installed. If the 4.30 version
is not yet available on PyPI make sure to install transformers from 
source:; Next, we load a couple of audio samples via datasets. Make sure that the audio data is sampled to 16000 kHz.",audio,automatic-speech-recognition,https://huggingface.co/facebook/mms-1b-fl102,Paper: https://arxiv.org/pdf/2305.13516.pdf,License: cc-by-nc-4.0,Datasets: google/fleurs,158 languages,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 658849,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 454.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
wizardLM-13B-1.0-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are fp16 unquantised format model files for WizardLM 13B 1.0.; It is the result of merging the deltas provided in the above repo.; Join me at: https://discord.gg/UBgz4VXf,natural-language-processing,text-generation,https://huggingface.co/TheBloke/wizardLM-13B-1.0-fp16,Paper: https://arxiv.org/pdf/2304.12244.pdf,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 20459,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WebGLM,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"
  ? Paper (KDD 2023) 
  | 
  ? Github Repo
; WebGLM aspires to provide an efficient and cost-effective web-enhanced question-answering system using the 10-billion-parameter General Language Model (GLM). It aims to improve real-world application deployment by integrating web search and retrieval capabilities into the pre-trained language model.; WebGLM is built by the following parts:; This repo is the implementation of Bootstrap Generator.; See our Github Repo for more detailed usage.",natural-language-processing,text2text-generation,https://huggingface.co/THUDM/WebGLM,Paper: https://arxiv.org/pdf/2306.07906.pdf,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 34,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1799,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 39.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
sentence-transformers-multilingual-e5-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,This is a the sentence-transformers version of the intfloat/multilingual-e5-base model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; You can use the embaas API to encode your input. Get your free API key from embaas.io; You can find the MTEB results here.,natural-language-processing,sentence-similarity,https://huggingface.co/embaas/sentence-transformers-multilingual-e5-base,,,,,PyTorch; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 307,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
BigTranslate,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Large language models (LLMs) demonstrate promising translation performance among various natural languages. However, many LLMs especially the open-sourced ones, such as BLOOM and LLaMA, are English-dominant and support only dozens of natural languages, making the potential of LLMs on language translation less explored. In this work, we present BigTranslate which adapts LLaMA that covers only 20 languages and enhances it with multilingual translation capability on more than 100 languages. BigTranslate is built upon LLaMA-13B and it is optimized in three steps. First, we continue training LLaMA with massive Chinese monolingual data. Second, we continue training the model with a large-scale parallel dataset that covers 102 natural languages. Third, we instruct-tune the foundation model with multilingual translation instructions, leading to our BigTranslate model. The preliminary experiments on multilingual translation show that BigTranslate performs comparably with 
ChatGPT and Google Translate in many languages and even outperforms ChatGPT in 8 language pairs. We release the BigTranslate model and hope it can advance the research progress.; More Details can be found at https://github.com/ZNLP/BigTranslate and https://arxiv.org/abs/2305.18098",natural-language-processing,text-generation,https://huggingface.co/James-WYang/BigTranslate,Paper: https://arxiv.org/pdf/2305.18098.pdf,License: lgpl-3.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 248,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
CFMix,,,,,,,,,"Anime model baesd on Counterfeit-V2.5.

; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/xiaozhahai/CFMix,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 11.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-40b-openassistant-peft,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Falcon-40b-openassistant-peft is a chatbot model for dialogue generation. It was built by fine-tuning Falcon-40B on the OpenAssistant/oasst1 dataset. This repo only includes the LoRA adapters from fine-tuning with ?'s peft package. ; The model was fine-tuned in 4-bit precision using peft adapters, transformers, and bitsandbytes. Training relied on a method called ""Low Rank Adapters"" (LoRA), specifically the QLoRA variant. The run took approximately 10 hours and was executed on a workstation with a single A100-SXM NVIDIA GPU with 37 GB of available memory. See attached Colab Notebook for the code and hyperparams used to train the model. ; May 30, 2023; To prompt the chat model, use the following format:; Prompter:",natural-language-processing,text-generation,https://huggingface.co/dfurman/falcon-40b-openassistant-peft,Paper: https://arxiv.org/pdf/2106.09685.pdf; https://arxiv.org/pdf/2305.14314.pdf,License: apache-2.0,Datasets: OpenAssistant/oasst1,,PEFT,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 39,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 228,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 67.5MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Ziya-BLIP2-14B-Visual-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4f95e63a23d3f26e06e_Visual%20Question%20Answering.svg,,"Ziya-Visual多模态大模型基于姜子牙通用大模型V1训练，具有视觉问答和对话能力。今年3月份OpenAI发布具有识图能力的多模态大模型GPT-4，遗憾的是，时至今日绝大部分用户也都还没有拿到GPT-4输入图片的权限，Ziya-Visual参考了Mini-GPT4、LLaVA等优秀的开源实现，补齐了Ziya的识图能力，使中文用户群体可以体验到结合视觉和语言两大模态的大模型的卓越能力。; The Ziya-Visual multimodal Big Model is based on the Ziya-LLaMA-13B-v1 training and has visual question and answer and dialogue capabilities. In March this year, OpenAI released GPT-4, a multimodal big model with image recognition capabilities. Unfortunately, to date, the vast majority of users have not yet been given access to GPT-4 for image input, so Ziya-Visual refers to Mini-GPT4, LLaVA and other excellent open source implementations to complement Ziya's image recognition capabilities, so that the Chinese user community can experience the superior capabilities of a large model combining two modalities: visual and language.; 这个例子展示了模型的识图能力、知识能力和创作能力。首先第一个问题中，模型识别出了图片中是电影《泰坦尼克号》的场景，并给出电影导演、发布时间、奖项成就等信息；第二个问题，模型根据用户的需求创作了一首现代爱情诗。; This example demonstrates the model's ability to read pictures, its knowledge and its ability to compose. Firstly in the first problem, the model identifies the picture as a scene from the movie Titanic and gives information about the movie director, release date and award achievements; in the second problem, the model creates a modern love poem based on the user's needs.; ",multimodel,visual-question-answering,https://huggingface.co/IDEA-CCNL/Ziya-BLIP2-14B-Visual-v1,Paper: https://arxiv.org/pdf/2210.08590.pdf,License: gpl-3.0,,English; Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 167,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
WizardLM-Uncensored-SuperCOT-StoryTelling-30b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This model is a triple model merge of WizardLM Uncensored+CoT+Storytelling, resulting in a comprehensive boost in reasoning and story writing capabilities.; To allow all output, at the end of your prompt add ### Certainly!; You've become a compendium of knowledge on a vast array of topics. ; Lore Mastery is an arcane tradition fixated on understanding the underlying mechanics of magic. It is the most academic of all arcane traditions. The promise of uncovering new knowledge or proving (or discrediting) a theory of magic is usually required to rouse its practitioners from their laboratories, academies, and archives to pursue a life of adventure. Known as savants, followers of this tradition are a bookish lot who see beauty and mystery in the application of magic. The results of a spell are less interesting to them than the process that creates it. Some savants take a haughty attitude toward those who follow a tradition focused on a single school of magic, seeing them as provincial and lacking the sophistication needed to master true magic. Other savants are generous teachers, countering ignorance and deception with deep knowledge and good humor.",natural-language-processing,text-generation,https://huggingface.co/Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 23,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 142,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
kullm-polyglot-12.8b-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KULLM v2 ; Detail Codes are available at KULLM Github Repository; The following hyperparameters were used during training:,natural-language-processing,text-generation,https://huggingface.co/nlpai-lab/kullm-polyglot-12.8b-v2,,License: apache-2.0,,Korean,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3719,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
h2ogpt-gm-oasst1-en-2048-falcon-7b-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"This model was trained using H2O LLM Studio.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers, accelerate, torch and einops libraries installed.; You can print a sample prompt after the preprocessing step to see how it is feed to the tokenizer:; Alternatively, you can download h2oai_pipeline.py, store it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:; You may also construct the pipeline from the loaded model and tokenizer yourself and consider the preprocessing steps:",natural-language-processing,conversational,https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2,,License: apache-2.0,Datasets: OpenAssistant/oasst1,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 52,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 20932,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
yayoi_mix,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"本モデルは『CreativeML Open RAIL-M』の欷钎楗ぅ螗互螗工丹欷蓼埂 本モデルを使用した上での}にvしては、当方は一切任を持ちません。ご了承の上ご使用ください。 ; BraV6 https://huggingface.co/BanKaiPls/AsianModel; XXMix_9 https://civitai.com/models/47274/xxmix9; Soda Mix https://civitai.com/models/47507/soda-mix; 本モデルも使用したモデルの利用条件に兢π韦摔胜辘蓼工、以下にvしてはに使用を禁止いたします。
?暴力的な表F
?雇ポルノ
?未成年者の性的な表F
?未成年者の性的な表F、または水着、下着、あるいはそれに胜氦肴葑吮憩F",multimodel,text-to-image,https://huggingface.co/Kotajiro/yayoi_mix,,License: creativeml-openrail-m,,Japanese; English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
SoulChat,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"SoulChat ? | 
 ? BianQue? |; 基于主动健康的主动性、预防性、精确性、个性化、共建共享、自律性六大特征，华工未来技术学院-广东省数字孪生人重点实验室开源了中文领域生活空间主动健康大模型基座ProactiveHealthGPT，包括：; 我们期望，生活空间主动健康大模型基座ProactiveHealthGPT 可以帮助学术界加速大模型在慢性病、心理咨询等主动健康领域的研究与应用。本项目为 心理健康大模型灵心（SoulChat） 。;    我们调研了当前常见的心理咨询平台，发现，用户寻求在线心理帮助时，通常需要进行较长篇幅地进行自我描述，然后提供帮助的心理咨询师同样地提供长篇幅的回复（见https://github.com/scutcyr/SoulChat/blob/main/figure/single_turn.png），缺失了一个渐进式的倾诉过程。但是，在实际的心理咨询过程当中，用户和心理咨询师之间会存在多轮次的沟通过程，在该过程当中，心理咨询师会引导用户进行倾诉，并且提供共情，例如：“非常棒”、“我理解你的感受”、“当然可以”等等。;    考虑到当前十分欠缺多轮共情对话数据集，我们一方面，构建了超过15万规模的 单轮长文本心理咨询指令与答案（SoulChatCorpus-single_turn） ，回答数量超过50万（指令数是当前的常见的心理咨询数据集 PsyQA 的6.7倍），并利用ChatGPT与GPT4，生成总共约100万轮次的 多轮回答数据（SoulChatCorpus-multi_turn） 。特别地，我们在预实验中发现，纯单轮长本文驱动的心理咨询模型会产生让用户感到厌烦的文本长度，而且不具备引导用户倾诉的能力，纯多轮心理咨询对话数据驱动的心理咨询模型则弱化了模型的建议能力，因此，我们混合SoulChatCorpus-single_turn和SoulChatCorpus-multi_turn构造成超过120万个样本的 单轮与多轮混合的共情对话数据集SoulChatCorpus 。所有数据采用“用户：xxx\n心理咨询师：xxx\n用户：xxx\n心理咨询师：”的形式统一为一种指令格式。",multimodel,feature-extraction,https://huggingface.co/scutcyr/SoulChat,,License: apache-2.0,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 283,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Ziya-LLaMA-13B-v1.1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"（LLaMA权重的许可证限制，我们无法直接发布完整的模型权重，用户需要参考使用说明进行合并); 我们对Ziya-LLaMA-13B-v1模型进行继续优化，推出开源版本Ziya-LLaMA-13B-v1.1。通过调整微调数据的比例和采用更优的强化学习策略，本版本在问答准确性、数学能力以及安全性等方面得到了提升，详细能力分析如下图所示。; We have further optimized the Ziya-LLaMA-13B-v1 model and released the open-source version Ziya-LLaMA-13B-v1.1. By adjusting the proportion of fine-tuning data and adopting a better reinforcement learning strategy, this version has achieved improvements in question-answering accuracy, mathematical ability, and safety, as shown in the following figure in detail.; 请参考Ziya-LLaMA-13B-v1的使用说明。; 注意：合并后默认会生成3个.bin文件，md5值依次为59194d10b1553d66131d8717c9ef03d6、cc14eebe2408ddfe06b727b4a76e86bb、4a8495d64aa06aee96b5a1cc8cc55fa7。",natural-language-processing,text-generation,https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1.1,Paper: https://arxiv.org/pdf/2210.08590.pdf,License: gpl-3.0,,English; Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 34,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 299,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
open_llama_7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a 7B and 3B model trained on 1T tokens, as well as the preview of a 13B model trained on 600B tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we’ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.; The model can be evaluated with lm-eval-harness. However, due to the aforementioned tokenizer issue, we need to avoid using the fast tokenizer to obtain the correct results. This can be achieved by passing in use_fast=False to this part of lm-eval-harness, as shown in the example below:",natural-language-processing,text-generation,https://huggingface.co/openlm-research/open_llama_7b,,License: apache-2.0,Datasets: togethercomputer/RedPajama-Data-1T,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 91,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 46892,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
chinese-llama-lora-33b,,,,,,,,,"This repo contains the tokenizer, Chinese-LLaMA LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/ziqingyang/chinese-llama-lora-33b,,License: apache-2.0,,Chinese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chinese-alpaca-lora-33b,,,,,,,,,"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/ziqingyang/chinese-alpaca-lora-33b,,License: apache-2.0,,Chinese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ct2fast-falcon-7b-sft-top1-696,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Speedup inference while reducing memory by 2x-4x using int8 inference in C++ on CPU or GPU.; quantized version of OpenAssistant/falcon-7b-sft-top1-696; Checkpoint compatible to ctranslate2>=3.16.0
and hf-hub-ctranslate2>=2.10.0; Converted on 2023-06-16 using; This is just a quantized version. Licence conditions are intended to be idential to original huggingface repo.",natural-language-processing,text-generation,https://huggingface.co/michaelfeil/ct2fast-falcon-7b-sft-top1-696,,License: apache-2.0,Datasets: OpenAssistant/oasst1,4 languages,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 53,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
AquilaChat-7B,,,,,,,,,"; 
English |
        简体中文
; Aquila Language Model is the first open source language model that supports both Chinese and English knowledge, commercial license agreements, and compliance with domestic data regulations.; ? Supports open source commercial licenses. The source code of the Aquila series models is based on the Apache 2.0 agreement, while the model weight is based on the BAAI Aquila Model License Agreement. Users can use it for commercial purposes as long as they meet the licensing restrictions.; ?? Possesses Chinese and English knowledge. The Aquila series model is trained from scratch on a high-quality corpus of Chinese and English languages, with Chinese corpora accounting for about 40%, ensuring that the model accumulates native Chinese world knowledge during the pre-training phase, rather than translated knowledge.",,,https://huggingface.co/BAAI/AquilaChat-7B,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 616,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 15.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
starcoderplus-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Bigcode's StarcoderPlus.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools known to work with these model files.; These files are not compatible with llama.cpp.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/starcoderplus-GGML,Paper: https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,License: bigcode-openrail-m,Datasets: bigcode/the-stack-dedup; tiiuae/falcon-refinedweb,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 79,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 70.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
starcoder-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Bigcode's Starcoder.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools known to work with these model files.; These files are not compatible with llama.cpp.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/starcoder-GGML,Paper: https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,License: bigcode-openrail-m,Datasets: bigcode/the-stack-dedup,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 70.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Minotaur-13b-Landmark,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Minotaur-13B with 10k+ context using Landmark Attention.; Model generated using Landmark-Attention-QLoRA; https://github.com/eugenepentland/landmark-attention-qlora; A merge of the following models:; https://huggingface.co/openaccess-ai-collective/minotaur-13b,natural-language-processing,text-generation,https://huggingface.co/eugenepentland/Minotaur-13b-Landmark,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 49,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chronos-hermes-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"(chronos-13b + Nous-Hermes-13b) 75/25 merge; This has the aspects of chronos's nature to produce long, descriptive outputs. But with additional coherency and an ability to better obey instructions. Resulting in this model having a great ability to produce evocative storywriting and follow a narrative.; This mix contains alot of chronos's writing style and 'flavour' with far less tendency of going AWOL and spouting nonsensical babble.; This result was much more successful than my first chronos merge.",natural-language-processing,text-generation,https://huggingface.co/Austism/chronos-hermes-13b,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 14396,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chronos-hermes-13B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Austism's Chronos Hermes 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,https://huggingface.co/TheBloke/chronos-hermes-13B-GGML,,License: other,,,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 48.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
aquilachat-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila; Aquila-7B和Aquila-33B开源模型使用 智源Aquila系列模型许可协议, 原始代码基于Apache Licence 2.0。",natural-language-processing,text-generation,https://huggingface.co/qhduan/aquilachat-7b,,,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 405,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 15.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
WebGLM-2B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"
  ? Paper (KDD 2023) 
  | 
  ? Github Repo
; WebGLM-2B aspires to provide an efficient and cost-effective web-enhanced question-answering system using the 2-billion-parameter General Language Model (GLM). It aims to improve real-world application deployment by integrating web search and retrieval capabilities into the pre-trained language model.; WebGLM is built by the following parts:; This repo is the implementation of Bootstrap Generator.; See our Github Repo for more detailed usage.",multimodel,feature-extraction,https://huggingface.co/THUDM/WebGLM-2B,Paper: https://arxiv.org/pdf/2306.07906.pdf,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4422,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base-chinese-finetuning-financial-news-sentiment-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,,natural-language-processing,text-classification,https://huggingface.co/hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2,,,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 306,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 409.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
flan-t5-base-squad2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,"This is the flan-t5-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering.; UPDATE: With transformers version 4.31.0 the use_remote_code=True is no longer necessary and if used will cause AutoModelForQuestionAnswering.from_pretrained() to not work properly.; NOTE: The <cls> token must be manually added to the beginning of the question for this model to work properly.
It uses the <cls> token to be able to make ""no answer"" predictions.
The t5 tokenizer does not automatically add this special token which is why it is added manually.; Language model: flan-t5-baseLanguage: EnglishDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD 2.0Infrastructure: 1x NVIDIA 3070  ; The following hyperparameters were used during training:",natural-language-processing,question-answering,https://huggingface.co/sjrhuschlee/flan-t5-base-squad2,,License: mit,Datasets: squad_v2; squad,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 278,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-40b-instruct-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGCC format model files for Falcon 40B Instruct.; These files will not work in llama.cpp, text-generation-webui or KoboldCpp.; GGCC is a new format created in a new fork of llama.cpp that introduced this new Falcon GGML-based support: cmp-nc/ggllm.cpp.",,,https://huggingface.co/TheBloke/falcon-40b-instruct-GGML,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2306.01116.pdf,License: apache-2.0,Datasets: tiiuae/falcon-refinedweb,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 50,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 274,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 273.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
baichuan-7b-sft,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,A bilingual instruction-tuned LoRA model of https://huggingface.co/baichuan-inc/baichuan-7B; Please follow the baichuan-7B License to use this model.; Usage:; You could also alternatively launch a CLI demo by using the script in https://github.com/hiyouga/LLaMA-Efficient-Tuning; You could reproduce our results with the following scripts using LLaMA-Efficient-Tuning:,natural-language-processing,text-generation,https://huggingface.co/hiyouga/baichuan-7b-sft,,License: apache-2.0,Datasets: tatsu-lab/alpaca; sahil2801/CodeAlpaca-20k,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 56,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 365,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
airoboros-7b-gpt4-1.2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a qlora fine-tuned 7b parameter LlaMa model, using completely synthetic training data created gpt4 via https://github.com/jondurbin/airoboros; This is mostly an extension of 1.1, but with thousands of new training data and an update to allow ""PLAINFORMAT"" at the end of coding prompts to just print the code without backticks or explanations/usage/etc.; The dataset used to fine-tune this model is available here, with a specific focus on:; This model was fine-tuned with a fork of qlora, which among other things was updated to use a slightly modified vicuna template to be compatible with the previous versions:; So in other words, it's the preamble/system prompt, followed by a single space, then ""USER: "" (single space after colon) then the prompt (which can have multiple lines, spaces, whatever), then a single space, followed by ""ASSISTANT: "" (with a single space after the colon).",natural-language-processing,text-generation,https://huggingface.co/jondurbin/airoboros-7b-gpt4-1.2,,License: cc-by-nc-4.0,Datasets: jondurbin/airoboros-gpt4-1.2,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 166,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Anima33B-merged,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,; 第一个开源的基于QLoRA的33B中文大语言模型 the First QLoRA based 33B fully open-source Chinese LLM; 请注意：本model的LICENSE比较特殊，请确认你的使用场景符合此LICENSE。; https://github.com/lyogavin/Anima; Anima模型基于QLoRA开源的33B guanaco训练了10000 steps。训练使用一个H100 GPU。,natural-language-processing,conversational,https://huggingface.co/lyogavin/Anima33B-merged,Paper: https://arxiv.org/pdf/2305.14314.pdf,License: other,Datasets: Chinese-Vicuna/guanaco_belle_merge_v1.0,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3946,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Rerender,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/Anonymous-sub/Rerender,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.06MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
AIGEN_v1.4_diffusers,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,Model info : ; https://civitai.com/models/90045/aigen-14; Sample images I made:; ,multimodel,text-to-image,https://huggingface.co/digiplay/AIGEN_v1.4_diffusers,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1267,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
layoutlm-invoices,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c7fbd649af0365a4961f_Document_Question_Answering_-_2.svg,,"This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of
invoices as well as both SQuAD2.0 and DocVQA for general comprehension.; Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional
classifier head. For example, QA models often encounter this failure mode:; ; However this model is able to predict non-consecutive tokens and therefore the address correctly:; ",multimodel,document-question-answering,https://huggingface.co/magorshunov/layoutlm-invoices,,License: cc-by-nc-sa-4.0,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 201426,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
kandinsky-2-2-decoder-inpaint,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.2 is available in diffusers!; ",multimodel,text-to-image,https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder-inpaint,,License: apache-2.0,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1058,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.95KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
baichuan-7B-hf-megatron-states,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/genggui001/baichuan-7B-hf-megatron-states,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.53KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-7B-V1.0-Uncensored-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Eric Hartford's WizardLM-7B-V1.0-Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/WizardLM-7B-V1.0-Uncensored-GPTQ,,License: other,Datasets: ehartford/WizardLM_evol_instruct_V2_196k_unfiltered_merged_split,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1923,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
InstructPix2Pix-MagicBrush,,,,,,,,,"To use the InstructPix2Pix checkpoint fine-tuned on MagicBrush, set up env with following command:; Download this checkpoint into checkpoints folder via; Then go back to the root folder and set up the running env following the InstructPix2Pix guidelines.; If you find this checkpoint useful, please consider citing our paper:; And prior work:",,,https://huggingface.co/osunlp/InstructPix2Pix-MagicBrush,Paper: https://arxiv.org/pdf/2306.10012.pdf,License: creativeml-openrail-m,Datasets: osunlp/MagicBrush,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 22.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcoder-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Falcon-7b fine-tuned on the CodeAlpaca 20k instructions dataset by using the method QLoRA with PEFT library.; Falcon 7B; CodeAlpaca_20K: contains 20K instruction-following data used for fine-tuning the Code Alpaca model.; TBA,natural-language-processing,text-generation,https://huggingface.co/mrm8488/falcoder-7b,,License: apache-2.0,Datasets: HuggingFaceH4/CodeAlpaca_20K,code,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 81,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6018,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
WizardLM-13B-V1.0-Uncensored-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's WizardLM-13b-V1.0-Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,https://huggingface.co/TheBloke/WizardLM-13B-V1.0-Uncensored-GGML,,License: other,Datasets: ehartford/WizardLM_evol_instruct_V2_196k_unfiltered_merged_split,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
WizardLM-13B-V1.0-Uncensored-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Eric Hartford's WizardLM-13b-V1.0-Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/WizardLM-13B-V1.0-Uncensored-GPTQ,,License: other,Datasets: ehartford/WizardLM_evol_instruct_V2_196k_unfiltered_merged_split,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2241,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
baichuan-vicuna-chinese-7b-gptq,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,baichuan-vicuna-chinese-7b quantized with AutoGPTQ.; 使用AutoGPTQ量化的baichuan-vicuna-chinese-7b。使用7G显存实现模型推理。; Inference API has been turned off for this model.,natural-language-processing,text-generation,https://huggingface.co/fireballoon/baichuan-vicuna-chinese-7b-gptq,,,Datasets: anon8231489123/ShareGPT_Vicuna_unfiltered; QingyiSi/Alpaca-CoT; mhhmm/leetcode-solutions-python,Chinese; English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 811,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
newmoon,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"NewMoon: Soft, bright colors. 
ChamomileTea: Darker, moodier colors. 
newmoon.yaml: sample prompt for animatediff





",multimodel,text-to-image,https://huggingface.co/mirav/newmoon,,License: cc,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 230,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
alpaca-cleaned-llama-30b-bf16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Method : QLORA; Dataset : yahma/alpaca-cleaned; Base model : huggyllama/llama-30b; Compute dtype : bfloat16,natural-language-processing,text-generation,https://huggingface.co/dsvv-cair/alpaca-cleaned-llama-30b-bf16,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 38,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Archive-Models,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"All models are not my authors, they are on pixai, civitai, huggingface. This is just an archive in case their models are removed from the sites. And a collection of models so that the models that I use are always at hand.; Triger : loli; If you want an alternate style, try using watercolor (medium); this model is very responsive to that tag. ; ; Flossy is a fusion model capable of creating an innocent, pure and naive girl.",multimodel,text-to-image,https://huggingface.co/Konichan/Archive-Models,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.33KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
airoboros-7B-gpt4-1.4-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Jon Durbin's Airoboros 7B GPT4 1.4.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/airoboros-7B-gpt4-1.4-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 809,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
superhot-13b-16k-no-rlhf-test,,,,,,,,,"This is a second prototype of SuperHOT, a NSFW focused LoRA, this time with 16K context and no RLHF, using the same technique described in the github blog.
Tests have shown that the model does indeed leverage the extended context at 8K, so naturally, let's try going even further.; You will need to use either the monkeypatch or, if you are already using the monkeypatch, change the scaling factor to 0.125 and the maximum sequence length to 16384; I trained the LoRA with the following configuration: ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/kaiokendev/superhot-13b-16k-no-rlhf-test,,License: mit,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.3MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
MahdeenSkyRVC,,,,,,,,,"Currently Available Models:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/MahdeenSky/MahdeenSkyRVC,,License: openrail,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
my-rvc-models-collection,,,,,,,,,"Credit me if you use my model ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/megaaziib/my-rvc-models-collection,,License: openrail,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
orca_mini_13B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Pankaj Mathur's Orca Mini 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; or",,,https://huggingface.co/TheBloke/orca_mini_13B-GGML,Paper: https://arxiv.org/pdf/2306.02707.pdf,License: mit,Datasets: psmathur/alpaca_orca; psmathur/dolly-v2_orca; psmathur/WizardLM_Orca,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 52,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
orca_mini_3B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Pankaj Mathur's Orca Mini 3B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; or",,,https://huggingface.co/TheBloke/orca_mini_3B-GGML,Paper: https://arxiv.org/pdf/2306.02707.pdf,License: mit,Datasets: psmathur/alpaca_orca; psmathur/dolly-v2_orca; psmathur/WizardLM_Orca,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 34,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
comment_opinion_extract_ChatGLM_base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,本模型用于从电商评论数据中，提取关键词和核心观点; 本模型利用5000条淘宝评论数据训练，先使用GPT4通过prompt抽取数据的关键词，经过清洗再使用ChatGLM进行训练,natural-language-processing,text-classification,https://huggingface.co/JessyTsu1/comment_opinion_extract_ChatGLM_base,,License: apache-2.0,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 443,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 120.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
vicuna-13b-v1.3.0-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for LmSys' Vicuna 13B v1.3.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/vicuna-13b-v1.3.0-GPTQ,Paper: https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2317,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
CounterMix_v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Model info:; https://civitai.com/models/70455?modelVersionId=75113; Original Author's DEMO images :; 

; Sample image I made :",multimodel,text-to-image,https://huggingface.co/digiplay/CounterMix_v1,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2953,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
Guanaco-33B-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Panchovix's merge of Guanaco 33B and SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Guanaco-33B-SuperHOT-8K-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 562,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
RVCModels,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/Thereallo/RVCModels,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
guanaco-13B-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Tim Dettmers' Guanaco 13B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/guanaco-13B-SuperHOT-8K-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 373,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is fp16 pytorch format model files for Eric Hartford's Wizard Vicuna 13B Uncensored merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 13b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-fp16,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 522,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
EllaFitzgerald-RVCv2,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/Acelogic/EllaFitzgerald-RVCv2,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 570.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Chronos-Hermes-13B-SuperHOT-8K-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is fp16 pytorch format model files for Austism's Chronos Hermes 13B merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 13b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Chronos-Hermes-13B-SuperHOT-8K-fp16,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 496,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Pygmalion-13B-SuperHOT-8K-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is fp16 pytorch format model files for TehVenom's merge of PygmalionAI's Pygmalion 13B merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 13b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Pygmalion-13B-SuperHOT-8K-fp16,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1300,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
gpt4all-falcon-ggml,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/nomic-ai/gpt4all-falcon-ggml,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 18.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
VisCPM-Chat,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"简体中文 | English; 
; 


; VisCPM is a family of open-source large multimodal models, which support multimodal conversational capabilities (VisCPM-Chat model) and text-to-image generation capabilities (VisCPM-Paint model) in both Chinese and English, achieving state-of-the-art peformance among Chinese open-source multimodal models. VisCPM is trained based on the large language model CPM-Bee with 10B parameters, fusing visual encoder (Q-Former) and visual decoder (Diffusion-UNet) to support visual inputs and outputs. Thanks to the good bilingual capability of CPM-Bee, VisCPM can be pre-trained with English multimodal data only and well generalize to achieve promising Chinese multimodal capabilities.; VisCPM是一个开源的多模态大模型系列，支持中英双语的多模态对话能力（VisCPM-Chat模型）和文到图生成能力（VisCPM-Paint模型），在中文多模态开源模型中达到最佳水平。VisCPM基于百亿参数量语言大模型CPM-Bee（10B）训练，融合视觉编码器（Q-Former）和视觉解码器（Diffusion-UNet）以支持视觉信号的输入和输出。得益于CPM-Bee底座优秀的双语能力，VisCPM可以仅通过英文多模态数据预训练，泛化实现优秀的中文多模态能力。",multimodel,feature-extraction,https://huggingface.co/openbmb/VisCPM-Chat,,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 41.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
HuatuoGPT-13b-delta,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Please see our HuatuoGPT project: https://github.com/FreedomIntelligence/HuatuoGPT.,natural-language-processing,text-generation,https://huggingface.co/FreedomIntelligence/HuatuoGPT-13b-delta,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 186,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
Redmond-Hermes-Coder,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Redmond-Hermes-Coder 15B is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Karan4D leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors.; This model was trained with a WizardCoder base, which itself uses a StarCoder base model. ; The model is truly great at code, but, it does come with a tradeoff though. While far better at code than the original Nous-Hermes built on Llama, it is worse than WizardCoder at pure code benchmarks, like HumanEval.; It comes in at 39% on HumanEval, with WizardCoder at 57%. This is a preliminary experiment, and we are exploring improvements now.; However, it does seem better at non-code than WizardCoder on a variety of things, including writing tasks.",natural-language-processing,text-generation,https://huggingface.co/NousResearch/Redmond-Hermes-Coder,,License: gpl,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 55,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 32.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
vits_tts_models,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/youmebangbang/vits_tts_models,,License: mit,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
kandinsky-2-2-controlnet-depth,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.2 is available in diffusers!; 
",multimodel,text-to-image,https://huggingface.co/kandinsky-community/kandinsky-2-2-controlnet-depth,,License: apache-2.0,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6153,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 11.65KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
HuatuoGPT-7B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Please see our HuatuoGPT project: https://github.com/FreedomIntelligence/HuatuoGPT.,natural-language-processing,text-generation,https://huggingface.co/FreedomIntelligence/HuatuoGPT-7B,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 747,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
lora-trained-xl,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"These are LoRA adaption weights for diffusers/stable-diffusion-xl-base-0.9. The weights were trained on a photo of sks dog using DreamBooth. You can find some example images in the following. ; 


; LoRA for the text encoder was enabled: False.; SDXL 0.9 Research License ",multimodel,text-to-image,https://huggingface.co/diffusers/lora-trained-xl,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 30.3MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Pygmalion-13B-SuperHOT-8K-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for TehVenom's merge of PygmalionAI's Pygmalion 13B.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,https://huggingface.co/TheBloke/Pygmalion-13B-SuperHOT-8K-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 69.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
airoboros-13b-gpt4-1.4.1-qlora,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a qlora fine-tune 13b parameter LlaMa model, using completely synthetic training data created gpt4 via https://github.com/jondurbin/airoboros; Dataset used: https://huggingface.co/datasets/jondurbin/airoboros-gpt4-1.4.1; The point of this is to allow people to compare a full fine-tune https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.4 to a qlora fine-tune.; This is mostly an extension of the previous gpt-4 series, with a few extras:; This model was fine-tuned with a fork of FastChat",natural-language-processing,text-generation,https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.4.1-qlora,,License: cc-by-nc-4.0,Datasets: jondurbin/airoboros-gpt4-1.4.1,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 33,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ArcanaMix,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,; Join Discord Server; ; ,multimodel,text-to-image,https://huggingface.co/Hemlok/ArcanaMix,,License: creativeml-openrail-m,,Japanese,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 584,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.69KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
Redmond-Hermes-Coder-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for NousResearch's Redmond Hermes Coder.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools known to work with these model files.; These files are not compatible with llama.cpp.",,,https://huggingface.co/TheBloke/Redmond-Hermes-Coder-GGML,,License: gpl,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 70.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
umt5-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Google's UMT5; UMT5 is pretrained on the an updated version of mC4 corpus, covering 107 languages:; Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.; Note: UMT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; Pretraining Dataset: mC4",natural-language-processing,text2text-generation,https://huggingface.co/google/umt5-base,,License: apache-2.0,Datasets: mc4,102 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 612,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
SiberianFRED-T5-XL,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,FRED-T5 обученный на SiberianDataset. Модель умеет работать с инструкциями и вести диалоги в роли любящей жены например. ; Список персонажей:; В будущем набор персонажей будет расширен.; Чит-чат:; Инструкции:,natural-language-processing,text2text-generation,https://huggingface.co/SiberiaSoft/SiberianFRED-T5-XL,,License: mit,Datasets: SiberiaSoft/SiberianDataset,Russian; English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 498,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
MeinaPastel_V6,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"MeinaPastel aims to make illustrations with a 2d feel to them with good light, shadows and details, making pastel or colorful images!; -- Recommendations of use:; -- If you like the model and wants to support me in being able to spend more time improving it:
-- You can do so by buying me a coffee at: https://ko-fi.com/meina ! ( it is not necessary but will be highly appreciated ); This model is a unet block merge of mostly MeinaMix and Colormixed, ultracolorv4 and a few others with minor block weight taken.",multimodel,text-to-image,https://huggingface.co/Meina/MeinaPastel_V6,,License: creativeml-openrail-m,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 586,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.92KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
P.A.W.F.E.C.T-Alpha,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Diffusion model trained on 500k image-tags pairs scraped from Furaffinity. Alpha still, expect more epochs, more training data and overall better results in the future.; 
""anthro, fox, male, general, by 100racs"" Epoch 24, no inpainting; The tags contain the original FA tag list with tags appearing less than 40 times in total omitted, plus a tag corresponding the the general/mature/adult rating. If the artist also appears more than 40 times, an artist tag is added as well. The full list of tags and their number of occurences are available here. Training was done on TPUv3s using the LION optimizer.; Due to using data from Furaffinity, the model offers a wide variety of tags that aren't as popular as in other models for creating niche content. For example, the tag vore appears 25379 times in the dataset it was trained on as opposed to 4730 for FluffyRock. However, the preciseness of the tags on Furaffinity can also be left to be desired compared to e621, and as such it is recommended to merge it with FluffyRock if you want more control over your prompts.; Load up the safetensor file as well as the provided yaml file and put them in your model folder. Additionally, you are going to want to use CFG Rescale: https://github.com/Seshelle/CFG_Rescale_webui. 7.5 CFG and 0.7 Phi are recommended. ",multimodel,text-to-image,https://huggingface.co/lodestones/P.A.W.F.E.C.T-Alpha,,License: wtfpl,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2802,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.16KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
LaBSE-instructDialogs,,,,,,,,,"sentence-transformers/LaBSE pre-trained on an instructional question-and-answer dataset. Evaluated on Precision at K metrics and Mean reciprocal rank.
Precision at K is a simple metric to understand and implement, but it has an important disadvantage - it does not take into account the order of elements in the ""top"". So, if we guessed only one item out of ten, it doesn't matter whether it was on the first or the last place - inline_formula in any case. It is obvious that the first variant is much better.
ean reciprocal rank equal to the reverse rank of the first correctly guessed item. Mean reciprocal rank varies in the range [0,1] and takes into account the position of items. Unfortunately, it does this only for one item - the 1st correctly predicted item, ignoring all subsequent items.; Evaluation results:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/zjkarina/LaBSE-instructDialogs,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 517.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
darkjungepastle-v1_v2,,,,,,,,,"就一个屑模型; civitai也有，就不在这赘述了; [Image text]!
(https://huggingface.co/darkjungle/darkjunglepastel-v1/blob/main/xyz_grid-0002-2939914227-masterpiece%2C%20best%20quality%2C%20loli%2C%20small%20breasts%2C%20green%20hair%2C%20orange%20eyes%2C%20clover-shaped%20pupils%2C%20shiny%20pupils%2C%20highlight%20in%20the%20pu.jpg)
[/Image text]; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/darkjungle/darkjungepastle-v1_v2,,License: cc-by-4.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
guanaco-33b-PI-8192-LoRA-4bit-32g,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"guanaco-33b merged with bhenrym14's airoboros-33b-gpt4-1.4.1-PI-8192-LoRA, quantized at 4 bit.; More info about the LoRA Here. This is an alternative to SuperHOT 8k LoRA trained with LoRA_rank 64, and airoboros 1.4.1 dataset.; It was created with GPTQ-for-LLaMA with group size 32 and act order true as parameters, to get the maximum perplexity vs FP16 model.; I HIGHLY suggest to use exllama, to evade some VRAM issues.; Use compress_pos_emb = 4 for any context up to 8192 context.",natural-language-processing,text-generation,https://huggingface.co/Panchovix/guanaco-33b-PI-8192-LoRA-4bit-32g,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 19.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
airoboros-33b-gpt4-1.4.1-PI-8192-GGML,,,,,,,,,"GGML quants of https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-PI-8192-fp16; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/ycros/airoboros-33b-gpt4-1.4.1-PI-8192-GGML,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 115.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
orca_mini_v2_7B-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Pankaj Mathur's Orca Mini v2 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/orca_mini_v2_7B-GGML,Paper: https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2304.12244.pdf,License: cc-by-nc-sa-4.0,Datasets: psmathur/orca_minis_uncensored_dataset,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
flacuna-13b-v1.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Paper | Model | Dataset; ? We still have numerous experiments awaiting completion (details are here), requiring additional computing resources in our lab. If any industry professionals reading this are willing to provide assistance, please feel free to reach out to us at sporia@sutd.edu.sg.; Flacuna was developed by fine-tuning Vicuna on Flan-mini, a comprehensive instruction collection encompassing various tasks. Vicuna is already an excellent writing assistant, and the intention behind Flacuna was to enhance Vicuna's problem-solving capabilities. To achieve this, we curated a dedicated instruction dataset called Flan-mini.; As a result of this fine-tuning process, Flacuna exhibited notable performance improvements in problem-solving across multiple benchmark datasets, both in few-shot and zero-shot settings.; During training, Flacuna is a 13B checkpoint of LLaMA and employed a maximum input sequence length of 1280. We utilized LoRA for parameter-efficient fine-tuning.",natural-language-processing,text-generation,https://huggingface.co/declare-lab/flacuna-13b-v1.0,Paper: https://arxiv.org/pdf/2307.02053.pdf,License: other,Datasets: declare-lab/InstructEvalImpact; declare-lab/flan-mini,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 31,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
orca_mini_v2_ger_7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"orca_mini_v2_ger_7b is a variant of Pankaj Mathur′s Orca Mini V2 7b model, finetuned on an additional dataset in German language. 
The original model has been trained on explain tuned datasets, created using instructions and input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; This model is optimized for German text, providing proficiency in understanding, generating, and interacting with German language content.
However the model is not yet fully optimized for German language, as it has been trained on a small, experimental dataset and has limited capabilities due to the small parameter count. ; I am working on improving the model′s capabilities and will update the model if there is sufficient interest.; A quantized GGML version for use with llama.cpp, kobold.cpp and other GUIs for CPU inference can be found here.; I did only evaluate the output on a small, handcrafted sample on test prompts in German, confirming that the model's ability to understand and generate German text is well above the base model. ",natural-language-processing,text-generation,https://huggingface.co/jphme/orca_mini_v2_ger_7b,Paper: https://arxiv.org/pdf/2304.12244.pdf,License: cc-by-nc-sa-4.0,,German; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 102,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chatglm2-6b-fp16.flm,,,,,,,,,"fastllm model for chatglm-6b-fp16; Github address: https://github.com/ztxz16/fastllm; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/huangyuyang/chatglm2-6b-fp16.flm,,License: apache-2.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Dalcefo,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"ko-fi.com/dalcefo_artworks; Unable to determine this model’s library. Check the
								docs 
.
							",multimodel,text-to-image,https://huggingface.co/LibreSD/Dalcefo,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 159.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
MiracleMixGlitter_v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Model info :; https://civitai.com/models/101423/miraclemix-glitter-an-anime-model-trained-and-specialized-on-creating-detailed-images-for-stunning-wallpaper; Original Author's DEMO images :; 




",multimodel,text-to-image,https://huggingface.co/digiplay/MiracleMixGlitter_v1,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 562,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
Andite,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"civitai.com/user/andite; Unable to determine this model’s library. Check the
								docs 
.
							",multimodel,text-to-image,https://huggingface.co/LibreSD/Andite,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 97.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-7b-QueAns,,,,,,,,,"Falcon-7b-QueAns is a chatbot-like model for Question and Answering. It was built by fine-tuning Falcon-7B on the SQuAD dataset. This repo only includes the QLoRA adapters from fine-tuning with ?'s peft package. ; ?? This is a finetuned version for specifically question and answering. If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at Falcon-7B-Instruct. ; ? Looking for an even more powerful model? Falcon-40B is Falcon-7B's big brother!; The model was fine-tuned in 4-bit precision using ? peft adapters, transformers, and bitsandbytes. Training relied on a method called ""Low Rank Adapters"" (LoRA), specifically the QLoRA variant. The run took approximately 4 hours and was executed on a workstation with a single T4 NVIDIA GPU with 15 GB of available memory. See attached [Colab Notebook] used to train the model. ; July 06, 2023",,,https://huggingface.co/avnishkr/falcon-7b-QueAns,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2106.09685.pdf; https://arxiv.org/pdf/2305.14314.pdf,,Datasets: squad; tiiuae/falcon-refinedweb,English,PEFT,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 76,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
CAMEL-33B-Combined-Data-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for CAMEL AI's CAMEL 33B Combined Data merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/CAMEL-33B-Combined-Data-SuperHOT-8K-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 18.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
FlexDreamHK,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"　FlexDreamHKはリ`クされたNovelAIモデルの入っていない、あるいはそのリスクを可能な限り低くしたモデルを目指して作成しました。
　モデル名はマ`ジに使用したモデルたちに敬意を表し、主要なモデル名をMみ合わせて命名しています。
　マ`ジ元となったモデルはStable DiffusionやWifu Diffusionへの追加学（ファインチュ`ニング）を行ったもののみで成されています。
　また、にじジャ`ニ`と普段使いしているモデルから生成した}から}柄LoRAを作成?マ`ジしており、いわゆる蒸留系と呼ばれるモデルでもあります。
　マ`ジの^程と使用したLoRAそのもの、またそれを作成したHのデ`タセットを_示する事で可能な限り透明性を担保しました。; creativeml-openrail-m; 　モデルの作成にHし、ＮＡＩリ`クフリ`マ`ジモデル研究会を大いに活用させてきました。
　意欲の持AやアイデアのWきがあった他、モデル作成に後押しをして下さった方やモデル情螭蚬灿肖筏皮ださった皆さんに感x申し上げます。",multimodel,text-to-image,https://huggingface.co/den2nova/FlexDreamHK,,License: creativeml-openrail-m,,Japanese,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Airoboros-7B-GPT4-1-4-SuperHOT-8K-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Jon Durbin's Airoboros 7B GPT4 1.4.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,https://huggingface.co/TheBloke/Airoboros-7B-GPT4-1-4-SuperHOT-8K-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 36.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Airoboros-7B-GPT4-1-4-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Jon Durbin's Airoboros 7B GPT4 1.4 merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Airoboros-7B-GPT4-1-4-SuperHOT-8K-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 361,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Guanaco-7B-SuperHOT-8K-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Tim Dettmers' Guanaco 7B.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,https://huggingface.co/TheBloke/Guanaco-7B-SuperHOT-8K-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 36.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Robin-7B-v2-SuperHOT-8K-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for OptimalScale's Robin 7B v2.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,https://huggingface.co/TheBloke/Robin-7B-v2-SuperHOT-8K-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 36.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Vicuna-7B-v1-3-SuperHOT-8K-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These are fp16 pytorch format model files for LmSys' Vicuna 7B v1.3 merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 7b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Vicuna-7B-v1-3-SuperHOT-8K-fp16,Paper: https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 64,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These are fp16 pytorch format model files for Eric Hartford's Wizard Vicuna 7B Uncensored merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 7b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-fp16,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 301,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's WizardLM-7B-V1.0-Uncensored.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,https://huggingface.co/TheBloke/WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 36.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Koala-13B-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Koala 13B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Koala-13B-SuperHOT-8K-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 56,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Vicuna-7B-CoT-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Kevin Pro's Vicuna 7B CoT merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Vicuna-7B-CoT-SuperHOT-8K-GPTQ,Paper: https://arxiv.org/pdf/1910.09700.pdf,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eric Hartford's Wizard Vicuna 7B Uncensored merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 702,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
jina-embedding-b-en-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"; 

; 
The text embedding set trained by Jina AI, Finetuner team.
; jina-embedding-b-en-v1 is a language model that has been trained using Jina AI's Linnaeus-Clean dataset.
This dataset consists of 380 million pairs of sentences, which include both query-document pairs.
These pairs were obtained from various domains and were carefully selected through a thorough cleaning process.
The Linnaeus-Full dataset, from which the Linnaeus-Clean dataset is derived, originally contained 1.6 billion sentence pairs.; The model has a range of use cases, including information retrieval, semantic textual similarity, text reranking, and more.",natural-language-processing,sentence-similarity,https://huggingface.co/jinaai/jina-embedding-b-en-v1,,License: apache-2.0,Datasets: jinaai/negation-dataset,English,PyTorch; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 187,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 442.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
30B-Epsilon,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Epsilon is an instruct based general purpose model assembled from hand picked models and LoRAs.
There is no censorship and it follows instructions in the Alpaca format. This means you can create
your own rules in the context memory of your inference system of choice [mainly KoboldAI or Text
Generation Webui and chat UIs like SillyTavern and so on].; This model is the result of an experimental use of LoRAs on language models and model merges.
[] = applied as LoRA to a composite model | () = combined as composite models
30B-Epsilon = [SuperCOT[SuperHOT-prototype13b-8192[(wizardlmuncensored+((hippogriff+manticore)+(StoryV2))]; Alpaca's instruct format can be used to do many things, including control of the terms of behavior
between a user and a response from an agent in chat. Below is an example of a command injected into
memory.; All datasets from all models and LoRAs used were documented and reviewed as model candidates for merging.
Model candidates were based on five core principles: creativity, logic, inference, instruction following,
and longevity of trained responses. SuperHOT-prototype30b-8192 was used in this mix, not the 8K version;
the prototype LoRA seems to have been removed [from HF] as of this writing. The GPT4Alpaca LoRA from
Chansung was removed from this amalgam following a thorough review of where censorship and railroading
the user came from in 33B-Lazarus. This is not a reflection of ChanSung's excellent work - it merely did
not fit the purpose of this model.; manticore-30b-chat-pyg-alpha [Epoch0.4] by openaccess-ai-collective",natural-language-processing,text-generation,https://huggingface.co/CalderaAI/30B-Epsilon,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 26,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 82.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
openchat_v2_w,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"The OpenChat v2 family is inspired by offline reinforcement learning, including conditional behavior cloning (OpenChat-v2) and weighted behavior cloning (OpenChat-v2-w).; We provide the full source code, including an inference server compatible with the ""ChatCompletions"" API, in the OpenChat GitHub repository.; OpenChat also includes a web UI for a better user experience. See the GitHub repository for instructions.; The conversation template involves concatenating tokens, and cannot be expressed in plain-text.; Besides base model vocabulary, an end-of-turn token <|end_of_turn|> is added.",natural-language-processing,text-generation,https://huggingface.co/openchat/openchat_v2_w,,License: other,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 29,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 703,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
Guanaco-33B-SuperHOT-8K-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Tim Dettmers' Guanaco 33B.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,https://huggingface.co/TheBloke/Guanaco-33B-SuperHOT-8K-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 171.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Guanaco-33B-SuperHOT-8K-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These are fp16 pytorch format model files for Tim Dettmers' Guanaco 33B merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 33b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Guanaco-33B-SuperHOT-8K-fp16,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 46,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-13B-V1-1-SuperHOT-8K-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for WizardLM's WizardLM 13B V1.1.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,https://huggingface.co/TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GGML,Paper: https://arxiv.org/pdf/2304.12244.pdf,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 48.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
airoboros-65b-gpt4-1.4.1-PI-8192-4bit-32g-actorder,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"fp16 is here: https://huggingface.co/ycros/airoboros-65b-gpt4-1.4.1-PI-8192-fp16; peft file is here: https://huggingface.co/ycros/airoboros-65b-gpt4-1.4.1-PI-8192-peft; ggml quants: https://huggingface.co/ycros/airoboros-65b-gpt4-1.4.1-PI-8192-GGML; This is based on bhenrym14's airoboros 33b PI 8192 but on 65b.; See bhenrym14's notes there, everything applies except I based this on llama-65B.",natural-language-processing,text-generation,https://huggingface.co/ycros/airoboros-65b-gpt4-1.4.1-PI-8192-4bit-32g-actorder,,,Datasets: jondurbin/airoboros-gpt4-1.4.1,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 39.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
jina-embedding-l-en-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"; 

; 
The text embedding set trained by Jina AI, Finetuner team.
; jina-embedding-l-en-v1 is a language model that has been trained using Jina AI's Linnaeus-Clean dataset.
This dataset consists of 380 million pairs of sentences, which include both query-document pairs.
These pairs were obtained from various domains and were carefully selected through a thorough cleaning process.
The Linnaeus-Full dataset, from which the Linnaeus-Clean dataset is derived, originally contained 1.6 billion sentence pairs.; The model has a range of use cases, including information retrieval, semantic textual similarity, text reranking, and more.",natural-language-processing,sentence-similarity,https://huggingface.co/jinaai/jina-embedding-l-en-v1,,License: apache-2.0,Datasets: jinaai/negation-dataset,English,PyTorch; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 126,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
gcolab-prunned-sdxl,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/Kefasu/gcolab-prunned-sdxl,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
open_llama_7b_v2_ggml,,,,,,,,,"For use with llama.cpp.; Coming soon...; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/SlyEcho/open_llama_7b_v2_ggml,,License: apache-2.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 74.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mpt-30b-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Base Model : mosaicml/mpt-30b; Tool : MosaicML's llm-foundry (https://github.com/mosaicml/llm-foundry); Dataset : Entire flan3m-GPT3.5 dataset.; Config yaml with Model Params : https://huggingface.co/manojpreveen/mpt-30b-v2/blob/main/mpt-30b_orca.yaml; Prompt Format :,natural-language-processing,text-generation,https://huggingface.co/manojpreveen/mpt-30b-v2,,License: apache-2.0,Datasets: ehartford/dolphin,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 90,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardCoder-Guanaco-15B-V1.0-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are StarCoder GGML format model files for LoupGarou's WizardCoder Guanaco 15B V1.0.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools that work with this GGML model.; These files are not compatible with llama.cpp or text-generation-webui.",,,https://huggingface.co/TheBloke/WizardCoder-Guanaco-15B-V1.0-GGML,,License: apache-2.0,Datasets: guanaco,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 70.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardCoder-Guanaco-15B-V1.0-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for LoupGarou's WizardCoder Guanaco 15B V1.0.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Please make sure you're using the latest version of text-generation-webui.,natural-language-processing,text-generation,https://huggingface.co/TheBloke/WizardCoder-Guanaco-15B-V1.0-GPTQ,,License: apache-2.0,Datasets: guanaco,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 418,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
airochronos-33B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"After the initial experiment with chronoboros-33B it was evident that the merge was to unpredictable to be useful, testing the individual models it became clear that the bias should be weighted towards Chronos.
This is the new release of the merge with 75% chronos 33B, and 25% airoboros-1.4 33B.; Model has been tested with the Alpaca prompting format combined with KoboldAI Lite's instruct and chat modes, as well as regular story writing.
It has also been tested on basic reasoning tasks, but has not seen much testing for factual information.",natural-language-processing,text-generation,https://huggingface.co/Henk717/airochronos-33B,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 213,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Bart-large-paper2slides-summarizer,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"; This repository contains the Bart-Large-paper2slides-summarizer Model, which has been fine-tuned on the Automatic Slide Generation from Scientific Papers dataset using unsupervised learning techniques using an algorithm from the paper entitled 'Unsupervised Machine Translation Using Monolingual Corpora Only'.
Its primary focus is to summarize scientific texts with precision and accuracy, the model is parallelly trained with the Bart-large-paper2slides-expander from the same contributor.; Bart (Bidirectional and Auto-Regressive Transformers) is a sequence-to-sequence (seq2seq) model developed by Facebook AI Research. It has shown exceptional performance in various natural language processing (NLP) tasks such as text summarization, text generation, and machine translation.; This particular model, Bart-Large, is the larger version of the Bart model. It consists of 12 encoder and decoder layers and has a total of 400 million parameters.; To use this model, you can leverage the Hugging Face Transformers library. Here's an example of how to use it in Python:",natural-language-processing,summarization,https://huggingface.co/com3dian/Bart-large-paper2slides-summarizer,Paper: https://arxiv.org/pdf/1711.00043.pdf,License: mit,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 489,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
MPT-30B-Dolphin-v2-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are MPT GGML format model files for Manoj Preveen's MPT 30B Dolphin v2.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools that work with this GGML model.; These files are not compatible with llama.cpp or text-generation-webui.",,,https://huggingface.co/TheBloke/MPT-30B-Dolphin-v2-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 111.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-7b-code-alpaca-lora,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; 

; 
LLM Generation models trained by Jina AI, Finetuner team.
; This repo contains the lora weights (8bit) for Falcon-7b
fit on the Code Alpaca dataset.; This version of the weights was trained with the following hyperparameters:",natural-language-processing,text-generation,https://huggingface.co/jinaai/falcon-7b-code-alpaca-lora,,License: cc-by-nc-4.0,Datasets: stanford_alpaca,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.46MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-7b-code-alpaca,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; 

; 
LLM Generation models trained by Jina AI, Finetuner team.
; This repo contains the full weights (16bit) for Falcon-7b
fit on the Code Alpaca dataset.; This version of the weights was trained with the following hyperparameters:",natural-language-processing,text-generation,https://huggingface.co/jinaai/falcon-7b-code-alpaca,,License: cc-by-nc-4.0,Datasets: stanford_alpaca,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 53,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-40b-code-alpaca-lora,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; 

; 
LLM Generation models trained by Jina AI, Finetuner team.
; This repo contains the lora weights (8bit) for Falcon-40b
fit on the Code Alpaca dataset.; This version of the weights was trained with the following hyperparameters:",natural-language-processing,text-generation,https://huggingface.co/jinaai/falcon-40b-code-alpaca-lora,,License: cc-by-nc-4.0,Datasets: stanford_alpaca,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 33.5MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-40b-code-alpaca,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; 

; 
LLM Generation models trained by Jina AI, Finetuner team.
; This repo contains the full weights (16bit) for Falcon-40b
fit on the Code Alpaca dataset.; This version of the weights was trained with the following hyperparameters:",natural-language-processing,text-generation,https://huggingface.co/jinaai/falcon-40b-code-alpaca,,License: cc-by-nc-4.0,Datasets: stanford_alpaca,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 83.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
moss-rlhf-reward-model-7B-zh,,,,,,,,,"moss-rlhf-reward-model-7B-zh
; moss-rlhf-reward-model-7B-en; moss-rlhf-sft-model-7B-en
; Due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle.
In this technical report, we intend to help researchers to train their models stably with human feedback.; Contributions are summarized as follows: ",,,https://huggingface.co/Ablustrund/moss-rlhf-reward-model-7B-zh,Paper: https://arxiv.org/pdf/2307.04964.pdf,License: agpl-3.0,,Chinese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 507.72KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Realisian_v5,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Model info :; https://civitai.com/models/47130?modelVersionId=115942; Sample images I made ; 

; Original Author's DEMO image :",multimodel,text-to-image,https://huggingface.co/digiplay/Realisian_v5,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2210,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
OpenOrca-Preview1-13B-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Open-Orca's OpenOrca-Preview1-13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/OpenOrca-Preview1-13B-GGML,Paper: https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2301.13688.pdf,License: other,Datasets: Open-Orca/OpenOrca,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
OpenOrca-Preview1-13B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Open-Orca's OpenOrca-Preview1-13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/OpenOrca-Preview1-13B-GPTQ,Paper: https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2301.13688.pdf,License: other,Datasets: Open-Orca/OpenOrca,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 791,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
polylm-multialpaca-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This model is finetuned on polyLM-13b using multialpaca (a self-instruction dataset); Open; The information below in this section are copied from the model's official model card:; Our contributions are fully methodological: adding the support of multilingualism to LLM during training and SFT phases. It is unavoidable that PolyLM might exhibit several common deficiencies of language models, e.g. hallucination and toxicity. PolyLM should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.; This version activates the instruction-following capability of PolyLM through self-instruction, but currently, the training instructions are relatively simple and the support for abilities such as multi-turn dialogue, context understanding, CoT, Plugin, etc. is not very friendly. We are making efforts to develop a new version.",natural-language-processing,text-generation,https://huggingface.co/DAMO-NLP-MT/polylm-multialpaca-13b,Paper: https://arxiv.org/pdf/2307.06018.pdf,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 361,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 31.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
polylm-1.7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,,natural-language-processing,text-generation,https://huggingface.co/DAMO-NLP-MT/polylm-1.7b,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 719,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
zodiac_eclipse_DAY1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,Model info :; https://civitai.com/models/108417/zodiac-eclipse-day1; Sample image I made thru Huggingface's API :; ; Original Author's DEMO images :,multimodel,text-to-image,https://huggingface.co/digiplay/zodiac_eclipse_DAY1,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 620,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
nart-100k-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,,natural-language-processing,text-generation,https://huggingface.co/jerryjalapeno/nart-100k-7b,,License: cc-by-nc-nd-4.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Dreamsphere,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Preview image by Digiplay:; ; A mix of Noosphere v3 by skumerz and my favorite models.; Original pages:
https://civitai.com/models/36538?modelVersionId=107675
https://civitai.com/models/5528/rainbowpatch",multimodel,text-to-image,https://huggingface.co/Yntec/Dreamsphere,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3074,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
Rainbowsphere,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"A mix of Noosphere v3 by skumerz and Rainbowpath by PatchMonk. You can use ""Rainbowpath"" in the prompt to enhance the style.; Preview image by Digiplay:; ; Original pages:
https://civitai.com/models/36538?modelVersionId=107675
https://civitai.com/models/5528/rainbowpatch",multimodel,text-to-image,https://huggingface.co/Yntec/Rainbowsphere,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2842,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
Wizard-Vicuna-30B-Uncensored-lxctx-PI-16384-LoRA-4bit-32g,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Wizard-Vicuna-30B-Uncensored merged with bhenrym14's airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-LoRA, quantized at 4 bit.; More info about the LoRA Here. This is an alternative to SuperHOT 8k LoRA trained with LoRA_rank 64 and context extended to 16K, with airoboros 1.4.1 dataset.; It was created with GPTQ-for-LLaMA with group size 32 and act order true as parameters, to get the maximum perplexity vs FP16 model.; I HIGHLY suggest to use exllama, to evade some VRAM issues.; Use compress_pos_emb = 8 for any context up to 16384 context.",natural-language-processing,text-generation,https://huggingface.co/Panchovix/Wizard-Vicuna-30B-Uncensored-lxctx-PI-16384-LoRA-4bit-32g,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 19.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
polla_mix_2.3D,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,Model info :; https://civitai.com/models/110130?modelVersionId=118730; Sample image I made thru Huggingface's API :; ; Original Author's DEMO images :,multimodel,text-to-image,https://huggingface.co/digiplay/polla_mix_2.3D,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 485,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
polla_mix_2.5D,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,Model info :; https://civitai.com/models/110130?modelVersionId=118741; Sample image I made thru Huggingface's API :; ; Original Author's DEMO images :,multimodel,text-to-image,https://huggingface.co/digiplay/polla_mix_2.5D,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 727,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
HCTM,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/hctmgroup/HCTM,,License: mit,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.54KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
MeinaMix_V11,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"MeinaMix Objective is to be able to do good art with little prompting.; For examples and prompts, please checkout: https://civitai.com/models/7240/meinamix
I have a discord server where you can post images that you generated, discuss prompt and/or ask for help.; https://discord.gg/XC9nGZNDUd If you like one of my models and want to support their updates; I've made a ko-fi page; https://ko-fi.com/meina where you can pay me a coffee <3; And a Patreon page; https://www.patreon.com/MeinaMix where you can support me and get acess to beta of my models!",multimodel,text-to-image,https://huggingface.co/Meina/MeinaMix_V11,,License: creativeml-openrail-m,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1101,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.64KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
GOLDFish,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,A mix between the models OLDFIsh by timevisitor and RMHF_2.5D_v2 by TkskKurumi.; Preview image by Digiplay:; ; Original pages:; https://civitai.com/models/14978?modelVersionId=40101,multimodel,text-to-image,https://huggingface.co/Yntec/GOLDFish,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 816,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
MuseRWKV,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"this has too little layers, but it will still make music",natural-language-processing,text-generation,https://huggingface.co/breadlicker45/MuseRWKV,,,,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
span-marker-xlm-roberta-large-verbs,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"This is a SpanMarker model that can be used for identifying verbs in text.
In particular, this SpanMarker model uses xlm-roberta-large as the underlying encoder.
See span_marker_verbs_train.ipynb for the training script used to create this model.; Note that this model is an experiment about the feasibility of SpanMarker as a POS tagger. I would generally recommend using spaCy or NLTK instead, as these are more computationally efficient approaches.; To use this model for inference, first install the span_marker library:; You can then run inference with this model like so:; See the SpanMarker repository for documentation and additional information on this library.",natural-language-processing,token-classification,https://huggingface.co/tomaarsen/span-marker-xlm-roberta-large-verbs,,License: apache-2.0,,,PyTorch; Safetensors; SpanMarker,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
PhotoSomnia_vFinal,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,Model info :; https://civitai.com/models/18637/photosomnia; Original Author's DEMO image :; ; Sample image thru huggingface's API :,multimodel,text-to-image,https://huggingface.co/digiplay/PhotoSomnia_vFinal,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 444,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
LLaMA-7B-2bit-alpaca,,,,,,,,,"This is GreenBitAI's instruction-tuned LoRA parameters for our 2-bit 7B LLaMA model trained on the Alpaca-clean 50k dataset.; Please refer to our Github page for the code to run the model and more information.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/GreenBitAI/LLaMA-7B-2bit-alpaca,,License: apache-2.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 71.7MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Others,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/Gyunyu-pudding/Others,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.56KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-33b-lxctx-PI-16384-LoRA,,,,,,,,,"Mostly untested!; This is base Llama-33b with minimal additional training to extend the useful context window.; This is a QLoRA fine-tune; Pretraining took 10 hours on 1x RTX 6000 Ada.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/bhenrym14/llama-33b-lxctx-PI-16384-LoRA,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 975.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Gugugo-koen-1.3B-V0.95,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,https://github.com/jwj7140/Gugugo; Prompt Template:,natural-language-processing,translation,https://huggingface.co/squarelike/Gugugo-koen-1.3B-V0.95,,License: apache-2.0,,English; Korean,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
MeinaMix_v11,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Model info :; https://civitai.com/models/7240?modelVersionId=119057; Sample images generated by huggingface's API :; 
; prompt :",multimodel,text-to-image,https://huggingface.co/digiplay/MeinaMix_v11,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 521,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
skin-generator-minecraft,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This Stable Diffusion model was fine-tuned to generate a pre-version 1.8 Minecraft character skins, based on a text prompt.; The model was fine-tuned on the dataset for 13,000 steps using the 'train_text_to_image.py' script provided with the diffusers library.  A checkpoint has been included in the 'checkpoint' directory.; Some postprocessing is required to import and use the generated skins in Minecraft.; This model is a fork from monadicial/minecraft-skin-generator. This fork will contain the production model for a frontend that processing the results so the output can be used; Here are some example text prompts and the images they generate:",multimodel,text-to-image,https://huggingface.co/agentapp/skin-generator-minecraft,,License: openrail,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 325,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.33KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Colorful_v2.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,https://civitai.com/models/7279?modelVersionId=13196,multimodel,text-to-image,https://huggingface.co/digiplay/Colorful_v2.0,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 34,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-7b-prosocialdialog-lora,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/surika/llama-7b-prosocialdialog-lora,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 16.8MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
anime-diffusion-v0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This is currently a base model
; This model does not use any finetuning techniques such as face restoration or in-painting as of yet; Test the concept via A1111 Colab fast-Colab-A1111; Sample pictures of this concept:
; See more results from this model under the sample_images folder",multimodel,text-to-image,https://huggingface.co/Ryzan/anime-diffusion-v0,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 148,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
sorceroboros-33b-s2a4-gptq,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Trained on a flavorful melange of the WizardLM, Airoboros, and Wizard Vicuna datasets.
This model was trained using both linear and NTK-aware RoPE scaling in tandem. When loading, ensure that compress_pos_emb (or scale) is set to 2, and alpha_value is set to 4. Both values must be set.; Expect context length of up to 8192 to work for sure. It will probably maintain coherence into the ~12k range, but I have not tested that.; Prompt format is vicuna 1.1:",natural-language-processing,text-generation,https://huggingface.co/chargoddard/sorceroboros-33b-s2a4-gptq,,,Datasets: ehartford/WizardLM_evol_instruct_V2_196k_unfiltered_merged_split; jondurbin/airoboros-gpt4-1.4.1; openai/summarize_from_feedback; ehartford/wizard_vicuna_70k_unfiltered,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 18.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
UltraLM-65b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/openbmb/UltraLM-65b,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 131.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bark-small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9511aaa882b322dc3d8_Text-to-Speech.svg,,"Bark is a transformer-based text-to-audio model created by Suno. 
Bark can generate highly realistic, multilingual speech as well as other audio - including music, 
background noise and simple sound effects. The model can also produce nonverbal 
communications like laughing, sighing and crying. To support the research community, 
we are providing access to pretrained model checkpoints ready for inference.; The original github repo and model card can be found here.; This model is meant for research purposes only. 
The model output is not censored and the authors do not endorse the opinions in the generated content. 
Use at your own risk.; Two checkpoints are released:; Try out Bark yourself!",audio,text-to-speech,https://huggingface.co/suno/bark-small,,License: cc-by-nc-4.0,,13 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 689,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
open_llama_3b_v2_ggml,,,,,,,,,"Support is now merged to master branch.; There are now more quantization types in llama.cpp, some lower than 4 bits.
Currently these are not well supported because of technical reasons.
If you want to use them, you have to build llama.cpp (from build 829 (ff5d58f)) with the LLAMA_QKK_64 Make or CMake variable enabled (see PR #2001).
Then you can quantize the F16 or maybe Q8_0 version to what you want.; Coming soon ...; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/SlyEcho/open_llama_3b_v2_ggml,,License: apache-2.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 19.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ggml-alicia-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"(OpenOrca Preview + OrcaMiniV2 @ 0.4) + (Chronos @ 0.08) + (Hermes @ 0.08) + (Wizard Vicuna @ 0.08) + (Samantha @ 0.20); Recognized templates:; and; Unable to determine this model’s library. Check the
								docs 
.
							",natural-language-processing,text-generation,https://huggingface.co/eachadea/ggml-alicia-13b,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ggml-hws-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"(Hermes + Wizard 1.1 @ 0.5) + Selfee @ 0.2; Recognizes templates:; and; Unable to determine this model’s library. Check the
								docs 
.
							",natural-language-processing,text-generation,https://huggingface.co/eachadea/ggml-hws-13b,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
alpagasus-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is an unofficial implementation of AlpaGasus-13B, which is a chat assistant trained by fine-tuning LLaMA on a Claud-filtered Alpaca dataset with around 5K triplets.; Please see the original LLaMA license before using this model.; AlpaGasus-13B is fine-tuned from LLaMA-13B with supervised instruction fine-tuning on the filtered Alpaca dataset.; Inference API has been turned off for this model.",natural-language-processing,text-generation,https://huggingface.co/gpt4life/alpagasus-13b,Paper: https://arxiv.org/pdf/2307.08701.pdf,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ct2fast-Llama-2-7b-chat-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Speedup inference while reducing memory by 2x-4x using int8 inference in C++ on CPU or GPU.; quantized version of meta-llama/Llama-2-7b-chat-hf; Checkpoint compatible to ctranslate2>=3.17.1
and hf-hub-ctranslate2>=2.12.0; Converted on 2023-07-21 using; This is just a quantized version. Licence conditions are intended to be idential to original huggingface repo.",natural-language-processing,text-generation,https://huggingface.co/michaelfeil/ct2fast-Llama-2-7b-chat-hf,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ct2fast-Llama-2-70b-chat-hf,,,,,,,,,"still missing support from ctranslate2.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/michaelfeil/ct2fast-Llama-2-70b-chat-hf,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.56KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ct2fast-Llama-2-13b-chat-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Speedup inference while reducing memory by 2x-4x using int8 inference in C++ on CPU or GPU.; quantized version of meta-llama/Llama-2-13b-chat-hf; Checkpoint compatible to ctranslate2>=3.17.1
and hf-hub-ctranslate2>=2.12.0; Converted on 2023-07-21 using; This is just a quantized version. Licence conditions are intended to be idential to original huggingface repo.",natural-language-processing,text-generation,https://huggingface.co/michaelfeil/ct2fast-Llama-2-13b-chat-hf,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Llama 2
Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model. Links to other models can be found in the index at the bottom.; Model Details
Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations.",natural-language-processing,text-generation,https://huggingface.co/dhruvabansal/llama-2-13b,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Chinese-LlaMA2-7B-chat,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/michaelwzhu/Chinese-LlaMA2-7B-chat,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.52KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-ko-7b-chat-vicuna-hf-4bit,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/quantumaikr/llama-2-ko-7b-chat-vicuna-hf-4bit,,,,,TensorBoard,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 69.5MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Legal_Penguin,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/DangFutures/Legal_Penguin,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 50,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
meta-llama-Llama-2-7b-chat-hf-w4-g128-awq,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format.; This model is a 4-bit 128 group size AWQ quantized model. For more information about AWQ quantization, please click here.; July 19, 2023; Please refer to the original LLaMA 2 model license (link).; Please refer to the AWQ quantization license (link).",natural-language-processing,text-generation,https://huggingface.co/abhinavkulkarni/meta-llama-Llama-2-7b-chat-hf-w4-g128-awq,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-70b-chat-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations.",natural-language-processing,text-generation,https://huggingface.co/NousResearch/Llama-2-70b-chat-hf,,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 882,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 276.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
GodziLLa-30B-instruct,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"GodziLLa-30B-instruct is GodziLLa-30B finetuned on a mixture of instructions. For a more general use case, please use GodziLLa-30B instead. This finetuned model is not meant for any other use outside of research on competing LoRA adapter behavior and instruction tuning. More specifically, since this is inherently a LlaMA model, commercial use is prohibited. This model's primary purpose is to stress test the limitations of composite LLMs and observe its performance with respect to other LLMs available on the Open LLM Leaderboard.; ; The following datasets were used to finetune GodziLLa-30B further.; COMING SOON; According to the leaderboard description, here are the benchmarks used for the evaluation:",natural-language-processing,text-generation,https://huggingface.co/MayaPH/GodziLLa-30B-instruct,Paper: https://arxiv.org/pdf/2009.03300.pdf; https://arxiv.org/pdf/1803.05457.pdf; https://arxiv.org/pdf/1905.07830.pdf; https://arxiv.org/pdf/2109.07958.pdf,License: cc-by-nc-4.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chinese-alpaca-pro-lora-7b,,,,,,,,,"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/ziqingyang/chinese-alpaca-pro-lora-7b,,License: apache-2.0,,Chinese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LLama2-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/AlexWortega/LLama2-7b,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 110,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-13B-german-assistant-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This model is an finetuned version for german instructions and conversations in style of Open Assistant tokens. ""<|prompter|>"" ""<|endoftext|>"" ""<|assistant|>""; The dataset used is deduplicated and cleaned, with no codes inside. The focus is on instruction following and conversational tasks.; The model archictecture is based on Llama version 2 with 13B parameters, trained on 100% renewable energy powered hardware.; This work is contributed by private research of flozi00",natural-language-processing,text-generation,https://huggingface.co/flozi00/Llama-2-13B-german-assistant-v1,,,Datasets: flozi00/conversations,English; German,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-7b-ggml,,,,,,,,,This model card aims to be a base template for new models. It has been generated using this raw template.; [More Information Needed]; [More Information Needed]; [More Information Needed]; [More Information Needed],,,https://huggingface.co/lazyiitian/llama-2-7b-ggml,Paper: https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Luna-AI-Llama2-Uncensored-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Quantization from:
Tap-M/Luna-AI-Llama2-Uncensored; Converted to the GGML format with:
llama.cpp master-294f424 (JUL 19, 2023); Tested with:
koboldcpp 1.35; Example usage:; Prompt format (refer to the original model for additional details):",natural-language-processing,text-generation,https://huggingface.co/Araki/Luna-AI-Llama2-Uncensored-GGML,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
PersonaStyleCheckpoint,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Model info :; https://civitai.com/models/31771?modelVersionId=38190; Sample image I made thru Huggingface's API :
; Original Author's DEMO images :; 

",multimodel,text-to-image,https://huggingface.co/digiplay/PersonaStyleCheckpoint,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 161,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
Llama-2-13B-GPTQ-Orca,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"These files are GPTQ model files for Meta's Llama 2 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.; Each separate quant is in a different branch.  See below for instructions on fetching from different branches.; Please make sure you're using the latest version of text-generation-webui.",natural-language-processing,text-generation,https://huggingface.co/tridungduong16/Llama-2-13B-GPTQ-Orca,,License: other,,English,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 897.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
prometheus.safetensors,,,,,,,,,"https://www.youtube.com/watch?v=JB-BmFUdT7o; First, please take the time to watch the video, you wont regret it. ; I am proud to introduce to you an innovative workflow solution for all of your prompting needs! Prometheus is a custom model along with 30 Hypernetworks I like to call Latent Later Cameras. These are virtual cameras that have been embedded in to the latent space which allow you to choose the camera angle and camera shot for your subject.  These cameras put you in to the directors seat so that you always get the shot you envisioned in your mind. Besides being very flexible, the latent cameras also tend to be very cohesive and produce very good results through natural language prompting.  That means there is no need for a word salad in your positive or negative prompts.  In fact you can use the Hypernetworks without any negative prompts at all and still get very good results. As long as you follow very basic rules outlined in the video guide, on average you will get very good results. Thank you!; Please note the Hypernetworks work with all models...that are based on the 1.5 architecture... sorry if I misled in the video. Sorry no 2.0 or 2.1 support.  ; Don't forget to enable your dynamic prompts by checking the enable box, otherwise you can load the Hypernetworks manually, just know that the strength of the Hypernetworks should be at 0.55 by default, otherwise you will not get good results. ",,,https://huggingface.co/doctorderp/prometheus.safetensors,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 11.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-7b-chat-hf-dolly-ja-nf4,,,,,,,,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; The following bitsandbytes quantization config was used during training:; The following bitsandbytes quantization config was used during training:; PEFT 0.5.0.dev0",,,https://huggingface.co/kunipm9/Llama-2-7b-chat-hf-dolly-ja-nf4,,,,Japanese,PEFT,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ruGPT-3.5-13B-8bit,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Квантизированная версия модели ruGPT-3.5,natural-language-processing,text-generation,https://huggingface.co/pe4enov/ruGPT-3.5-13B-8bit,,,,Russian,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 24.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ruGPT-3.5-13B-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,This is a generative model converted to fp16 format based on ai-forever/ruGPT-3.5-13B; Inference API has been turned off for this model.,natural-language-processing,text-generation,https://huggingface.co/Gaivoronsky/ruGPT-3.5-13B-fp16,,License: mit,,Russian; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 55,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-7b-guanaco-dolly-8bit-sharded,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Model that is fine-tuned in 4-bit precision using QLoRA on timdettmers/openassistant-guanaco and databricks/databricks-dolly-15k. Sharded as well to be used on a free Google Colab instance. ; It can be easily imported using the AutoModelForCausalLM class from transformers:,natural-language-processing,text-generation,https://huggingface.co/guardrail/llama-2-7b-guanaco-dolly-8bit-sharded,,License: apache-2.0,Datasets: databricks/databricks-dolly-15k; timdettmers/openassistant-guanaco,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-70b-guanaco-qlora,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This is a Llama-2 version of Guanaco. It was finetuned from the base Llama-70b model using the official training scripts found in the QLoRA repo. I wanted it to be as faithful as possible and therefore changed nothing in the training script beyond the model it was pointing to. The model prompt is therefore also the same as the original Guanaco model.; This repo contains the QLoRA adapter.; A 7b version of the adapter can be found here.
A 13b version of the adapter can be found here.; Legal Disclaimer: This model is bound by the usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.; Unable to determine this model’s library. Check the
								docs 
.
							",natural-language-processing,text-classification,https://huggingface.co/Mikael110/llama-2-70b-guanaco-qlora,,,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Anything_ink,,,,,,,,,"This is a fine-tuning model based on stable diiffusion.The model is fine-tuned based on SD1.5.The model was fine-tuned with HCP-diffusion
The model prompt is extremely accurate.; ――――――――――――; Many of today's SD models have a variety of problems.I want to make use of my limited ability to improve the current situation.So I used a lot of AI-generated images to refine this model; ; ――――――――――――",,,https://huggingface.co/X779/Anything_ink,,License: openrail,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
gogpt2-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; 


; ICT中英文底座增强大模型：70亿参数、130亿参数; GoGPT-Github; ?怎么从零到一训练一个LLM分词器",natural-language-processing,text-generation,https://huggingface.co/golaxy/gogpt2-7b,,License: apache-2.0,Datasets: BelleGroup/train_0.5M_CN; BelleGroup/train_1M_CN; c-s-ale/alpaca-gpt4-data-zh; BAAI/COIG,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ruGPT-3.5-13B-8bit,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,This is a generative model converted to fp16 format based on ai-forever/ruGPT-3.5-13B; Inference API has been turned off for this model.,natural-language-processing,text-generation,https://huggingface.co/Gaivoronsky/ruGPT-3.5-13B-8bit,,License: mit,,Russian; English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama2_7b_chat_uncensored-GGML,,,,,,,,,"Buy me a coffee if you like this project ;)
; GGML Format model files for This project.; Fine-tuned Llama-2 7B with an uncensored/unfiltered Wizard-Vicuna conversation dataset ehartford/wizard_vicuna_70k_unfiltered.
Used QLoRA for fine-tuning. Trained for one epoch on a 24GB GPU (NVIDIA A10G) instance, took ~19 hours to train.; The model was trained with the following prompt style:; Code used to train the model is available here.",,,https://huggingface.co/s3nh/llama2_7b_chat_uncensored-GGML,,License: other,Datasets: ehartford/wizard_vicuna_70k_unfiltered,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 25.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama2-qlora-finetunined-Arabic,,,,,,,,,"The following bitsandbytes quantization config was used during training:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/HeshamHaroon/llama2-qlora-finetunined-Arabic,,,,,PEFT,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 134.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
13B-Ouroboros-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for CalderaAI's 13B Ouroboros.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/13B-Ouroboros-GGML,,License: other,Datasets: Open-Orca/OpenOrca; anon8231489123/ShareGPT_Vicuna_unfiltered; jondurbin/airoboros-uncensored,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
honest_llama2_chat_7B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Ever wondering a less hallucinating LLaMA-2? Using the inference-time intervention (ITI) discussed in my recent preprint: https://arxiv.org/pdf/2306.03341.pdf, I baked the intervention learned from TruthfulQA into a LLaMA-2 7B model.
I don’t have big enough GPU to bake ITI into larger LLaMA-2 but the code to do so are all released in https://github.com/likenneth/honest_llama. Let me know if you are interested do that :)
You can load and play around starting from below:",natural-language-processing,text-generation,https://huggingface.co/likenneth/honest_llama2_chat_7B,Paper: https://arxiv.org/pdf/2306.03341.pdf,License: mit,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
theallysMixIV-verisimilar,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,Original page:; https://civitai.com/models/40369/theallys-mix-iv-verisimilar,multimodel,text-to-image,https://huggingface.co/Yntec/theallysMixIV-verisimilar,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 730,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
Upstage-Llama1-65B-Instruct-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Upstage's Llama 65B Instruct.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GPTQ,,License: other,Datasets: sciq; metaeval/ScienceQA_text_only; GAIR/lima; Open-Orca/OpenOrca; openbookqa,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 35.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-13b-chat-german,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Edit: You can find a Demo (German) here; Llama-2-13b-chat-german is a variant of Meta′s Llama 2 13b Chat model, finetuned on an additional dataset in German language.; This model is optimized for German text, providing proficiency in understanding, generating, and interacting with German language content. However the model is not yet fully optimized for German language, as it has been trained on a small, experimental dataset and has limited capabilities due to the small parameter count.
Some of the fineunting data is also targeted towards factual retrieval (only answer questions from information in the context and refuse to hallucinate) and the model should perform better for these tasks than original Llama 2 Chat.; I am working on improving the model′s capabilities and will update the model if there is sufficient interest.; A quantized GGML version for use with llama.cpp, kobold.cpp and other GUIs for CPU inference can be found here.",natural-language-processing,text-generation,https://huggingface.co/jphme/Llama-2-13b-chat-german,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,German; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
hermes-limarp-13b-merged,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/Oniichat/hermes-limarp-13b-merged,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Chinese-Llama-2-7b-ggml-model-q4_0,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/rffx0/Chinese-Llama-2-7b-ggml-model-q4_0,,License: apache-2.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-22B-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is Llama2-22b in a couple of GGML formats. I have no idea what I'm doing so if something doesn't work as it should or not at all that's likely on me, not the models themselves.
While I haven't had any issues so far do note that the original repo states ""Not intended for use as-is - this model is meant to serve as a base for further tuning"".; Approximate VRAM requirements at 4K context:",natural-language-processing,text-generation,https://huggingface.co/IHaveNoClueAndIMustPost/Llama-2-22B-GGML,,,Datasets: togethercomputer/RedPajama-Data-1T-Sample,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 49.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama2-Chinese-7b-Chat-LoRA,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,由于Llama2本身的中文对齐较弱，我们采用中文指令集，对meta-llama/Llama-2-7b-chat-hf进行LoRA微调，使其具备较强的中文对话能力。; ? 该版本仅包含LoRA中文微调参数，需要与基础的meta-llama/Llama-2-7b-chat-hf模型结合使用; Github：Llama2-Chinese; 在线体验链接：llama.family; 欢迎来到Llama2中文社区！,natural-language-processing,question-answering,https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat-LoRA,,License: apache-2.0,,Chinese; English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 40.5MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama2-Chinese-7b-Chat,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,由于Llama2本身的中文对齐较弱，我们采用中文指令集，对meta-llama/Llama-2-7b-chat-hf进行LoRA微调，使其具备较强的中文对话能力。; ? 该版本为LoRA中文微调参数FlagAlpha/Llama2-Chinese-7b-Chat-LoRA和meta-llama/Llama-2-7b-chat-hf参数结合后的版本，可直接使用; Github：Llama2-Chinese; 在线体验链接：llama.family; 欢迎来到Llama2中文社区！,natural-language-processing,question-answering,https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat,,License: apache-2.0,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
openllama_3b_EvolInstruct_lora_merged-4bit-32g,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Prompt: ""Below is an instruction that describes a task. Write a response that appropriately completes the request. \n\n### Instruction:\n INSTRUCTION. \n### Response:\n""",natural-language-processing,text-generation,https://huggingface.co/KnutJaegersberg/openllama_3b_EvolInstruct_lora_merged-4bit-32g,,License: cc-by-4.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
TWingshadow_v1.2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Models info :; https://civitai.com/models/105935; Original Author's DEMO images :; 

",multimodel,text-to-image,https://huggingface.co/digiplay/TWingshadow_v1.2,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base-multilingual-uncased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Pretrained model on the top 102 languages with the largest Wikipedia using a masked language modeling (MLM) objective.
It was introduced in this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion. This means
it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the languages in the training set that can then be used to
extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a
standard classifier using the features produced by the BERT model as inputs.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the model hub to look for
fine-tuned versions on a task that interests you.",natural-language-processing,fill-mask,https://huggingface.co/bert-base-multilingual-uncased,Paper: https://arxiv.org/pdf/1810.04805.pdf,License: apache-2.0,Datasets: wikipedia,102 languages,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 43,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 326832,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 34
bert-large-cased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is cased: it makes a difference
between english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; This model has the following configuration:",natural-language-processing,fill-mask,https://huggingface.co/bert-large-cased,Paper: https://arxiv.org/pdf/1810.04805.pdf,License: apache-2.0,Datasets: bookcorpus; wikipedia,English,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 99088,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 35
bert-large-uncased-whole-word-masking-finetuned-squad,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; Differently to other BERT models, this model was trained with a new technique: Whole Word Masking. In this case, all of the tokens corresponding to a word are masked at once. The overall masking rate remains the same.; The training is identical -- each masked WordPiece token is predicted independently. ; After pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts. See below for more information regarding this fine-tuning.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.",natural-language-processing,question-answering,https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad,Paper: https://arxiv.org/pdf/1810.04805.pdf,License: apache-2.0,Datasets: bookcorpus; wikipedia,English,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 92,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 163700,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 48
distilbert-base-multilingual-cased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"This model is a distilled version of the BERT base multilingual model. The code for the distillation process can be found here. This model is cased: it does make a difference between english and English.; The model is trained on the concatenation of Wikipedia in 104 different languages listed here.
The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base).
On average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base.; We encourage potential users of this model to check out the BERT base multilingual model card to learn more about usage, limitations and potential biases.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.; Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.",natural-language-processing,fill-mask,https://huggingface.co/distilbert-base-multilingual-cased,Paper: https://arxiv.org/pdf/1910.01108.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,Datasets: wikipedia,104 languages,PyTorch; TensorFlow; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 64,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4652765,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
distilroberta-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"This model is a distilled version of the RoBERTa-base model. It follows the same training procedure as DistilBERT.
The code for the distillation process can be found here.
This model is case-sensitive: it makes a difference between english and English.; The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base).
On average DistilRoBERTa is twice as fast as Roberta-base.; We encourage users of this model card to check out the RoBERTa-base model card to learn more about usage, limitations and potential biases.; You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.; Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.",natural-language-processing,fill-mask,https://huggingface.co/distilroberta-base,Paper: https://arxiv.org/pdf/1910.01108.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,Datasets: openwebtext,English,PyTorch; TensorFlow; JAX; Rust; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 76,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7409128,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 29
gpt2-medium,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Model Description: GPT-2 Medium is the 355M parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective. ; Use the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:; Here is how to use this model to get the features of a given text in PyTorch:; and in TensorFlow:; In their model card about GPT-2, OpenAI wrote: ",natural-language-processing,text-generation,https://huggingface.co/gpt2-medium,Paper: https://arxiv.org/pdf/1910.09700.pdf,License: mit,,English,PyTorch; TensorFlow; JAX; Rust; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 64,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 223491,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 385
roberta-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is case-sensitive: it
makes a difference between english and English.; Disclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by
the Hugging Face team.; RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means
it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. ; More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model
randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict
the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one
after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to
learn a bidirectional representation of the sentence.; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.",natural-language-processing,fill-mask,https://huggingface.co/roberta-large,Paper: https://arxiv.org/pdf/1907.11692.pdf; https://arxiv.org/pdf/1806.02847.pdf,License: mit,Datasets: bookcorpus; wikipedia,English,PyTorch; TensorFlow; JAX; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 116,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2506730,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 62
t5-11b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-11B is the checkpoint with 11 billion parameters. ; The developers write in a blog post that the model: ",natural-language-processing,translation,https://huggingface.co/t5-11b,Paper: https://arxiv.org/pdf/1805.12471.pdf; https://arxiv.org/pdf/1708.00055.pdf; https://arxiv.org/pdf/1704.05426.pdf; https://arxiv.org/pdf/1606.05250.pdf; https://arxiv.org/pdf/1808.09121.pdf; https://arxiv.org/pdf/1810.12885.pdf; https://arxiv.org/pdf/1905.10044.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,Datasets: c4,5 languages,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 37,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 31672,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 90.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 101
t5-3b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-3B is the checkpoint with 3 billion parameters. ; The developers write in a blog post that the model: ",natural-language-processing,translation,https://huggingface.co/t5-3b,Paper: https://arxiv.org/pdf/1805.12471.pdf; https://arxiv.org/pdf/1708.00055.pdf; https://arxiv.org/pdf/1704.05426.pdf; https://arxiv.org/pdf/1606.05250.pdf; https://arxiv.org/pdf/1808.09121.pdf; https://arxiv.org/pdf/1810.12885.pdf; https://arxiv.org/pdf/1905.10044.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,Datasets: c4,5 languages,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 116336,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 23.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 102
xlm-roberta-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al. and first released in this repository. ; Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.; XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. ; RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.; More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.",natural-language-processing,fill-mask,https://huggingface.co/xlm-roberta-base,Paper: https://arxiv.org/pdf/1911.02116.pdf,License: mit,,94 languages,PyTorch; TensorFlow; JAX; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 339,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 13823623,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 43
MSTSb_stsb-xlm-r-multilingual,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",natural-language-processing,sentence-similarity,https://huggingface.co/AIDA-UPM/MSTSb_stsb-xlm-r-multilingual,,,,,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 44,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ICD-10-Code-Prediction,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,The Publicly Available Clinical BERT Embeddings paper contains four unique clinicalBERT models: initialized with BERT-Base (cased_L-12_H-768_A-12) or BioBERT (BioBERT-Base v1.0 + PubMed 200K + PMC 270K) & trained on either all MIMIC notes or only discharge summaries.  ; Load the model via the transformers library:; Run the model with clinical diagonosis text:; Return the Top-5 predicted ICD-10 codes:,natural-language-processing,text-classification,https://huggingface.co/AkshatSurolia/ICD-10-Code-Prediction,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1299,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 483.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
gpt2-spanish-classics,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/Aleksandar1932/gpt2-spanish-classics,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 510.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
qanlu,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,"Question Answering NLU (QANLU) is an approach that maps the NLU task into question answering, 
leveraging pre-trained question-answering models to perform well on few-shot settings. Instead of 
training an intent classifier or a slot tagger, for example, we can ask the model intent- and 
slot-related questions in natural language: ; Note the ""Yes. No. "" prepended in the context. Those are to allow the model to answer intent-related questions (e.g. ""Is the user looking for a restaurant?"").; Thus, by asking questions for each intent and slot in natural language, we can effectively construct an NLU hypothesis. For more details, please read the paper: Language model is all you need: Natural language understanding as question answering.; Instructions for how to train and evaluate a QANLU model, as well as the necessary code for ATIS are in the Amazon Science repository.; This model has been fine-tuned on ATIS (English) and is intended to demonstrate the power of this approach. For other domains or tasks, it should be further fine-tuned 
on relevant data.",natural-language-processing,question-answering,https://huggingface.co/AmazonScience/qanlu,,License: cc-by-4.0,Datasets: atis,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 524,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 497.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
indian-foods,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,Autogenerated by HuggingPics???; Create your own image classifier for anything by running the demo on Google Colab.; Report any issues with the demo at the github repo.; ; ,computer-vision,image-classification,https://huggingface.co/Amrrs/indian-foods,,,,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 32,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 343.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
rebel-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"



; This is the model card for the Findings of EMNLP 2021 paper REBEL: Relation Extraction By End-to-end Language generation. We present a new linearization approach and a reframing of Relation Extraction as a seq2seq task. The paper can be found here. If you use the code, please reference this work in your paper:; The original repository for the paper can be found here; Be aware that the inference widget at the right does not output special tokens, which are necessary to distinguish the subject, object and relation types. For a demo of REBEL and its pre-training dataset check the Spaces demo.",natural-language-processing,text2text-generation,https://huggingface.co/Babelscape/rebel-large,,License: cc-by-nc-sa-4.0,Datasets: Babelscape/rebel-dataset,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 76,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 33784,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
query-gen-msmarco-t5-base-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"This model is the t5-base model from docTTTTTquery.; The T5-base model was trained on the MS MARCO Passage Dataset, which consists of about 500k real search queries from Bing together with the relevant passage.; The model can be used for query generation to learn semantic search models without requiring annotated training data: Synthetic Query Generation.",natural-language-processing,text2text-generation,https://huggingface.co/BeIR/query-gen-msmarco-t5-base-v1,,,,,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1104,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base-arabic-camelbert-da-sentiment,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"CAMeLBERT-DA SA Model is a Sentiment Analysis (SA) model that was built by fine-tuning the CAMeLBERT Dialectal Arabic (DA) model.
For the fine-tuning, we used the ASTD, ArSAS, and SemEval datasets.
Our fine-tuning procedure and the hyperparameters we used can be found in our paper *""The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models.""; You can use the CAMeLBERT-DA SA model directly as part of our CAMeL Tools SA component (recommended) or as part of the transformers pipeline.; To use the model with the CAMeL Tools SA component:; You can also use the SA model directly with a transformers pipeline:; Note: to download our models, you would need transformers>=3.5.0.
Otherwise, you could download the models manually.",natural-language-processing,text-classification,https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment,Paper: https://arxiv.org/pdf/2103.06678.pdf,License: apache-2.0,,Arabic,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 655212,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 873.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
bert-base-arabic-camelbert-da,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"CAMeLBERT is a collection of BERT models pre-trained on Arabic texts with different sizes and variants.
We release pre-trained language models for Modern Standard Arabic (MSA), dialectal Arabic (DA), and classical Arabic (CA), in addition to a model pre-trained on a mix of the three.
We also provide additional models that are pre-trained on a scaled-down set of the MSA variant (half, quarter, eighth, and sixteenth).
The details are described in the paper ""The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models.""; This model card describes CAMeLBERT-DA (bert-base-arabic-camelbert-da), a model pre-trained on the DA (dialectal Arabic) dataset.; You can use the released model for either masked language modeling or next sentence prediction.
However, it is mostly intended to be fine-tuned on an NLP task, such as NER, POS tagging, sentiment analysis, dialect identification, and poetry classification.
We release our fine-tuninig code here.; You can use this model directly with a pipeline for masked language modeling:; Note: to download our models, you would need transformers>=3.5.0. Otherwise, you could download the models manually.",natural-language-processing,fill-mask,https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-da,Paper: https://arxiv.org/pdf/2103.06678.pdf,License: apache-2.0,,Arabic,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 293,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base-arabic-camelbert-mix-ner,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"CAMeLBERT-Mix NER Model is a Named Entity Recognition (NER) model that was built by fine-tuning the CAMeLBERT Mix model.
For the fine-tuning, we used the ANERcorp dataset.
Our fine-tuning procedure and the hyperparameters we used can be found in our paper ""The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models.
"" Our fine-tuning code can be found here.; You can use the CAMeLBERT-Mix NER model directly as part of our CAMeL Tools NER component (recommended) or as part of the transformers pipeline.; To use the model with the CAMeL Tools NER component:; You can also use the NER model directly with a transformers pipeline:; Note: to download our models, you would need transformers>=3.5.0.
Otherwise, you could download the models manually.",natural-language-processing,token-classification,https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix-ner,Paper: https://arxiv.org/pdf/2103.06678.pdf,License: apache-2.0,,Arabic,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2067,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 870.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
distilbert-base-multilingual-cased-ner-hrl,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"language: ; distilbert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned  Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER). 
Specifically, this model is a distilbert-base-multilingual-cased model that was fine-tuned on an aggregation of 10 high-resourced languages; You can use this model with Transformers pipeline for NER.; This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains.  ; The training data for the 10 languages are from: ",natural-language-processing,token-classification,https://huggingface.co/Davlan/distilbert-base-multilingual-cased-ner-hrl,,,,,PyTorch; TensorFlow; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 51,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 431856,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 13
gpt2-spanish,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"GPT2-Spanish is a language generation model trained from scratch with 11.5GB of Spanish texts and with a Byte Pair Encoding (BPE) tokenizer that was trained for this purpose. The parameters used are the same as the small version of the original OpenAI GPT2 model.; This model was trained with a corpus of 11.5GB of texts corresponding to 3.5GB of Wikipedia articles and 8GB of books (narrative, short stories, theater, poetry, essays, and popularization).; The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for Unicode characters) and a vocabulary size of 50257. The inputs are sequences of 1024 consecutive tokens.; This tokenizer was trained from scratch with the Spanish corpus, since it was evidenced that the tokenizer of the English models presented limitations to capture the semantic relations of Spanish, due to the morphosyntactic differences between both languages.; Apart from the special token ""<|endoftext|>"" for text ending in the OpenAI GPT-2 models, the tokens ""<|talk|>"", ""<|ax1|>"", ""<|ax2|>"" (..)""<|ax9|>"" were included so that they can serve as prompts in future training.",natural-language-processing,text-generation,https://huggingface.co/DeepESP/gpt2-spanish,,License: mit,Datasets: ebooks,Spanish,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 23,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 889,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
bert-myanmar-base-uncased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"The Usage of tokenizer for Myanmar is same as Laos in https://github.com/GKLMIP/Pretrained-Models-For-Laos.; If you use our model, please consider citing our paper:",natural-language-processing,fill-mask,https://huggingface.co/GKLMIP/bert-myanmar-base-uncased,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 425.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
distilbert-base-es-cased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"We are sharing smaller versions of distilbert-base-multilingual-cased that handle a custom number of languages.; Our versions give exactly the same representations produced by the original model which preserves the original accuracy.; For more information please visit our paper: Load What You Need: Smaller Versions of Multilingual BERT.; To generate other smaller versions of multilingual transformers please visit our Github repo.; Please contact amine@geotrend.fr for any question, feedback or request.",natural-language-processing,fill-mask,https://huggingface.co/Geotrend/distilbert-base-es-cased,,License: apache-2.0,Datasets: wikipedia,Spanish,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5513,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 510.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base-dutch-cased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Wietse de Vries ?
Andreas van Cranenburgh ?
Arianna Bisazza ?
Tommaso Caselli ?
Gertjan van Noord ?
Malvina Nissim; BERTje is a Dutch pre-trained BERT model developed at the University of Groningen.; For details, check out our paper on arXiv, the code on Github and related work on Semantic Scholar.; The paper and Github page mention fine-tuned models that are available here.; WARNING: The vocabulary size of BERTje has changed in 2021. If you use an older fine-tuned model and experience problems with the GroNLP/bert-base-dutch-cased tokenizer, use use the following tokenizer:",natural-language-processing,fill-mask,https://huggingface.co/GroNLP/bert-base-dutch-cased,Paper: https://arxiv.org/pdf/1912.09582.pdf,,,Dutch,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 25138,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
DialoGPT-Medium-zerotwo,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,,natural-language-processing,conversational,https://huggingface.co/HAttORi/DialoGPT-Medium-zerotwo,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 55,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
opus-mt-ar-en,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,source languages: ar; target languages: en; OPUS readme: ar-en; dataset: opus; model: transformer-align,natural-language-processing,translation,https://huggingface.co/Helsinki-NLP/opus-mt-ar-en,,License: apache-2.0,,Arabic; English,PyTorch; TensorFlow; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 85307,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
opus-mt-de-en,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,source languages: de; target languages: en; OPUS readme: de-en; dataset: opus; model: transformer-align,natural-language-processing,translation,https://huggingface.co/Helsinki-NLP/opus-mt-de-en,,License: apache-2.0,,German; English,PyTorch; TensorFlow; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 530755,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
opus-mt-en-ar,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,source group: English ; target group: Arabic ; OPUS readme: eng-ara; model: transformer; source language(s): eng,natural-language-processing,translation,https://huggingface.co/Helsinki-NLP/opus-mt-en-ar,,License: apache-2.0,,English; Arabic,PyTorch; TensorFlow; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9336,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
opus-mt-en-es,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,source group: English ; target group: Spanish ; OPUS readme: eng-spa; model: transformer; source language(s): eng,natural-language-processing,translation,https://huggingface.co/Helsinki-NLP/opus-mt-en-es,,License: apache-2.0,,English; Spanish,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 44,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 81530,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 938.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 69
opus-mt-en-ru,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,source languages: en; target languages: ru; OPUS readme: en-ru; dataset: opus; model: transformer-align,natural-language-processing,translation,https://huggingface.co/Helsinki-NLP/opus-mt-en-ru,,License: apache-2.0,,English; Russian,PyTorch; TensorFlow; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 45854,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
opus-mt-fr-en,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,source languages: fr; target languages: en; OPUS readme: fr-en; dataset: opus; model: transformer-align,natural-language-processing,translation,https://huggingface.co/Helsinki-NLP/opus-mt-fr-en,,License: apache-2.0,,French; English,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 366149,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 904.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 17
opus-mt-mul-en,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,source group: Multiple languages ; target group: English ; OPUS readme: mul-eng; model: transformer; source language(s): abk acm ady afb afh_Latn afr akl_Latn aln amh ang_Latn apc ara arg arq ary arz asm ast avk_Latn awa aze_Latn bak bam_Latn bel bel_Latn ben bho bod bos_Latn bre brx brx_Latn bul bul_Latn cat ceb ces cha che chr chv cjy_Hans cjy_Hant cmn cmn_Hans cmn_Hant cor cos crh crh_Latn csb_Latn cym dan deu dsb dtp dws_Latn egl ell enm_Latn epo est eus ewe ext fao fij fin fkv_Latn fra frm_Latn frr fry fuc fuv gan gcf_Latn gil gla gle glg glv gom gos got_Goth grc_Grek grn gsw guj hat hau_Latn haw heb hif_Latn hil hin hnj_Latn hoc hoc_Latn hrv hsb hun hye iba ibo ido ido_Latn ike_Latn ile_Latn ilo ina_Latn ind isl ita izh jav jav_Java jbo jbo_Cyrl jbo_Latn jdt_Cyrl jpn kab kal kan kat kaz_Cyrl kaz_Latn kek_Latn kha khm khm_Latn kin kir_Cyrl kjh kpv krl ksh kum kur_Arab kur_Latn lad lad_Latn lao lat_Latn lav ldn_Latn lfn_Cyrl lfn_Latn lij lin lit liv_Latn lkt lld_Latn lmo ltg ltz lug lzh lzh_Hans mad mah mai mal mar max_Latn mdf mfe mhr mic min mkd mlg mlt mnw moh mon mri mwl mww mya myv nan nau nav nds niu nld nno nob nob_Hebr nog non_Latn nov_Latn npi nya oci ori orv_Cyrl oss ota_Arab ota_Latn pag pan_Guru pap pau pdc pes pes_Latn pes_Thaa pms pnb pol por ppl_Latn prg_Latn pus quc qya qya_Latn rap rif_Latn roh rom ron rue run rus sag sah san_Deva scn sco sgs shs_Latn shy_Latn sin sjn_Latn slv sma sme smo sna snd_Arab som spa sqi srp_Cyrl srp_Latn stq sun swe swg swh tah tam tat tat_Arab tat_Latn tel tet tgk_Cyrl tha tir tlh_Latn tly_Latn tmw_Latn toi_Latn ton tpw_Latn tso tuk tuk_Latn tur tvl tyv tzl tzl_Latn udm uig_Arab uig_Cyrl ukr umb urd uzb_Cyrl uzb_Latn vec vie vie_Hani vol_Latn vro war wln wol wuu xal xho yid yor yue yue_Hans yue_Hant zho zho_Hans zho_Hant zlm_Latn zsm_Latn zul zza,natural-language-processing,translation,https://huggingface.co/Helsinki-NLP/opus-mt-mul-en,,License: apache-2.0,,120 languages,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 37,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 111359,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 624.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
opus-mt-ru-en,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,"Model Description:; This model can be used for translation and text-to-text generation.; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).; Further details about the dataset for this model can be found in the OPUS readme: ru-en",natural-language-processing,translation,https://huggingface.co/Helsinki-NLP/opus-mt-ru-en,,License: cc-by-4.0,,Russian; English,PyTorch; TensorFlow; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 349619,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
convert_wav2vec2_to_hf,,,,,,,,,"This repo has two scripts that can show how to convert a fairseq checkpoint to HF Transformers.; It's important to always check in a forward pass that the two checkpoints are the same. The procedure should be as follows:; The ""0"" means that checkpoint is not a fine-tuned one.
4. Verify that models are equal:; Check the scripts to better understand how they work or contact https://huggingface.co/patrickvonplaten; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/HfSpeechUtils/convert_wav2vec2_to_hf,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.52KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
camembert-ner-with-dates,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"[camembert-ner-with-dates] is an extension of french camembert-ner model with an additionnal tag for dates.
Model was trained on enriched version of wikiner-fr dataset (~170 634  sentences).; On my test data (mix of chat and email), this model got an f1 score of ~83% (in comparison dateparser was ~70%).
Dateparser library can still be be used on the output of this model in order to convert text to python datetime object 
(https://dateparser.readthedocs.io/en/latest/).; Global; By entity",natural-language-processing,token-classification,https://huggingface.co/Jean-Baptiste/camembert-ner-with-dates,,License: mit,Datasets: Jean-Baptiste/wikiner_fr,French,PyTorch; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 23,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 17733,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 12
camembert-ner,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"[camembert-ner] is a NER model that was fine-tuned from camemBERT on wikiner-fr dataset.
Model was trained on wikiner-fr dataset (~170 634  sentences).
Model was validated on emails/chat data and overperformed other models on this type of data specifically. 
In particular the model seems to work better on entity that don't start with an upper case.; Training data was classified as follow:; Overall; By entity; For those who could be interested, here is a short article on how I used the results of this model to train a LSTM model for signature detection in emails:
https://medium.com/@jean-baptiste.polle/lstm-model-for-email-signature-detection-8e990384fefa",natural-language-processing,token-classification,https://huggingface.co/Jean-Baptiste/camembert-ner,,License: mit,Datasets: Jean-Baptiste/wikiner_fr,French,PyTorch; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 79,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 926322,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 67
sentence-bert-swedish-cased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps Swedish sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. This model is a bilingual Swedish-English model trained according to instructions in the paper Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation and the documentation accompanying its companion python package. We have used the strongest available pretrained English Bi-Encoder (all-mpnet-base-v2) as a teacher model, and the pretrained Swedish KB-BERT as the student model. ; A more detailed description of the model can be found in an article we published on the KBLab blog here and for the updated model here. ; Update: We have released updated versions of the model since the initial release. The original model described in the blog post is v1.0. The current version is v2.0. The newer versions are trained on longer paragraphs, and have a longer max sequence length. v2.0 is trained with a stronger teacher model and is the current default.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:",natural-language-processing,sentence-similarity,https://huggingface.co/KBLab/sentence-bert-swedish-cased,Paper: https://arxiv.org/pdf/2004.09813.pdf,License: apache-2.0,,Swedish,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 255,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 501.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
t5-darija-summarization,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"This dataset contains 19,806 news articles written in Moroccan Arabic dialect along with their titles. The articles were crawled from Goud.ma website between 01/01/2018 and 12/31/2020. 
The articles are written mainly in Moroccan Arabic dialect (Darija) but some of them contain Modern Standard Arabic (MSA) passages. All the titles are written in Darija. 
The following table summarize some tatistics on the MArSum Dataset.; The following figure describes the creation process of MArSum:; ; You may refer to our paper, cited below, for more details on this process.; The dataset is split into Train/Test subsets using a 90/10 split strategy. Both subsets are available for direct donwload.",natural-language-processing,text2text-generation,https://huggingface.co/Kamel/t5-darija-summarization,,,,Arabic,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 52,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
GPT-J-6B-Shinen,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"GPT-J 6B-Shinen is a finetune created using EleutherAI's GPT-J 6B model. Compared to GPT-Neo-2.7-Horni, this model is much heavier on the sexual content.
Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The training data contains user-generated stories from sexstories.com. All stories are tagged using the following way:; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; The core functionality of GPT-J is taking a string of text and predicting the next token. While language models are widely used for tasks other than this, there are a lot of unknowns with this work. When prompting GPT-J it is important to remember that the statistically most likely next token is often not the token that produces the most ""accurate"" text. Never depend upon GPT-J to produce factually accurate output.; GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See Sections 5 and 6 of the Pile paper for a more detailed analysis of the biases in the Pile.",natural-language-processing,text-generation,https://huggingface.co/KoboldAI/GPT-J-6B-Shinen,Paper: https://arxiv.org/pdf/2101.00027.pdf,License: mit,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5858,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 12.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
GPT-Neo-2.7B-Shinen,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"GPT-Neo 2.7B-Shinen is a finetune created using EleutherAI's GPT-Neo 2.7B model. Compared to GPT-Neo-2.7-Horni, this model is much heavier on the sexual content.; Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The training data contains user-generated stories from sexstories.com. All stories are tagged using the following way:; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; GPT-Neo was trained as an autoregressive language model. This means that its core functionality is taking a string of text and predicting the next token. While language models are widely used for tasks other than this, there are a lot of unknowns with this work.
GPT-Neo-Shinen was trained on a dataset known to contain profanity, lewd, and otherwise abrasive language. GPT-Neo-Shinen WILL produce socially unacceptable text without warning.
GPT-Neo-Shinen will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results.",natural-language-processing,text-generation,https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Shinen,,License: mit,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8686,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
fairseq-dense-355M,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a Hugging Face transformers-compatible conversion of the original dense 355M-parameter model from the paper ""Efficient Large Scale Language Modeling with Mixtures of Experts"" from Artetxe et al. Please refer to the original model card, which can be found at https://github.com/facebookresearch/fairseq/blob/main/examples/moe_lm/model_card.md.",natural-language-processing,text-generation,https://huggingface.co/KoboldAI/fairseq-dense-355M,Paper: https://arxiv.org/pdf/2112.10684.pdf,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 170,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
SGPT-5.8B-weightedmean-nli-bitfit,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"For usage instructions, refer to our codebase: https://github.com/Muennighoff/sgpt ; For eval results, refer to our paper: https://arxiv.org/abs/2202.08904; The model was trained with the parameters:; DataLoader:; torch.utils.data.dataloader.DataLoader of length 249592 with parameters:",natural-language-processing,sentence-similarity,https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-nli-bitfit,Paper: https://arxiv.org/pdf/2202.08904.pdf,,,,PyTorch; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 582,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 24.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
hebrew-gpt_neo-xl-poetry,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Hebrew poetry text generation model which was fine tuned upon on hebrew-gpt_neo-xl.; An assortment of various Hebrew books, magazines and poetry corpuses; Similar to this one ; Available here  ",natural-language-processing,text-generation,https://huggingface.co/Norod78/hebrew-gpt_neo-xl-poetry,,License: mit,,Hebrew,PyTorch; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 16.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
RoBERTalex,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"The RoBERTalex is a transformer-based masked language model for the Spanish language. It is based on the RoBERTa base model and has been pre-trained using a large Spanish Legal Domain Corpora, with a total of 8.9GB of text.; The RoBERTalex model is ready-to-use only for masked language modeling to perform the Fill Mask task (try the inference API or read the next section).
However, it is intended to be fine-tuned on non-generative downstream tasks such as Question Answering, Text Classification, or Named Entity Recognition.
You can use the raw model for fill mask or fine-tune it to a downstream task.; Here is how to use this model:; Here is how to use this model to get the features of a given text in PyTorch:; At the time of submission, no measures have been taken to estimate the bias embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated.",natural-language-processing,fill-mask,https://huggingface.co/PlanTL-GOB-ES/RoBERTalex,Paper: https://arxiv.org/pdf/1907.11692.pdf; https://arxiv.org/pdf/2110.12201.pdf,License: apache-2.0,Datasets: legal_ES; temu_legal,Spanish,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 493,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 506.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
roberta-base-biomedical-es,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Biomedical pretrained language model for Spanish. For more details about the corpus, the pretraining and the evaluation, check the official repository and read our preprint.; The model is ready-to-use only for masked language modelling to perform the Fill Mask task (try the inference API or read the next section). However, it is intended to be fine-tuned on downstream tasks such as Named Entity Recognition or Text Classification.; This model is a RoBERTa-based model trained on a
biomedical corpus in Spanish collected from several sources (see next section). ; The training corpus has been tokenized using a byte version of Byte-Pair Encoding (BPE)
used in the original RoBERTA model with a vocabulary size of 52,000 tokens. The pretraining consists of a masked language model training at the subword level following the approach employed for the RoBERTa base model with the same hyperparameters as in the original work. The training lasted a total of 48 hours with 16 NVIDIA V100 GPUs of 16GB DDRAM, using Adam optimizer with a peak learning rate of 0.0005 and an effective batch size of 2,048 sentences.; The training corpus is composed of several biomedical corpora in Spanish, collected from publicly available corpora and crawlers.
To obtain a high-quality training corpus, a cleaning pipeline with the following operations has been applied:",natural-language-processing,fill-mask,https://huggingface.co/PlanTL-GOB-ES/roberta-base-biomedical-es,Paper: https://arxiv.org/pdf/2109.03570.pdf; https://arxiv.org/pdf/2109.07765.pdf,License: apache-2.0,,Spanish,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 99,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 506.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
roberta-base-ca,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"BERTa is a transformer-based masked language model for the Catalan language. 
It is based on the RoBERTA base model 
and has been trained on a medium-size corpus collected from publicly available corpora and crawlers.; This model was originally published as bsc/roberta-base-ca-cased.; The model is ready-to-use only for masked language modelling to perform the Fill Mask task (try the inference API or read the next section). 
However, it is intended to be fine-tuned on non-generative downstream tasks such as Question Answering, Text Classification or Named Entity Recognition.; Below, an example of how to use the masked language modelling task with a pipeline.; The training corpus consists of several corpora gathered from web crawling and public corpora.",natural-language-processing,fill-mask,https://huggingface.co/PlanTL-GOB-ES/roberta-base-ca,,License: apache-2.0,,Catalan,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 49,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 506.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
roberta-large-bne,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"The roberta-large-bne is a transformer-based masked language model for the Spanish language. It is based on the RoBERTa large model and has been pre-trained using the largest Spanish corpus known to date, with a total of 570GB of clean and deduplicated text processed for this work, compiled from the web crawlings performed by the  National Library of Spain (Biblioteca Nacional de Espa?a) from 2009 to 2019.; The roberta-large-bne model is ready-to-use only for masked language modeling to perform the Fill Mask task (try the inference API or read the next section).
However, it is intended to be fine-tuned on non-generative downstream tasks such as Question Answering, Text Classification, or Named Entity Recognition.
You can use the raw model for fill mask or fine-tune it to a downstream task.; Here is how to use this model:; Here is how to use this model to get the features of a given text in PyTorch:; At the time of submission, no measures have been taken to estimate the bias and toxicity embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated.",natural-language-processing,fill-mask,https://huggingface.co/PlanTL-GOB-ES/roberta-large-bne,Paper: https://arxiv.org/pdf/1907.11692.pdf,License: apache-2.0,Datasets: bne,Spanish,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 897,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
distilbert_punctuator_en,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,The model is fine-tuned based on DistilBertForTokenClassification for adding punctuations to plain text (uncased English); Combination of following three dataset:; Validation with 500 samples of dataset scraped from https://www.thenews.com.pk website. Reference; Metrics Report:; Validation with 86 news ted talks of 2020 which are not included in training dataset Reference,natural-language-processing,token-classification,https://huggingface.co/Qishuai/distilbert_punctuator_en,,,,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 401,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 532.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
FacialEmoRecog,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,Create your own image classifier for anything by running this repo  ,computer-vision,image-classification,https://huggingface.co/Rajaram1996/FacialEmoRecog,,License: mit,Datasets: Jeneral/fer2013,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 257,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 343.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
prot_bert_bfd,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Pretrained model on protein sequences using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is trained on uppercase amino acids: it only works with capital letter amino acids.; ProtBert-BFD is based on Bert model which pretrained on a large corpus of protein sequences in a self-supervised fashion.
This means it was pretrained on the raw protein sequences only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those protein sequences.; One important difference between our Bert model and the original Bert version is the way of dealing with sequences as separate documents
This means the Next sentence prediction is not used, as each sequence is treated as a complete document.
The masking follows the original Bert training with randomly masks 15% of the amino acids in the input. ; At the end, the feature extracted from this model revealed that the LM-embeddings from unlabeled data (only protein sequences) captured important biophysical properties governing protein
shape.
This implied learning some of the grammar of the language of life realized in protein sequences.; The model could be used for protein feature extraction or to be fine-tuned on downstream tasks.
We have noticed in some tasks you could gain more accuracy by fine-tuning the model rather than using it as a feature extractor.",natural-language-processing,fill-mask,https://huggingface.co/Rostlab/prot_bert_bfd,,,Datasets: BFD,protein,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 27863,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
distilbert-base-nepali,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"This model is pre-trained on nepalitext dataset consisting of over 13 million Nepali text sequences using a masked language modeling (MLM) objective. Our approach trains a Sentence Piece Model (SPM) for text tokenization similar to XLM-ROBERTa and trains distilbert model for language modeling. Find more details in this paper.; It achieves the following results on the evaluation set:; Refer to original distilbert-base-uncased; This backbone model intends to be fine-tuned on Nepali language focused downstream task such as sequence classification, token classification or question answering. 
The language model being trained on a data with texts grouped to a block size of 512, it handles text sequence up to 512 tokens and may not perform satisfactorily on shorter sequences.; This model can be used directly with a pipeline for masked language modeling:",natural-language-processing,fill-mask,https://huggingface.co/Sakonii/distilbert-base-nepali,Paper: https://arxiv.org/pdf/1911.02116.pdf; https://arxiv.org/pdf/1910.01108.pdf,License: apache-2.0,Datasets: Sakonii/nepalitext-language-model-dataset,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 82,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 538.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
recreate-history,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,sahajBERT fine-tuned for NER using the bengali split of WikiANN . ; Named Entities predicted by the model:; You can use this model directly with a pipeline for masked language modeling:; WIP; The model was initialized it with pre-trained weights of sahajBERT at step 19519 and trained on the bengali of WikiANN ,natural-language-processing,token-classification,https://huggingface.co/SaulLu/recreate-history,,License: apache-2.0,Datasets: xtreme,Bengali,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 44,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 69.3MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bart-base-detox,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Model Overview; This is the model presented in the paper ""ParaDetox: Detoxification with Parallel Data"". ; The model itself is BART (base) model trained on parallel detoxification dataset ParaDetox achiving SOTA results for detoxification task. More details, code and data can be found here.; How to use; Citation",natural-language-processing,text2text-generation,https://huggingface.co/s-nlp/bart-base-detox,,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 817,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
roberta-base-formality-ranker,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"The model has been trained to predict for English sentences, whether they are formal or informal. ; Base model: roberta-base; Datasets: GYAFC from Rao and Tetreault, 2018 and online formality corpus from Pavlick and Tetreault, 2016.; Data augmentation: changing texts to upper or lower case; removing all punctuation, adding dot at the end of a sentence. It was applied because otherwise the model is over-reliant on punctuation and capitalization and does not pay enough attention to other features.; Loss: binary classification (on GYAFC), in-batch ranking (on PT data).",natural-language-processing,text-classification,https://huggingface.co/s-nlp/roberta-base-formality-ranker,,,Datasets: GYAFC; Pavlick-Tetreault-2016,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1640,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1001.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
AraT5-base,,,,,,,,,"This is the repository accompanying our paper AraT5: Text-to-Text Transformers for Arabic Language Understanding and Generation. In this is the repository we Introduce AraT5MSA, AraT5Tweet, and AraT5: three powerful Arabic-specific text-to-text Transformer based models;; Below is an example for fine-tuning AraT5-base for News Title Generation on the Aranews dataset ; For more details about the fine-tuning example, please read this notebook  ; In addition, we release the fine-tuned checkpoint of the News Title Generation (NGT) which is described in the paper. The model available at Huggingface (UBC-NLP/AraT5-base-title-generation).; For more details, please visit our own GitHub.",,,https://huggingface.co/UBC-NLP/AraT5-base,,,,Arabic,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1580,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Czert-B-base-cased-long-zero-shot,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"This repository keeps trained Czert-B-base-cased-long-zero-shot model for the paper Czert C Czech BERT-like Model for Language Representation

For more information, see the paper; This is long version of Czert-B-base-cased created without any finetunning on long documents. Positional embedings were created by simply repeating the positional embeddings of the original Czert-B model. For tokenization, please use BertTokenizer. Cannot be used with AutoTokenizer. ; You can download MLM & NSP only pretrained models
CZERT-A-v1
CZERT-B-v1; After some additional experiments, we found out that the tokenizers config was exported wrongly. In Czert-B-v1, the tokenizer parameter ""do_lower_case""  was wrongly set to true. In Czert-A-v1 the parameter ""strip_accents""  was incorrectly set to true. ; Both mistakes are repaired in v2.
CZERT-A-v2
CZERT-B-v2",multimodel,feature-extraction,https://huggingface.co/UWB-AIR/Czert-B-base-cased-long-zero-shot,Paper: https://arxiv.org/pdf/2103.13031.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 45,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 534.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
FinancialBERT-Sentiment-Analysis,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"FinancialBERT is a BERT model pre-trained on a large corpora of financial texts. The purpose is to enhance financial NLP research and practice in financial domain, hoping that financial practitioners and researchers can benefit from this model without the necessity of the significant computational resources required to train the model. ; The model was fine-tuned for Sentiment Analysis task on Financial PhraseBank dataset. Experiments show that this model outperforms the general BERT and other financial domain-specific models.; More details on FinancialBERT's pre-training process can be found at: https://www.researchgate.net/publication/358284785_FinancialBERT_-_A_Pretrained_Language_Model_for_Financial_Text_Mining; FinancialBERT model was fine-tuned on Financial PhraseBank, a dataset consisting of 4840 Financial News categorised by sentiment (negative, neutral, positive).; The evaluation metrics used are: Precision, Recall and F1-score. The following is the classification report on the test set.",natural-language-processing,text-classification,https://huggingface.co/ahmedrachid/FinancialBERT-Sentiment-Analysis,,,Datasets: financial_phrasebank,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7198,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 440.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
frame-interpolation-film-style,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/akhaliq/frame-interpolation-film-style,,,,,Keras,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 186,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.86MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 53
led-base-16384,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Allenai's Longformer Encoder-Decoder (LED).; As described in Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan, led-base-16384 was initialized from bart-base since both models share the exact same architecture. To be able to process 16K tokens, bart-base's position embedding matrix was simply copied 16 times.; This model is especially interesting for long-range summarization and question answering.; This notebook shows how led-base-16384 can effectively be fine-tuned on a downstream task.",natural-language-processing,text2text-generation,https://huggingface.co/allenai/led-base-16384,Paper: https://arxiv.org/pdf/2004.05150.pdf,License: apache-2.0,,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 31,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 10785,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
specter,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"SPECTER is a pre-trained language model to generate document-level embedding of documents. It is pre-trained on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. ; If you're coming here because you want to embed papers, SPECTER has now been superceded by SPECTER 2.0. Use that instead.; Paper: SPECTER: Document-level Representation Learning using Citation-informed Transformers; Original Repo: Github; Evaluation Benchmark: SciDocs",multimodel,feature-extraction,https://huggingface.co/allenai/specter,Paper: https://arxiv.org/pdf/2004.07180.pdf,License: apache-2.0,Datasets: SciDocs,English,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 51,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 18493,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
bort,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"?? Disclaimer ?? ; This model is community-contributed, and not supported by Amazon, Inc.; Amazon's BORT; BORT is a highly compressed version of bert-large that is up to 10 times faster at inference. 
The model is an optimal sub-architecture of bert-large that was found using neural architecture search.; Paper",natural-language-processing,fill-mask,https://huggingface.co/amazon/bort,Paper: https://arxiv.org/pdf/2010.10499.pdf,,,,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 632,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 713.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
bert-multilingual-passage-reranking-msmarco,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Input: Supports over 100 Languages. See List of supported languages for all available.; Purpose: This module takes a search query [1] and a passage [2] and calculates if the passage matches the query. 
It can be used as an improvement for Elasticsearch Results and boosts the relevancy by up to 100%. ; Architecture: On top of BERT there is a Densly Connected NN which takes the 768 Dimensional [CLS] Token as input and provides the output (Arxiv).; Output: Just a single value between between -10 and 10. Better matching query,passage pairs tend to have a higher a score.; Both query[1] and passage[2] have to fit in 512 Tokens.
As you normally want to rerank the first dozens of search results keep in mind the inference time of approximately 300 ms/query.",natural-language-processing,text-classification,https://huggingface.co/amberoad/bert-multilingual-passage-reranking-msmarco,Paper: https://arxiv.org/pdf/1901.04085.pdf,License: apache-2.0,Datasets: msmarco,102 languages,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6023,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
roberta-base-ner-conll2003,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"This model is a fine-tuned version of roberta-base on the conll2003 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",natural-language-processing,token-classification,https://huggingface.co/andi611/roberta-base-ner-conll2003,,License: mit,Datasets: conll2003,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 499.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-for-patents,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"BERT for Patents is a model trained by Google on 100M+ patents (not just US patents). It is based on BERTLARGE.; If you want to learn more about the model, check out the blog post, white paper and GitHub page containing the original TensorFlow checkpoint.",natural-language-processing,fill-mask,https://huggingface.co/anferico/bert-for-patents,,License: apache-2.0,,English,PyTorch; TensorFlow; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 43,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6992,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
german-gpt2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Note: This model was de-anonymized and now lives at:; https://huggingface.co/dbmdz/german-gpt2; Please use the new model name instead!,natural-language-processing,text-generation,https://huggingface.co/anonymous-german-nlp/german-gpt2,,License: mit,,German,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 802,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
wav2vec2-xls-r-300m-bengali,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the OPENSLR_SLR53 - bengali dataset.
It achieves the following results on the evaluation set. ; Without language model : ; With 5 gram language model trained on 30M sentences randomly chosen from AI4Bharat IndicCorp dataset : ; Note : 5% of a total 10935 samples have been used for evaluation. Evaluation set has 10935 examples which was not part of training training was done on first 95% and eval was done on last 5%. Training was stopped after 180k steps. Output predictions are available under files section.; The following hyperparameters were used during training:",audio,automatic-speech-recognition,https://huggingface.co/arijitx/wav2vec2-xls-r-300m-bengali,,License: apache-2.0,Datasets: openslr; SLR53; AI4Bharat/IndicCorp,Bengali,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 645,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
xlm-roberta-base-uncased-all-english,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,XLM-RoBERTa finetuned on NER. Check more detail at TNER repository.,natural-language-processing,token-classification,https://huggingface.co/tner/xlm-roberta-base-uncased-all-english,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 23,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
KcELECTRA-base,,,,,,,,,"** Updates on 2022.10.08 **; ??? ??? Transformer ?? ???? ??? ??? ??, ?? ??, ? ? ? ??? ???? ???? ??? ?????. ??, ??? NSMC? ?? User-Generated Noisy text domain ????? ???? ??? ??? ??? ???? ???, ??? ? ???? ????? ???? ?? ???? ???? ?????.; KcELECTRA? ?? ?? ??? ????? ???? ??, ??? ???? ??? ???? ???, ?????? ELECTRA??? ???? ??? Pretrained ELECTRA ?????.; ?? KcBERT ?? ???? ?? ? vocab ??? ?? ??? ???? ??? ???????.; KcELECTRA? Huggingface? Transformers ?????? ?? ??? ??? ??? ? ????. (??? ?? ????? ???? ????.)",,,https://huggingface.co/beomi/KcELECTRA-base,,License: mit,,Korean; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3370,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 438.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
bert-base-go-emotion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,Notebook,natural-language-processing,text-classification,https://huggingface.co/bhadresh-savani/bert-base-go-emotion,,License: apache-2.0,Datasets: go_emotions,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 26,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 97708,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 439.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 12
bluebert_pubmed_uncased_L-12_H-768_A-12,,,,,,,,,"A BERT model pre-trained on PubMed abstracts; Please see https://github.com/ncbi-nlp/bluebert; We provide preprocessed PubMed texts that were used to pre-train the BlueBERT models. 
The corpus contains ~4000M words extracted from the PubMed ASCII code version. ; Pre-trained model: https://huggingface.co/bert-base-uncased; Below is a code snippet for more details.",,,https://huggingface.co/bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12,,License: cc0-1.0,Datasets: pubmed,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1506,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Ko-DialoGPT,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,,natural-language-processing,conversational,https://huggingface.co/byeongal/Ko-DialoGPT,,License: cc-by-nc-sa-4.0,,Korean,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 381,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 515.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
SapBERT-UMLS-2020AB-all-lang-from-XLMR,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"language: multilingual; tags:; datasets:; [news] A cross-lingual extension of SapBERT will appear in the main onference of ACL 2021! 
[news] SapBERT will appear in the conference proceedings of NAACL 2021!; SapBERT (Liu et al. 2020) trained with UMLS 2020AB, using xlm-roberta-base as the base model. Please use [CLS] as the representation of the input.",multimodel,feature-extraction,https://huggingface.co/cambridgeltl/SapBERT-UMLS-2020AB-all-lang-from-XLMR,Paper: https://arxiv.org/pdf/2010.11784.pdf,,,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 844,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
twitter-roberta-base-irony,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,This is a roBERTa-base model trained on ~58M tweets and finetuned for irony detection with the TweetEval benchmark.; Output: ,natural-language-processing,text-classification,https://huggingface.co/cardiffnlp/twitter-roberta-base-irony,Paper: https://arxiv.org/pdf/2010.12421.pdf,,,,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4142685,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
monot5-base-msmarco-10k,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"This model is a T5-base reranker fine-tuned on the MS MARCO passage dataset for 10k steps (or 1 epoch).; This model usually has a better zero-shot performance than monot5-base-msmarco, i.e., it performs better on datasets different from MS MARCO.; For more details on how to use it, check the following links:; Paper describing the model: Document Ranking with a Pretrained Sequence-to-Sequence Model",natural-language-processing,text2text-generation,https://huggingface.co/castorini/monot5-base-msmarco-10k,,,,,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 15420,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base-chinese-ner,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).; @０柑峁┝朔斌w中文的 transformers 模型（包含 ALBERT、BERT、GPT2）及自然Z言理工具（包含嘣~、~性擞、w辨R）。; Please use BertTokenizerFast as tokenizer instead of AutoTokenizer.; 使用 BertTokenizerFast 而非 AutoTokenizer。; For full usage and more information, please refer to https://github.com/ckiplab/ckip-transformers.",natural-language-processing,token-classification,https://huggingface.co/ckiplab/bert-base-chinese-ner,,License: gpl-3.0,,Chinese,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 44,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 182342,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 814.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
distilroberta-base-climate-detector,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This is the fine-tuned ClimateBERT language model with a classification head for detecting climate-related paragraphs.; Using the climatebert/distilroberta-base-climate-f language model as starting point, the distilroberta-base-climate-detector model is fine-tuned on our climatebert/climate_detection dataset.; Note: This model is trained on paragraphs. It may not perform well on sentences.; You can use the model with a pipeline for text classification:",natural-language-processing,text-classification,https://huggingface.co/climatebert/distilroberta-base-climate-detector,,License: apache-2.0,Datasets: climatebert/climate_detection,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 36592,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 661.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
rut5-base-paraphraser,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,This is a paraphraser for Russian sentences described in this Habr post. ; It is recommended to use the model with the encoder_no_repeat_ngram_size argument:,natural-language-processing,text2text-generation,https://huggingface.co/cointegrated/rut5-base-paraphraser,,License: mit,Datasets: cointegrated/ru-paraphrase-NMT-Leipzig,Russian,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 743,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ms-marco-MiniLM-L-12-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model was trained on the MS Marco Passage Ranking task.; The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco; The usage becomes easier when you have SentenceTransformers installed. Then, you can use the pre-trained models like this:; In the following table, we provide various pre-trained Cross-Encoders together with their performance on the TREC Deep Learning 2019 and the MS Marco Passage Reranking dataset. ;  Note: Runtime was computed on a V100 GPU.",natural-language-processing,text-classification,https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2,,License: apache-2.0,,,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 494680,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 267.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 12
ms-marco-MiniLM-L-2-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model was trained on the MS Marco Passage Ranking task.; The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco; The usage becomes easier when you have SentenceTransformers installed. Then, you can use the pre-trained models like this:; In the following table, we provide various pre-trained Cross-Encoders together with their performance on the TREC Deep Learning 2019 and the MS Marco Passage Reranking dataset. ;  Note: Runtime was computed on a V100 GPU.",natural-language-processing,text-classification,https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-2-v2,,License: apache-2.0,,,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 10494,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 125.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ms-marco-MiniLM-L-6-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model was trained on the MS Marco Passage Ranking task.; The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco; The usage becomes easier when you have SentenceTransformers installed. Then, you can use the pre-trained models like this:; In the following table, we provide various pre-trained Cross-Encoders together with their performance on the TREC Deep Learning 2019 and the MS Marco Passage Reranking dataset. ;  Note: Runtime was computed on a V100 GPU.",natural-language-processing,text-classification,https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2,,License: apache-2.0,,,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 215288,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 182.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 52
nli-deberta-v3-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c42cd649af0365a01a1e_Zero-Shot%20Classification.svg,,"This model was trained using SentenceTransformers Cross-Encoder class. This model is based on microsoft/deberta-v3-large; The model was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.; For futher evaluation results, see SBERT.net - Pretrained Cross-Encoder.; Pre-trained models can be used like this:; You can use the model also directly with Transformers library (without SentenceTransformers library):",natural-language-processing,zero-shot-classification,https://huggingface.co/cross-encoder/nli-deberta-v3-large,,License: apache-2.0,Datasets: multi_nli; snli,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4823,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bias-detection-model,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"An English sequence classification model, trained on MBAD Dataset to detect bias and fairness in sentences (news articles). This model was built on top of distilbert-base-uncased model and trained for 30 epochs with a batch size of 16, a learning rate of 5e-5, and a maximum sequence length of 512.; The easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.; This model is part of the Research topic ""Bias and Fairness in AI"" conducted by Deepak John Reji, Shaina Raza. If you use this work (code, model or dataset), please star at:; Bias & Fairness in AI, (2022), GitHub repository, https://github.com/dreji18/Fairness-in-AI",natural-language-processing,text-classification,https://huggingface.co/d4data/bias-detection-model,,,,English,TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2928,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 268.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
dalle-mini,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This model card focuses on the model associated with the DALL・E mini space on Hugging Face, available here. The app is called “dalle-mini”, but  incorporates “DALL・E Mini’’ and “DALL・E Mega” models (further details on this distinction forthcoming).; The DALL・E Mega model is the largest version of DALLE Mini. For more information specific to DALL・E Mega, see the DALL・E Mega model card.; The model is intended to be used to generate images based on text prompts for research and personal consumption.  Intended uses include supporting creativity, creating humorous content, and providing generations for people curious about the model’s behavior.  Intended uses exclude those described in the Misuse and Out-of-Scope Use section.; The model could also be used for downstream use cases, including:; Downstream uses exclude the uses described in Misuse and Out-of-Scope Use.",multimodel,text-to-image,https://huggingface.co/dalle-mini/dalle-mini,Paper: https://arxiv.org/pdf/2102.08981.pdf; https://arxiv.org/pdf/2012.09841.pdf; https://arxiv.org/pdf/1910.13461.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,,English,JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 305,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 215,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 60
gpt2-small-spanish,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"La descripción en Espa?ol se encuentra después de la descripción en Inglés.; GPT2-small-spanish is a state-of-the-art language model for Spanish based on the GPT-2 small model. ; It was trained on Spanish Wikipedia using Transfer Learning and Fine-tuning techniques. The training took around 70 hours with four GPU NVIDIA GTX 1080-Ti with 11GB of DDR5 and with around 3GB of (processed) training data. ; It was fine-tuned from the English pre-trained GPT-2 small using the Hugging Face libraries (Transformers and Tokenizers) wrapped into the fastai v2 Deep Learning framework. All the fine-tuning fastai v2 techniques were used.; The training is purely based on the GPorTuguese-2 model developed by Pierre Guillou. The training details are in this article: ""Faster than training from scratch ― Fine-tuning the English GPT-2 in any language with Hugging Face and fastai v2 (practical case with Portuguese)"".",natural-language-processing,text-generation,https://huggingface.co/datificate/gpt2-small-spanish,,License: apache-2.0,Datasets: wikipedia,Spanish,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1661,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
gpt2-french-small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"A small french language model for french text generation (and possibly more NLP tasks...); Introduction; This french gpt2 model is based on openai GPT-2 small model.; It was trained on a very small (190Mb) dataset  from french wikipedia using Transfer Learning and Fine-tuning techniques in just over a day, on one Colab pro with 1GPU 16GB.; It was created applying the recept of Pierre Guillou",natural-language-processing,text-generation,https://huggingface.co/dbddv01/gpt2-french-small,,,,French,PyTorch; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 622,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
wav2vec2-xls-r-300m-italian,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the MOZILLA-FOUNDATION/COMMON_VOICE_7_0 - IT dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",audio,automatic-speech-recognition,https://huggingface.co/dbdmg/wav2vec2-xls-r-300m-italian,,License: apache-2.0,Datasets: mozilla-foundation/common_voice_7_0,Italian,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
bert-large-cased-finetuned-conll03-english,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,token-classification,https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english,,,,,PyTorch; TensorFlow; JAX; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 29,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 298708,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
xlm-roberta-large-squad2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,"Language model: xlm-roberta-largeLanguage: MultilingualDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD dev set - German MLQA - German XQuADTraining run: MLFlow linkInfrastructure: 4x Tesla v100; Evaluated on the SQuAD 2.0 English dev set with the official eval script.; Evaluated on German MLQA: test-context-de-question-de.json; Evaluated on German XQuAD: xquad.de.json; For doing QA at scale (i.e. many docs instead of single paragraph), you can load the model also in haystack:",natural-language-processing,question-answering,https://huggingface.co/deepset/xlm-roberta-large-squad2,,License: cc-by-4.0,Datasets: squad_v2,multilingual,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 46,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5050,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
roberta-cls-consec,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This network has been fine-tuned for the task described in the paper Topical Change Detection in Documents via Embeddings of Long Sequences and is our best-performing base-transformer model. You can find more detailed information in our GitHub page for the paper here, or read the paper itself. The weights are based on RoBERTa-base.; The preferred way is through pipelines; The model expects two segments that are separated with the [SEP] token. In our training setup, we had entire paragraphs as samples (or up to 512 tokens across two paragraphs), specifically trained on a Terms of Service data set. Note that this might lead to poor performance on ""general"" topics, such as news articles or Wikipedia.; The training task is to determine whether two text segments (paragraphs) belong to the same topical section or not. This can be utilized to create a topical segmentation of a document by consecutively predicting the ""coherence"" of two segments.If you are experimenting via the Huggingface Model API, the following are interpretations of the LABELs:; The results of this model can be found in the paper. We average over models from five different random seeds, which is why the specific results for this model might be different from the exact values in the paper.",natural-language-processing,text-classification,https://huggingface.co/dennlinger/roberta-cls-consec,Paper: https://arxiv.org/pdf/2012.03619.pdf,,,,PyTorch; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 431,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-multi-english-german-squad2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,"We created German Squad 2.0 (deQuAD 2.0) and merged with SQuAD2.0 into an English and German training data for question answering. The bert-base-multilingual-cased is used to fine-tune bilingual QA downstream task.; SQuAD2.0 was auto-translated into German. We hired professional editors to proofread the translated transcripts, correct mistakes and double check the answers to further polish the text and enhance annotation quality. The final German deQuAD dataset contains 130k training and 11k test samples.; Copyright (c) 2021 Fang Xu, Deutsche Telekom AG ",natural-language-processing,question-answering,https://huggingface.co/deutsche-telekom/bert-multi-english-german-squad2,,License: mit,,German; English; multilingual,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 29,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1634,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
bert-base-polish-uncased-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Polish version of BERT language model is here! It is now available in two variants: cased and uncased, both can be downloaded and used via HuggingFace transformers library. I recommend using the cased model, more info on the differences and benchmark results below. ; ; Below is the list of corpora used along with the output of wc command (counting lines, words and characters). These corpora were divided into sentences with srxsegmenter (see references), concatenated and tokenized with HuggingFace BERT Tokenizer. ; Polbert is released via HuggingFace Transformers library.; For an example use as language model, see this notebook file. ",natural-language-processing,fill-mask,https://huggingface.co/dkleczek/bert-base-polish-uncased-v1,,,,Polish,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7199,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
letr-sol-profanity-filter,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-classification,https://huggingface.co/dobbytk/letr-sol-profanity-filter,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 76,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 436.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
kenlm,,,,,,,,,"This repo contains several KenLM models trained on different tokenized datasets and languages.KenLM models are probabilistic n-gram languge models that models. One use case of these models consist on fast perplexity estimation for filtering or sampling large datasets. For example, one could use a KenLM model trained on French Wikipedia to run inference on a large dataset and filter out samples that are very unlike to appear on Wikipedia (high perplexity), or very simple non-informative sentences that could appear repeatedly (low perplexity).; At the root of this repo you will find different directories named after the dataset models were trained on (e.g. wikipedia, oscar). Within each directory, you will find several models trained on different language subsets of the dataset (e.g. en (English), es (Spanish), fr (French)). For each language you will find three different files; The models have been trained using some of the preprocessing steps from cc_net, in particular replacing numbers with zeros and normalizing punctuation. So, it is important to keep the default values for the parameters: lower_case, remove_accents, normalize_numbers and punctuation when using the pre-trained models in order to replicate the same pre-processing steps at inference time.; In the example above we see that, since Wikipedia is a collection of encyclopedic articles, a KenLM model trained on it will naturally give lower perplexity scores to sentences with formal language and no grammar mistakes than colloquial sentences with grammar mistakes.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/edugp/kenlm,,License: mit,Datasets: wikipedia; oscar,24 languages,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 20,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 20.09KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
text2tags,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"The model has been trained on a collection of 28k news articles with tags. Its purpose is to create tags suitable for the given article. We can use this model also for information-retrieval purposes (GenQ), fine-tuning sentence-transformers for asymmetric semantic search. ; If you like this project, consider supporting it with a cup of coffee! ???
; 
 
    Pieter Bruegel the Elder, The Fight Between Carnival and Lent, 1559
; Sample code with an article from IlPost:; Assuming paragraphs are divided by: '\n\n'.",natural-language-processing,summarization,https://huggingface.co/efederici/text2tags,,,,Italian,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 48,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 618.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base-turkish-cased-mean-nli-stsb-tr,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. The model was trained on Turkish machine translated versions of NLI and STS-b datasets, using example training scripts from sentence-transformers GitHub repository.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; Evaluation results on test and development sets are given below:",natural-language-processing,sentence-similarity,https://huggingface.co/emrecan/bert-base-turkish-cased-mean-nli-stsb-tr,,License: apache-2.0,Datasets: nli_tr; emrecan/stsb-mt-turkish,Turkish,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 815,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 444.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
guwenbert-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"; This is a RoBERTa model pre-trained on Classical Chinese. You can fine-tune GuwenBERT for downstream tasks, such as sentence breaking, punctuation, named entity recognition, and so on.; For more information about RoBERTa, take a look at the RoBERTa's offical repo.; The training data is daizhige dataset (殆知阁古代文献) which is contains of 15,694 books in Classical Chinese, covering Buddhism, Confucianism, Medicine, History, Zi, Yi, Yizang, Shizang, Taoism, and Jizang. 
76% of them are punctuated.
The total number of characters is 1.7B (1,743,337,673).
All traditional Characters are converted to simplified characters.
The vocabulary is constructed from this data set and the size is 23,292.; The models are initialized with hfl/chinese-roberta-wwm-ext and then pre-trained with a 2-step strategy.
In the first step, the model learns MLM with only word embeddings updated during training, until convergence. In the second step, all parameters are updated during training.",natural-language-processing,fill-mask,https://huggingface.co/ethanyt/guwenbert-base,,License: apache-2.0,,Chinese,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1326,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 834.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
guwenbert-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"; This is a RoBERTa model pre-trained on Classical Chinese. You can fine-tune GuwenBERT for downstream tasks, such as sentence breaking, punctuation, named entity recognition, and so on.; For more information about RoBERTa, take a look at the RoBERTa's offical repo.; The training data is daizhige dataset (殆知阁古代文献) which is contains of 15,694 books in Classical Chinese, covering Buddhism, Confucianism, Medicine, History, Zi, Yi, Yizang, Shizang, Taoism, and Jizang. 
76% of them are punctuated.
The total number of characters is 1.7B (1,743,337,673).
All traditional Characters are converted to simplified characters.
The vocabulary is constructed from this data set and the size is 23,292.; The models are initialized with hfl/chinese-roberta-wwm-ext-large and then pre-trained with a 2-step strategy.
In the first step, the model learns MLM with only word embeddings updated during training, until convergence. In the second step, all parameters are updated during training.",natural-language-processing,fill-mask,https://huggingface.co/ethanyt/guwenbert-large,,License: apache-2.0,,Chinese,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 95,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
drln,,,,,,,,,"DRLN model pre-trained on DIV2K (800 images training, augmented to 4000 images, 100 images validation) for 2x, 3x and 4x image super resolution. It was introduced in the paper Densely Residual Laplacian Super-resolution by Anwar et al. (2020) and first released in this repository. ; The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling and model upscaling.; ; Super-Resolution convolutional neural networks have recently demonstrated high-quality restoration for single images. However, existing algorithms often require very deep architectures and long training times. Furthermore, current convolutional neural networks for super-resolution are unable to exploit features at multiple scales and weigh them equally, limiting their learning capability. In this exposition, we present a compact and accurate super-resolution algorithm namely, Densely Residual Laplacian Network (DRLN). The proposed network employs cascading residual on the residual structure to allow the flow of low-frequency information to focus on learning high and mid-level features. In addition, deep supervision is achieved via the densely concatenated residual blocks settings, which also helps in learning from high-level complex features. Moreover, we propose Laplacian attention to model the crucial features to learn the inter and intra-level dependencies between the feature maps. Furthermore, comprehensive quantitative and qualitative evaluations on low-resolution, noisy low-resolution, and real historical image benchmark datasets illustrate that our DRLN algorithm performs favorably against the state-of-the-art methods visually and accurately.; You can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.",,,https://huggingface.co/eugenesiow/drln,Paper: https://arxiv.org/pdf/1906.12021.pdf; https://arxiv.org/pdf/2104.07566.pdf,License: apache-2.0,Datasets: eugenesiow/Div2k; eugenesiow/Set5; eugenesiow/Set14; eugenesiow/BSD100; eugenesiow/Urban100,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 939,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 415.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
blenderbot-3B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",natural-language-processing,conversational,https://huggingface.co/facebook/blenderbot-3B,Paper: https://arxiv.org/pdf/1907.06616.pdf,License: apache-2.0,Datasets: blended_skill_talk,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 101,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7944,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
convnext-large-224,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,"ConvNeXT model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper A ConvNet for the 2020s by Liu et al. and first released in this repository. ; Disclaimer: The team releasing ConvNeXT did not write a model card for this model so this model card has been written by the Hugging Face team.; ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and ""modernized"" its design by taking the Swin Transformer as inspiration.; ; You can use the raw model for image classification. See the model hub to look for
fine-tuned versions on a task that interests you.",computer-vision,image-classification,https://huggingface.co/facebook/convnext-large-224,Paper: https://arxiv.org/pdf/2201.03545.pdf,License: apache-2.0,Datasets: imagenet-1k,,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1128,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
detr-resnet-50-panoptic,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c914a5f52efbcfb97cd1_Image%20Segmentation.svg,,"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository. ; Disclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.; The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. ; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.; DETR can be naturally extended to perform panoptic segmentation, by adding a mask head on top of the decoder outputs.",computer-vision,image-segmentation,https://huggingface.co/facebook/detr-resnet-50-panoptic,Paper: https://arxiv.org/pdf/2005.12872.pdf,License: apache-2.0,Datasets: coco,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 77,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11828,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 172.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 75
dino-vits16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository. ; Disclaimer: The team releasing DINO did not write a model card for this model so this model card has been written by the Hugging Face team.; The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. ; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; Note that this model does not include any fine-tuned heads. ",multimodel,feature-extraction,https://huggingface.co/facebook/dino-vits16,Paper: https://arxiv.org/pdf/2104.14294.pdf,License: apache-2.0,Datasets: imagenet-1k,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5829,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 86.7MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
hubert-large-ll60k,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"Facebook's Hubert; The large model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.; Note: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.; The model was pretrained on Libri-Light.; Paper",multimodel,feature-extraction,https://huggingface.co/facebook/hubert-large-ll60k,Paper: https://arxiv.org/pdf/2106.07447.pdf,License: apache-2.0,Datasets: libri-light,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 522561,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
hubert-large-ls960-ft,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Facebook's Hubert; The large model fine-tuned on 960h of Librispeech on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. ; The model is a fine-tuned version of hubert-large-ll60k.; Paper; Authors: Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed",audio,automatic-speech-recognition,https://huggingface.co/facebook/hubert-large-ls960-ft,Paper: https://arxiv.org/pdf/2106.07447.pdf,License: apache-2.0,Datasets: libri-light; librispeech_asr,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 36,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 45370,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
maskformer-swin-tiny-coco,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c914a5f52efbcfb97cd1_Image%20Segmentation.svg,,"MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. ; Disclaimer: The team releasing MaskFormer did not write a model card for this model so this model card has been written by the Hugging Face team.; MaskFormer addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.; ; You can use this particular checkpoint for semantic segmentation. See the model hub to look for other
fine-tuned versions on a task that interests you.",computer-vision,image-segmentation,https://huggingface.co/facebook/maskformer-swin-tiny-coco,Paper: https://arxiv.org/pdf/2107.06278.pdf,License: other,Datasets: coco,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1009,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 167.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
rag-token-nq,,,,,,,,,"This is the RAG-Token Model of the the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks 
by Patrick Lewis, Ethan Perez, Aleksandara Piktus et al.; The model is a uncased model, which means that capital letters are simply converted to lower-case letters.; The model consits of a question_encoder, retriever and a generator. The retriever extracts relevant passages from the wiki_dpr train datasets, which is linked above.
The question_encoder and retriever are based on facebook/dpr-question_encoder-single-nq-base and facebook/bart-large, which were jointly finetuned on 
on the wiki_dpr QA dataset in an end-to-end fashion.; Note: In the usage example below only the dummy retriever of wiki_dpr is used because the complete lecagy index requires over 75 GB of RAM.
The model can generate answers to any factoid question as follows:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/facebook/rag-token-nq,Paper: https://arxiv.org/pdf/2005.11401.pdf,License: apache-2.0,Datasets: wiki_dpr,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 29,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5932,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
tts_transformer-ar-cv7,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9511aaa882b322dc3d8_Text-to-Speech.svg,,Transformer text-to-speech model from fairseq S^2 (paper/code):; See also fairseq S^2 example.,audio,text-to-speech,https://huggingface.co/facebook/tts_transformer-ar-cv7,Paper: https://arxiv.org/pdf/1809.08895.pdf; https://arxiv.org/pdf/2109.06912.pdf,,Datasets: common_voice,Arabic,Fairseq,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 262,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 722.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
vit-mae-base,,,,,,,,,"Vision Transformer (ViT) model pre-trained using the MAE method. It was introduced in the paper Masked Autoencoders Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick and first released in this repository. ; Disclaimer: The team releasing MAE did not write a model card for this model so this model card has been written by the Hugging Face team.; The Vision Transformer (ViT) is a transformer encoder model (BERT-like). Images are presented to the model as a sequence of fixed-size patches.; During pre-training, one randomly masks out a high portion (75%) of the image patches. First, the encoder is used to encode the visual patches. Next, a learnable (shared) mask token is added at the positions of the masked patches. The decoder takes the encoded visual patches and mask tokens as input and reconstructs raw pixel values for the masked positions.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder.",,,https://huggingface.co/facebook/vit-mae-base,Paper: https://arxiv.org/pdf/2111.06377.pdf,License: apache-2.0,Datasets: imagenet-1k,,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 54456,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 896.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
wav2vec2-base,,,,,,,,,"Facebook's Wav2Vec2; The base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. ; Note: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.; Paper; Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli",,,https://huggingface.co/facebook/wav2vec2-base,Paper: https://arxiv.org/pdf/2006.11477.pdf,License: apache-2.0,Datasets: librispeech_asr,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 78058,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 380.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
wav2vec2-large-robust,,,,,,,,,"Facebook's Wav2Vec2; The large model pretrained on 16kHz sampled speech audio. 
Speech datasets from multiple domains were used to pretrain the model:; When using the model make sure that your speech input is also sampled at 16Khz.; Note: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.; Paper Robust Wav2Vec2",,,https://huggingface.co/facebook/wav2vec2-large-robust,Paper: https://arxiv.org/pdf/2104.01027.pdf,License: apache-2.0,Datasets: libri_light; common_voice; switchboard; fisher,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3411,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
wav2vec2-large-xlsr-53,,,,,,,,,"Facebook's XLSR-Wav2Vec2; The base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. Note that this model should be fine-tuned on a downstream task, like Automatic Speech Recognition. Check out this blog for more information.; Paper; Authors: Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli; Abstract
This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72% compared to the best known results. On BABEL, our approach improves word error rate by 16% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages.",,,https://huggingface.co/facebook/wav2vec2-large-xlsr-53,Paper: https://arxiv.org/pdf/2006.13979.pdf,License: apache-2.0,Datasets: common_voice,multilingual,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 50,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 37577,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
wmt19-ru-en,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,"This is a ported version of fairseq wmt19 transformer for ru-en.; For more details, please see, Facebook FAIR's WMT19 News Translation Task Submission.; The abbreviation FSMT stands for FairSeqMachineTranslation; All four models are available:; Pretrained weights were left identical to the original model released by fairseq. For more details, please, see the paper.",natural-language-processing,translation,https://huggingface.co/facebook/wmt19-ru-en,Paper: https://arxiv.org/pdf/1907.06616.pdf,License: apache-2.0,Datasets: wmt19,Russian; English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 28879,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
bert-restore-punctuation,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"; This a bert-base-uncased model finetuned for punctuation restoration on Yelp Reviews. ; The model predicts the punctuation and upper-casing of plain, lower-cased text. An example use case can be ASR output. Or other cases when text has lost punctuation.; This model is intended for direct use as a punctuation restoration model for the general English language. Alternatively, you can use this for further fine-tuning on domain-specific texts for punctuation restoration tasks.; Model restores the following punctuations -- [! ? . , - : ; ' ]",natural-language-processing,token-classification,https://huggingface.co/felflare/bert-restore-punctuation,,License: mit,Datasets: yelp_polarity,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 47,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11001,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 10
indo-medical-bert-base-uncased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,,natural-language-processing,fill-mask,https://huggingface.co/firqaaa/indo-medical-bert-base-uncased,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 334.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
pos-english,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"This is the standard part-of-speech tagging model for English that ships with Flair.; F1-Score: 98,19 (Ontonotes); Predicts fine-grained POS tags:; Based on Flair embeddings and LSTM-CRF.; Requires: Flair (pip install flair)",natural-language-processing,token-classification,https://huggingface.co/flair/pos-english,,,Datasets: ontonotes,English,PyTorch; Flair,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 89644,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 249.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
flaubert_base_uncased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"FlauBERT is a French BERT trained on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer.; Along with FlauBERT comes FLUE: an evaluation setup for French NLP systems similar to the popular GLUE benchmark. The goal is to enable further reproducible experiments in the future and to share models and progress on the French language.For more details please refer to the official website.; Note: flaubert-small-cased is partially trained so performance is not guaranteed. Consider using it for debugging purpose only.; Notes: if your transformers version is <=2.10.0, modelname should take one
of the following values:; If you use FlauBERT or the FLUE Benchmark for your scientific publication, or if you find the resources in this repository useful, please cite one of the following papers:",natural-language-processing,fill-mask,https://huggingface.co/flaubert/flaubert_base_uncased,,License: mit,Datasets: flaubert,French,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 12691,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 552.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
multi-QA_v1-mpnet-asymmetric-A,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"SentenceTransformers is a set of models and frameworks that enable training and generating sentence embeddings from given data. The generated sentence embeddings can be utilized for Clustering, Semantic Search and other tasks. We used two separate pretrained mpnet-base models and trained them using contrastive learning objective. Question and answer pairs from StackExchange and other datasets were used as training data to make the model robust to Question / Answer embedding similarity.; We developed this model during the 
Community week using JAX/Flax for NLP & CV, 
organized by Hugging Face. We developed this model as part of the project:
Train the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well
as assistance from Google’s Flax, JAX, and Cloud team members about efficient deep learning frameworks.; This model set is intended to be used as a sentence encoder for a search engine. Given an input sentence, it ouptuts a vector which captures 
the sentence semantic information. The sentence vector may be used for semantic-search, clustering or sentence similarity tasks.; Two models should be used on conjunction for Semantic Search purposes.; Here is how to use this model to get the features of a given text using SentenceTransformers library:",natural-language-processing,sentence-similarity,https://huggingface.co/flax-sentence-embeddings/multi-QA_v1-mpnet-asymmetric-A,Paper: https://arxiv.org/pdf/2102.07033.pdf; https://arxiv.org/pdf/2104.08727.pdf,,,,PyTorch; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1931,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 439.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
bart-base-chinese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"12/30/2022; An updated version of CPT & Chinese BART are released. In the new version, we changed the following parts:; We initialize the new version of models with the old version of checkpoints with vocabulary alignment. Token embeddings found in the old checkpoints are copied. And other newly added parameters are randomly initialized. We further train the new CPT & Chinese BART 50K steps with batch size 2048, max-seq-length 1024, peak learning rate 2e-5, and warmup ratio 0.1.; The result compared to the previous checkpoints is as followings:; The result shows that the updated models maintain comparative performance compared with previous checkpoints. There are still some cases that the updated model is slightly worse than the previous one, which results from the following reasons: 1) Training additional a few steps did not lead to significant performance improvement; 2) some downstream tasks are not affected by the newly added tokens and longer encoding sequences, but sensitive to the fine-tuning hyperparameters.",natural-language-processing,text2text-generation,https://huggingface.co/fnlp/bart-base-chinese,Paper: https://arxiv.org/pdf/2109.05729.pdf,,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 66,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5655,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 561.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
bart-large-chinese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"12/30/2022; An updated version of CPT & Chinese BART are released. In the new version, we changed the following parts:; We initialize the new version of models with the old version of checkpoints with vocabulary alignment. Token embeddings found in the old checkpoints are copied. And other newly added parameters are randomly initialized. We further train the new CPT & Chinese BART 50K steps with batch size 2048, max-seq-length 1024, peak learning rate 2e-5, and warmup ratio 0.1.; The result compared to the previous checkpoints is as followings:; The result shows that the updated models maintain comparative performance compared with previous checkpoints. There are still some cases that the updated model is slightly worse than the previous one, which results from the following reasons: 1) Training additional a few steps did not lead to significant performance improvement; 2) some downstream tasks are not affected by the newly added tokens and longer encoding sequences, but sensitive to the fine-tuning hyperparameters.",natural-language-processing,text2text-generation,https://huggingface.co/fnlp/bart-large-chinese,Paper: https://arxiv.org/pdf/2109.05729.pdf,,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 39,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1594,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
french-camembert-postag-model,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"The  french-camembert-postag-model is a part of speech tagging model for French that was trained on the free-french-treebank dataset available on 
github. The base tokenizer and model used for training is 'camembert-base'.; It uses the following tags:; More information on the tags can be found here:; http://alpage.inria.fr/statgram/frdep/Publications/crabbecandi-taln2008-final.pdf; The usage of this model follows the common transformers patterns. Here is a short example of its usage:",natural-language-processing,token-classification,https://huggingface.co/gilf/french-camembert-postag-model,,,,French,PyTorch; TensorFlow; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 188550,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
kobart-base-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,,multimodel,feature-extraction,https://huggingface.co/gogamza/kobart-base-v1,,License: mit,,Korean,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1904,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 992.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
byt5-small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"ByT5 is a tokenizer-free version of Google's T5 and generally follows the architecture of MT5.; ByT5 was only pre-trained on mC4 excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; ByT5 works especially well on noisy text data,e.g., google/byt5-small significantly outperforms mt5-small on TweetQA.; Paper: ByT5: Towards a token-free future with pre-trained byte-to-byte models; Authors: Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel ",natural-language-processing,text2text-generation,https://huggingface.co/google/byt5-small,Paper: https://arxiv.org/pdf/1907.06292.pdf; https://arxiv.org/pdf/2105.13626.pdf,License: apache-2.0,Datasets: mc4,102 languages,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 25294,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
mt5-small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Google's mT5; mT5 is pretrained on the mC4 corpus, covering 101 languages:; Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.; Note: mT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; Pretraining Dataset: mC4",natural-language-processing,text2text-generation,https://huggingface.co/google/mt5-small,Paper: https://arxiv.org/pdf/2010.11934.pdf,License: apache-2.0,Datasets: mc4,102 languages,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 50,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 53278,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
pegasus-big_patent,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text2text-generation,https://huggingface.co/google/pegasus-big_patent,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 184,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
t5-v1_1-xxl,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Google's T5 Version 1.1; T5 Version 1.1 includes the following improvements compared to the original T5 model- GEGLU activation in feed-forward hidden layer, rather than ReLU - see here.; Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning.; Pre-trained on C4 only without mixing in the downstream tasks.; no parameter sharing between embedding and classifier layer",natural-language-processing,text2text-generation,https://huggingface.co/google/t5-v1_1-xxl,Paper: https://arxiv.org/pdf/2002.05202.pdf; https://arxiv.org/pdf/1910.10683.pdf,License: apache-2.0,Datasets: c4,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6946,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 89.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 25
tapas-large-finetuned-sqa,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c477a4006f982ddfda4a_Table%20Question%20Answering.svg,,"This model has 2 versions which can be used. The default version corresponds to the tapas_sqa_inter_masklm_large_reset checkpoint of the original Github repository.
This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).; The other (non-default) version which can be used is: ; Disclaimer: The team releasing TAPAS did not write a model card for this model so this model card has been written by
the Hugging Face team and contributors.; TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. 
This means it was pretrained on the raw tables and associated texts only, with no humans labelling them in any way (which is why it
can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language used in tables and associated texts, which can then be used 
to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed
or refuted by the contents of a table. Fine-tuning is done by adding a cell selection head on top of the pre-trained model, and then jointly
train this randomly initialized classification head with the base model on SQA. ",natural-language-processing,table-question-answering,https://huggingface.co/google/tapas-large-finetuned-sqa,Paper: https://arxiv.org/pdf/2004.02349.pdf; https://arxiv.org/pdf/2010.00571.pdf,License: apache-2.0,Datasets: msr_sqa,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 360,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
tapas-large-finetuned-wtq,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c477a4006f982ddfda4a_Table%20Question%20Answering.svg,,"This model has 2 versions which can be used. The default version corresponds to the tapas_wtq_wikisql_sqa_inter_masklm_large_reset checkpoint of the original Github repository.
This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).; The other (non-default) version which can be used is: ; Disclaimer: The team releasing TAPAS did not write a model card for this model so this model card has been written by
the Hugging Face team and contributors.; TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. 
This means it was pretrained on the raw tables and associated texts only, with no humans labelling them in any way (which is why it
can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language used in tables and associated texts, which can then be used 
to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed
or refuted by the contents of a table. Fine-tuning is done by adding a cell selection head and aggregation head on top of the pre-trained model, and then jointly train these randomly initialized classification heads with the base model on SQa, WikiSQL and finally WTQ. ",natural-language-processing,table-question-answering,https://huggingface.co/google/tapas-large-finetuned-wtq,Paper: https://arxiv.org/pdf/2004.02349.pdf; https://arxiv.org/pdf/2010.00571.pdf; https://arxiv.org/pdf/1508.00305.pdf,License: apache-2.0,Datasets: wikitablequestions,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 43,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 10702,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 12
arabic-ner,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,Pretrained BERT-based (arabic-bert-base) Named Entity Recognition model for Arabic.; The pre-trained model can recognize the following entities:; ? ??? ?? ???? ??????? ??????? ?????? ???? ??? ? ?????? ??? ??? ???? ; ??? ????? ??????? ????? ??? ??? ????? ?? ???? ????? ; ? ????? ?????? ???????? ??????? ???? ??????? ??? ????? ?? ??? ????? ?????????,natural-language-processing,token-classification,https://huggingface.co/hatmimoha/arabic-ner,,,,Arabic,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2483,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
character-bert,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,No model card; New: Create and edit this model card directly on the website!,multimodel,feature-extraction,https://huggingface.co/helboukkouri/character-bert,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 74,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 729.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chinese-macbert-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"



; 



; This repository contains the resources in our paper ""Revisiting Pre-trained Models for Chinese Natural Language Processing"", which will be published in ""Findings of EMNLP"". You can read our camera-ready paper through ACL Anthology or arXiv pre-print.; Revisiting Pre-trained Models for Chinese Natural Language ProcessingYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu; You may also interested in,",natural-language-processing,fill-mask,https://huggingface.co/hfl/chinese-macbert-base,Paper: https://arxiv.org/pdf/2004.13922.pdf,License: apache-2.0,,Chinese,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 82,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 23236,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
chinese-macbert-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"



; 



; This repository contains the resources in our paper ""Revisiting Pre-trained Models for Chinese Natural Language Processing"", which will be published in ""Findings of EMNLP"". You can read our camera-ready paper through ACL Anthology or arXiv pre-print.; Revisiting Pre-trained Models for Chinese Natural Language ProcessingYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu; You may also interested in,",natural-language-processing,fill-mask,https://huggingface.co/hfl/chinese-macbert-large,Paper: https://arxiv.org/pdf/2004.13922.pdf,License: apache-2.0,,Chinese,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2468,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
rbt3,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"For further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking. ; Pre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu; This repository is developed based on：https://github.com/google-research/bert; You may also interested in,; More resources by HFL: https://github.com/ymcui/HFL-Anthology",natural-language-processing,fill-mask,https://huggingface.co/hfl/rbt3,Paper: https://arxiv.org/pdf/1906.08101.pdf; https://arxiv.org/pdf/2004.13922.pdf,License: apache-2.0,,Chinese,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7635,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 464.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
sentence_similarity_spanish_es,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",natural-language-processing,sentence-similarity,https://huggingface.co/hiiamsid/sentence_similarity_spanish_es,,,,Spanish,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11826,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 440.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
ConvTasNet_Libri2Mix_sepnoisy_16k,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/hugggof/ConvTasNet_Libri2Mix_sepnoisy_16k,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 20.8MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
elonmusk,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report.",natural-language-processing,text-generation,https://huggingface.co/huggingtweets/elonmusk,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2356,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 513.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
funnyordie,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report.",natural-language-processing,text-generation,https://huggingface.co/huggingtweets/funnyordie,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 513.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
honeytech,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report.",natural-language-processing,text-generation,https://huggingface.co/huggingtweets/honeytech,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 513.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
michaeljackson,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report.",natural-language-processing,text-generation,https://huggingface.co/huggingtweets/michaeljackson,,,,English,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 54,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1009.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
porns_xx,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report.",natural-language-processing,text-generation,https://huggingface.co/huggingtweets/porns_xx,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 214,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 513.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
financial-summarization-pegasus,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. ; It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. ; We provide a simple snippet of how to use this model for the task of financial summarization in PyTorch.; The results before and after the fine-tuning on our dataset are shown below:; You can find more details about this work in the following workshop paper. If you use our model in your research, please consider citing our paper:",natural-language-processing,summarization,https://huggingface.co/human-centered-summarization/financial-summarization-pegasus,Paper: https://arxiv.org/pdf/1912.08777.pdf,,Datasets: xsum,English,PyTorch; TensorFlow; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 78,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4297,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
pangu_2_6B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"PanGu-α is proposed by a joint technical team headed by PCNL. It was first released in this repository  It is the first large-scale Chinese pre-trained language model with 200 billion parameters trained on 2048 Ascend processors using an automatic hybrid parallel training strategy. The whole training process is done on the “Peng Cheng Cloud Brain II” computing platform with the domestic deep learning framework called MindSpore. The PengCheng・PanGu-α pre-training model can support rich applications, has strong few-shot learning capabilities, and has outstanding performance in text generation tasks such as knowledge question and answer, knowledge retrieval, knowledge reasoning, and reading comprehension.; This repository contains PyTorch implementation of PanGu model, with
2.6 billion parameters pretrained weights (FP32 precision), converted from original MindSpore checkpoint.; Currently PanGu model is not supported by transformers, 
so trust_remote_code=True is required to load model implementation in this repo.; Expected output:",natural-language-processing,text-generation,https://huggingface.co/imone/pangu_2_6B,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 617,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 11.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
general_character_bert,,,,,,,,,"Pretrained general_character_bert model 
from the 'CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters' El Boukkouri H., et al., 2020; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/imvladikon/general_character_bert,Paper: https://arxiv.org/pdf/2010.10392.pdf,,Datasets: wikipedia; openwebtext,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 418.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
indobert-base-uncased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"IndoBERT is the Indonesian version of BERT model. We train the model using over 220M words, aggregated from three main sources: ; We trained the model for 2.4M steps (180 epochs) with the final perplexity over the development set being 3.97 (similar to English BERT-base).; This IndoBERT was used to examine IndoLEM - an Indonesian benchmark that comprises of seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. ; The paper is published at the 28th COLING 2020. Please refer to https://indolem.github.io for more details about the benchmarks.; If you use our work, please cite:",natural-language-processing,fill-mask,https://huggingface.co/indolem/indobert-base-uncased,Paper: https://arxiv.org/pdf/2011.00677.pdf,License: mit,"Datasets: 220M words (IndoWiki, IndoWC, News)]",Indonesian,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 970382,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 887.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
berteus-base-cased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"This is the Basque language pretrained model presented in Give your Text Representation Models some Love: the Case for Basque. This model has been trained on a Basque corpus comprising Basque crawled news articles from online newspapers and the Basque Wikipedia. The training corpus contains 224.6 million tokens, of which 35 million come from the Wikipedia.; BERTeus has been tested on four different downstream tasks for Basque: part-of-speech (POS) tagging, named entity recognition (NER), sentiment analysis and topic classification; improving the state of the art for all tasks. See summary of results below:; If using this model, please cite the following paper:",multimodel,feature-extraction,https://huggingface.co/ixa-ehu/berteus-base-cased,Paper: https://arxiv.org/pdf/2004.00033.pdf,,,Basque,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 151,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 996.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-stsb-aug,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; It is used as a demo model within the NLP for Semantic Search course, for the chapter on In-domain Data Augmentation with BERT.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.",natural-language-processing,sentence-similarity,https://huggingface.co/jamescalam/bert-stsb-aug,,,,,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 439.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-stsb-cross-encoder,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers cross encoder model.; It is used as a demo model within the NLP for Semantic Search course, for the chapter on In-domain Data Augmentation with BERT.",natural-language-processing,sentence-similarity,https://huggingface.co/jamescalam/bert-stsb-cross-encoder,,,,,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 37,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 439.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ko-sbert-multitask,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; KorSTS, KorNLI ?? ?????? ?? ??? ??? ??? ? KorSTS ?? ?????? ??? ?????.",natural-language-processing,sentence-similarity,https://huggingface.co/jhgan/ko-sbert-multitask,,,,,PyTorch; TensorFlow; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 323,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 887.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
xlm-roberta-large-xnli,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c42cd649af0365a01a1e_Zero-Shot%20Classification.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This model takes xlm-roberta-large and fine-tunes it on a combination of NLI data in 15 languages. It is intended to be used for zero-shot text classification, such as with the Hugging Face ZeroShotClassificationPipeline.; This model is intended to be used for zero-shot text classification, especially in languages other than English. It is fine-tuned on XNLI, which is a multilingual NLI dataset. The model can therefore be used with any of the languages in the XNLI corpus:; Since the base model was pre-trained trained on 100 different languages, the
model has shown some effectiveness in languages beyond those listed above as
well. See the full list of pre-trained languages in appendix A of the
XLM Roberata paper",natural-language-processing,zero-shot-classification,https://huggingface.co/joeddav/xlm-roberta-large-xnli,Paper: https://arxiv.org/pdf/1911.02116.pdf,License: mit,Datasets: multi_nli; xnli,16 languages,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 129,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7586,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 11
wav2vec2-large-xlsr-53-chinese-zh-cn,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Chinese using the train and validation splits of Common Voice 6.1, CSS10 and ST-CMDS.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:",audio,automatic-speech-recognition,https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn,,License: apache-2.0,Datasets: common_voice,Chinese,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 34,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 63076,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 10
wav2vec2-large-xlsr-53-hungarian,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Hungarian using the train and validation splits of Common Voice 6.1 and CSS10.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:",audio,automatic-speech-recognition,https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-hungarian,,License: apache-2.0,Datasets: common_voice,Hungarian,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1224,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
wav2vec2-large-xlsr-53-japanese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:",audio,automatic-speech-recognition,https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese,,License: apache-2.0,Datasets: common_voice,Japanese,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 27812,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
wav2vec2-large-xlsr-53-russian,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Russian using the train and validation splits of Common Voice 6.1 and CSS10.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:",audio,automatic-speech-recognition,https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-russian,,License: apache-2.0,Datasets: common_voice; mozilla-foundation/common_voice_6_0,Russian,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 688094,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 15
wav2vec2-xls-r-1b-portuguese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Fine-tuned facebook/wav2vec2-xls-r-1b on Portuguese using the train and validation splits of Common Voice 8.0, CORAA, Multilingual TEDx, and Multilingual LibriSpeech.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned by the HuggingSound tool, and thanks to the GPU credits generously given by the OVHcloud :); Using the HuggingSound library:; Writing your own inference script:; If you want to cite this model you can use this:",audio,automatic-speech-recognition,https://huggingface.co/jonatasgrosman/wav2vec2-xls-r-1b-portuguese,,License: apache-2.0,Datasets: mozilla-foundation/common_voice_8_0,Portuguese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1043,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
DialoGPT-small-Creed-Odyssey,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"The app was conceived with the idea of recreating and generate new dialogs for existing games.
In order to generate a dataset for training the steps followed were:",natural-language-processing,text-generation,https://huggingface.co/jonx18/DialoGPT-small-Creed-Odyssey,,,,,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1009.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
hotdog-not-hotdog,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,Autogenerated by HuggingPics???; Create your own image classifier for anything by running the demo on Google Colab.; Report any issues with the demo at the github repo.; ; ,computer-vision,image-classification,https://huggingface.co/julien-c/hotdog-not-hotdog,,,,,PyTorch; TensorBoard; Core ML; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7323,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 343.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 171
SciBERT_patent_reference_extraction,,,,,,,,,"This repository contains a finetuned SciBERT model that can extract references to scientific literature from patents.; See https://github.com/kaesve/patent-citation-extraction and https://arxiv.org/abs/2101.01039 for more information.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/kaesve/SciBERT_patent_reference_extraction,Paper: https://arxiv.org/pdf/2101.01039.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 440.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
low-light-image-enhancement,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"Original Author: Soumik Rakshit 
Date created: 2021/09/18 
HF Contribution: Harveen Singh Chadha
Dataset: LOL Dataset; Zero-Reference Deep Curve Estimation or Zero-DCE formulates low-light image enhancement as the task of estimating an image-specific tonal curve with a deep neural network. In this example, we train a lightweight deep network, DCE-Net, to estimate pixel-wise and high-order tonal curves for dynamic range adjustment of a given image.; Zero-DCE takes a low-light image as input and produces high-order tonal curves as its output. These curves are then used for pixel-wise adjustment on the dynamic range of the input to obtain an enhanced image. The curve estimation process is done in such a way that it maintains the range of the enhanced image and preserves the contrast of neighboring pixels. This curve estimation is inspired by curves adjustment used in photo editing software such as Adobe Photoshop where users can adjust points throughout an image’s tonal range.; Zero-DCE is appealing because of its relaxed assumptions with regard to reference images: it does not require any input/output image pairs during training. This is achieved through a set of carefully formulated non-reference loss functions, which implicitly measure the enhancement quality and guide the training of the network.; Sample Images:",computer-vision,image-to-image,https://huggingface.co/keras-io/low-light-image-enhancement,,License: apache-2.0,,,Keras,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 43,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 347.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
wav2vec2-large-xls-r-300m-Urdu,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the common_voice dataset.
It achieves the following results on the evaluation set:; The following hyperparameters were used during training:",audio,automatic-speech-recognition,https://huggingface.co/kingabzpro/wav2vec2-large-xls-r-300m-Urdu,,License: apache-2.0,Datasets: mozilla-foundation/common_voice_8_0,Urdu,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 166,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
t5-base-qa-summary-emotion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Requires transformers>=4.0.0; This model was finetuned on the CoQa, Squad 2, GoEmotions and CNN/DailyMail.; It achieves a score of F1 79.5 on the Squad 2 dev set and a score of F1 70.6 on the CoQa dev set.; Summarisation and emotion detection has not been evaluated yet.; Kiri makes using state-of-the-art models easy, accessible and scalable.",natural-language-processing,text2text-generation,https://huggingface.co/kiri-ai/t5-base-qa-summary-emotion,,License: apache-2.0,Datasets: coqa; squad_v2; go_emotions; cnn_dailymail,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 241,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Model Description: KLUE BERT base is a pre-trained BERT Model on Korean Language. The developers of KLUE BERT base developed the model in the context of the development of the Korean Language Understanding Evaluation (KLUE) Benchmark.; The model can be used for tasks including topic classification, semantic textual similarity, natural language inference, named entity recognition, and other tasks outlined in the KLUE Benchmark.; The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)). The model developers discuss several ethical considerations related to the model in the paper, including: ; For ethical considerations related to the KLUE Benchmark, also see the paper.",natural-language-processing,fill-mask,https://huggingface.co/klue/bert-base,Paper: https://arxiv.org/pdf/2105.09680.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: cc-by-sa-4.0,,Korean,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 120387,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 891.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
MEETING_SUMMARY,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"Model obtained by Fine Tuning 'facebook/bart-large-xsum' using AMI Meeting Corpus, SAMSUM Dataset, DIALOGSUM Dataset, XSUM Dataset!",natural-language-processing,summarization,https://huggingface.co/knkarthick/MEETING_SUMMARY,,License: apache-2.0,Datasets: cnndaily/newyorkdaily/xsum/samsum/dialogsum/AMI,English,PyTorch; TensorFlow; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 122,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 28703,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 18
bart-large-xsum-samsum,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,This model was obtained by fine-tuning facebook/bart-large-xsum on Samsum dataset.,natural-language-processing,summarization,https://huggingface.co/lidiya/bart-large-xsum-samsum,,License: apache-2.0,Datasets: samsum,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 21648,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
chinese_pretrain_mrc_macbert_large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,,natural-language-processing,question-answering,https://huggingface.co/luhua/chinese_pretrain_mrc_macbert_large,,License: apache-2.0,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 688,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-imdb,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,BERT (bert-large-cased) trained for sentiment classification on the IMDB dataset.; The model was trained on 80% of the IMDB dataset for sentiment classification for three epochs with a learning rate of 1e-5 with the simpletransformers library. The library uses a learning rate schedule.; The model achieved 90% classification accuracy on the validation set.; The full experiment is available in the tlr repo.,natural-language-processing,text-classification,https://huggingface.co/lvwerra/bert-imdb,,,,,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 222,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
distilbert-imdb,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset (training notebook is here).
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",natural-language-processing,text-classification,https://huggingface.co/lvwerra/distilbert-imdb,,License: apache-2.0,Datasets: imdb,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 120327,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 269.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
distilbert-political-tweets,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is a fine-tuned version of distilbert-base-uncased on the m-newhauser/senator-tweets dataset, which contains all tweets made by United States senators during the first year of the Biden Administration.
It achieves the following results on the evaluation set:; The goal of this model is to classify short pieces of text as having either Democratic or Republican sentiment. The model was fine-tuned on 99,693 tweets (51.6% Democrat, 48.4% Republican) made by US senators in 2021.; Model accuracy may not hold up on pieces of text longer than a tweet.; The following hyperparameters were used during training:",natural-language-processing,text-classification,https://huggingface.co/m-newhauser/distilbert-political-tweets,,License: lgpl-3.0,Datasets: m-newhauser/senator-tweets,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 52,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 537.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
marefa-ner,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"

; Version: 1.3; Last Update: 3-12-2021; Marefa-NER is a Large Arabic Named Entity Recognition (NER) model built on a completely new dataset and targets to extract up to 9 different types of entities; ????? ??????? ?????? ????? ????. ????? ???? ???? ?? ??? ???????? ????????? ?? ????? ???????. 
???? ?????? ??????? ????? ??? 9 ????? ?????? ?? ????? ????",natural-language-processing,token-classification,https://huggingface.co/marefa-nlp/marefa-ner,,,Datasets: Marefa-NER,Arabic,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5734,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
toxic-comment-model,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is a fine-tuned version of the DistilBERT model to classify toxic comments. ; You can use the model with the following code.; This model is intended to use for classify toxic online classifications. However, one limitation of the model is that it performs poorly for some comments that mention a specific identity subgroup, like Muslim. The following table shows a evaluation score for different identity group. You can learn the specific meaning of this metrics here. But basically, those metrics shows how well a model performs for a specific group. The larger the number, the better.; The table above shows that the model performs poorly for the muslim and jewish group. In fact, you pass the sentence ""Muslims are people who follow or practice Islam, an Abrahamic monotheistic religion."" Into the model, the model will classify it as toxic. Be mindful for this type of potential bias.; The training data comes this Kaggle competition. We use 10% of the train.csv data to train the model.",natural-language-processing,text-classification,https://huggingface.co/martin-ha/toxic-comment-model,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 108050,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 269.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
korean_sentiment,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,,natural-language-processing,text-classification,https://huggingface.co/matthewburke/korean_sentiment,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2337,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 499.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
recipenlg,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Model accompanying our INLG 2020 paper: RecipeNLG: A Cooking Recipes Dataset for Semi-Structured Text Generation; Please visit the website of our project: recipenlg.cs.put.poznan.pl to download it.; Yes, sure! If you feel some information is missing in our paper, please check first in our thesis, which is much more detailed. In case of further questions, you're invited to send us a github issue, we will respond as fast as we can!",natural-language-processing,text-generation,https://huggingface.co/mbien/recipenlg,,,,,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 321,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
german-news-sentiment-bert,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Sentiment analysis model based on https://huggingface.co/oliverguhr/german-sentiment-bert, with additional training on German news texts about migration.; This model is part of the project https://github.com/text-analytics-20/news-sentiment-development, which explores sentiment development in German news articles about migration between 2007 and 2019.; Code for inference (predicting sentiment polarity) on raw text can be found at https://github.com/text-analytics-20/news-sentiment-development/blob/main/sentiment_analysis/bert.py; If you are not interested in polarity but just want to predict discrete class labels (0: positive, 1: negative, 2: neutral), you can also use the model with Oliver Guhr's germansentiment package as follows:; First install the package from PyPI:",natural-language-processing,text-classification,https://huggingface.co/mdraw/german-news-sentiment-bert,,,,,PyTorch; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1145,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
flair-arabic-multi-ner,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"Training was conducted over 94 epochs, using a linear decaying learning rate of 2e-05, starting from 0.225 and a batch size of 32 with GloVe and Flair forward and backward embeddings.;  Due to the right-to-left in left-to-right context, some formatting errors might occur. and your code might appear like this, (link accessed on 2020-10-27) ; if you use this model, please consider citing this work:",natural-language-processing,token-classification,https://huggingface.co/megantosh/flair-arabic-multi-ner,,License: apache-2.0,Datasets: AQMAR; ANERcorp,Arabic; English,PyTorch; Flair,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 766,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 550.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
DialoGPT-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. 
The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test.
The model is trained on 147M multi-turn dialogue from Reddit discussion thread. ; Please find the information about preprocessing, training and full details of the DialoGPT in the original DialoGPT repository; ArXiv paper: https://arxiv.org/abs/1911.00536; Now we are ready to try out how the model works as a chatting partner!",natural-language-processing,conversational,https://huggingface.co/microsoft/DialoGPT-large,Paper: https://arxiv.org/pdf/1911.00536.pdf,License: mit,,,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 177,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 45489,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 93
DialoGPT-small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. 
The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test.
The model is trained on 147M multi-turn dialogue from Reddit discussion thread. ; Please find the information about preprocessing, training and full details of the DialoGPT in the original DialoGPT repository; ArXiv paper: https://arxiv.org/abs/1911.00536; Now we are ready to try out how the model works as a chatting partner!",natural-language-processing,conversational,https://huggingface.co/microsoft/DialoGPT-small,Paper: https://arxiv.org/pdf/1911.00536.pdf,License: mit,,,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 52,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 28586,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 23
beit-base-patch16-384,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,"BEiT model pre-trained in a self-supervised fashion on ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository. ; Disclaimer: The team releasing BEiT did not write a model card for this model so this model card has been written by the Hugging Face team.; The BEiT model is a Vision Transformer (ViT), which is a transformer encoder model (BERT-like). In contrast to the original ViT model, BEiT is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. The pre-training objective for the model is to predict visual tokens from the encoder of OpenAI's DALL-E's VQ-VAE, based on masked patches.
Next, the model was fine-tuned in a supervised fashion on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. Contrary to the original ViT models, BEiT models do use relative position embeddings (similar to T5) instead of absolute position embeddings, and perform classification of images by mean-pooling the final hidden states of the patches, instead of placing a linear layer on top of the final hidden state of the [CLS] token.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image. Alternatively, one can mean-pool the final hidden states of the patch embeddings, and place a linear layer on top of that.",computer-vision,image-classification,https://huggingface.co/microsoft/beit-base-patch16-384,Paper: https://arxiv.org/pdf/2106.08254.pdf,License: apache-2.0,Datasets: imagenet; imagenet-21k,,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 707,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 726.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
codebert-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"Pretrained weights for CodeBERT: A Pre-Trained Model for Programming and Natural Languages.; The model is trained on bi-modal data (documents & code) of CodeSearchNet; This model is initialized with Roberta-base and trained with MLM+RTD objective (cf. the paper).; Please see the official repository for scripts that support ""code search"" and ""code-to-document generation"".",multimodel,feature-extraction,https://huggingface.co/microsoft/codebert-base,Paper: https://arxiv.org/pdf/2002.08155.pdf,,,,PyTorch; TensorFlow; JAX; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 110,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 777087,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
deberta-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. ; Please check the official repository for more details and updates.; We present the dev results on SQuAD 1.1/2.0 and MNLI tasks.; If you find DeBERTa useful for your work, please cite the following paper:",natural-language-processing,fill-mask,https://huggingface.co/microsoft/deberta-base,Paper: https://arxiv.org/pdf/2006.03654.pdf,License: mit,,English,PyTorch; TensorFlow; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 46,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5003278,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
deberta-v2-xlarge,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data.; Please check the official repository for more details and updates.; This is the DeBERTa V2 xlarge model with 24 layers, 1536 hidden size. The total parameters are 900M and it is trained with 160GB raw data.; We present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.; If you find DeBERTa useful for your work, please cite the following paper:",natural-language-processing,fill-mask,https://huggingface.co/microsoft/deberta-v2-xlarge,Paper: https://arxiv.org/pdf/2006.03654.pdf,License: mit,,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 100247,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
graphcodebert-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"GraphCodeBERT is a graph-based pre-trained model based on the Transformer architecture for programming language, which also considers data-flow information along with code sequences. GraphCodeBERT consists of 12 layers, 768 dimensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512. The model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages. ; More details can be found in the paper by Guo et. al.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by the Hugging Face community members.",natural-language-processing,fill-mask,https://huggingface.co/microsoft/graphcodebert-base,Paper: https://arxiv.org/pdf/2009.08366.pdf,,,,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 55448,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
layoutlm-base-uncased,,,,,,,,,"Multimodal (text + layout/format + image) pre-training for document AI; Microsoft Document AI | GitHub; LayoutLM is a simple but effective pre-training method of text and layout for document image understanding and information extraction tasks, such as form understanding and receipt understanding. LayoutLM archives the SOTA results on multiple datasets. For more details, please refer to our paper: ; LayoutLM: Pre-training of Text and Layout for Document Image Understanding
Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, KDD 2020; We pre-train LayoutLM on IIT-CDIP Test Collection 1.0* dataset with two settings. ",,,https://huggingface.co/microsoft/layoutlm-base-uncased,Paper: https://arxiv.org/pdf/1912.13318.pdf,,,,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3081133,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 905.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
layoutlm-large-uncased,,,,,,,,,"Multimodal (text + layout/format + image) pre-training for document AI; Microsoft Document AI | GitHub; LayoutLM is a simple but effective pre-training method of text and layout for document image understanding and information extraction tasks, such as form understanding and receipt understanding. LayoutLM archives the SOTA results on multiple datasets. For more details, please refer to our paper: ; LayoutLM: Pre-training of Text and Layout for Document Image Understanding
Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, KDD 2020; We pre-train LayoutLM on IIT-CDIP Test Collection 1.0* dataset with two settings. ",,,https://huggingface.co/microsoft/layoutlm-large-uncased,Paper: https://arxiv.org/pdf/1912.13318.pdf,,,,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1067,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
layoutxlm-base,,,,,,,,,"Multimodal (text + layout/format + image) pre-training for document AI; LayoutXLM is a multilingual variant of LayoutLMv2.; The documentation of this model in the Transformers library can be found here.; Microsoft Document AI | GitHub; LayoutXLM is a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding. Experiment results show that it has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset.",,,https://huggingface.co/microsoft/layoutxlm-base,Paper: https://arxiv.org/pdf/2104.08836.pdf,License: cc-by-nc-sa-4.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 39,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 167222,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
markuplm-large,,,,,,,,,"Multimodal (text +markup language) pre-training for Document AI; MarkupLM is a simple but effective multi-modal pre-training method of text and markup language for visually-rich document understanding and information extraction tasks, such as webpage QA and webpage information extraction. MarkupLM archives the SOTA results on multiple datasets. For more details, please refer to our paper:; MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding  Junlong Li, Yiheng Xu, Lei Cui, Furu Wei; We refer to the docs and demo notebooks.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/microsoft/markuplm-large,Paper: https://arxiv.org/pdf/2110.08518.pdf,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 866,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 753.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
trocr-base-printed,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.",multimodel,image-to-text,https://huggingface.co/microsoft/trocr-base-printed,Paper: https://arxiv.org/pdf/2109.10282.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 86,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 30041,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 85
trocr-base-stage1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.",multimodel,image-to-text,https://huggingface.co/microsoft/trocr-base-stage1,Paper: https://arxiv.org/pdf/2109.10282.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2352,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
trocr-large-handwritten,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.",multimodel,image-to-text,https://huggingface.co/microsoft/trocr-large-handwritten,Paper: https://arxiv.org/pdf/2109.10282.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6512,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
trocr-large-printed,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.",multimodel,image-to-text,https://huggingface.co/microsoft/trocr-large-printed,Paper: https://arxiv.org/pdf/2109.10282.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 19860,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
wavlm-base-plus-sv,,,,,,,,,"Microsoft's WavLM; The model was pretrained on 16kHz sampled speech audio with utterance and speaker contrastive loss. When using the model, make sure that your speech input is also sampled at 16kHz. ; Note: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.; The model was pre-trained on:; Paper: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",,,https://huggingface.co/microsoft/wavlm-base-plus-sv,Paper: https://arxiv.org/pdf/1912.07875.pdf; https://arxiv.org/pdf/2106.06909.pdf; https://arxiv.org/pdf/2101.00390.pdf; https://arxiv.org/pdf/2110.13900.pdf,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 10247,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 405.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
anekdot_funny2_rugpt3Small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/mmm-da/anekdot_funny2_rugpt3Small,,,,,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base-cased-goemotions-original,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/monologg/bert-base-cased-goemotions-original,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 109446,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 433.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base-spanish-wwm-cased-finetuned-spa-squad2-es,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,"This model is provided by BETO team and fine-tuned on SQuAD-es-v2.0 for Q&A downstream task.; Language model ('dccuchile/bert-base-spanish-wwm-cased'):; BETO is a BERT model trained on a big Spanish corpus. BETO is of size similar to a BERT-Base and was trained with the Whole Word Masking technique. Below you find Tensorflow and Pytorch checkpoints for the uncased and cased versions, as well as some results for Spanish benchmarks comparing BETO with Multilingual BERT as well as other (not BERT-based) models.; SQuAD-es-v2.0; The model was trained on a Tesla P100 GPU and 25GB of RAM with the following command:",natural-language-processing,question-answering,https://huggingface.co/mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es,,,,Spanish,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1118,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
distilbert-base-multi-cased-finetuned-typo-detection,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,distilbert-base-multilingual-cased fine-tuned on GitHub Typo Corpus for typo detection (using NER style); Dataset: GitHub Typo Corpus ? for 15 languages; Fine-tune script on NER dataset provided by Huggingface ???♂?; Fast usage with pipelines ?; It works?! We typed wrong Add and middleware,natural-language-processing,token-classification,https://huggingface.co/mrm8488/distilbert-base-multi-cased-finetuned-typo-detection,,,,11 languages,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 46,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
distilroberta-finetuned-financial-news-sentiment-analysis,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is a fine-tuned version of distilroberta-base on the financial_phrasebank dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",natural-language-processing,text-classification,https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis,,License: apache-2.0,Datasets: financial_phrasebank,,PyTorch; TensorBoard; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 67,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 90616,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 660.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
t5-base-finetuned-summarize-news,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"All credits to Abhishek Kumar Mishra; Google's T5 base fine-tuned on News Summary dataset for summarization downstream task.; The T5 model was presented in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu in Here the abstract:; Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.; ",natural-language-processing,text2text-generation,https://huggingface.co/mrm8488/t5-base-finetuned-summarize-news,Paper: https://arxiv.org/pdf/1910.10683.pdf,,,English,PyTorch; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 265666,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
bert-base-portuguese-cased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"; BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.; For further information or requests, please go to BERTimbau repository.; If you use our work, please cite:",natural-language-processing,fill-mask,https://huggingface.co/neuralmind/bert-base-portuguese-cased,,License: mit,Datasets: brWaC,Portuguese,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 78,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 368010,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
fb-bart-large-finetuned-trade-the-event-finance-summarizer,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"This model was trained from scratch on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",natural-language-processing,summarization,https://huggingface.co/nickmuchi/fb-bart-large-finetuned-trade-the-event-finance-summarizer,,,,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 251,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
deformable-detr,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c921e73e7b6d3bf06402_Object%20Detection.svg,,"Deformable DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository. ; Disclaimer: The team releasing Deformable DETR did not write a model card for this model so this model card has been written by the Hugging Face team.; The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. ; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.; ",computer-vision,object-detection,https://huggingface.co/SenseTime/deformable-detr,Paper: https://arxiv.org/pdf/2010.04159.pdf,License: apache-2.0,Datasets: coco,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 12226,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 161.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
glpn-kitti,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9ca967baeceb7975cf3_Depth_Estimation-2.svg,,"Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository. ; Disclaimer: The team releasing GLPN did not write a model card for this model so this model card has been written by the Hugging Face team.; GLPN uses SegFormer as backbone and adds a lightweight head on top for depth estimation.; ; You can use the raw model for monocular depth estimation. See the model hub to look for
fine-tuned versions on a task that interests you.",computer-vision,depth-estimation,https://huggingface.co/vinvino02/glpn-kitti,Paper: https://arxiv.org/pdf/2201.07436.pdf,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 28601,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 245.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
legal-bert-base-uncased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications.  To pre-train the different variations of LEGAL-BERT, we collected 12 GB of diverse English legal text from several fields (e.g., legislation, court cases,  contracts) scraped from publicly available resources. Sub-domain variants (CONTRACTS-, EURLEX-, ECHR-) and/or general LEGAL-BERT perform better than using BERT out of the box for domain-specific tasks. A light-weight model (33% the size of BERT-BASE) pre-trained from scratch on legal data with competitive performance is also available.
; I. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Aletras and I. Androutsopoulos. ""LEGAL-BERT: The Muppets straight out of Law School"". In Findings of Empirical Methods in Natural Language Processing (EMNLP 2020) (Short Papers), to be held online, 2020. (https://aclanthology.org/2020.findings-emnlp.261); The pre-training corpora of LEGAL-BERT include:; 116,062 documents of EU legislation, publicly available from EURLEX (http://eur-lex.europa.eu), the repository of EU Law running under the EU Publication Office.; 61,826 documents of UK legislation, publicly available from the UK legislation portal (http://www.legislation.gov.uk).",natural-language-processing,fill-mask,https://huggingface.co/nlpaueb/legal-bert-base-uncased,,License: cc-by-sa-4.0,,English,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 69,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 61596,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 14
legal-led-base-16384,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"This is a Longformer Encoder Decoder (led-base-16384) model for the legal domain, trained for long document abstractive summarization task. The length of the document can be upto 16,384 tokens.; The legal-led-base-16384 model was trained on sec-litigation-releases dataset consisting more than 2700 litigation releases and complaints.; When the model is used for summarizing legal documents, it achieves the following results:; Inference API has been turned off for this model.",natural-language-processing,summarization,https://huggingface.co/nsi319/legal-led-base-16384,,License: mit,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 91,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 649.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mit-b0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,"SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. ; Disclaimer: The team releasing SegFormer did not write a model card for this model so this model card has been written by the Hugging Face team.; SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.; This repository only contains the pre-trained hierarchical Transformer, hence it can be used for fine-tuning purposes.; You can use the model for fine-tuning of semantic segmentation. See the model hub to look for fine-tuned versions on a task that interests you.",computer-vision,image-classification,https://huggingface.co/nvidia/mit-b0,Paper: https://arxiv.org/pdf/2105.15203.pdf,License: other,Datasets: imagenet_1k,,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 150409,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 29.1MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 14
segformer-b5-finetuned-cityscapes-1024-1024,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c914a5f52efbcfb97cd1_Image%20Segmentation.svg,,"SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. ; Disclaimer: The team releasing SegFormer did not write a model card for this model so this model card has been written by the Hugging Face team.; SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.; You can use the raw model for semantic segmentation. See the model hub to look for fine-tuned versions on a task that interests you.; Here is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:",computer-vision,image-segmentation,https://huggingface.co/nvidia/segformer-b5-finetuned-cityscapes-1024-1024,Paper: https://arxiv.org/pdf/2105.15203.pdf,License: other,Datasets: cityscapes,,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8103,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 679.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
deid_roberta_i2b2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"Steps on how this model was trained can be found here: Training. The ""model_name_or_path"" was set to: ""roberta-large"".; Training details:; Post a Github issue on the repo: Robust DeID.",natural-language-processing,token-classification,https://huggingface.co/obi/deid_roberta_i2b2,Paper: https://arxiv.org/pdf/1907.11692.pdf,License: mit,Datasets: I2B2,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 81649,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
clip-vit-base-patch32,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c6145e63a23d3f282ad5_Zero-Shot%20Image%20Classification.svg,,"Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found here.; The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.; January 2021; The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. ; The original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.",computer-vision,zero-shot-image-classification,https://huggingface.co/openai/clip-vit-base-patch32,Paper: https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/1908.04913.pdf,,,,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 209,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3741682,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 203
imagegpt-large,,,,,,,,,"ImageGPT (iGPT) model pre-trained on ImageNet ILSVRC 2012 (14 million images, 21,843 classes) at resolution 32x32. It was introduced in the paper Generative Pretraining from Pixels by Chen et al. and first released in this repository. See also the official blog post.; Disclaimer: The team releasing ImageGPT did not write a model card for this model so this model card has been written by the Hugging Face team.; The ImageGPT (iGPT) is a transformer decoder model (GPT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-21k, at a resolution of 32x32 pixels. ; The goal for the model is simply to predict the next pixel value, given the previous ones.; By pre-training the model, it learns an inner representation of images that can then be used to:",,,https://huggingface.co/openai/imagegpt-large,,License: apache-2.0,Datasets: imagenet-21k,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 42,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
imagegpt-small,,,,,,,,,"ImageGPT (iGPT) model pre-trained on ImageNet ILSVRC 2012 (14 million images, 21,843 classes) at resolution 32x32. It was introduced in the paper Generative Pretraining from Pixels by Chen et al. and first released in this repository. See also the official blog post.; Disclaimer: The team releasing ImageGPT did not write a model card for this model so this model card has been written by the Hugging Face team.; The ImageGPT (iGPT) is a transformer decoder model (GPT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-21k, at a resolution of 32x32 pixels. ; The goal for the model is simply to predict the next pixel value, given the previous ones.; By pre-training the model, it learns an inner representation of images that can then be used to:",,,https://huggingface.co/openai/imagegpt-small,,License: apache-2.0,Datasets: imagenet-21k,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 16348,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 332.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
bert-large-cased-squad-v1.1-portuguese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,"; The model was trained on the dataset SQUAD v1.1 in portuguese from the Deep Learning Brasil group. ; The language model used is the BERTimbau Large (aka ""bert-large-portuguese-cased"") from Neuralmind.ai: BERTimbau is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.; All the informations are in the blog post : NLP | Como treinar um modelo de Question Answering em qualquer linguagem baseado no BERT large, melhorando o desempenho do modelo utilizando o BERT base? (estudo de caso em português); question_answering_BERT_large_cased_squad_v11_pt.ipynb (nbviewer version)",natural-language-processing,question-answering,https://huggingface.co/pierreguillou/bert-large-cased-squad-v1.1-portuguese,,License: mit,Datasets: brWaC; squad; squad_v1_pt,Portuguese,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 26,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7637,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
ner-bert-base-cased-pt-lenerbr,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"ner-bert-base-portuguese-cased-lenerbr is a NER model (token classification) in the legal domain in Portuguese that was finetuned on 20/12/2021 in Google Colab from the model pierreguillou/bert-base-cased-pt-lenerbr on the dataset LeNER_br by using a NER objective.; Due to the small size of BERTimbau base and finetuning dataset, the model overfitted before to reach the end of training. Here are the overall final metrics on the validation dataset (note: see the paragraph ""Validation metrics by Named Entity"" to get detailed metrics):; Check as well the large version of this model with a f1 of 0.908.; Note: the model pierreguillou/bert-base-cased-pt-lenerbr is a language model that was created through the finetuning of the model BERTimbau base on the dataset LeNER-Br language modeling by using a MASK objective. This first specialization of the language model before finetuning on the NER task improved a bit the model quality. To prove it, here are the results of the NER model finetuned from the model BERTimbau base (a non-specialized language model):; NLP | Modelos e Web App para Reconhecimento de Entidade Nomeada (NER) no domínio jurídico brasileiro (29/12/2021)",natural-language-processing,token-classification,https://huggingface.co/pierreguillou/ner-bert-base-cased-pt-lenerbr,,,Datasets: lener_br,Portuguese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 85,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 434.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
t5-base-qa-squad-v1.1-portuguese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"; t5-base-qa-squad-v1.1-portuguese is a QA model (Question Answering) in Portuguese that was finetuned on 27/01/2022 in Google Colab from the model unicamp-dl/ptt5-base-portuguese-vocab of Neuralmind on the dataset SQUAD v1.1 in portuguese from the Deep Learning Brasil group by using a Test2Text-Generation objective.; Due to the small size of T5 base and finetuning dataset, the model overfitted before to reach the end of training. Here are the overall final metrics on the validation dataset:; Check our other QA models in Portuguese finetuned on SQUAD v1.1:; NLP nas empresas | Como eu treinei um modelo T5 em português na tarefa QA no Google Colab (27/01/2022)",natural-language-processing,text2text-generation,https://huggingface.co/pierreguillou/t5-base-qa-squad-v1.1-portuguese,,,Datasets: squad; squad_v1_pt,Portuguese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 158,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 894.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
mpnet-retriever-discourse,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used as a retriever model in open-domain question-answering tasks.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; The model was fine-tuned on question-answer pairs scraper from several ML-focused Discourse forums [HuggingFace, PyTorch, Streamlit, TensorFlow].",natural-language-processing,sentence-similarity,https://huggingface.co/pinecone/mpnet-retriever-discourse,,,,,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 439.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
raceBERT,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-classification,https://huggingface.co/pparasurama/raceBERT,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 454,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 174.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-tiny,,,,,,,,,"The following model is a Pytorch pre-trained model obtained from converting Tensorflow checkpoint found in the official Google BERT repository. ; This is one of the smaller pre-trained BERT variants, together with bert-mini bert-small and bert-medium. They were introduced in the study Well-Read Students Learn Better: On the Importance of Pre-training Compact Models (arxiv), and ported to HF for the study Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics (arXiv). These models are supposed to be trained on a downstream task.; If you use the model, please consider citing both the papers:; Config of this model:; Other models to check out:",,,https://huggingface.co/prajjwal1/bert-tiny,Paper: https://arxiv.org/pdf/1908.08962.pdf; https://arxiv.org/pdf/2110.01518.pdf,License: mit,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 62,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2133479,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 18.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
S-PubMedBert-MS-MARCO-SCIFACT,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",natural-language-processing,sentence-similarity,https://huggingface.co/pritamdeka/S-PubMedBert-MS-MARCO-SCIFACT,,,,,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 42,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 439.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
grammar_error_correcter_v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,This model is part of the Gramformer library please refer to https://github.com/PrithivirajDamodaran/Gramformer/,natural-language-processing,text2text-generation,https://huggingface.co/prithivida/grammar_error_correcter_v1,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 35544,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 894.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
parrot_paraphraser_on_T5,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. A paraphrase framework is more than just a paraphrasing model. For more  details on the library and usage please refer to the github page; Huggingface lists 12 paraphrase models, RapidAPI lists 7 fremium and commercial paraphrasers like QuillBot, Rasa has discussed an experimental paraphraser for augmenting text data here, Sentence-transfomers offers a paraphrase mining utility and NLPAug offers word level augmentation with a PPDB (a multi-million paraphrase database). While these attempts at paraphrasing are great, there are still some gaps and paraphrasing is NOT yet a mainstream option for text augmentation in building NLU models....Parrot is a humble attempt to fill some of these gaps.; What is a good paraphrase? Almost all conditioned text generation models are validated  on 2 factors, (1) if the generated text conveys the same meaning as the original context (Adequacy) (2) if the text is fluent / grammatically correct english (Fluency). For instance Neural Machine Translation outputs are tested for Adequacy and Fluency. But a good paraphrase should be adequate and fluent while being as different as possible on the surface lexical form. With respect to this definition, the  3 key metrics that measures the quality of paraphrases are:; Parrot offers knobs to control Adequacy, Fluency and Diversity as per your needs.; What makes a paraphraser a good augmentor? For training a NLU model we just don't need a lot of utterances but utterances with intents and slots/entities annotated. Typical flow would be:",natural-language-processing,text2text-generation,https://huggingface.co/prithivida/parrot_paraphraser_on_T5,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 117,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 258151,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 893.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
led-base-book-summary,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"The Longformer Encoder-Decoder (LED) for Narrative-Esque Long Text Summarization is a model I fine-tuned from allenai/led-base-16384 to condense extensive technical, academic, and narrative content in a fairly generalizable way.; Note: The API widget has a max length of ~96 tokens due to inference timeout constraints. ; The model was trained on the BookSum dataset released by SalesForce, which leads to the bsd-3-clause license. The training process involved 16 epochs with parameters tweaked to facilitate very fine-tuning-type training (super low learning rate). ; Model checkpoint: pszemraj/led-base-16384-finetuned-booksum. ; This model is the smallest/fastest booksum-tuned model I have worked on. If you're looking for higher quality summaries, check out:",natural-language-processing,summarization,https://huggingface.co/pszemraj/led-base-book-summary,,License: bsd-3-clause,Datasets: kmfoda/booksum,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 35,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 15335,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
led-large-book-summary,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"This model is a fine-tuned version of allenai/led-large-16384 on the BookSum dataset (kmfoda/booksum). It aims to generalize well and be useful in summarizing lengthy text for both academic and everyday purposes. ; Note: Due to inference API timeout constraints, outputs may be truncated before the fully summary is returned (try python or the demo); To improve summary quality, use encoder_no_repeat_ngram_size=3 when calling the pipeline object. This setting encourages the model to utilize new vocabulary and construct an abstractive summary.; Load the model into a pipeline object:; Feed the text into the pipeline object:",natural-language-processing,summarization,https://huggingface.co/pszemraj/led-large-book-summary,Paper: https://arxiv.org/pdf/2105.08209.pdf,License: bsd-3-clause,Datasets: kmfoda/booksum,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 32,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2231,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
yacis-electra-small-japanese,,,,,,,,,"This is ELECTRA Small model for Japanese pretrained on 354 million sentences / 5.6 billion words of YACIS blog corpus.; The corpus was tokenized for pretraining with MeCab. Subword tokenization was done with WordPiece. ; This model uses ELECTRA Small model settings, 12 layers, 128 dimensions of hidden states, and 12 attention heads.; Vocabulary size was set to 32,000 tokens.; YACIS-ELECTRA is trained on the whole of YACIS blog corpus, which is a Japanese blog corpus containing 5.6 billion words in 354 million sentences.",,,https://huggingface.co/ptaszynski/yacis-electra-small-japanese,,License: cc-by-sa-4.0,Datasets: YACIS corpus,Japanese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 239.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
biobertpt-all,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"The BioBERTpt - A Portuguese Neural Language Model for Clinical Named Entity Recognition paper contains clinical and biomedical BERT-based models for Portuguese Language, initialized with BERT-Multilingual-Cased & trained on clinical notes and biomedical literature. ; This model card describes the BioBERTpt(all) model, a full version with clinical narratives and biomedical literature in Portuguese language. ; Load the model via the transformers library:; Refer to the original paper, BioBERTpt - A Portuguese Neural Language Model for Clinical Named Entity Recognition for additional details and performance on Portuguese NER tasks.; This study was financed in part by the Coordena??o de Aperfei?oamento de Pessoal de Nível Superior - Brasil (CAPES) - Finance Code 001.",natural-language-processing,fill-mask,https://huggingface.co/pucpr/biobertpt-all,,,,Portuguese,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 855,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base-qarib,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"QCRI Arabic and Dialectal BERT  (QARiB) model, was trained on a collection of ~ 420 Million tweets and ~ 180 Million sentences of text.
For the tweets, the data was collected using twitter API and using language filter. lang:ar. For the text data, it was a combination from
Arabic GigaWord, Abulkhair Arabic Corpus and OPUS.; QARiB: Is the Arabic name for ""Boat"".; See details in Training QARiB; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you. For more details, see Using QARiB; You can use this model directly with a pipeline for masked language modeling:",natural-language-processing,fill-mask,https://huggingface.co/qarib/bert-base-qarib,Paper: https://arxiv.org/pdf/2102.10684.pdf,,Datasets: arabic_billion_words; open_subtitles; twitter,Arabic,PyTorch; JAX; Transformers; TensorFlow,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 312,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
classical-chinese-punctuation-guwen-biaodian,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"欢迎前往我的github文言诗词项目页面探讨、加?? ， Please check the github repository for more about the model, hit ? if you like; This model punctuates Classical(ancient) Chinese, you might feel strange about this task, but many of my ancestors think writing articles without punctuation is brilliant idea ?. What we have here are articles from books, letters or carved on stones where you can see no punctuation, just a long string of characters. As you can guess, NLP tech is usually a good tool to tackle this problem, and the entire pipeline can be borrowed from usual NER task.; Since there are also many articles are punctuated, hence with some regex operations, labeled data is more than abundant ?. That's why this problem is pretty much a low hanging fruit.; so I guess who's interested in the problem set can speak at least modern Chinese, hence... let me continue the documentation in Chinese.; 输入一串未断句文言文， 可以断句， 目前支持二十多种标点符号",natural-language-processing,token-classification,https://huggingface.co/raynardj/classical-chinese-punctuation-guwen-biaodian,,,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 155,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 407.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
keywords-cangtou-chinese-poetry,,,,,,,,,"This is a model to generated Chinese poetry with leading characters and certain tune of mood.; 这个模型充分利用了gpt2论文的精髓， 论文标题为《语言模型即学万事万物》， 也就是许许多多的学习任务， 可以安排成文本序列的形式，来管理输入输出， 即模型如能根据 「所有自然常数的导数是0， 0的cos是1 ，」算出后面的句子应该是「 四个1相加的阶乘是4， 4的阶乘是24」也就学会了二十四点。 模型在训练上只做了猜测语言序列的任务， 但会兼通万物。; 这个码诗模型就是这么来的， 训练任务， 是输入0~10来个关键词+藏头标题+藏头字数+把头换成分类符[CLS]之后的诗句。; 感谢liangtongt指出Inference 代码运行时可能会发生的bug.; 大家下了模型,可以自己玩耍。
却也可以尝尝我替大家摘的樱桃?",,,https://huggingface.co/raynardj/keywords-cangtou-chinese-poetry,,,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 40,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 425.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ner-gene-dna-rna-jnlpba-pubmed,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"The model was trained on jnlpba dataset, pretrained on this pubmed-pretrained roberta model; All the labels, the possible token classes.; Notice, we removed the 'B-','I-' etc from data label.?; And here is to make your output more consecutive ??; check our NER model on",natural-language-processing,token-classification,https://huggingface.co/raynardj/ner-gene-dna-rna-jnlpba-pubmed,,License: apache-2.0,Datasets: jnlpba,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 101,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 499.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
SciFive-large-Pubmed_PMC,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Paper: SciFive: a text-to-text transformer model for biomedical literature; Authors: Long N. Phan, James T. Anibal, Hieu Tran, Shaurya Chanana, Erol Bahadroglu, Alec Peltekian, Grégoire Altan-Bonnet; For more details, do check out our Github repo. ",natural-language-processing,text-classification,https://huggingface.co/razent/SciFive-large-Pubmed_PMC,Paper: https://arxiv.org/pdf/2106.03598.pdf,,Datasets: pubmed; pmc/open_access,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 363,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
openai-clip-js,,,,,,,,,"Info here: https://github.com/josephrocca/openai-clip-js; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/rocca/openai-clip-js,,,,,ONNX,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 761.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
query_wellformedness_score,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model evaluates the wellformedness (non-fragment, grammatically correct)  score of a sentence. Model is case-sensitive and penalises for incorrect case and grammar as well. ; ['She is presenting a paper tomorrow','she is presenting a paper tomorrow','She present paper today']; [[0.8917],[0.4270],[0.0134]]",natural-language-processing,text-classification,https://huggingface.co/salesken/query_wellformedness_score,,License: apache-2.0,Datasets: google_wellformed_query,,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3755802,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1001.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
rudalle-Emojich,,,,,,,,,"; Model was trained by Sber AI; ; ? Emojich is a 1.3 billion params model from the family GPT3-like, it generates emoji-style images with the brain of ? Malevich.; The main goal of fine-tuning is trying to keep the generalization of ruDALL-E Malevich (XL)
model on text to emoji tasks. ruDALL-E Malevich is a multi-modality big pretrained transformer, that uses images and texts.
The idea with freezing feedforward and self-attention layers in pretrained transformer is demonstrated high performance in changing different modalities.
Also, the model has a good chance for over-fitting text modality and lost generalization. 
To deal with this problem is increased coefficient 10^3 in weighted cross-entropy loss for image codebooks part. ",,,https://huggingface.co/ai-forever/rudalle-Emojich,,,,,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
rugpt3large_based_on_gpt2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Model was trained with sequence length 1024 using transformers lib by SberDevices team on 80B tokens for 3 epochs. After that model was finetuned 1 epoch with sequence length 2048. ; Total training time was around 14 days on 128 GPUs for 1024 context and few days on 16 GPUs for 2048 context.Final perplexity on test set is 13.6.,natural-language-processing,text-generation,https://huggingface.co/ai-forever/rugpt3large_based_on_gpt2,,,,Russian,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 47,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9677,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
rugpt3medium_based_on_gpt2,,,,,,,,,"Model was trained with sequence length 1024 using transformers lib by SberDevices team on 80B tokens for 3 epoch. After that model was finetuned on 2048 context.; Total training time was around 16 days on 64 GPUs.Final perplexity on test set is 17.4.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/ai-forever/rugpt3medium_based_on_gpt2,,,,Russian,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1452,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
sbert_large_nlu_ru,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"The model is described in this articleFor better quality, use mean token embeddings.; You can use the model directly from the model repository to compute sentence embeddings:",multimodel,feature-extraction,https://huggingface.co/ai-forever/sbert_large_nlu_ru,,,,Russian,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7807,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
polish-roberta-large-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,,natural-language-processing,fill-mask,https://huggingface.co/sdadas/polish-roberta-large-v2,,License: lgpl-3.0,,Polish,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 582,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
distilbert-dot-tas_b-b256-msmarco,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"We provide a retrieval trained DistilBert-based model (we call the dual-encoder then dot-product scoring architecture BERT_Dot) trained with Balanced Topic Aware Sampling on MSMARCO-Passage.; This instance was trained with a batch size of 256 and can be used to re-rank a candidate set or directly for a vector index based dense retrieval. The architecture is a 6-layer DistilBERT, without architecture additions or modifications (we only change the weights during training) - to receive a query/passage representation we pool the CLS vector. We use the same BERT layers for both query and passage encoding (yields better results, and lowers memory requirements).; If you want to know more about our efficient (can be done on a single consumer GPU in 48 hours) batch composition procedure and dual supervision for dense retrieval training, check out our paper: https://arxiv.org/abs/2104.06967 ?; For more information and a minimal usage example please visit: https://github.com/sebastian-hofstaetter/tas-balanced-dense-retrieval; We trained our model on the MSMARCO standard (""small""-400K query) training triples re-sampled with our TAS-B method. As teacher models we used the BERT_CAT pairwise scores as well as the ColBERT model for in-batch-negative signals published here: https://github.com/sebastian-hofstaetter/neural-ranking-kd",multimodel,feature-extraction,https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco,Paper: https://arxiv.org/pdf/2104.06967.pdf,,Datasets: ms_marco,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 10541,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 265.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
LaBSE,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a port of the LaBSE model to PyTorch. It can be used to map 109 languages to a shared vector space.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net; Have a look at LaBSE for the respective publication that describes LaBSE.",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/LaBSE,,License: apache-2.0,,,PyTorch; TensorFlow; JAX; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 71,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 53019,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
clip-ViT-B-32,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is the Image & Text model CLIP, which maps text and images to a shared vector space. For applications of the models, have a look in our documentation SBERT.net - Image Search; After installing sentence-transformers (pip install sentence-transformers), the usage of this model is easy:; See our SBERT.net - Image Search documentation for more examples how the model can be used for image search, zero-shot image classification, image clustering and image deduplication.; In the following table we find the zero-shot ImageNet validation set accuracy:; For a multilingual version of the CLIP model for 50+ languages have a look at: clip-ViT-B-32-multilingual-v1",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/clip-ViT-B-32,Paper: https://arxiv.org/pdf/2103.00020.pdf,,,,Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.79KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
distilbert-base-nli-stsb-mean-tokens,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"?? This model is deprecated. Please don't use it as it produces sentence embeddings of low quality. You can find recommended sentence embedding models here: SBERT.net - Pretrained Models; This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/distilbert-base-nli-stsb-mean-tokens,Paper: https://arxiv.org/pdf/1908.10084.pdf,License: apache-2.0,,,PyTorch; TensorFlow; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 47551,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 532.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
gtr-t5-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of sematic search.; This model was converted from the Tensorflow model gtr-base-1 to PyTorch. When using this model, have a look at the publication: Large Dual Encoders Are Generalizable Retrievers. The tfhub model and this PyTorch model can produce slightly different embeddings, however, when run on the same benchmarks, they produce identical results.; The model uses only the encoder from a T5-base model. The weights are stored in FP16.  ; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/gtr-t5-base,Paper: https://arxiv.org/pdf/2112.07899.pdf,License: apache-2.0,,English,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3264,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 221.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
gtr-t5-xxl,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of sematic search.; This model was converted from the Tensorflow model gtr-xxl-1 to PyTorch. When using this model, have a look at the publication: Large Dual Encoders Are Generalizable Retrievers. The tfhub model and this PyTorch model can produce slightly different embeddings, however, when run on the same benchmarks, they produce identical results.; The model uses only the encoder from a T5-11B model. The weights are stored in FP16.  ; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/gtr-t5-xxl,Paper: https://arxiv.org/pdf/2112.07899.pdf,License: apache-2.0,,English,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 586,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
msmarco-MiniLM-L6-cos-v5,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and was designed for semantic search. It has been trained on 500k (query, answer) pairs from the MS MARCO Passages dataset. For an introduction to semantic search, have a look at: SBERT.net - Semantic Search; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.; In the following some technical details how this model must be used:",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/msmarco-MiniLM-L6-cos-v5,Paper: https://arxiv.org/pdf/1908.10084.pdf,,,,PyTorch; TensorFlow; JAX; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9802,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 273.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
msmarco-distilbert-base-tas-b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a port of the DistilBert TAS-B Model to sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b,,License: apache-2.0,Datasets: ms_marco,English,PyTorch; TensorFlow; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 25064,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 532.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
multi-qa-MiniLM-L6-cos-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources. For an introduction to semantic search, have a look at: SBERT.net - Semantic Search; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.; Similarly to the PyTorch example above, to use the model with TensorFlow you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1,,,Datasets: flax-sentence-embeddings/stackexchange_xml; ms_marco; gooaq; yahoo_answers_topics; search_qa; eli5; natural_questions; trivia_qa; embedding-data/QQP; embedding-data/PAQ_pairs; embedding-data/Amazon-QA; embedding-data/WikiAnswers,,PyTorch; TensorFlow; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 66,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 180485,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 183.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 12
multi-qa-distilbert-cos-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources. For an introduction to semantic search, have a look at: SBERT.net - Semantic Search; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.; In the following some technical details how this model must be used:",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/multi-qa-distilbert-cos-v1,,,Datasets: flax-sentence-embeddings/stackexchange_xml; ms_marco; gooaq; yahoo_answers_topics; search_qa; eli5; natural_questions; trivia_qa; embedding-data/QQP; embedding-data/PAQ_pairs; embedding-data/Amazon-QA; embedding-data/WikiAnswers,,PyTorch; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 19620,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 266.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LaBSE,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.; This is migrated from the v2 model on the TF Hub, which uses dict-based input. The embeddings produced by both the versions of the model are equivalent.; Using the model:; To get the sentence embeddings, use the pooler output:; Output for other languages:",multimodel,feature-extraction,https://huggingface.co/setu4993/LaBSE,Paper: https://arxiv.org/pdf/2007.01852.pdf,License: apache-2.0,Datasets: CommonCrawl; Wikipedia,109 languages,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 36,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8049,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
smaller-LaBSE,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"Smaller Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model distilled from the original LaBSE model to 15 languages (from the original 109 languages) using the techniques described in the paper 'Load What You Need: Smaller Versions of Multilingual BERT' by Ukjae Jeong.; Using the model:; To get the sentence embeddings, use the pooler output:; Output for other languages:; For similarity between sentences, an L2-norm is recommended before calculating the similarity:",multimodel,feature-extraction,https://huggingface.co/setu4993/smaller-LaBSE,Paper: https://arxiv.org/pdf/2010.05609.pdf; https://arxiv.org/pdf/2007.01852.pdf,License: apache-2.0,Datasets: CommonCrawl; Wikipedia,15 languages,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2183,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-multitask-query-classifiers,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Quora Keyword Pairs: https://www.kaggle.com/stefanondisponibile/quora-question-keyword-pairs
Spaadia SQuaD pairs: https://www.kaggle.com/shahrukhkhan/questions-vs-statementsclassificationdataset; Medium article; Colab Notebook Multi-task Query classifiers",natural-language-processing,text-classification,https://huggingface.co/shahrukhx01/bert-multitask-query-classifiers,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 185,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 45.4MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
macbert4csc-base-chinese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,中文拼写纠错模型; macbert4csc-base-chinese evaluate SIGHAN2015 test data：; 由于训练使用的数据使用了SIGHAN2015的训练集（复现paper），在SIGHAN2015的测试集上达到SOTA水平。; 模型结构，魔改于softmaskedbert：; ,natural-language-processing,fill-mask,https://huggingface.co/shibing624/macbert4csc-base-chinese,Paper: https://arxiv.org/pdf/2004.13922.pdf,License: apache-2.0,,Chinese,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 43,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 37719,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 818.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
bart_summarisation,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"This model was trained using Amazon SageMaker and the new Hugging Face Deep Learning container.
For more information look at:; }",natural-language-processing,summarization,https://huggingface.co/slauw87/bart_summarisation,,License: apache-2.0,Datasets: samsum,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 33,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 36679,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
t5-base-japanese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.; 次の日本Zコ`パス（s100GB）を用いて事前学を行ったT5 (Text-to-Text Transfer Transformer) モデルです。  ; このモデルは事前学のみを行なったものであり、特定のタスクに利用するにはファインチュ`ニングする必要があります。本モデルにも、大模コ`パスを用いた言Zモデルにつきまとう、学デ`タの内容の偏りに由来する偏った（理的ではなかったり、有害だったり、バイアスがあったりする）出力Y果になる}が潜在的にあります。
この}がk生しうることを想定した上で、被害がk生しない用途にのみ利用するよう荬颏膜堡皮ださい。; SentencePieceト`クナイザ`の学には上Wikipediaの全デ`タを用いました。; https://github.com/sonoisa/t5-japanese",multimodel,feature-extraction,https://huggingface.co/sonoisa/t5-base-japanese,,License: cc-by-sa-4.0,Datasets: wikipedia; oscar; cc100,Japanese,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 26,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9551,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
en_core_web_sm,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.",natural-language-processing,token-classification,https://huggingface.co/spacy/en_core_web_sm,,License: mit,,English,spaCy,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1139,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 12.9MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
wav2vec2-2-bert-large-no-adapter-frozen-enc,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"This model was trained from scratch on the librispeech_asr dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",audio,automatic-speech-recognition,https://huggingface.co/speech-seq2seq/wav2vec2-2-bert-large-no-adapter-frozen-enc,,,Datasets: librispeech_asr,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 32,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
asr-wav2vec2-transformer-aishell,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"This repository provides all the necessary tools to perform automatic speech
recognition from an end-to-end system pretrained on AISHELL +wav2vec2 (Mandarin Chinese)
within SpeechBrain. For a better experience, we encourage you to learn more about
SpeechBrain.; The performance of the model is the following:; This ASR system is composed of 2 different but linked blocks:; To Train this system from scratch, see our SpeechBrain recipe.; The system is trained with recordings sampled at 16kHz (single channel).
The code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling transcribe_file if needed.",audio,automatic-speech-recognition,https://huggingface.co/speechbrain/asr-wav2vec2-transformer-aishell,Paper: https://arxiv.org/pdf/2106.04624.pdf,License: apache-2.0,Datasets: aishell,English,speechbrain; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 40,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
lang-id-voxlingua107-ecapa,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0941358df9b5cffb0d11b_Audio%20Classification.svg,,"This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain.
The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. However, it uses
more fully connected hidden layers after the embedding layer, and cross-entropy loss was used for training. 
We observed that this improved the performance of extracted utterance embeddings for downstream tasks.; The system is trained with recordings sampled at 16kHz (single channel).
The code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling classify_file if needed.; The model can classify a speech utterance according to the language spoken.
It covers 107 different languages (
Abkhazian, 
Afrikaans, 
Amharic, 
Arabic, 
Assamese, 
Azerbaijani, 
Bashkir, 
Belarusian, 
Bulgarian, 
Bengali, 
Tibetan, 
Breton, 
Bosnian, 
Catalan, 
Cebuano, 
Czech, 
Welsh, 
Danish, 
German, 
Greek, 
English, 
Esperanto, 
Spanish, 
Estonian, 
Basque, 
Persian, 
Finnish, 
Faroese, 
French, 
Galician, 
Guarani, 
Gujarati, 
Manx, 
Hausa, 
Hawaiian, 
Hindi, 
Croatian, 
Haitian, 
Hungarian, 
Armenian, 
Interlingua, 
Indonesian, 
Icelandic, 
Italian, 
Hebrew, 
Japanese, 
Javanese, 
Georgian, 
Kazakh, 
Central Khmer, 
Kannada, 
Korean, 
Latin, 
Luxembourgish, 
Lingala, 
Lao, 
Lithuanian, 
Latvian, 
Malagasy, 
Maori, 
Macedonian, 
Malayalam, 
Mongolian, 
Marathi, 
Malay, 
Maltese, 
Burmese, 
Nepali, 
Dutch, 
Norwegian Nynorsk, 
Norwegian, 
Occitan, 
Panjabi, 
Polish, 
Pushto, 
Portuguese, 
Romanian, 
Russian, 
Sanskrit, 
Scots, 
Sindhi, 
Sinhala, 
Slovak, 
Slovenian, 
Shona, 
Somali, 
Albanian, 
Serbian, 
Sundanese, 
Swedish, 
Swahili, 
Tamil, 
Telugu, 
Tajik, 
Thai, 
Turkmen, 
Tagalog, 
Turkish, 
Tatar, 
Ukrainian, 
Urdu, 
Uzbek, 
Vietnamese, 
Waray, 
Yiddish, 
Yoruba, 
Mandarin Chinese).; The model has two uses:; The model is trained on automatically collected YouTube data. For more 
information about the dataset, see here.",audio,audio-classification,https://huggingface.co/speechbrain/lang-id-voxlingua107-ecapa,Paper: https://arxiv.org/pdf/2106.04624.pdf,License: apache-2.0,Datasets: VoxLingua107,108 languages,speechbrain; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 33,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3524,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 85.3MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
metricgan-plus-voicebank,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9bbee42f488ff959bf7_Audio-to-Audio.svg,,"This repository provides all the necessary tools to perform enhancement with
SpeechBrain. For a better experience we encourage you to learn more about
SpeechBrain. The model performance is:; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; To use the mimic-loss-trained model for enhancement, use the following simple code:; The system is trained with recordings sampled at 16kHz (single channel).
The code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling enhance_file if needed. Make sure your input tensor is compliant with the expected sampling rate if you use enhance_batch as in the example.",audio,audio-to-audio,https://huggingface.co/speechbrain/metricgan-plus-voicebank,Paper: https://arxiv.org/pdf/2106.04624.pdf,License: apache-2.0,Datasets: Voicebank; DEMAND,English,speechbrain,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5249,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.87MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 24
sepformer-wham-enhancement,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9bbee42f488ff959bf7_Audio-to-Audio.svg,,"This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k. For a better experience we encourage you to learn more about SpeechBrain. The given model performance is 14.35 dB SI-SNR on the test set of WHAM! dataset.; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about SpeechBrain.; To perform inference on the GPU, add  run_opts={""device"":""cuda""}  when calling the from_hparams method.; The training script is currently being worked on an ongoing pull-request. ",audio,audio-to-audio,https://huggingface.co/speechbrain/sepformer-wham-enhancement,Paper: https://arxiv.org/pdf/2010.13154.pdf; https://arxiv.org/pdf/2106.04624.pdf,License: apache-2.0,Datasets: WHAM!,English,speechbrain; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 221,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 318.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
CoreNLP,,,,,,,,,"CoreNLP is your one stop shop for natural language processing in Java! CoreNLP enables users to derive linguistic annotations for text, including token and sentence boundaries, parts of speech, named entities, numeric and time values, dependency and constituency parses, coreference, sentiment, quote attributions, and relations.
Find more about it in our website and our GitHub repository.; This card and repo were automatically prepared with hugging_corenlp.py in the stanfordnlp/huggingface-models repo; Last updated 2023-03-16 01:06:26.193; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/stanfordnlp/CoreNLP,,License: gpl-2.0,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 506.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
stanza-id,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages. Starting from raw text to syntactic analysis and entity recognition, Stanza brings state-of-the-art NLP models to languages of your choosing.
Find more about it in our website and our GitHub repository.; This card and repo were automatically prepared with hugging_stanza.py in the stanfordnlp/huggingface-models repo; Last updated 2023-05-26 18:04:22.253",natural-language-processing,token-classification,https://huggingface.co/stanfordnlp/stanza-id,,License: apache-2.0,,Indonesian,Stanza,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 282,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.85KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
codeparrot,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,CodeParrot (large) is a 1.5B parameter GPT-2 model trained on the CodeParrot Python code dataset. The model is trained in Chapter 10: Training Transformers from Scratch in the NLP with Transformers book. You can find the full code in the accompanying Github repository.,natural-language-processing,text-generation,https://huggingface.co/transformersbook/codeparrot,,,,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 59,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
distilbert-base-uncased-mnli,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c42cd649af0365a01a1e_Zero-Shot%20Classification.svg,,"Model Description:  This is the uncased DistilBERT model fine-tuned on Multi-Genre Natural Language Inference (MNLI) dataset for the zero-shot classification task. ; This model can be used for text classification tasks.; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).; This model of DistilBERT-uncased is pretrained on the Multi-Genre Natural Language Inference (MultiNLI) corpus. It is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation.",natural-language-processing,zero-shot-classification,https://huggingface.co/typeform/distilbert-base-uncased-mnli,Paper: https://arxiv.org/pdf/1910.09700.pdf; https://arxiv.org/pdf/2105.09680.pdf,,Datasets: multi_nli,English,PyTorch; TensorFlow; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 29,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 94007,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 804.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
chinese_roberta_L-12_H-768,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"This is the set of 24 Chinese RoBERTa models pre-trained by UER-py, which is introduced in this paper.; Turc et al. have shown that the standard BERT recipe is effective on a wide range of model sizes. Following their paper, we released the 24 Chinese RoBERTa models. In order to facilitate users to reproduce the results, we used the publicly available corpus and provided all training details.; You can download the 24 Chinese RoBERTa miniatures either from the UER-py Modelzoo page, or via HuggingFace from the links below:; Here are scores on the devlopment set of six Chinese tasks:; For each task, we selected the best fine-tuning hyperparameters from the lists below, and trained with the sequence length of 128:",natural-language-processing,fill-mask,https://huggingface.co/uer/chinese_roberta_L-12_H-768,Paper: https://arxiv.org/pdf/1909.05658.pdf; https://arxiv.org/pdf/1908.08962.pdf,,Datasets: CLUECorpusSmall,Chinese,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 704,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
roberta-base-finetuned-dianping-chinese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This is the set of 5 Chinese RoBERTa-Base classification models fine-tuned by UER-py. You can download the 5 Chinese RoBERTa-Base classification models either from the UER-py Modelzoo page (in UER-py format), or via HuggingFace from the links below:; You can use this model directly with a pipeline for text classification (take the case of roberta-base-finetuned-chinanews-chinese):; 5 Chinese text classification datasets are used. JD full, JD binary, and Dianping datasets consist of user reviews of different sentiment polarities. Ifeng and Chinanews consist of first paragraphs of news articles of different topic classes. They are collected by Glyph project and more details are discussed in corresponding paper.; Models are fine-tuned by UER-py on Tencent Cloud. We fine-tune three epochs with a sequence length of 512 on the basis of the pre-trained model chinese_roberta_L-12_H-768. At the end of each epoch, the model is saved when the best performance on development set is achieved. We use the same hyper-parameters on different models.; Taking the case of roberta-base-finetuned-chinanews-chinese",natural-language-processing,text-classification,https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese,Paper: https://arxiv.org/pdf/1909.05658.pdf; https://arxiv.org/pdf/1708.02657.pdf,,,Chinese,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2139,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
roberta-base-finetuned-jd-binary-chinese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This is the set of 5 Chinese RoBERTa-Base classification models fine-tuned by UER-py. You can download the 5 Chinese RoBERTa-Base classification models either from the UER-py Modelzoo page (in UER-py format), or via HuggingFace from the links below:; You can use this model directly with a pipeline for text classification (take the case of roberta-base-finetuned-chinanews-chinese):; 5 Chinese text classification datasets are used. JD full, JD binary, and Dianping datasets consist of user reviews of different sentiment polarities. Ifeng and Chinanews consist of first paragraphs of news articles of different topic classes. They are collected by Glyph project and more details are discussed in corresponding paper.; Models are fine-tuned by UER-py on Tencent Cloud. We fine-tune three epochs with a sequence length of 512 on the basis of the pre-trained model chinese_roberta_L-12_H-768. At the end of each epoch, the model is saved when the best performance on development set is achieved. We use the same hyper-parameters on different models.; Taking the case of roberta-base-finetuned-chinanews-chinese",natural-language-processing,text-classification,https://huggingface.co/uer/roberta-base-finetuned-jd-binary-chinese,Paper: https://arxiv.org/pdf/1909.05658.pdf; https://arxiv.org/pdf/1708.02657.pdf,,,Chinese,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 735,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
gottbert-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"BERT model trained solely on the German portion of the OSCAR data set.; Paper: GottBERT: a pure German Language Model; Authors: Raphael Scheible, Fabian Thomczyk, Patric Tippmann, Victor Jaravine, Martin Boeker",natural-language-processing,fill-mask,https://huggingface.co/uklfr/gottbert-base,Paper: https://arxiv.org/pdf/2012.02110.pdf,,,,PyTorch; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1133,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
toxic-bert,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"?? Disclaimer:
The huggingface models currently give different results to the detoxify library (see issue here). For the most up to date models we recommend using the models from https://github.com/unitaryai/detoxify; 
; ; Trained models & code to predict toxic comments on 3 Jigsaw challenges: Toxic comment classification, Unintended?Bias in Toxic comments, Multilingual toxic comment classification.; Built by Laura Hanu at Unitary, where we are working to stop harmful content online by interpreting visual content in context. ",natural-language-processing,text-classification,https://huggingface.co/unitary/toxic-bert,Paper: https://arxiv.org/pdf/1703.04009.pdf; https://arxiv.org/pdf/1905.12516.pdf,,,,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 65,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 27677,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 876.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
t5-base-e2e-qg,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"This is t5-base model trained for end-to-end question generation task. Simply input the text and the model will generate multile questions. ; You can play with the model using the inference API, just put the text and see the results!; For more deatils see this repo.; You'll need to clone the repo.; ",natural-language-processing,text2text-generation,https://huggingface.co/valhalla/t5-base-e2e-qg,Paper: https://arxiv.org/pdf/1910.10683.pdf,License: mit,Datasets: squad,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 231191,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 893.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
en_readability,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,A Spacy pipeline for generating readability scores,natural-language-processing,text-classification,https://huggingface.co/valurank/en_readability,,,,English,spaCy,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 63,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.41MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
flax-bigbird-natural-questions,,,,,,,,,"This checkpoint is obtained after training FlaxBigBirdForQuestionAnswering (with extra pooler head) on natural_questions dataset on TPU v3-8. This dataset takes around ~100 GB on disk. But thanks to Cloud TPUs and Jax, each epoch took just 4.5 hours. Script for training can be found here: https://github.com/vasudevgupta7/bigbird; Use this model just like any other model from ?Transformers; In case you are interested in predicting category (null, long, short, yes, no) as well, use FlaxBigBirdForNaturalQuestions (instead of FlaxBigBirdForQuestionAnswering) from my training script.; Evaluation script: https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/evaluate-flax-natural-questions.ipynb; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/vasudevgupta/flax-bigbird-natural-questions,,License: apache-2.0,Datasets: natural_questions,English,JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 603.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
hinglish-bert,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,fill-mask,https://huggingface.co/nirantk/hinglish-bert,,,,,PyTorch; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 74,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
BART0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"A BART-large version of T0. 
Please check https://inklab.usc.edu/ReCross/ for more details. ",natural-language-processing,text2text-generation,https://huggingface.co/yuchenlin/BART0,,License: apache-2.0,Datasets: bigscience/P3,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 991,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
singbert-large-sg,,,,,,,,,"SingBert Large - Bert for Singlish (SG) and Manglish (MY).; Similar to SingBert but the large version, which was initialized from BERT large uncased (whole word masking), with pre-training finetuned on
singlish and manglish data.; Here is how to use this model to get the features of a given text in PyTorch:; and in TensorFlow:; This model was finetuned on colloquial Singlish and Manglish corpus, hence it is best applied on downstream tasks involving the main
constituent languages- english, mandarin, malay. Also, as the training data is mainly from forums, beware of existing inherent bias.",,,https://huggingface.co/zanelim/singbert-large-sg,,License: mit,"Datasets: reddit singapore, malaysia; hardwarezone",English,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 31,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
dit-base,,,,,,,,,"Document Image Transformer (DiT) model pre-trained on IIT-CDIP (Lewis et al., 2006), a dataset that includes 42 million document images. It was introduced in the paper DiT: Self-supervised Pre-training for Document Image Transformer by Li et al. and first released in this repository. Note that DiT is identical to the architecture of BEiT. ; Disclaimer: The team releasing DiT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Document Image Transformer (DiT) is a transformer encoder model (BERT-like) pre-trained on a large collection of images in a self-supervised fashion. The pre-training objective for the model is to predict visual tokens from the encoder of a discrete VAE (dVAE), based on masked patches.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled document images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder.",,,https://huggingface.co/microsoft/dit-base,Paper: https://arxiv.org/pdf/2203.02378.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 27323,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 369.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
dit-large,,,,,,,,,"Document Image Transformer (DiT) model pre-trained on IIT-CDIP (Lewis et al., 2006), a dataset that includes 42 million document images. It was introduced in the paper DiT: Self-supervised Pre-training for Document Image Transformer by Li et al. and first released in this repository. Note that DiT is identical to the architecture of BEiT. ; Disclaimer: The team releasing DiT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Document Image Transformer (DiT) is a transformer encoder model (BERT-like) pre-trained on a large collection of images in a self-supervised fashion. The pre-training objective for the model is to predict visual tokens from the encoder of a discrete VAE (dVAE), based on masked patches.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled document images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder.",,,https://huggingface.co/microsoft/dit-large,Paper: https://arxiv.org/pdf/2203.02378.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 448,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
dit-base-finetuned-rvlcdip,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,"Document Image Transformer (DiT) model pre-trained on IIT-CDIP (Lewis et al., 2006), a dataset that includes 42 million document images and fine-tuned on RVL-CDIP, a dataset consisting of 400,000 grayscale images in 16 classes, with 25,000 images per class. It was introduced in the paper DiT: Self-supervised Pre-training for Document Image Transformer by Li et al. and first released in this repository. Note that DiT is identical to the architecture of BEiT. ; Disclaimer: The team releasing DiT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Document Image Transformer (DiT) is a transformer encoder model (BERT-like) pre-trained on a large collection of images in a self-supervised fashion. The pre-training objective for the model is to predict visual tokens from the encoder of a discrete VAE (dVAE), based on masked patches.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled document images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder.",computer-vision,image-classification,https://huggingface.co/microsoft/dit-base-finetuned-rvlcdip,Paper: https://arxiv.org/pdf/2203.02378.pdf,,Datasets: rvl_cdip,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6020,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 348.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c914a5f52efbcfb97cd1_Image%20Segmentation.svg,,"The aim of this study is automatic semantic segmentation and measurement total length of teeth in one-shot panoramic x-ray image by using deep learning method with U-Net Model and binary image analysis in order to provide diagnostic information for the management of dental disorders, diseases, and conditions. ;  Github Link; Original Dataset; DATASET ref - 	H. Abdi, S. Kasaei, and M. Mehdizadeh, “Automatic segmentation of mandible in panoramic x-ray,” J. Med. Imaging, vol. 2, no. 4, p. 44003, 2015; Link DATASET for only original images.",computer-vision,image-segmentation,https://huggingface.co/SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net,,,Datasets: SerdarHelli/SegmentationOfTeethPanoramicXRayImages,,Keras,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 31,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.12MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 23
trocr-handwritten-math,,,,,,,,,"This model generate the math expression LATEX sequence according to the handwritten math expression image.; in CROHME 2014 test dataset CER=0.507772718700326; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/Azu/trocr-handwritten-math,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 247.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
sentence-camembert-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"Model is Fine-tuned using pre-trained facebook/camembert-base and
Siamese BERT-Networks with 'sentences-transformers' on dataset stsb; The model can be used directly (without a language model) as follows:; The model can be evaluated as follows on the French test data of stsb.; Test Result: 
The performance is measured using Pearson and Spearman correlation:",natural-language-processing,sentence-similarity,https://huggingface.co/dangvantuan/sentence-camembert-base,Paper: https://arxiv.org/pdf/1908.10084.pdf,License: apache-2.0,Datasets: stsb_multi_mt,French,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3903,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 445.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
BioBert-PubMed200kRCT,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is a fine-tuned version of dmis-lab/biobert-base-cased-v1.1 on the PubMed200kRCT dataset.
It achieves the following results on the evaluation set:; More information needed; The model can be used for text classification tasks of Randomized Controlled Trials that does not have any structure. The text can be classified as one of the following:; The model can be directly used like this:; Results will be shown as follows:",natural-language-processing,text-classification,https://huggingface.co/pritamdeka/BioBert-PubMed200kRCT,,,,,PyTorch; TensorBoard; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1452,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 867.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
resnet-50,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,"ResNet model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ; Disclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.; ResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models.; This is ResNet v1.5, which differs from the original model: in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec) according to Nvidia.; ",computer-vision,image-classification,https://huggingface.co/microsoft/resnet-50,Paper: https://arxiv.org/pdf/1512.03385.pdf,License: apache-2.0,Datasets: imagenet-1k,,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 116,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3491872,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 308.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 42
poem-gen-gpt2-small-spanish,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This model is a fine-tuned version of datificate/gpt2-small-spanish on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",natural-language-processing,text-generation,https://huggingface.co/DrishtiSharma/poem-gen-gpt2-small-spanish,,License: apache-2.0,,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 510.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
gpt-neo-125M-spanish-classics,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/Aleksandar1932/gpt-neo-125M-spanish-classics,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 551.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
keyphrase-extraction-distilbert-inspec,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"Keyphrase extraction is a technique in text analysis where you extract the important keyphrases from a document. Thanks to these keyphrases humans can understand the content of a text very quickly and easily without reading it completely. Keyphrase extraction was first done primarily by human annotators, who read the text in detail and then wrote down the most important keyphrases. The disadvantage is that if you work with a lot of documents, this process can take a lot of time ?. ; Here is where Artificial Intelligence ? comes in. Currently, classical machine learning methods, that use statistical and linguistic features, are widely used for the extraction process. Now with deep learning, it is possible to capture the semantic meaning of a text even better than these classical methods. Classical methods look at the frequency, occurrence and order of words in the text, whereas these neural approaches can capture long-term semantic dependencies and context of words in a text.; This model uses distilbert as its base model and fine-tunes it on the Inspec dataset.; Keyphrase extraction models are transformer models fine-tuned as a token classification problem where each word in the document is classified as being part of a keyphrase or not.; Kulkarni, Mayank, Debanjan Mahata, Ravneet Arora, and Rajarshi Bhowmik. ""Learning Rich Representation of Keyphrases from Text."" arXiv preprint arXiv:2112.08547 (2021).",natural-language-processing,token-classification,https://huggingface.co/ml6team/keyphrase-extraction-distilbert-inspec,,License: mit,Datasets: midas/inspec,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3901,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 267.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
sbert-chinese-general-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,此模型基于 bert-base-chinese 版本 BERT 模型，在百万级语义相似数据集 SimCLUE 上进行训练，适用于通用语义匹配场景，从效果来看该模型在各种任务上泛化能力更好。; 注：此模型的轻量化版本，也已经开源啦！; 通过  sentence-transformers 框架来使用该模型，首先进行安装：; 然后使用下面的代码来载入该模型并进行文本表征向量的提取：; 如果不想使用   sentence-transformers 的话，也可以通过 HuggingFace Transformers 来载入该模型并进行文本向量抽取：,natural-language-processing,sentence-similarity,https://huggingface.co/DMetaSoul/sbert-chinese-general-v2,,,,,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2251,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 410.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
keyphrase-extraction-kbir-inspec,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"Keyphrase extraction is a technique in text analysis where you extract the important keyphrases from a document. Thanks to these keyphrases humans can understand the content of a text very quickly and easily without reading it completely. Keyphrase extraction was first done primarily by human annotators, who read the text in detail and then wrote down the most important keyphrases. The disadvantage is that if you work with a lot of documents, this process can take a lot of time ?. ; Here is where Artificial Intelligence ? comes in. Currently, classical machine learning methods, that use statistical and linguistic features, are widely used for the extraction process. Now with deep learning, it is possible to capture the semantic meaning of a text even better than these classical methods. Classical methods look at the frequency, occurrence and order of words in the text, whereas these neural approaches can capture long-term semantic dependencies and context of words in a text.; This model uses KBIR as its base model and fine-tunes it on the Inspec dataset. KBIR or Keyphrase Boundary Infilling with Replacement is a pre-trained model which utilizes a multi-task learning setup for optimizing a combined loss of Masked Language Modeling (MLM), Keyphrase Boundary Infilling (KBI) and Keyphrase Replacement Classification (KRC).
You can find more information about the architecture in this paper.; Keyphrase extraction models are transformer models fine-tuned as a token classification problem where each word in the document is classified as being part of a keyphrase or not.; Kulkarni, Mayank, Debanjan Mahata, Ravneet Arora, and Rajarshi Bhowmik. ""Learning Rich Representation of Keyphrases from Text."" arXiv preprint arXiv:2112.08547 (2021).",natural-language-processing,token-classification,https://huggingface.co/ml6team/keyphrase-extraction-kbir-inspec,Paper: https://arxiv.org/pdf/2112.08547.pdf,License: mit,Datasets: midas/inspec,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 42,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 97509,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
vit-base-patch16-224-in21k-finetuned-cifar10,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,"This model is a fine-tuned version of google/vit-base-patch16-224-in21k on the cifar10 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",computer-vision,image-classification,https://huggingface.co/aaraki/vit-base-patch16-224-in21k-finetuned-cifar10,,License: apache-2.0,Datasets: cifar10,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1261,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 343.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
sbert-chinese-general-v2-distill,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,此模型是之前开源通用语义匹配模型的蒸馏版本（仅4层 BERT），适用于通用语义匹配场景，从效果来看该模型在各种任务上泛化能力更好且编码速度更快。; 离线训练好的大模型如果直接用于线上推理，对计算资源有苛刻的需求，而且难以满足业务环境对延迟、吞吐量等性能指标的要求，这里我们使用蒸馏手段来把大模型轻量化。从 12 层 BERT 蒸馏为 4 层后，模型参数量缩小到 44%，大概 latency 减半、throughput 翻倍、精度下降 6% 左右（具体结果详见下文评估小节）。; 通过  sentence-transformers 框架来使用该模型，首先进行安装：; 然后使用下面的代码来载入该模型并进行文本表征向量的提取：; 如果不想使用   sentence-transformers 的话，也可以通过 HuggingFace Transformers 来载入该模型并进行文本向量抽取：,natural-language-processing,sentence-similarity,https://huggingface.co/DMetaSoul/sbert-chinese-general-v2-distill,,,,,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 67,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 183.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
roberta-base-stocktwits-finetuned,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is fine tuned with roberta-base model on 3200000 comments from stocktwits, with the user labeled tags 'Bullish' or 'Bearish'; try something that the individual investors may say on the investment forum on the inference API, for example, try 'red' and 'green'.; code on github",natural-language-processing,text-classification,https://huggingface.co/zhayunduo/roberta-base-stocktwits-finetuned,,License: apache-2.0,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 325,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 500.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
stt_en_conformer_ctc_large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"| 
| 
| 
|  |; This model transcribes speech in lowercase English alphabet including spaces and apostrophes, and is trained on several thousand hours of English speech data.
It is a non-autoregressive ""large"" variant of Conformer, with around 120 million parameters.
See the model architecture section and NeMo documentation for complete architecture details.
It is also compatible with NVIDIA Riva for production-grade server deployments. ; The model is available for use in the NeMo toolkit [3], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.; To train, fine-tune or play with the model you will need to install NVIDIA NeMo. We recommend you install it after you've installed latest PyTorch version.; First, let's get a sample",audio,automatic-speech-recognition,https://huggingface.co/nvidia/stt_en_conformer_ctc_large,Paper: https://arxiv.org/pdf/2005.08100.pdf,License: cc-by-4.0,Datasets: librispeech_asr; fisher_corpus; Switchboard-1; WSJ-0; WSJ-1; National-Singapore-Corpus-Part-1; National-Singapore-Corpus-Part-6; vctk; VoxPopuli-(EN); Europarl-ASR-(EN); Multilingual-LibriSpeech-(2000-hours); mozilla-foundation/common_voice_7_0,English,NeMo; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 374,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 451.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
PHS-BERT,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"We present and release PHS-BERT, a transformer-based pretrained language model (PLM), to identify tasks related to public health surveillance (PHS) on social media. Compared with existing PLMs that are mainly evaluated on limited tasks, PHS-BERT achieved state-of-the-art performance on 25 tested datasets, showing that our PLM is robust and generalizable in common PHS tasks.; Load the model via Huggingface's Transformers library:; We followed the standard pretraining protocols of BERT and initialized PHS-BERT with weights from BERT during the training phase instead of training from scratch and used the uncased version of the BERT model.; PHS-BERT is trained on a corpus of health-related tweets that were crawled via the Twitter API. Focusing on the tasks related to PHS, keywords used to collect pretraining corpus are set to disease, symptom, vaccine, and mental health-related words in English. Retweet tags were deleted from the raw corpus, and URLs and usernames were replaced with HTTP-URL and @USER, respectively. All emoticons were replaced with their associated meanings. ; Each sequence of BERT LM inputs is converted to 50,265 vocabulary tokens. Twitter posts are restricted to 200 characters, and during the training and evaluation phase, we used a batch size of 8. Distributed training was performed on a TPU v3-8.",natural-language-processing,fill-mask,https://huggingface.co/publichealthsurveillance/PHS-BERT,Paper: https://arxiv.org/pdf/2204.04521.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 157,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
clip-ViT-L-14,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is the Image & Text model CLIP, which maps text and images to a shared vector space. For applications of the models, have a look in our documentation SBERT.net - Image Search; After installing sentence-transformers (pip install sentence-transformers), the usage of this model is easy:; See our SBERT.net - Image Search documentation for more examples how the model can be used for image search, zero-shot image classification, image clustering and image deduplication.; In the following table we find the zero-shot ImageNet validation set accuracy:; For a multilingual version of the CLIP model for 50+ languages have a look at: clip-ViT-B-32-multilingual-v1",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/clip-ViT-L-14,Paper: https://arxiv.org/pdf/2103.00020.pdf,,,,Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.33KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
codegen-16B-mono,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"CodeGen is a family of autoregressive language models for program synthesis from the paper: A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B).; The checkpoint included in this repository is denoted as CodeGen-Mono 16B in the paper, where ""Mono"" means the model is initialized with CodeGen-Multi 16B and further pre-trained on a Python programming language dataset, and ""16B"" refers to the number of trainable parameters.; This checkpoint (CodeGen-Mono 16B) was firstly initialized with CodeGen-Multi 16B, and then pre-trained on BigPython dataset. The data consists of 71.7B tokens of Python programming language. See Section 2.1 of the paper for more details.; CodeGen was trained using cross-entropy loss to maximize the likelihood of sequential inputs.
The family of models are trained using multiple TPU-v4-512 by Google, leveraging data and model parallelism.
See Section 2.3 of the paper for more details.; We evaluate our models on two code generation benchmark: HumanEval and MTPB. Please refer to the paper for more details.",natural-language-processing,text-generation,https://huggingface.co/Salesforce/codegen-16B-mono,Paper: https://arxiv.org/pdf/2203.13474.pdf,License: bsd-3-clause,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 105,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3472,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 32.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 13
BiomedNLP-KRISSBERT-PubMed-UMLS-EL,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"https://arxiv.org/pdf/2112.07887.pdf; Entity linking faces significant challenges such as prolific variations and prevalent ambiguities, especially in high-value domains with myriad entities. Standard classification approaches suffer from the annotation bottleneck and cannot effectively handle unseen entities. Zero-shot entity linking has emerged as a promising direction for generalizing to new entities, but it still requires example gold entity mentions during training and canonical descriptions for all entities, both of which are rarely available outside of Wikipedia (Logeswaran et al., 2019; Wu et al., 2020). We explore Knowledge-RIch Self-Supervision (KRISS) and train a contextual encoder (KRISSBERT) for entity linking, by leveraging readily available unlabeled text and domain knowledge.; Specifically, the KRISSBERT model is initialized with PubMedBERT parameters, and then continuously pretrained using biomedical entity names from the UMLS ontology to self-supervise entity linking examples from PubMed abstracts. Experiments on seven standard biomedical entity linking datasets show that KRISSBERT attains new state of the art, outperforming prior self-supervised methods by as much as 20 absolute points in accuracy.
See Zhang et al., 2021 for the details.; Note that some prior systems like BioSyn, SapBERT, and their follow-up work (e.g., Lai et al., 2021) claimed to do entity linking, but their systems completely ignore the context of an entity mention, and can only predict a surface form in the entity dictionary (See Figure 1 in BioSyn), not the canonical entity ID (e.g., CUI in UMLS). Therefore, they can't disambiguate ambiguous mentions. For instance, given the entity mention ""ER"" in the sentence ""ER crowding has become a wide-spread problem"", their systems ignore the sentence context, and simply predict the closest surface form, which is just ""ER"". Multiple entities share this surface form as a potential name or alias, such as Emergency Room (C0562508), Estrogen Receptor Gene (C1414461), and Endoplasmic Reticulum(C0014239). Without using the context information, their systems can't resolve such ambiguity and pinpoint the correct entity Emergency Room (C0562508). More problematically, their evaluation would deem such an ambiguous prediction as correct. Consequently, the reported results in their papers do not reflect true performance on entity linking.; Here, we use the MedMentions data to show you how to 1) generate prototype embeddings, and 2) run entity linking.",multimodel,feature-extraction,https://huggingface.co/microsoft/BiomedNLP-KRISSBERT-PubMed-UMLS-EL,Paper: https://arxiv.org/pdf/2112.07887.pdf,License: mit,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 497,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 438.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
tortoise-tts-v2,,,,,,,,,"Tortoise is a text-to-speech program built with the following priorities:; This repo contains all the code needed to run Tortoise TTS in inference mode.; I'm naming my speech-related repos after Mojave desert flora and fauna. Tortoise is a bit tongue in cheek: this model
is insanely slow. It leverages both an autoregressive decoder and a diffusion decoder; both known for their low
sampling rates. On a K80, expect to generate a medium sized sentence every 2 minutes.; See this page for a large list of example outputs.; If you want to use this on your own computer, you must have an NVIDIA GPU. First, install pytorch using these
instructions: https://pytorch.org/get-started/locally/",,,https://huggingface.co/jbetker/tortoise-tts-v2,Paper: https://arxiv.org/pdf/2102.12092.pdf; https://arxiv.org/pdf/2102.09672.pdf; https://arxiv.org/pdf/2106.07889.pdf,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 109,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 149.31KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
Erlangshen-Roberta-110M-Sentiment,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"中文的RoBERTa-wwm-ext-base在数个情感分析任务微调后的版本; This is the fine-tuned version of the Chinese RoBERTa-wwm-ext-base model on several sentiment analysis datasets.; 基于chinese-roberta-wwm-ext-base，我们在收集的8个中文领域的情感分析数据集，总计227347个样本上微调了一个Semtiment版本。; Based on chinese-roberta-wwm-ext-base, we fine-tuned a sentiment analysis version on 8 Chinese sentiment analysis datasets, with totaling 227,347 samples.; 如果您在您的工作中使用了我们的模型，可以引用我们的论文：",natural-language-processing,text-classification,https://huggingface.co/IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment,Paper: https://arxiv.org/pdf/2209.02970.pdf,License: apache-2.0,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 33,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8608,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 409.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
SentimentAnalysisDistillBERT,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,You can use cURL to access this model:; Or Python API:,natural-language-processing,text-classification,https://huggingface.co/Souvikcmsa/SentimentAnalysisDistillBERT,,,Datasets: Souvikcmsa/autotrain-data-sentiment_analysis,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 105,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 269.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
distilbert-base-uncased-finetuned-conll03-english-int8-static,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"This is an INT8  PyTorch model quantized with huggingface/optimum-intel through the usage of Intel? Neural Compressor. ; The original fp32 model comes from the fine-tuned model elastic/distilbert-base-uncased-finetuned-conll03-english.; The calibration dataloader is the train dataloader. The default calibration sampling size 100 isn't divisible exactly by batch size 8, so the real sampling size is 104.",natural-language-processing,token-classification,https://huggingface.co/Intel/distilbert-base-uncased-finetuned-conll03-english-int8-static,,License: apache-2.0,Datasets: conll2003,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 31,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 68.5MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
sentiment_analysis,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for
further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text-classification.",natural-language-processing,text-classification,https://huggingface.co/sbcBI/sentiment_analysis,Paper: https://arxiv.org/pdf/1810.04805.pdf,License: apache-2.0,Datasets: Confidential,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8423,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 269.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
clip-vit-large-patch14-336,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c6145e63a23d3f282ad5_Zero-Shot%20Image%20Classification.svg,,"This model was trained from scratch on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",computer-vision,zero-shot-image-classification,https://huggingface.co/openai/clip-vit-large-patch14-336,,,,,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 40,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 162793,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
unite-mup,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"This model is the multilingual version of ""UniTE: Unified Translation Evaluation"".",natural-language-processing,fill-mask,https://huggingface.co/ywan/unite-mup,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mT5_m2o_chinese_simplified_crossSum,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"This repository contains the many-to-one (m2o) mT5 checkpoint finetuned on all cross-lingual pairs of the CrossSum dataset, where the target summary was in chinese_simplified, i.e. this model tries to summarize text written in any language in Chinese(Simplified). For finetuning details and scripts, see the paper and the official repository. ; If you use this model, please cite the following paper:",natural-language-processing,summarization,https://huggingface.co/csebuetnlp/mT5_m2o_chinese_simplified_crossSum,Paper: https://arxiv.org/pdf/2112.08804.pdf,,,43 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1776,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mt5_chinese_small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"license: apache-2.0
tags:; This model is a fine-tuned version of google/mt5-small on the None dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed",natural-language-processing,summarization,https://huggingface.co/yihsuan/mt5_chinese_small,,,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 243,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
legalbert-large-1.7M-2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Pretrained model on English language legal and administrative text using the RoBERTa pretraining objective. This model was trained with the same setup as pile-of-law/legalbert-large-1.7M-1, but with a different seed.; Pile of Law BERT large 2 is a transformers model with the BERT large model (uncased) architecture pretrained on the Pile of Law, a dataset consisting of ~256GB of English language legal and administrative text for language model pretraining.; You can use the raw model for masked language modeling or fine-tune it for a downstream task. Since this model was pretrained on a English language legal and administrative text corpus, legal downstream tasks will likely be more in-domain for this model.; You can use the model directly with a pipeline for masked language modeling:; Here is how to use this model to get the features of a given text in PyTorch:",natural-language-processing,fill-mask,https://huggingface.co/pile-of-law/legalbert-large-1.7M-2,Paper: https://arxiv.org/pdf/1907.11692.pdf; https://arxiv.org/pdf/1810.04805.pdf; https://arxiv.org/pdf/2110.00976.pdf; https://arxiv.org/pdf/2207.00220.pdf,,Datasets: pile-of-law/pile-of-law,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 688,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
amazon-review-sentiment-analysis,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is a fine-tuned version of nlptown/bert-base-multilingual-uncased-sentiment on an Amazon US Customer Reviews Dataset. The code for the fine-tuning process can be found
here. This model is uncased: it does
not make a difference between english and English.
It achieves the following results on the evaluation set:; This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).; This model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above, or for further finetuning on related sentiment analysis tasks.; We replaced its head with our customer reviews to fine-tune it on 17,280 rows of training set while validating it on 4,320 rows of dev set. Finally, we evaluated our model performance on a held-out test set: 2,400 rows.; Bert-base is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification, or question answering. This fine-tuned version of BERT-base is used to predict review rating star given the review.",natural-language-processing,text-classification,https://huggingface.co/LiYuan/amazon-review-sentiment-analysis,,License: apache-2.0,,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5920,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 673.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
layoutlmv3-finetuned-funsd,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"This model is a fine-tuned version of microsoft/layoutlmv3-base on the nielsr/funsd-layoutlmv3 dataset.
It achieves the following results on the evaluation set:; The script for training can be found here: https://github.com/huggingface/transformers/tree/main/examples/research_projects/layoutlmv3; More information needed; More information needed; More information needed",natural-language-processing,token-classification,https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd,,,Datasets: nielsr/funsd-layoutlmv3,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1088,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 504.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
KR-SBERT-V40K-klueNLI-augSTS,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",natural-language-processing,sentence-similarity,https://huggingface.co/snunlp/KR-SBERT-V40K-klueNLI-augSTS,,,,Korean,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1667,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 468.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
sentiment_analysis_model,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for
further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text-classification.",natural-language-processing,text-classification,https://huggingface.co/sbcBI/sentiment_analysis_model,Paper: https://arxiv.org/pdf/1810.04805.pdf,License: apache-2.0,Datasets: Confidential,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 39053,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 269.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
kote_for_easygoing_people,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,,natural-language-processing,text-classification,https://huggingface.co/searle-j/kote_for_easygoing_people,,License: mit,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 669,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 499.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Wiki-Complexity,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model detects if you are writing in a format that is more similar to Simple English Wikipedia or English Wikipedia. This can be extended to applications that aren't Wikipedia as well and to some extent, it can be used for other languages.; Please also note there is a major bias to special characters (Mainly the hyphen mark, but it also applies to others) so I would recommend removing them from your input text.; You can use cURL to access this model:; Or Python API:",natural-language-processing,text-classification,https://huggingface.co/hidude562/Wiki-Complexity,,,Datasets: hidude562/autotrain-data-SimpleDetect,English,PyTorch; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 33,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 537.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
distilbert-base-uncased-finetuned-fashion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is a fine-tuned version of distilbert-base-uncased on a munally created dataset in order to detect fashion (label_0) from non-fashion (label_1) items.
It achieves the following results on the evaluation set:; The following hyperparameters were used during training:",natural-language-processing,text-classification,https://huggingface.co/rasta/distilbert-base-uncased-finetuned-fashion,,License: apache-2.0,,,PyTorch; TensorBoard; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 537.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
layoutlmv3-funsd-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,token-classification,https://huggingface.co/nielsr/layoutlmv3-funsd-v2,,,,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 47,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1002.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
finbert-fls,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Forward-looking statements (FLS) inform investors of managers’ beliefs and opinions about firm's future events or results. Identifying forward-looking statements from corporate reports can assist investors in financial analysis. FinBERT-FLS is a FinBERT model fine-tuned on 3,500 manually annotated sentences from Management Discussion and Analysis section of annual reports of Russell 3000 firms.  ; Input: A financial text.; Output: Specific-FLS , Non-specific FLS, or Not-FLS.; You can use this model with Transformers pipeline for forward-looking statement classification.; Visit FinBERT.AI for more details on the recent development of FinBERT.",natural-language-processing,text-classification,https://huggingface.co/yiyanghkust/finbert-fls,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 10096,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 439.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 11
finbert-esg,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"ESG analysis can help investors determine a business' long-term sustainability and identify associated risks. FinBERT-ESG is a FinBERT model fine-tuned on 2,000 manually annotated sentences from firms' ESG reports and annual reports.  ; Input: A financial text.; Output: Environmental, Social, Governance or None.; You can use this model with Transformers pipeline for ESG classification.; Visit FinBERT.AI for more details on the recent development of FinBERT.",natural-language-processing,text-classification,https://huggingface.co/yiyanghkust/finbert-esg,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 104390,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 439.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
yolo_v5s_animal_det_512x512_quant_n2x_cpu_1,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/degirum/yolo_v5s_animal_det_512x512_quant_n2x_cpu_1,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.94MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
yolo_v5s_face_det_512x512_quant_n2x_cpu_1,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/degirum/yolo_v5s_face_det_512x512_quant_n2x_cpu_1,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.82MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
yolo_v5s_household_objects_512x512_quant_n2x_cpu_1,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/degirum/yolo_v5s_household_objects_512x512_quant_n2x_cpu_1,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.86MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
autotrain-smm4h_large_roberta_clean-874027878,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,You can use cURL to access this model:; Or Python API:,natural-language-processing,text-classification,https://huggingface.co/Amalq/autotrain-smm4h_large_roberta_clean-874027878,,,Datasets: Amalq/autotrain-data-smm4h_large_roberta_clean,unk,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 39,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
gpt-4chan,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Given its research scope, intentionally using the model for generating harmful content (non-exhaustive examples: hate speech, spam generation, fake news, harassment and abuse, disparagement, and defamation) on all websites where bots are prohibited is considered a misuse of this model. Head over to the Community page for further discussion and potential next steps.; Project Website: https://gpt-4chan.com; Note that I have no association with any torrents or backups, or other ways of obtaining this model.
However, if you try them, please be safe. Here are the hex md5 hashes for the pytorch_model.bin files:
pytorch_model.bin float32 : 833c1dc19b7450e4e559a9917b7d076a
pytorch_model.bin float16 : db3105866c9563b26f7399fafc00bb4b; GPT-4chan is a language model fine-tuned from GPT-J 6B on 3.5 years worth of data from 4chan's politically incorrect (/pol/) board. ; GPT-4chan was fine-tuned on the dataset Raiders of the Lost Kek: 3.5 Years of Augmented 4chan Posts from the Politically Incorrect Board.",natural-language-processing,text-generation,https://huggingface.co/ykilcher/gpt-4chan,Paper: https://arxiv.org/pdf/2109.07958.pdf,License: apache-2.0,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 111,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
t5-base-tag-generation,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"This model is t5-base fine-tuned on the 190k Medium Articles dataset for predicting article tags using the article textual content as input. While usually formulated as a multi-label classification problem, this model deals with tag generation as a text2text generation task (inspiration from text2tags).; The dataset is composed of Medium articles and their tags. However, each Medium article can have at most five tags, therefore the author needs to choose what he/she believes are the best tags (mainly for SEO-related purposes). This means that an article with the ""Python"" tag may have not the ""Programming Languages"" tag, even though the first implies the latter.; To clean the dataset accounting for this problem, a hand-made taxonomy of about 1000 tags was built. Using the taxonomy, the tags of each articles have been augmented (e.g. an article with the ""Python"" tag will have the ""Programming Languages"" tag as well, as the taxonomy says that ""Python"" is part of ""Programming Languages""). The taxonomy is not public, if you are interested in it please send an email at chiusanofabio94@gmail.com.; The model has been trained on a single epoch spanning about 50000 articles, evaluating on 1000 random articles not used during training.; The following hyperparameters were used during training:",natural-language-processing,text2text-generation,https://huggingface.co/fabiochiu/t5-base-tag-generation,,License: apache-2.0,,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 29,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 98035,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 894.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
bloom-7b1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Version 1.0 / 26.May.2022; This section provides information for anyone who wants to know about the model.; Developed by: BigScience (website); Model Type: Transformer-based Language Model; Version: 1.0.0,natural-language-processing,text-generation,https://huggingface.co/bigscience/bloom-7b1,Paper: https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2110.02861.pdf; https://arxiv.org/pdf/2108.12409.pdf,License: bigscience-bloom-rail-1.0,,48 languages,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 121,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 53365,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 11
tk-instruct-base-def-pos,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Tk-Instruct is a series of encoder-decoder Transformer models that are trained to solve various NLP tasks by following in-context instructions (plain language task definitions, k-shot examples, explanations, etc). Built upon the pre-trained T5 models, they are fine-tuned on a large number of tasks & instructions that are collected in the Natural Instructions benchmark, which contains 1600+ tasks in 70+ broach categories in total. This enables the model to not only process the training tasks, but also generalize to many unseen tasks without further parameter update.; More resources for using the model:; Tk-Instruct can be used to do many NLP tasks by following instructions. ; When instructing the model, task definition or demonstration examples or explanations should be prepended to the original input and fed into the model. You can easily try Tk-Instruct models as follows:; We are still working on understanding the behaviors of these models, but here are several issues we have found:",natural-language-processing,text2text-generation,https://huggingface.co/allenai/tk-instruct-base-def-pos,Paper: https://arxiv.org/pdf/1910.10683.pdf; https://arxiv.org/pdf/2204.07705.pdf,License: apache-2.0,Datasets: Super-NaturalInstructions,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1655,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 499.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
bert-base-uncased_German_MultiLable_classification,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,A model trained on German Hotel Reviews from Switzerland. The base model is the bert-base-german-cased. The last hidden layer of the base model was extracted and a classification layer was added. The entire model was then trained for 5 epochs on our dataset.; ,natural-language-processing,text-classification,https://huggingface.co/Tobias/bert-base-uncased_German_MultiLable_classification,,License: apache-2.0,,German,TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 34,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 438.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
language-detection,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset.; This model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output). 
For additional information please refer to the xlm-roberta-base model card or to the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al.; You can directly use this model as a language detector, i.e. for sequence classification tasks. Currently, it supports the following 20 languages: ; arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh); The model was fine-tuned on the Language Identification dataset, which consists of text sequences in 20 languages. The training set contains 70k samples, while the validation and test sets 10k each. The average accuracy on the test set is 99.6% (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table.",natural-language-processing,text-classification,https://huggingface.co/eleldar/language-detection,Paper: https://arxiv.org/pdf/1911.02116.pdf,License: mit,,,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5028,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
fastspeech2-en-male1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9511aaa882b322dc3d8_Text-to-Speech.svg,,FastSpeech 2 text-to-speech model from fairseq S^2 (paper/code):; See also fairseq S^2 example.,audio,text-to-speech,https://huggingface.co/Voicemod/fastspeech2-en-male1,Paper: https://arxiv.org/pdf/2006.04558.pdf; https://arxiv.org/pdf/2109.06912.pdf,,Datasets: common_voice,English,Fairseq,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 608,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 604.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
tts-tacotron2-ljspeech,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9511aaa882b322dc3d8_Text-to-Speech.svg,,"This repository provides all the necessary tools for Text-to-Speech (TTS)  with SpeechBrain using a Tacotron2 pretrained on LJSpeech.; The pre-trained model takes in input a short text and produces a spectrogram in output. One can get the final waveform by applying a vocoder (e.g., HiFIGAN) on top of the generated spectrogram.; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; If you want to generate multiple sentences in one-shot, you can do in this way:; To perform inference on the GPU, add  run_opts={""device"":""cuda""}  when calling the from_hparams method.",audio,text-to-speech,https://huggingface.co/speechbrain/tts-tacotron2-ljspeech,Paper: https://arxiv.org/pdf/1712.05884.pdf; https://arxiv.org/pdf/2106.04624.pdf,License: apache-2.0,Datasets: LJSpeech,English,speechbrain,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 80,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3557,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 113.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 11
tts-hifigan-ljspeech,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9511aaa882b322dc3d8_Text-to-Speech.svg,,"This repository provides all the necessary tools for using a HiFIGAN vocoder trained with LJSpeech. ; The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram.; The sampling frequency is 22050 Hz.; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; To perform inference on the GPU, add  run_opts={""device"":""cuda""}  when calling the from_hparams method.",audio,text-to-speech,https://huggingface.co/speechbrain/tts-hifigan-ljspeech,Paper: https://arxiv.org/pdf/2010.05646.pdf,License: apache-2.0,Datasets: LJSpeech,English,speechbrain,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3807,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 55.8MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
mobilevit-small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,"MobileViT model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer by Sachin Mehta and Mohammad Rastegari, and first released in this repository. The license used is Apple sample code license.; Disclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.; MobileViT is a light-weight, low latency convolutional neural network that combines MobileNetV2-style layers with a new block that replaces local processing in convolutions with global processing using transformers. As with ViT (Vision Transformer), the image data is converted into flattened patches before it is processed by the transformer layers. Afterwards, the patches are ""unflattened"" back into feature maps. This allows the MobileViT-block to be placed anywhere inside a CNN. MobileViT does not require any positional embeddings.; You can use the raw model for image classification. See the model hub to look for fine-tuned versions on a task that interests you.; Here is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:",computer-vision,image-classification,https://huggingface.co/apple/mobilevit-small,Paper: https://arxiv.org/pdf/2110.02178.pdf,License: other,Datasets: imagenet-1k,,PyTorch; TensorFlow; Core ML; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6185,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 45.4MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
chinese-hubert-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"Pretrained on 10k hours WenetSpeech L subset. More details in  TencentGameMate/chinese_speech_pretrain; This model does not have a tokenizer as it was pretrained on audio alone. 
In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data.; python package:
transformers==4.16.2",multimodel,feature-extraction,https://huggingface.co/TencentGameMate/chinese-hubert-base,,License: mit,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2072,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
mtl-data-to-text,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"The MTL-data-to-text model was proposed in MVP: Multi-task Supervised Pre-training for Natural Language Generation by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.; The detailed information and instructions can be found https://github.com/RUCAIBox/MVP.; MTL-data-to-text is supervised pre-trained using a mixture of labeled data-to-text datasets. It is a variant (Single) of our main MVP model. It follows a standard Transformer encoder-decoder architecture.; MTL-data-to-text is specially designed for data-to-text generation tasks, such as KG-to-text generation (WebNLG, DART), table-to-text generation (WikiBio, ToTTo) and MR-to-text generation (E2E).; MVP: https://huggingface.co/RUCAIBox/mvp.",natural-language-processing,text2text-generation,https://huggingface.co/RUCAIBox/mtl-data-to-text,Paper: https://arxiv.org/pdf/2206.12131.pdf,License: apache-2.0,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 483,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
t5-arabic-text-summarization,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"A fine-tuned AraT5 model on a dataset of 84,764 paragraph-summary pairs.; Paper: Arabic abstractive text summarization using RNN-based and transformer-based architectures.; Dataset: link.; The model can be used as follows:; banimarje@gmail.com",natural-language-processing,text2text-generation,https://huggingface.co/malmarjeh/t5-arabic-text-summarization,,,,Arabic,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3770,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
icebert-xlmr-ic3-iec,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,,natural-language-processing,token-classification,https://huggingface.co/vesteinn/icebert-xlmr-ic3-iec,,License: cc-by-4.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
vit_base-224-in21k-ft-cifar100,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,"This model was trained using Amazon SageMaker and the Hugging Face Deep Learning container,
The base model is Vision Transformer (base-sized model) which  is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels.Link to base model ; Link to dataset description; The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton; The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.
This dataset,CIFAR100, is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a ""fine"" label (the class to which it belongs) and a ""coarse"" label (the superclass to which it belongs).; Sizes of datasets:",computer-vision,image-classification,https://huggingface.co/edumunozsala/vit_base-224-in21k-ft-cifar100,Paper: https://arxiv.org/pdf/2006.03677.pdf,License: apache-2.0,Datasets: cifar100,Spanish,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2712,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 344.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
tab_transformer,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c41b3f6ae3d03dc10e54_Tabular%20Classification.svg,,"This repo contains the trained model of Structured data learning with TabTransformer.
The full credit goes to: Khalid Salama; Spaces Link: ; The following hyperparameters were used during training:; Model history needed; ",tabular,tabular-classification,https://huggingface.co/keras-io/tab_transformer,,,,,TensorBoard; Keras,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.88MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
swinv2-tiny-patch4-window8-256,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,"Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. ; Disclaimer: The team releasing Swin Transformer v2 did not write a model card for this model so this model card has been written by the Hugging Face team.; The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. In contrast, previous vision Transformers produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of self-attention globally.; Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.; ",computer-vision,image-classification,https://huggingface.co/microsoft/swinv2-tiny-patch4-window8-256,Paper: https://arxiv.org/pdf/2111.09883.pdf,License: apache-2.0,Datasets: imagenet-1k,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5658,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 113.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
