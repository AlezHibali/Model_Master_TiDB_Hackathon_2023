model_name,task,libraries,language,license,paper,likes,datasets,downloads_last_month,model_size,model_usage,huggingface_link,abstract
Llama-2-7b,Text Generation,PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,896,,0,0,2,https://huggingface.co/meta-llama/Llama-2-7b,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-70b-chat-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,https://arxiv.org/pdf/2307.09288.pdf,550,,"41,430",0,94,https://huggingface.co/meta-llama/Llama-2-70b-chat-hf,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-7b-chat-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,https://arxiv.org/pdf/2307.09288.pdf,328,,"77,918",0,57,https://huggingface.co/meta-llama/Llama-2-7b-chat-hf,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-70b-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,https://arxiv.org/pdf/2307.09288.pdf,307,,"155,399",0,45,https://huggingface.co/meta-llama/Llama-2-70b-hf,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-13B-chat-GGML,Text Generation,Transformers; PyTorch,English,other,,264,,796,119316.5128,6,https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 13B-chat.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
stable-diffusion-xl-base-0.9,Text-to-Image,Diffusers,,other,https://arxiv.org/pdf/2108.01073.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2307.01952.pdf,1.2k,,"245,508",0,49,https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9,"SDXL 0.9 Research License Agreement; Copyright (c) Stability AI Ltd.; This License Agreement (as may be amended in accordance with this License Agreement, “License”), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (“Licensee” or “you”) and Stability AI Ltd. (“Stability AI” or “we”) applies to your use of any computer program, algorithm, source code, object code, software, models, or model weights that is made available by Stability AI under this License (“Software”) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (“Documentation”). By using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; LICENSE GRANTa. Subject to your compliance with the Documentation and Sections 2, 3, and 5, Stability AI grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty free and limited license under Stability AI’s copyright interests to use, reproduce, and create derivative works of the Software solely for your non-commercial research purposes. The foregoing license is personal to you, and you may not assign, sublicense, distribute, publish, host, or otherwise make available this Software, derivative works of the Software, models or model weights associated with the Software, this License, or any other rights or obligations under this License without Stability AI’s prior written consent; any such assignment or sublicense without Stability AI’s prior written consent will be void and will automatically and immediately terminate this License.  For sake of clarity, this License does not grant to you the right or ability to extend any license to the Software, derivative works of the Software, or associated models or model weights to a non-Licensee, nor does this License permit you to create a new Licensee, such as by making available a copy of this License.  If you would like rights not granted by this License, you may seek permission by sending an email to legal@stability.ai.b. You may make a reasonable number of copies of the Documentation solely for your use in connection with the license to the Software granted above.c. The grant of rights expressly set forth in this Section 1 (License Grant) are the complete grant of rights to you in the Software Products, and no other licenses are granted, whether by waiver, estoppel, implication, equity or otherwise. Stability AI and its licensors reserve all rights not expressly granted by this License.; RESTRICTIONSYou will not, and will not permit, assist or cause any third party to:a. use, modify, copy, reproduce, create derivative works of, or distribute the Software Products (or any derivative works thereof, works incorporating the Software Products, or any data produced by the Software), in whole or in part, for (i) any commercial or production purposes, (ii) military purposes or in the service of nuclear technology, (iii) purposes of surveillance, including any research or development relating to surveillance, (iv) biometric processing, (v) in any manner that infringes, misappropriates, or otherwise violates any third-party rights, or (vi) in any manner that violates any applicable law and violating any privacy or security laws, rules, regulations, directives, or governmental requirements (including the General Data Privacy Regulation (Regulation (EU) 2016/679), the California Consumer Privacy Act, and any and all laws governing the processing of biometric information), as well as all amendments and successor laws to any of the foregoing;b. alter or remove copyright and other proprietary notices which appear on or in the Software Products;c. utilize any equipment, device, software, or other means to circumvent or remove any security or protection used by Stability AI in connection with the Software, or to circumvent or remove any usage restrictions, or to enable functionality disabled by Stability AI; ord. offer or impose any terms on the Software Products that alter, restrict, or are inconsistent with the terms of this License.e. 1) violate any applicable U.S. and non-U.S. export control and trade sanctions laws (“Export Laws”); 2) directly or indirectly export, re-export, provide, or otherwise transfer Software Products: (a) to any individual, entity, or country prohibited by Export Laws; (b) to anyone on U.S. or non-U.S. government restricted parties lists; or (c) for any purpose prohibited by Export Laws, including nuclear, chemical or biological weapons, or missile technology applications; 3) use or download Software Products if you or they are: (a) located in a comprehensively sanctioned jurisdiction, (b) currently listed on any U.S. or non-U.S. restricted parties list, or (c) for any purpose prohibited by Export Laws; and (4) will not disguise your location through IP proxying or other methods."
FreeWilly2,Text Generation,PyTorch; Transformers,English,cc-by-nc-4.0,https://arxiv.org/pdf/2307.09288.pdf; https://arxiv.org/pdf/2306.02707.pdf,220,conceptofmind/cot_submix_original; conceptofmind/flan2021_submix_original; conceptofmind/t0_submix_original; conceptofmind/niv2_submix_original,18,282554.7217,5,https://huggingface.co/stabilityai/FreeWilly2,"FreeWilly2 is a Llama2 70B model finetuned on an Orca style Dataset; Start chatting with FreeWilly2 using the following code snippet:; FreeWilly should be used with this prompt format:; FreeWilly2 is trained on our internal Orca-style dataset; Models are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (BF16), and optimized with AdamW. We outline the following hyperparameters:"
Llama-2-13b-chat-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,https://arxiv.org/pdf/2307.09288.pdf,213,,"14,790",0,30,https://huggingface.co/meta-llama/Llama-2-13b-chat-hf,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-7b-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,https://arxiv.org/pdf/2307.09288.pdf,187,,"19,069",0,10,https://huggingface.co/meta-llama/Llama-2-7b-hf,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-13b-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,https://arxiv.org/pdf/2307.09288.pdf,155,,"39,218",0,11,https://huggingface.co/meta-llama/Llama-2-13b-hf,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-70b,Text Generation,PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,151,,0,0,3,https://huggingface.co/meta-llama/Llama-2-70b,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-70b-chat,Text Generation,PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,151,,0,0,2,https://huggingface.co/meta-llama/Llama-2-70b-chat,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
chatglm2-6b,,PyTorch; Transformers,Chinese; English,,https://arxiv.org/pdf/2103.10360.pdf; https://arxiv.org/pdf/2210.02414.pdf; https://arxiv.org/pdf/1911.02150.pdf,1.24k,,"1,402,185",12801.13068,65,https://huggingface.co/THUDM/chatglm2-6b,"
  ? Github Repo ? ? Twitter ? ? [GLM@ACL 22] [GitHub] ? ? [GLM-130B@ICLR 23] [GitHub] 
; 
    ? Join our Slack and WeChat
; ChatGLM2-6B 是开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM2-6B 引入了如下新特性：; ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the following new features:; 可以通过如下代码调用 ChatGLM-6B 模型来生成对话："
Llama-2-7b-chat,Text Generation,PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,142,,0,0,9,https://huggingface.co/meta-llama/Llama-2-7b-chat,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-13B-chat-GPTQ,Text Generation,Transformers; PyTorch,English,other,,122,,"7,726",7436.603779,,https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 13B-chat.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; To continue a conversation:"
Llama-2-13B-GGML,Text Generation,Transformers; PyTorch,English,other,,110,,139,119316.5121,4,https://huggingface.co/TheBloke/Llama-2-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
Baichuan-13B-Chat,Text Generation,PyTorch; Transformers,Chinese; English,,https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2009.03300.pdf,386,,"983,805",27168.33809,2,https://huggingface.co/baichuan-inc/Baichuan-13B-Chat,"Baichuan-13B-Chat为Baichuan-13B系列模型中对齐后的版本，预训练模型可见Baichuan-13B-Base。; Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。本次发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。Baichuan-13B 有如下几个特点：; Baichuan-13B-Chat is the aligned version in the Baichuan-13B series of models, and the pre-trained model can be found at Baichuan-13B-Base.; Baichuan-13B is an open-source, commercially usable large-scale language model developed by Baichuan Intelligence, following Baichuan-7B. With 13 billion parameters, it achieves the best performance in standard Chinese and English benchmarks among models of its size. This release includes two versions: pre-training (Baichuan-13B-Base) and alignment (Baichuan-13B-Chat). Baichuan-13B has the following features:; 如下是一个使用Baichuan-13B-Chat进行对话的示例，正确输出为""乔戈里峰。世界第二高峰―――乔戈里峰西方登山者称其为k2峰，海拔高度是8611米，位于喀喇昆仑山脉的中巴边境上"""
Llama-2-13b,Text Generation,PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,98,,0,0,,https://huggingface.co/meta-llama/Llama-2-13b,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-7B-GGML,Text Generation,Transformers; PyTorch,English,other,,95,,351,61870.11286,,https://huggingface.co/TheBloke/Llama-2-7B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
Llama-2-7B-Chat-GGML,Text Generation,Transformers; PyTorch,English,other,,95,,574,61870.11276,6,https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 7b Chat.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
stable-diffusion-v1-5,Text-to-Image,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/2207.12598.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/1910.09700.pdf,8.74k,,"6,901,572",24514.57802,1546,https://huggingface.co/runwayml/stable-diffusion-v1-5,"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
For more information about how Stable Diffusion functions, please have a look at ?'s Stable Diffusion blog.; The Stable-Diffusion-v1-5 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 
checkpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on ""laion-aesthetics v2 5+"" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.; You can use this both with the ?Diffusers library and the RunwayML GitHub repository.; For more detailed instructions, use-cases and examples in JAX follow the instructions here; Download the weights "
chatglm-fitness-RLHF,,PyTorch; PEFT,Chinese; English,apache-2.0,,131,,0,12392.44954,1,https://huggingface.co/fb700/chatglm-fitness-RLHF,模型体验地址：https://huggingface.co/spaces/fb700/chatglm-fitness-RLHF; ChatGLM-6B 是开源中英双语对话模型，本次训练基于ChatGLM-6B 的第一代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上开展训练。通过训练我们对模型有了更深刻的认知，LLM在一直在进化，好的方法和数据可以挖掘出模型的更大潜能。; 首先，用40万条高质量数据进行强化训练，以提高模型的基础能力；; 第二，使用30万条人类反馈数据，构建一个表达方式规范优雅的语言模式（RM模型）；; 第三，在保留SFT阶段三分之一训练数据的同时，增加了30万条fitness数据，叠加RM模型，对ChatGLM-6B进行强化训练。
Llama-2-13b-chat,Text Generation,PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,83,,0,0,4,https://huggingface.co/meta-llama/Llama-2-13b-chat,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
ruGPT-3.5-13B,Text Generation,PyTorch; Transformers,English; Russian,mit,,79,,970,53885.92852,3,https://huggingface.co/ai-forever/ruGPT-3.5-13B,"Language model for Russian. Model has 13B parameters as you can guess from it's name. This is our biggest model so far and it was used for trainig GigaChat (read more about it in the article).; Model was pretrained on a 300Gb of various domains, than additionaly trained on the 100 Gb of code and legal documets. Here is the dataset structure:; ; Training data was deduplicated, the text deduplication includes 64-bit hashing of each text in the corpus for keeping texts with a unique hash. We also filter the documents based on their text compression rate using zlib4. The most strongly and weakly compressing deduplicated texts are discarded.; Model was trained using Deepspeed and Megatron libraries, on 300B tokens dataset for 3 epochs, around 45 days on 512 V100. After that model was finetuned 1 epoch with sequence length 2048 around 20 days on 200 GPU A100 on additional data (see above)."
ControlNet-v1-1,,,,openrail,,2.06k,,0,20787.22835,,https://huggingface.co/lllyasviel/ControlNet-v1-1,"This is the model files for ControlNet 1.1.
This model card will be filled in a more detailed way after 1.1 is officially merged into ControlNet.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
falcon-40b,Text Generation,PyTorch; Transformers,4 languages,apache-2.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2306.01116.pdf,2.14k,tiiuae/falcon-refinedweb,"151,418",85660.43192,40,https://huggingface.co/tiiuae/falcon-40b,"Falcon-40B is a 40B parameters causal decoder-only model built by TII and trained on 1,000B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.; Paper coming soon ?.; We get it. AI is everywhere! Is it taking over? ; Before we debate the scant likelihood of a cyborg assassin from the future terminating humanity, let’s get to know the newbie that has soared to top-spot on the leaderboard C Falcon 40B.; Falcon 40B is the UAE’s and the Middle East’s first home-grown, open-source large language model (LLM) with 40 billion parameters trained on one trillion tokens. The brainchild of the Technology Innovation Institute (TII), Falcon 40B has generated a tremendous amount of global interest and intrigue, but what really sweetens the deal is its transparent, open-source feature. "
Llama-2-70B-chat-GPTQ,Text Generation,Transformers; PyTorch,English,other,,71,,"8,020",36149.56646,,https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 70B Chat.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware for these quantisations!"
LLongMA-2-7b,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2212.10554.pdf,69,,272,13824.55466,1,https://huggingface.co/conceptofmind/LLongMA-2-7b,"LLongMA-2, a suite of Llama-2 models, trained at 8k context length using linear positional interpolation scaling. The model was trained in collaboration with Emozilla of NousResearch and Kaiokendev.; We worked directly with Kaiokendev, to extend the context length of the Llama-2 7b model through fine-tuning. The models pass all our evaluations and maintain the same perplexity at 8k extrapolation surpassing the performance of other recent methodologies.; The model has identical performance to LLaMA 2 under 4k context length, performance scales directly to 8k, and works out-of-the-box with the new version of transformers (4.31) or with trust_remote_code for <= 4.30.; A Llama-2 13b model trained at 8k will release soon on huggingface here: https://huggingface.co/conceptofmind/LLongMA-2-13b; Applying the method to the rotary position embedding requires only slight changes to the model's code by dividing the positional index, t, by a scaling factor."
llama-30b-instruct-2048,Text Generation,PyTorch; Transformers,English,,,67,sciq; metaeval/ScienceQA_text_only; GAIR/lima; Open-Orca/OpenOrca; openbookqa,725,66634.06468,5,https://huggingface.co/upstage/llama-30b-instruct-2048,No other data was used except for the dataset mentioned above
Nous-Hermes-Llama2-13b,Text Generation,PyTorch; Transformers,English,,,63,,362,26657.95247,1,https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b,"Compute provided by our project sponsor Redmond AI, thank you! Follow RedmondAI on Twitter @RedmondAI.; Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Emozilla leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors.; This Hermes model uses the exact same dataset as Hermes on Llama-1. This is to ensure consistency between the old Hermes and new, for anyone who wanted to keep Hermes as similar to the old one, just more capable.; This model stands out for its long responses, lower hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 4096 sequence length on an 8x a100 80GB DGX machine.; 


"
control_v1p_sd15_qrcode_monster,,Diffusers,English,openrail++,,218,,"2,805",5177.407102,4,https://huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster,"; This model is made to generate creative QR codes that still scan.
Keep in mind that not all generated codes might be readable, but you can try different parameters and prompts to get the desired results.; NEW VERSION; Introducing the upgraded version of our model - Controlnet QR code Monster v2.
V2 is a huge upgrade over v1, for scannability AND creativity.; QR codes can now seamlessly blend the image by using a gray-colored background (#808080)."
Luna-AI-Llama2-Uncensored,Text Generation,PyTorch; Transformers,,cc-by-sa-4.0,,61,,498,27597.57421,3,https://huggingface.co/Tap-M/Luna-AI-Llama2-Uncensored,"“Luna AI Llama2 Uncensored” is a Llama2 based Chat model fine-tuned on over 40,000 long form chat discussions 
  This model was fine-tuned by Tap, the creator of Luna AI.  
  The result is an enhanced Llama2 7b model that rivals ChatGPT in performance across a variety of tasks.; This model stands out for its long responses,  low hallucination rate, and absence of censorship mechanisms. ; The fine-tuning process was performed on an 8x a100 80GB machine.
  The model was trained almost entirely on synthetic outputs.
  This includes data from diverse sources which we included to create our custom dataset, it includes multiple rounds of chats between Human & AI.
; 4bit GPTQ Version provided by @TheBloke - for GPU inference
GGML Version provided by @TheBloke - For CPU inference; The model follows the Vicuna 1.1/ OpenChat format:"
Llama-2-13B-GPTQ,Text Generation,Transformers; PyTorch,English,other,,60,,"2,021",7436.602048,,https://huggingface.co/TheBloke/Llama-2-13B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements."
waifu-diffusion-xl,Text-to-Image,,English,unknown,,102,,0,14233.62598,,https://huggingface.co/hakurei/waifu-diffusion-xl,"waifu-diffusion-xl is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning StabilityAI's SDXL 0.9 model provided as a research preview.; ; masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck; This model has been released under the SDXL 0.9 RESEARCH LICENSE AGREEMENT due to the repository containing the SDXL 0.9 weights before an official release. We have been given permission to release this model.; This model can be used for entertainment purposes and as a generative art assistant."
Llama-2-7b-chat-coreml,Text Generation,Core ML; Transformers,,other,,56,,239,1.860422974,1,https://huggingface.co/pcuenq/Llama-2-7b-chat-coreml,"This is a Core ML version of meta-llama/Llama-2-7b-chat-hf. For license information, model details and acceptable use policy, please refer to the original model card.; This conversion was performed in float16 mode with a fixed sequence length of 64, and is intended for evaluation and test purposes. Please, open a conversation in the Community tab if you have questions or want to report an issue."
stable-diffusion-2-1,Text-to-Image,Diffusers,,openrail++,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,2.93k,,"842,185",21340.17387,796,https://huggingface.co/stabilityai/stable-diffusion-2-1,"This model card focuses on the model associated with the Stable Diffusion v2-1 model, codebase available here.; This stable-diffusion-2-1 model is fine-tuned from stable-diffusion-2 (768-v-ema.ckpt) with an additional 55k steps on the same dataset (with punsafe=0.1), and then fine-tuned for another 155k extra steps with punsafe=0.98.; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model; Language(s): English"
stable-diffusion-xl-refiner-0.9,Image-to-Image,Diffusers,,other,https://arxiv.org/pdf/2108.01073.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2307.01952.pdf,273,,"91,225",0,13,https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9,"SDXL 0.9 Research License Agreement; Copyright (c) Stability AI Ltd.; This License Agreement (as may be amended in accordance with this License Agreement, “License”), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (“Licensee” or “you”) and Stability AI Ltd. (“Stability AI” or “we”) applies to your use of any computer program, algorithm, source code, object code, software, models, or model weights that is made available by Stability AI under this License (“Software”) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (“Documentation”). By using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; LICENSE GRANTa. Subject to your compliance with the Documentation and Sections 2, 3, and 5, Stability AI grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty free and limited license under Stability AI’s copyright interests to use, reproduce, and create derivative works of the Software solely for your non-commercial research purposes. The foregoing license is personal to you, and you may not assign, sublicense, distribute, publish, host, or otherwise make available this Software, derivative works of the Software, models or model weights associated with the Software, this License, or any other rights or obligations under this License without Stability AI’s prior written consent; any such assignment or sublicense without Stability AI’s prior written consent will be void and will automatically and immediately terminate this License.  For sake of clarity, this License does not grant to you the right or ability to extend any license to the Software, derivative works of the Software, or associated models or model weights to a non-Licensee, nor does this License permit you to create a new Licensee, such as by making available a copy of this License.  If you would like rights not granted by this License, you may seek permission by sending an email to legal@stability.ai.b. You may make a reasonable number of copies of the Documentation solely for your use in connection with the license to the Software granted above.c. The grant of rights expressly set forth in this Section 1 (License Grant) are the complete grant of rights to you in the Software Products, and no other licenses are granted, whether by waiver, estoppel, implication, equity or otherwise. Stability AI and its licensors reserve all rights not expressly granted by this License.; RESTRICTIONSYou will not, and will not permit, assist or cause any third party to:a. use, modify, copy, reproduce, create derivative works of, or distribute the Software Products (or any derivative works thereof, works incorporating the Software Products, or any data produced by the Software), in whole or in part, for (i) any commercial or production purposes, (ii) military purposes or in the service of nuclear technology, (iii) purposes of surveillance, including any research or development relating to surveillance, (iv) biometric processing, (v) in any manner that infringes, misappropriates, or otherwise violates any third-party rights, or (vi) in any manner that violates any applicable law and violating any privacy or security laws, rules, regulations, directives, or governmental requirements (including the General Data Privacy Regulation (Regulation (EU) 2016/679), the California Consumer Privacy Act, and any and all laws governing the processing of biometric information), as well as all amendments and successor laws to any of the foregoing;b. alter or remove copyright and other proprietary notices which appear on or in the Software Products;c. utilize any equipment, device, software, or other means to circumvent or remove any security or protection used by Stability AI in connection with the Software, or to circumvent or remove any usage restrictions, or to enable functionality disabled by Stability AI; ord. offer or impose any terms on the Software Products that alter, restrict, or are inconsistent with the terms of this License.e. 1) violate any applicable U.S. and non-U.S. export control and trade sanctions laws (“Export Laws”); 2) directly or indirectly export, re-export, provide, or otherwise transfer Software Products: (a) to any individual, entity, or country prohibited by Export Laws; (b) to anyone on U.S. or non-U.S. government restricted parties lists; or (c) for any purpose prohibited by Export Laws, including nuclear, chemical or biological weapons, or missile technology applications; 3) use or download Software Products if you or they are: (a) located in a comprehensively sanctioned jurisdiction, (b) currently listed on any U.S. or non-U.S. restricted parties list, or (c) for any purpose prohibited by Export Laws; and (4) will not disguise your location through IP proxying or other methods."
starcoder,Text Generation,PyTorch; Transformers,,bigcode-openrail-m,https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,2.11k,bigcode/the-stack-dedup,"70,238",0,68,https://huggingface.co/bigcode/starcoder,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Please read the BigCode OpenRAIL-M license agreement before accepting it.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; ; Play with the model on the StarCoder Playground."
m3e-base,,PyTorch; Safetensors; Sentence Transformers,Chinese; English,,,350,,"85,440",818.5652066,4,https://huggingface.co/moka-ai/m3e-base,m3e-small | m3e-base; M3E 是 Moka Massive Mixed Embedding 的缩写; 说明：; Tips:; 您需要先安装 sentence-transformers
Luna-AI-Llama2-Uncensored-GGML,,Transformers,,other,,46,,30,61870.10553,1,https://huggingface.co/TheBloke/Luna-AI-Llama2-Uncensored-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Tap-M's Luna AI Llama2 Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!"
llama2_7b_chat_uncensored,Text Generation,PyTorch; TensorBoard; Transformers,,other,,45,ehartford/wizard_vicuna_70k_unfiltered,61,27597.3305,6,https://huggingface.co/georgesung/llama2_7b_chat_uncensored,"Fine-tuned Llama-2 7B with an uncensored/unfiltered Wizard-Vicuna conversation dataset ehartford/wizard_vicuna_70k_unfiltered.
Used QLoRA for fine-tuning. Trained for one epoch on a 24GB GPU (NVIDIA A10G) instance, took ~19 hours to train.; The version here is the fp16 HuggingFace model.; Thanks to TheBloke, he has created the GGML and GPTQ versions:; The model was trained with the following prompt style:; Code used to train the model is available here."
Chinese-Llama-2-7b,Text Generation,PyTorch; Transformers,Chinese; English,openrail,,43,LinkSoul/instruction_merge_set,147,27597.32159,1,https://huggingface.co/LinkSoul/Chinese-Llama-2-7b,"全部开源，完全可商用的中文版 Llama2 模型及中英文 SFT 数据集，输入格式严格遵循 llama-2-chat 格式，兼容适配所有针对原版 llama-2-chat 模型的优化。; ; ; Talk is cheap, Show you the Demo.; 模型下载：Chinese Llama2 Chat Model"
whisper-large-v2,Automatic Speech Recognition,PyTorch; TensorFlow; JAX; Transformers,99 languages,apache-2.0,https://arxiv.org/pdf/2212.04356.pdf,762,,"199,338",18958.00115,122,https://huggingface.co/openai/whisper-large-v2,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al. from OpenAI. The original code repository can be found here.; Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization 
for improved performance.; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. "
Redmond-Puffin-13B,Text Generation,PyTorch; Transformers,eng,mit,,41,,308,26657.0927,1,https://huggingface.co/NousResearch/Redmond-Puffin-13B,"; The first commercially available language model released by Nous Research!; Redmond-Puffin-13B is one of the worlds first llama-2 based, fine-tuned language models, leveraging a hand curated set of 3K high quality examples, many of which take full advantage of the 4096 context length of Llama 2. This model was fine-tuned by Nous Research, with LDJ leading the training and dataset curation, along with significant dataset formation contributions by J-Supha.; Special thank you to Redmond AI for sponsoring the compute.; Special thank you to Emozilla for assisting with training experimentations and many issues encountered during training."
llama2-7b-chat-hf-codeCherryPop-qLoRA-merged,,PyTorch; Transformers,,,,37,,40,13805.88207,,https://huggingface.co/TokenBender/llama2-7b-chat-hf-codeCherryPop-qLoRA-merged,"  description:;   additional_info:;   next_steps: ""I've a few things in mind and after that this will be more valuable."";   tasks:;   commercial_use: |
    So far I think this can be used commercially but this is a adapter on Meta's llama2 with
    some gating issues so that is there.
  contact_info: ""If you find any issues or want to just holler at me, you can reach out to me - https://twitter.com/4evaBehindSOTA"""
falcon-7b-instruct,Text Generation,PyTorch; Core ML; Transformers,English,apache-2.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2306.01116.pdf,516,tiiuae/falcon-refinedweb,"526,338",14779.12882,77,https://huggingface.co/tiiuae/falcon-7b-instruct,"Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ? To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF!; ? This is an instruct model, which may not be ideal for further finetuning. If you are interested in building your own instruct/chat model, we recommend starting from Falcon-7B. ; ? Looking for an even more powerful model? Falcon-40B-Instruct is Falcon-7B-Instruct's big brother!"
Nous-Hermes-Llama2-GGML,,Transformers,English,other,,33,,4,121057.3112,,https://huggingface.co/TheBloke/Nous-Hermes-Llama2-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Nous Research's Nous Hermes Llama 2 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
bloom,Text Generation,PyTorch; TensorBoard; Safetensors; Transformers,46 languages,bigscience-bloom-rail-1.0,https://arxiv.org/pdf/2211.05100.pdf; https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2110.02861.pdf; https://arxiv.org/pdf/2108.12409.pdf,3.85k,,"15,365",229488.7332,322,https://huggingface.co/bigscience/bloom,"BigScience Large Open-science Open-access Multilingual Language ModelVersion 1.3 / 6 July 2022; Current Checkpoint: Training Iteration  95000; Link to paper: here; Total seen tokens: 366B; BLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks."
falcon-40b-instruct,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2306.01116.pdf,1.01k,tiiuae/falcon-refinedweb,"327,284",85660.42921,28,https://huggingface.co/tiiuae/falcon-40b-instruct,"Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ? To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF!; ? This is an instruct model, which may not be ideal for further finetuning. If you are interested in building your own instruct/chat model, we recommend starting from Falcon-40B. ; ? Looking for a smaller, less expensive model? Falcon-7B-Instruct is Falcon-40B-Instruct's little brother!"
OpenOrca-Preview1-13B,Text Generation,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2301.13688.pdf,128,Open-Orca/OpenOrca,"1,659",26655.61812,8,https://huggingface.co/Open-Orca/OpenOrca-Preview1-13B,"; ; ; We have used our own OpenOrca dataset to fine-tune LLaMA-13B.
This dataset is our attempt to reproduce the dataset generated for Microsoft Research's Orca Paper.; We have trained on less than 6% of our data, just to give a preview of what is possible while we further refine our dataset!
We trained a refined selection of 200k GPT-4 entries from OpenOrca.
We have filtered our GPT-4 augmentations to remove statements like, ""As an AI language model..."" and other responses which have been shown to harm model reasoning capabilities. Further details on our dataset curation practices will be forthcoming with our full model releases."
Llama-2-7b-Chat-GPTQ,Text Generation,Transformers; PyTorch,English,other,,32,,"3,136",3995.96293,,https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 7b Chat.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; To continue a conversation:"
Luna-AI-Llama2-Uncensored-GPTQ,Text Generation,Transformers,,other,,32,,631,3994.115373,,https://huggingface.co/TheBloke/Luna-AI-Llama2-Uncensored-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Tap-M's Luna AI Llama2 Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!"
Redmond-Puffin-13B-Preview,Text Generation,PyTorch; Transformers,eng,mit,,31,,26,26657.08699,,https://huggingface.co/NousResearch/Redmond-Puffin-13B-Preview,"; The first commercially available language model released by Nous Research!; Redmond-Puffin-13B is one of the worlds first llama-2 based, fine-tuned language models, leveraging a hand curated set of 3K high quality examples, many of which take full advantage of the 4096 context length of Llama 2. This model was fine-tuned by Nous Research, with LDJ leading the training and dataset curation, along with significant dataset formation contributions by J-Supha.; Special thank you to Redmond AI for sponsoring the compute.; Special thank you to Emozilla for assisting with training experimentations and many issues encountered during training."
vicuna-33b-v1.3,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,175,,"20,883",66632.22223,6,https://huggingface.co/lmsys/vicuna-33b-v1.3,"Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning.
The training data is around 140K conversations collected from ShareGPT.com.
See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.; Vicuna is evaluated with standard benchmarks, human preference, and LLM-as-a-judge. See more details in this paper and leaderboard.; See vicuna_weights_version.md"
Llama-2-13B-fp16,Text Generation,PyTorch; Transformers,English,,,30,,"9,906",26657.10888,,https://huggingface.co/TheBloke/Llama-2-13B-fp16,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are fp16 format model files for Meta's Llama 2 13B.; They were produced by downloading the PTH files from Meta, and then converting to HF format using the latest Transformers 4.32.0.dev0, from Git, with the Llama 2 PR included: https://github.com/huggingface/transformers/pull/24891.; Command to convert was:"
Llama-2-70B-GPTQ,Text Generation,Transformers; PyTorch,English,other,,30,,"1,822",36149.5651,,https://huggingface.co/TheBloke/Llama-2-70B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 70B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware for these quantisations!"
FreeWilly2-GPTQ,Text Generation,Transformers,English,other,https://arxiv.org/pdf/2307.09288.pdf; https://arxiv.org/pdf/2306.02707.pdf,30,conceptofmind/cot_submix_original; conceptofmind/flan2021_submix_original; conceptofmind/t0_submix_original; conceptofmind/niv2_submix_original,176,36149.55829,,https://huggingface.co/TheBloke/FreeWilly2-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GPTQ model files for Stability AI's FreeWilly 2.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; None"
all-MiniLM-L6-v2,Sentence Similarity,PyTorch; TensorFlow; Rust; Sentence Transformers,English,apache-2.0,https://arxiv.org/pdf/1904.06472.pdf; https://arxiv.org/pdf/2102.07033.pdf; https://arxiv.org/pdf/2104.08727.pdf; https://arxiv.org/pdf/1704.05179.pdf; https://arxiv.org/pdf/1810.09305.pdf,733,s2orc; flax-sentence-embeddings/stackexchange_xml; ms_marco; gooaq; yahoo_answers_topics; code_search_net; search_qa; eli5; snli; multi_nli; wikihow; natural_questions; trivia_qa; embedding-data/sentence-compression; embedding-data/flickr30k-captions; embedding-data/altlex; embedding-data/simple-wiki; embedding-data/QQP; embedding-data/SPECTER; embedding-data/PAQ_pairs; embedding-data/WikiAnswers,"1,932,147",273.5459323,135,https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2,"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net"
ioc-controlnet,,,,,,126,,0,0.002258797,2,https://huggingface.co/ioclab/ioc-controlnet," Download our ControlNet Models for AUTOMATIC1111 Stable Diffusion Web UI!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
FreeWilly1-Delta-SafeTensor,Text Generation,Safetensors; Transformers,English,cc-by-nc-4.0,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.02707.pdf,29,conceptofmind/cot_submix_original; conceptofmind/flan2021_submix_original; conceptofmind/t0_submix_original; conceptofmind/niv2_submix_original,0,133654.8837,1,https://huggingface.co/stabilityai/FreeWilly1-Delta-SafeTensor,"FreeWilly is a Llama65B model fine-tuned on an Orca style Dataset; FreeWilly1 cannot be used from the stabilityai/FreeWilly1-Delta-SafeTensor weights alone. To obtain the correct model, one must add back the difference between LLaMA 65B and stabilityai/FreeWilly1-Delta-SafeTensor weights. We provide the apply_delta.py script to automate the conversion, which you can run as:; Start chatting with FreeWilly using the following code snippet:; FreeWilly should be used with prompts formatted similarly to Alpaca as below:; FreeWilly is trained on our internal Orca-style dataset"
chatglm-6b,,PyTorch; Transformers,Chinese; English,,https://arxiv.org/pdf/2103.10360.pdf; https://arxiv.org/pdf/2210.02414.pdf,2.53k,,"586,003",13734.71223,200,https://huggingface.co/THUDM/chatglm-6b,"
   ? Blog ? ? Github Repo ? ? Twitter ? ? [GLM@ACL 22] [GitHub] ? ? [GLM-130B@ICLR 23] [GitHub] 
; 
    ? Join our Slack and WeChat
; 我们发布了 ChatGLM2-6B，ChatGLM-6B 的升级版本，在保留了了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，引入了更强大的性能、更长的上下文、更高效的推理等升级。; ChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 ChatGLM 相同的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。 ChatGLM-6B 权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。; ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM-6B uses technology similar to ChatGPT, optimized for Chinese QA and dialogue. The model is trained for about 1T tokens of Chinese and English corpus, supplemented by supervised fine-tuning, feedback bootstrap, and reinforcement learning with human feedback. With only about 6.2 billion parameters, the model is able to generate answers that are in line with human preference. ChatGLM-6B weights are completely open for academic research, and free commercial use is also allowed after completing the questionnaire."
dolphin-llama-13b,Text Generation,PyTorch; Transformers,,other,,33,,0,26657.09109,,https://huggingface.co/ehartford/dolphin-llama-13b,"Dolphin ?
https://erichartford.com/dolphin; This model is based on llama1, so it is for non-commercial use only.  Future versions will be trained on llama2 and other open models that are suitable for commercial use.; This model is uncensored.  I have filtered the dataset to remove alignment and bias.  This makes the model compliant to any requests.  You are advised to implement your own alignment layer before exposing the model as a service.  It will be highly compliant to any requests, even unethical ones.  Please read my blog post about uncensored models.  https://erichartford.com/uncensored-models
You are responsible for any content you create using this model.  Enjoy responsibly.; This dataset is an open source implementation of Microsoft's Orca; After uncensoring, deduping, and cleaning, our dataset consists of:"
Nous-Hermes-Llama2-13b-GPTQ,Text Generation,PyTorch; Transformers,English,llama2,,27,,30,15628.61246,1,https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b-GPTQ,"Compute provided by our project sponsor Redmond AI, thank you! Follow RedmondAI on Twitter @RedmondAI.; Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Emozilla leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors.; This Hermes model uses the exact same dataset as Hermes on Llama-1. This is to ensure consistency between the old Hermes and new, for anyone who wanted to keep Hermes as similar to the old one, just more capable.; This model stands out for its long responses, lower hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 4096 sequence length on an 8x a100 80GB DGX machine.; 


"
llama2_7b_chat_uncensored-GGML,,Transformers,,other,,27,ehartford/wizard_vicuna_70k_unfiltered,32,61870.10417,2,https://huggingface.co/TheBloke/llama2_7b_chat_uncensored-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for George Sung's Llama2 7B Chat Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
stable-diffusion-v1-4,Text-to-Image,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/2207.12598.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/1910.09700.pdf,5.79k,,"585,404",0.088640938,862,https://huggingface.co/CompVis/stable-diffusion-v1-4,"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
For more information about how Stable Diffusion functions, please have a look at ?'s Stable Diffusion with ?Diffusers blog.; The Stable-Diffusion-v1-4 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 
checkpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on ""laion-aesthetics v2 5+"" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.; This weights here are intended to be used with the ? Diffusers library. If you are looking for the weights to be loaded into the CompVis Stable Diffusion codebase, come here; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model"
falcon-7b,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2306.01116.pdf,691,tiiuae/falcon-refinedweb,"1,107,310",14779.12843,27,https://huggingface.co/tiiuae/falcon-7b,"Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ? To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF!; ?? This is a raw, pretrained model, which should be further finetuned for most usecases. If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at Falcon-7B-Instruct. ; ? Looking for an even more powerful model? Falcon-40B is Falcon-7B's big brother!"
control_v1p_sd15_brightness,Image-to-Image,Diffusers,English,creativeml-openrail-m,,85,ioclab/grayscale_image_aesthetic_3M,"33,556",1484.804014,7,https://huggingface.co/ioclab/control_v1p_sd15_brightness,"This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.; huggingface.co/spaces/ioclab/brightness-controlnet; [More Information Needed]; [More Information Needed]; [More Information Needed]"
detailed_eye-10,,,,creativeml-openrail-m,,26,,0,19.00148251,,https://huggingface.co/casque/detailed_eye-10,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
WizardCoder-15B-V1.0,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/2306.08568.pdf,404,,"13,626",31747.26413,32,https://huggingface.co/WizardLM/WizardCoder-15B-V1.0,"This is the Full-Weight of WizardCoder.; Repository: https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder; Twitter: https://twitter.com/WizardLM_AI/status/1669109414559911937; Paper: WizardCoder: Empowering Code Large Language Models with Evol-Instruct; To develop our WizardCoder model, we begin by adapting the Evol-Instruct method specifically for coding tasks. This involves tailoring the prompt to the domain of code-related instructions. Subsequently, we fine-tune the Code LLM, StarCoder, utilizing the newly created instruction-following training set."
zeroscope_v2_576w,Text-to-Video,Diffusers,,cc-by-nc-4.0,,220,,"21,059",0.005004883,33,https://huggingface.co/cerspense/zeroscope_v2_576w,"; A watermark-free Modelscope-based video model optimized for producing high-quality 16:9 compositions and a smooth video output. This model was trained from the original weights using 9,923 clips and 29,769 tagged frames at 24 frames, 576x320 resolution.
zeroscope_v2_567w is specifically designed for upscaling with zeroscope_v2_XL using vid2vid in the 1111 text2video extension by kabachuha. Leveraging this model as a preliminary step allows for superior overall compositions at higher resolutions in zeroscope_v2_XL, permitting faster exploration in 576x320 before transitioning to a high-resolution render. See some example outputs that have been upscaled to 1024x576 using zeroscope_v2_XL. (courtesy of dotsimulate); zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320; For upscaling, it's recommended to use zeroscope_v2_XL via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. ; Let's first install the libraries required:"
llama-2-13B-Guanaco-QLoRA-GGML,Text Classification,Transformers,English,other,,24,,87,119316.5044,,https://huggingface.co/TheBloke/llama-2-13B-Guanaco-QLoRA-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Mikael110's Llama2 13B Guanaco QLoRA.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!"
OrangeMixs,Text-to-Image,Diffusers,,creativeml-openrail-m,,3.24k,Nerfgun3/bad_prompt,"11,766",0.071629257,138,https://huggingface.co/WarriorMama777/OrangeMixs,"""OrangeMixs"" shares various Merge models that can be used with StableDiffusionWebui:Automatic1111 and others.
?
; Maintain a repository for the following purposes.; 
Hero image prompts(AOM3B2):https://majinai.art/ja/i/jhw20Z_; We support a Gradio Web UI to run OrangeMixs:
; +/hdg/ Stable Diffusion Models Cookbook - https://rentry.org/hdgrecipes#g-anons-unnamed-mix-e93c3bf7
Model names are named after Cookbook precedents?"
llama-7b-hf,Text Generation,PyTorch; Transformers,,other,,1.15k,,"245,723",13484.53376,138,https://huggingface.co/decapoda-research/llama-7b-hf,"LLaMA-7B converted to work with Transformers/HuggingFace. This is under a special license, please see the LICENSE file for details.; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model."
text2vec-large-chinese,Sentence Similarity,PyTorch; Safetensors; Transformers,Chinese,apache-2.0,,489,,"288,225",2662.939382,56,https://huggingface.co/GanymedeNil/text2vec-large-chinese,"Based on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replace MacBERT with LERT, and keep other training conditions unchanged。; Talk to me: https://twitter.com/GanymedeNil"
Llama-2-7B-GPTQ,Text Generation,Transformers; PyTorch,English,other,,23,,"1,626",3995.961163,,https://huggingface.co/TheBloke/Llama-2-7B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 7B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements."
dreamlike-photoreal-2.0,Text-to-Image,Diffusers,English,other,,1.45k,,"116,056",4369.132147,129,https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0,"Warning: This model is horny! Add ""nude, naked"" to the negative prompt if want to avoid NSFW.  ; You can add photo to your prompt to make your gens look more photorealistic.Non-square aspect ratios work better for some prompts. If you want a portrait photo, try using a vertical aspect ratio. If you want a landscape photo, try using a horizontal aspect ratio.This model was trained on 768x768px images, so use 768x768px, 640x896px, 896x640px, etc. It also works pretty good with higher resolutions such as 768x1024px or 1024x768px.  ; You can use this model for free on dreamlike.art!; Download dreamlike-photoreal-2.0.ckpt (2.13GB); Download dreamlike-photoreal-2.0.safetensors (2.13GB)"
Baichuan-7B,Text Generation,PyTorch; Transformers,Chinese; English,,https://arxiv.org/pdf/1910.07467.pdf; https://arxiv.org/pdf/2009.03300.pdf,704,,"44,902",14338.12457,,https://huggingface.co/baichuan-inc/Baichuan-7B,"Baichuan-7B是由百川智能开发的一个开源的大规模预训练模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。; 如果希望使用Baichuan-7B（如进行推理、Finetune等），我们推荐使用配套代码库Baichuan-7B。; Baichuan-7B is an open-source large-scale pre-trained model developed by Baichuan Intelligent Technology. Based on the Transformer architecture, it is a model with 7 billion parameters trained on approximately 1.2 trillion tokens. It supports both Chinese and English, with a context window length of 4096. It achieves the best performance of its size on standard Chinese and English authoritative benchmarks (C-EVAL/MMLU).; If you wish to use Baichuan-7B (for inference, finetuning, etc.), we recommend using the accompanying code library Baichuan-7B.; 在同尺寸模型中Baichuan-7B达到了目前SOTA的水平，参考下面MMLU指标"
openchat_v2_openorca_preview,Text Generation,PyTorch; Transformers,,other,,35,Open-Orca/OpenOrca,79,0,,https://huggingface.co/openchat/openchat_v2_openorca_preview,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This is a preview version of OpenChat V2 trained for 2 epochs (total 5 epochs) on full (4.5M) OpenOrca dataset.; Important Notice: Beta Release for Limited Testing Purposes Only; This release is intended solely for a small group of beta testers and is not an official release or preview. We caution against publicizing or sharing this version as it may contain bugs, errors, or incomplete features that could negatively impact performance. We are actively working on improving the model and preparing it for an official release."
Redmond-Puffin-13B-GPTQ,Text Generation,Transformers,eng,other,,22,,268,7436.603962,,https://huggingface.co/TheBloke/Redmond-Puffin-13B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for NousResearch's Redmond Puffin 13B V1.3.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!"
Nous-Hermes-13b,Text Generation,PyTorch; Transformers,English,gpl,,350,,"6,780",26624.49558,20,https://huggingface.co/NousResearch/Nous-Hermes-13b,"Nous-Hermes-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Karan4D leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors. The result is an enhanced Llama 13b model that rivals GPT-3.5-turbo in performance across a variety of tasks.; This model stands out for its long responses, low hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 2000 sequence length on an 8x a100 80GB DGX machine for over 50 hours. ; The model was trained almost entirely on synthetic GPT-4 outputs. This includes data from diverse sources such as GPTeacher, the general, roleplay v1&2, code instruct datasets, Nous Instruct & PDACTL (unpublished), CodeAlpaca, Evol_Instruct Uncensored, GPT4-LLM, and Unnatural Instructions. ; Additional data inputs came from Camel-AI's Biology/Physics/Chemistry and Math Datasets, Airoboros' GPT-4 Dataset, and more from CodeAlpaca. The total volume of data encompassed over 300,000 instructions.; The model fine-tuning and the datasets were a collaboration of efforts and resources between Teknium, Karan4D, Nous Research, Huemin Art, and Redmond AI. "
vicuna-7b-v1.3,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,86,,"93,274",13804.03948,1,https://huggingface.co/lmsys/vicuna-7b-v1.3,"Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning.
The training data is around 140K conversations collected from ShareGPT.com.
See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.; Vicuna is evaluated with standard benchmarks, human preference, and LLM-as-a-judge. See more details in this paper and leaderboard.; See vicuna_weights_version.md"
zeroscope_v2_XL,,Diffusers,,cc-by-nc-4.0,,395,,"12,029",0.005767555,12,https://huggingface.co/cerspense/zeroscope_v2_XL,"
example outputs (courtesy of dotsimulate); A watermark-free Modelscope-based video model capable of generating high quality video at 1024 x 576. This model was trained from the original weights with offset noise using 9,923 clips and 29,769 tagged frames at 24 frames, 1024x576 resolution.
zeroscope_v2_XL is specifically designed for upscaling content made with zeroscope_v2_576w using vid2vid in the 1111 text2video extension by kabachuha. Leveraging this model as an upscaler allows for superior overall compositions at higher resolutions, permitting faster exploration in 576x320 (or 448x256) before transitioning to a high-resolution render.; zeroscope_v2_XL uses 15.3gb of vram when rendering 30 frames at 1024x576; For upscaling, it's recommended to use the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip.; Let's first install the libraries required:"
llama-2-7b-chat-hf,Text Generation,PyTorch; Transformers,,,,21,,"91,504",13805.88471,1,https://huggingface.co/daryl149/llama-2-7b-chat-hf,"These are the converted model weights for Llama-2-7B-chat in Huggingface format.; Courtesy of Mirage-Studio.io, home of MirageGPT: the private ChatGPT alternative.; license: other
LLAMA 2 COMMUNITY LICENSE AGREEMENT	
Llama 2 Version Release Date: July 18, 2023; ""Agreement"" means the terms and conditions for use, reproduction, distribution and 
modification of the Llama Materials set forth herein.; ""Documentation"" means the specifications, manuals and documentation 
accompanying Llama 2 distributed by Meta at ai.meta.com/resources/models-and-
libraries/llama-downloads/."
Llama-2-70B-fp16,Text Generation,Safetensors; Transformers; PyTorch,English,other,,21,,"1,161",141285.4621,,https://huggingface.co/TheBloke/Llama-2-70B-fp16,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are fp16 format model files for Meta's Llama 2 70B.; They were produced by downloading the PTH files from Meta, and then converting to HF format using the latest Transformers 4.32.0.dev0, from Git, with the Llama 2 PR included: https://github.com/huggingface/transformers/pull/24891.; Command to convert was:"
Nous-Hermes-Llama2-13b-GGML,,,English,llama2,,21,,0,34877.44935,1,https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b-GGML,"Compute provided by our project sponsor Redmond AI, thank you! Follow RedmondAI on Twitter @RedmondAI.; Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Emozilla leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors.; This Hermes model uses the exact same dataset as Hermes on Llama-1. This is to ensure consistency between the old Hermes and new, for anyone who wanted to keep Hermes as similar to the old one, just more capable.; This model stands out for its long responses, lower hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 4096 sequence length on an 8x a100 80GB DGX machine.; 


"
waifu-diffusion,Text-to-Image,Diffusers,English,creativeml-openrail-m,,2.21k,,"267,780",0.006028786,227,https://huggingface.co/hakurei/waifu-diffusion,"waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.; ; masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck; Original Weights; We also support a Gradio Web UI and Colab with Diffusers to run Waifu Diffusion:

"
VoiceConversionWebUI,,ONNX,,mit,,411,,0,45704.65624,12,https://huggingface.co/lj1995/VoiceConversionWebUI,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
rwkv-4-world,Text Generation,PyTorch,12 languages,apache-2.0,,147,EleutherAI/pile; togethercomputer/RedPajama-Data-1T,0,175243.9232,3,https://huggingface.co/BlinkDL/rwkv-4-world,"RWKV-4 trained on 100+ world languages (70% English, 15% multilang, 15% code).; World = Some_Pile + Some_RedPajama + Some_OSCAR + All_Wikipedia + All_ChatGPT_Data_I_can_find; XXXtuned = finetune of World on MC4, OSCAR, wiki, etc.; How to use:; The differences between World & Raven:"
DreamShaper,Text-to-Image,Diffusers,English,other,,678,,"71,907",170824.3595,73,https://huggingface.co/Lykon/DreamShaper,"Read more about this model here: https://civitai.com/models/4384/dreamshaper; Also please support by giving 5 stars and a heart, which will notify new updates.; Please consider supporting me on Patreon or buy me a coffee; You can run this model on:; Inference API has been turned off for this model."
bark,Text-to-Speech,PyTorch; Transformers,13 languages,cc-by-nc-4.0,,216,,736,22726.53357,61,https://huggingface.co/suno/bark,"Bark is a transformer-based text-to-audio model created by Suno. 
Bark can generate highly realistic, multilingual speech as well as other audio - including music, 
background noise and simple sound effects. The model can also produce nonverbal 
communications like laughing, sighing and crying. To support the research community, 
we are providing access to pretrained model checkpoints ready for inference.; The original github repo and model card can be found here.; This model is meant for research purposes only. 
The model output is not censored and the authors do not endorse the opinions in the generated content. 
Use at your own risk.; Two checkpoints are released:; Try out Bark yourself!"
m3e-large,,PyTorch; Sentence Transformers,Chinese; English,,,47,,"1,874",1331.764153,,https://huggingface.co/moka-ai/m3e-large,m3e-small | m3e-base | m3e-large; M3E 是 Moka Massive Mixed Embedding 的缩写; 说明：; Tips:; 您需要先安装 sentence-transformers
Llama-2-70B-Chat-fp16,Text Generation,Safetensors; Transformers; PyTorch,English,other,,19,,"1,080",141285.4623,1,https://huggingface.co/TheBloke/Llama-2-70B-Chat-fp16,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are fp16 pytorch model files for Meta's Llama 2 70B Chat.; They were produced by downloading the PTH files from Meta, and then converting to HF format using the latest Transformers 4.32.0.dev0, from Git, with the Llama 2 PR included: https://github.com/huggingface/transformers/pull/24891.; Command to convert was:"
LLongMA-2-13b,Text Generation,PyTorch; Transformers,,,,19,,30,0,,https://huggingface.co/conceptofmind/LLongMA-2-13b,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; No model card; New: Create and edit this model card directly on the website!"
gpt2,Text Generation,PyTorch; TensorFlow; JAX; TF Lite; Rust; ONNX; Safetensors; Transformers,English,mit,,1.27k,,"14,736,272",3666.85439,850,https://huggingface.co/gpt2,"Test the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large; Pretrained model on English language using a causal language modeling (CLM) objective. It was introduced in
this paper
and first released at this page.; Disclaimer: The team releasing GPT-2 also wrote a
model card for their model. Content from this model card
has been written by the Hugging Face team to complete the information they provided and give specific examples of bias.; GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This
means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots
of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,
it was trained to guess the next word in sentences.; More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,
shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the
predictions for the token i only uses the inputs from 1 to i but not the future tokens."
gpt-j-6b,Text Generation,PyTorch; TensorFlow; JAX; Transformers,English,apache-2.0,https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2101.00027.pdf,1.22k,EleutherAI/pile,"384,727",74345.01182,24,https://huggingface.co/EleutherAI/gpt-j-6b,"GPT-J 6B is a transformer model trained using Ben Wang's Mesh Transformer JAX. ""GPT-J"" refers to the class of model, while ""6B"" represents the number of trainable parameters.; * Each layer consists of one feedforward block and one self attention block.; ? Although the embedding matrix has a size of 50400, only 50257 entries are used by the GPT-2 tokenizer.; The model consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model
dimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64
dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as
GPT-2/GPT-3.; GPT-J learns an inner representation of the English language that can be used to 
extract features useful for downstream tasks. The model is best at what it was 
pretrained for however, which is generating text from a prompt."
long_llama_3b,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2307.03170.pdf; https://arxiv.org/pdf/2305.16300.pdf,74,togethercomputer/RedPajama-Data-1T,"8,557",7015.004829,1,https://huggingface.co/syzymon/long_llama_3b,"; TLDR | Overview | Usage | LongLLaMA performance | Authors | Citation | License | Acknowledgments; This repository contains the research preview of LongLLaMA, a large language model capable of handling long contexts of 256k tokens or even more. ; LongLLaMA is built upon the foundation of OpenLLaMA and fine-tuned using the Focused Transformer (FoT) method.  We release a smaller 3B variant of the LongLLaMA model on a permissive license (Apache 2.0) and inference code supporting longer contexts on Hugging Face. Our model weights can serve as the drop-in replacement of LLaMA in existing implementations (for short context up to 2048 tokens). Additionally, we provide evaluation results and comparisons against the original OpenLLaMA models. Stay tuned for further updates.; Focused Transformer: Contrastive Training for Context Scaling (FoT) presents a simple method for endowing language models with the ability to handle context consisting possibly of millions of tokens while training on significantly shorter input. FoT permits a subset of attention layers to access a memory cache of (key, value) pairs to extend the context length. The distinctive aspect of FoT is its training procedure, drawing from contrastive learning. Specifically, we deliberately expose the memory attention layers to both relevant and irrelevant keys (like negative samples from unrelated documents). This strategy incentivizes the model to differentiate keys connected with semantically diverse values, thereby enhancing their structure. This, in turn, makes it possible to extrapolate the effective context length much beyond what is seen in training. "
Redmond-Puffin-13B-GGML,,Transformers,eng,other,,18,,26,121057.307,,https://huggingface.co/TheBloke/Redmond-Puffin-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for NousResearch's Redmond Puffin 13B V1.3.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!"
upstage-llama-30b-instruct-2048-GGML,Text Generation,Transformers,English,other,,18,,7,297881.6162,,https://huggingface.co/TheBloke/upstage-llama-30b-instruct-2048-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Upstage's Llama 30B Instruct 2048.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!"
stable-diffusion-v-1-4-original,Text-to-Image,,,creativeml-openrail-m,https://arxiv.org/pdf/2207.12598.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/1910.09700.pdf,2.46k,,0,12257.36237,215,https://huggingface.co/CompVis/stable-diffusion-v-1-4-original,"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.; The Stable-Diffusion-v-1-4 checkpoint was initialized with the weights of the Stable-Diffusion-v-1-2 
checkpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on ""laion-aesthetics v2 5+"" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.; These weights are intended to be used with the original CompVis Stable Diffusion codebase. If you are looking for the model to use with the D?iffusers library, come here.; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model"
flan-t5-xxl,Text2Text Generation,PyTorch; TensorFlow; JAX; Safetensors; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/2210.11416.pdf; https://arxiv.org/pdf/1910.09700.pdf,818,svakulenk0/qrecc; taskmaster2; djaym7/wiki_dialog; deepmind/code_contests; lambada; gsm8k; aqua_rat; esnli; quasc; qed,"723,038",183524.7096,232,https://huggingface.co/google/flan-t5-xxl,"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:"
instructor-xl,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,English,apache-2.0,https://arxiv.org/pdf/2212.09741.pdf,302,,"22,563",5082.306212,48,https://huggingface.co/hkunlp/instructor-xl,"We introduce Instructor???, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor?? achieves sota on 70 diverse embedding tasks!
The model is easy to use with our customized sentence-transformer library. For more details, check out our paper and project page! ; **************************** Updates ****************************; Then you can use the model like this to calculate domain-specific and task-aware embeddings:; If you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions: ; ??????????????????????????Represent the domain text_type for task_objective:"
instruct-pix2pix,Image-to-Image,Diffusers,,mit,,597,,"100,343",15769.60329,172,https://huggingface.co/timbrooks/instruct-pix2pix,"GitHub: https://github.com/timothybrooks/instruct-pix2pix
; To use InstructPix2Pix, install diffusers using main for now. The pipeline will be available in the next release"
whisper.cpp,,,,mit,,189,,0,14862.4626,8,https://huggingface.co/ggerganov/whisper.cpp,"Available models; For more information, visit:; https://github.com/ggerganov/whisper.cpp/tree/master/models; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
vicuna-13b-v1.3,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,127,,"45,658",26655.24592,2,https://huggingface.co/lmsys/vicuna-13b-v1.3,"Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning.
The training data is around 140K conversations collected from ShareGPT.com.
See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.; Vicuna is evaluated with standard benchmarks, human preference, and LLM-as-a-judge. See more details in this paper and leaderboard.; See vicuna_weights_version.md"
Baichuan-13B-Base,Text Generation,PyTorch; Transformers,Chinese; English,,https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2009.03300.pdf,119,,"11,182",27168.33373,,https://huggingface.co/baichuan-inc/Baichuan-13B-Base,"Baichuan-13B-Base为Baichuan-13B系列模型中的预训练版本，经过对齐后的模型可见Baichuan-13B-Chat。; Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。本次发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。Baichuan-13B 有如下几个特点：; Baichuan-13B is an open-source, commercially usable large-scale language model developed by Baichuan Intelligence, following Baichuan-7B. With 13 billion parameters, it achieves the best performance in standard Chinese and English benchmarks among models of its size. This release includes two versions: pre-training (Baichuan-13B-Base) and alignment (Baichuan-13B-Chat). Baichuan-13B has the following features:; Developed by: 百川智能(Baichuan Intelligent Technology); Email: opensource@baichuan-inc.com"
Llama-2-13B-Chat-fp16,Text Generation,PyTorch; Transformers,,,,17,,966,26657.09569,,https://huggingface.co/TheBloke/Llama-2-13B-Chat-fp16,No model card; New: Create and edit this model card directly on the website!
Nous-Hermes-Llama2-GPTQ,Text Generation,Transformers,English,other,,17,,107,7436.601263,,https://huggingface.co/TheBloke/Nous-Hermes-Llama2-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Nous Research's Nous Hermes Llama 2 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements."
llama2-7b-chat-codeCherryPop-qLoRA-GGML,,Transformers,,other,,17,,3,61870.10544,,https://huggingface.co/TheBloke/llama2-7b-chat-codeCherryPop-qLoRA-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for TokenBender's Llama-2-7B-Chat Code Cherry Pop.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
pygmalion-6b,Conversational,PyTorch; TensorBoard; Transformers,English,creativeml-openrail-m,,592,,"191,985",16735.55981,24,https://huggingface.co/PygmalionAI/pygmalion-6b,"Pymalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B.; Warning: This model is NOT suitable for use by minors. It will output X-rated content under certain circumstances.; The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations.; Model weights were initialized from the uft-6b ConvoGPT model made available in this commit.; The model was then further fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed."
mpt-7b-8k-chat,Text Generation,PyTorch; Transformers,,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2010.04245.pdf,16,camel-ai/code; ehartford/wizard_vicuna_70k_unfiltered; anon8231489123/ShareGPT_Vicuna_unfiltered; teknium1/GPTeacher/roleplay-instruct-v2-final; teknium1/GPTeacher/codegen-isntruct; timdettmers/openassistant-guanaco; camel-ai/math; project-baize/baize-chatbot/medical_chat_data; project-baize/baize-chatbot/quora_chat_data; project-baize/baize-chatbot/stackoverflow_chat_data; camel-ai/biology; camel-ai/chemistry; camel-ai/ai_society; jondurbin/airoboros-gpt4-1.2; LongConversations; camel-ai/physics,684,13621.4583,,https://huggingface.co/mosaicml/mpt-7b-8k-chat,"MPT-7B-Chat-8k is a chatbot-like model for dialogue generation.
It was built by finetuning MPT-7B-8k on the ShareGPT-Vicuna, Camel-AI,
 GPTeacher, Guanaco, Baize and some generated datasets.
 This is the same dataset that MPT-30B-Chat was trained on.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; July 18, 2023; CC-By-NC-SA-4.0 (non-commercial use only); This model is best used with the MosaicML llm-foundry repository for training and finetuning."
llama2-22b,Text Generation,PyTorch; Transformers,,,,16,togethercomputer/RedPajama-Data-1T-Sample,1,44708.37072,,https://huggingface.co/chargoddard/llama2-22b,"This is Llama 2 13b with some additional attention heads from original-flavor Llama 33b frankensteined on.; Fine-tuned on ~10M tokens from RedPajama to settle in the transplants a little.; Not intended for use as-is - this model is meant to serve as a base for further tuning, hopefully with a greater capacity for learning than 13b."
speaker-diarization,Automatic Speech Recognition,pyannote.audio,,mit,https://arxiv.org/pdf/2012.01477.pdf; https://arxiv.org/pdf/2110.07058.pdf; https://arxiv.org/pdf/2005.08072.pdf,367,ami; dihard; voxconverse; aishell; repere; voxceleb,"1,145,415",0,48,https://huggingface.co/pyannote/speaker-diarization,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; The collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Relies on pyannote.audio 2.1.1: see installation instructions.; In case the number of speakers is known in advance, one can use the num_speakers option:"
openjourney,Text-to-Image,Diffusers; Safetensors,English,creativeml-openrail-m,,2.79k,,"327,457",4854.244627,296,https://huggingface.co/prompthero/openjourney,"Include 'mdjrny-v4 style' in prompt. Here you'll find hundreds of Openjourney prompts; ; (Same parameters, just added ""mdjrny-v4 style"" at the beginning):



; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; You can also export the model to ONNX, MPS and/or FLAX/JAX."
ControlNet,,,,openrail,,3.06k,,0,0.005019531,244,https://huggingface.co/lllyasviel/ControlNet,This is the pretrained weights and some other detector weights of ControlNet.; See also: https://github.com/lllyasviel/ControlNet; ControlNet/models/control_sd15_canny.pth; ControlNet/models/control_sd15_depth.pth; ControlNet/models/control_sd15_hed.pth
Wizard-Vicuna-30B-Uncensored-GPTQ,Text Generation,Transformers,English,other,,244,ehartford/wizard_vicuna_70k_unfiltered,"8,220",17308.0362,1,https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Eric Hartford's Wizard-Vicuna-30B-Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
mpt-7b-8k-instruct,Text Generation,PyTorch; Transformers,,cc-by-sa-3.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2010.04245.pdf,15,competition_math; conceptofmind/cot_submix_original/cot_gsm8k; knkarthick/dialogsum; mosaicml/dolly_hhrlhf; duorc; tau/scrolls/qasper; emozilla/quality; scrolls/summ_screen_fd; spider,"1,889",13621.45871,,https://huggingface.co/mosaicml/mpt-7b-8k-instruct,"MPT-7B-Instruct-8k is a model for long-form instruction following, especially question-answering on and summarization of longer documents.
It is built by finetuning MPT-7B-8k on Dolly HHRLHF derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets. It is also trained on Competition Math, Duorc, CoT GSM8k, Qasper, Quality, Summ Screen FD and Spider.
This is the same dataset that MPT-30B-Instruct was trained on.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; July 18, 2023;  CC-By-SA-3.0; This model is best used with the MosaicML llm-foundry repository for training and finetuning."
chatglm2-6b-int4,,PyTorch; Transformers,Chinese; English,,https://arxiv.org/pdf/2103.10360.pdf; https://arxiv.org/pdf/2210.02414.pdf; https://arxiv.org/pdf/1911.02150.pdf,112,,"90,617",4017.784148,29,https://huggingface.co/THUDM/chatglm2-6b-int4,"
  ? Github Repo ? ? Twitter ? ? [GLM@ACL 22] [GitHub] ? ? [GLM-130B@ICLR 23] [GitHub] 
; 
    ? Join our Slack and WeChat
; ChatGLM2-6B 是开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM2-6B 引入了如下新特性：; ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the following new features:; 可以通过如下代码调用 ChatGLM-6B 模型来生成对话："
notwaifu-diffusion-xl,,,English,other,,15,,0,0,,https://huggingface.co/gmonsoon/notwaifu-diffusion-xl,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Please for anyone, don't reupload my finetuned models to other site, like Civitai or image generating site like Seeart, Yodayo or other sites. Thank you for understanding this. ; Like my works and want to collaboration or funding my projects? contact me.; Sample

 "
llava-llama-2-13b-chat-lightning-preview,Text Generation,PyTorch; Transformers,,,,15,,141,26665.4933,,https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview,"Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA-LLaMA-2-13B-Chat-Preview was trained in July 2023.; Paper or resources for more information:
https://llava-vl.github.io/; Llama 2 is licensed under the LLAMA 2 Community License, 
Copyright (c) Meta Platforms, Inc. All Rights Reserved.; Where to send questions or comments about the model:
https://github.com/haotian-liu/LLaVA/issues"
upstage-llama-30b-instruct-2048-GPTQ,Text Generation,Transformers,English,other,,15,,247,17307.94551,,https://huggingface.co/TheBloke/upstage-llama-30b-instruct-2048-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Upstage's Llama 30B Instruct 2048.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!"
LLongMA-2-7B-GGML,,Transformers,,other,https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2212.10554.pdf,15,,11,61870.10651,,https://huggingface.co/TheBloke/LLongMA-2-7B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for ConceptofMind's LLongMA 2 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; This is an extended context base Llama 2 model.  Please check if your GGML client supports extended context. llama.cpp and KoboldCpp do, but I have not verified the others."
bart-large-mnli,Zero-Shot Classification,PyTorch; JAX; Rust; Safetensors; Transformers,,mit,https://arxiv.org/pdf/1910.13461.pdf; https://arxiv.org/pdf/1909.00161.pdf,594,multi_nli,"3,898,557",7099.008516,124,https://huggingface.co/facebook/bart-large-mnli,"This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset.; Additional information about this model:; Yin et al. proposed a method for using pre-trained NLI models as a ready-made zero-shot sequence classifiers. The method works by posing the sequence to be classified as the NLI premise and to construct a hypothesis from each candidate label. For example, if we want to evaluate whether a sequence belongs to the class ""politics"", we could construct a hypothesis of This text is about politics.. The probabilities for entailment and contradiction are then converted to label probabilities.; This method is surprisingly effective in many cases, particularly when used with larger pre-trained models like BART and Roberta. See this blog post for a more expansive introduction to this and other zero shot methods, and see the code snippets below for examples of using this model for zero-shot classification both with Hugging Face's built-in pipeline and with native Transformers/PyTorch code.; The model can be loaded with the zero-shot-classification pipeline like so:"
sd-vae-ft-mse-original,Text-to-Image,,,mit,,1.01k,,0,670.0077051,19,https://huggingface.co/stabilityai/sd-vae-ft-mse-original,"These weights are intended to be used with the original CompVis Stable Diffusion codebase. If you are looking for the model to use with the ? diffusers library, come here.; We publish two kl-f8 autoencoder versions, finetuned from the original kl-f8 autoencoder on a 1:1 ratio of LAION-Aesthetics and LAION-Humans, an unreleased subset containing only SFW images of humans. The intent was to fine-tune on the Stable Diffusion training set (the autoencoder was originally trained on OpenImages) but also enrich the dataset with images of humans to improve the reconstruction of faces.
The first, ft-EMA, was resumed from the original checkpoint, trained for 313198 steps and uses EMA weights. It uses the same loss configuration as the original checkpoint (L1 + LPIPS).
The second, ft-MSE, was resumed from ft-EMA and uses EMA weights and was trained for another 280k steps using a different loss, with more emphasis 
on MSE reconstruction (MSE + 0.1 * LPIPS). It produces somewhat ``smoother'' outputs. The batch size for both versions was 192 (16 A100s, batch size 12 per GPU).
To keep compatibility with existing models, only the decoder part was finetuned; the checkpoints can be used as a drop-in replacement for the existing autoencoder..; Original kl-f8 VAE vs f8-ft-EMA vs f8-ft-MSE; Visualization of reconstructions on  256x256 images from the COCO2017 validation dataset. ; 


256x256: ft-EMA (left), ft-MSE (middle), original (right)
"
stable-diffusion-2-1-base,Text-to-Image,Diffusers,,openrail++,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,456,,"494,813",21340.17417,231,https://huggingface.co/stabilityai/stable-diffusion-2-1-base,"This model card focuses on the model associated with the Stable Diffusion v2-1-base model.; This stable-diffusion-2-1-base model fine-tunes stable-diffusion-2-base (512-base-ema.ckpt) with 220k extra steps taken, with punsafe=0.98 on the same dataset. ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model; Language(s): English"
AsianModel,,,,openrail,,148,,0,18790.40152,3,https://huggingface.co/BanKaiPls/AsianModel,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
starchat-beta,Text Generation,PyTorch; TensorBoard; Safetensors; Transformers,,bigcode-openrail-m,,164,,"2,021,106",64199.10244,8,https://huggingface.co/HuggingFaceH4/starchat-beta,"StarChat is a series of language models that are trained to act as helpful coding assistants. StarChat-β is the second model in the series, and is a fine-tuned version of StarCoderPlus that was trained on an ""uncensored"" variant of the openassistant-guanaco dataset. We found that removing the in-built alignment of the OpenAssistant dataset boosted performance on the Open LLM Leaderboard and made the model more helpful at coding tasks. However, this means that model is likely to generate problematic text when prompted to do so and should only be used for educational and research purposes.; The model was fine-tuned on a variant of the OpenAssistant/oasst1 dataset, which contains a diverse range of dialogues in over 35 languages. As a result, the model can be used for chat and you can check out our demo to test its coding capabilities. ; Here's how you can run the model using the pipeline() function from ? Transformers:; StarChat-β has not been aligned to human preferences with techniques like RLHF or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). 
Models trained primarily on code data will also have a more skewed demographic bias commensurate with the demographics of the GitHub community, for more on this see the StarCoder dataset which is derived from The Stack.; Since the base model was pretrained on a large corpus of code, it may produce code snippets that are syntactically valid but semantically incorrect. 
For example, it may produce code that does not compile or that produces incorrect results.It may also produce code that is vulnerable to security exploits.We have observed the model also has a tendency to produce false URLs which should be carefully inspected before clicking."
controlnet_qrcode-control_v1p_sd15,Image-to-Image,Diffusers,English,openrail++,,141,,"51,831",5900.408976,53,https://huggingface.co/DionTimmer/controlnet_qrcode-control_v1p_sd15,"; This repo holds the safetensors & diffusers versions of the QR code conditioned ControlNet for Stable Diffusion v1.5.
The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs. However, this 1.5 version model was also trained on the same dataset for those who are using the older version.; These models perform quite well in most cases, but please note that they are not 100% accurate. In some instances, the QR code shape might not come through as expected. You can increase the ControlNet weight to emphasize the QR code shape. However, be cautious as this might negatively impact the style of your output.To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).; To balance between style and shape, a gentle fine-tuning of the control weight might be required based on the individual input and the desired output, aswell as the correct prompt. Some prompts do not work until you increase the weight by a lot. The process of finding the right balance between these factors is part art and part science. For the best results, it is recommended to generate your artwork at a resolution of 768. This allows for a higher level of detail in the final product, enhancing the quality and effectiveness of the QR code-based artwork.; The simplest way to use this is to place the .safetensors model and its .yaml config file in the folder where your other controlnet models are installed, which varies per application. 
For usage in auto1111 they can be placed in the webui/models/ControlNet folder. They can be loaded using the controlnet webui extension which you can install through the extensions tab in the webui (https://github.com/Mikubill/sd-webui-controlnet). Make sure to enable your controlnet unit and set your input image as the QR code. Set the model to either the SD2.1 or 1.5 version depending on your base stable diffusion model, or it will error. No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail.
Make sure to look up additional info on how to use controlnet if you get stuck, once you have the webui up and running its really easy to install the controlnet extension aswell."
polylm-13b,Text Generation,PyTorch; Transformers,18 languages,apache-2.0,https://arxiv.org/pdf/2307.06018.pdf; https://arxiv.org/pdf/2104.09864.pdf,25,,"1,010",31472.31951,1,https://huggingface.co/DAMO-NLP-MT/polylm-13b," Large language models (LLMs) demonstrate remarkable ability to comprehend, reason, and generate following nature language instructions. However, the development of LLMs has been primarily focused on high-resource languages, such as English, thereby limiting their applicability and research in other languages. Consequently, we present PolyLM, a multilingual LLM trained on 640 billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its multilingual capabilities, we 1) integrate bilingual data into training data; and 2) adopt a curriculum learning strategy that increases the proportion of non-English data from 30% in the first stage to 60% in the final stage during pre-training. Further, we propose a multilingual self-instruct method which automatically generates 132.7K  diverse multilingual instructions for model fine-tuning. To assess the model's performance, we collect several existing multilingual tasks, including multilingual understanding, question answering, generation, and translation. Extensive experiments show that PolyLM surpasses other open-source models such as LLaMA and BLOOM on multilingual tasks while maintaining comparable performance in English.; Find below some example scripts on how to use the model in transformers:; The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models; See the research paper for further details.; More information needed."
limarp-llama2,,,,,https://arxiv.org/pdf/2305.11206.pdf; https://arxiv.org/pdf/2305.14314.pdf,14,,0,0.011640625,,https://huggingface.co/lemonilia/limarp-llama2,"LIMARP-Llama2 is an experimental Llama2 finetune narrowly focused on novel-style roleplay chatting.; To considerably facilitate uploading and distribution, LoRA adapters have been provided instead of the merged models. You should get the Llama2 base model first, either from Meta or from one of the reuploads on HuggingFace (for example here and here). It is also possible to apply the LoRAs on different Llama2-based models (e.g. LLongMA-2 or Nous-Hermes-Llama2), although this is largely untested and the final results may not work as intended.; This is an experimental attempt at creating an RP-oriented fine-tune using a manually-curated, high-quality dataset of human-generated conversations. The main rationale for this are the observations from Zhou et al.. The authors suggested that just 1000-2000 carefully curated training examples may yield high quality output for assistant-type chatbots. This is in contrast with the commonly employed strategy where a very large number of training examples (tens of thousands to even millions) of widely varying quality are used.; For LIMARP a similar approach was used, with the difference that the conversational data is almost entirely human-generated. Every training example is manually compiled and selected to comply with subjective quality parameters, with virtually no chance for OpenAI-style alignment responses to come up.; The model is intended to approximate the experience of 1-on-1 roleplay as observed on many Internet forums dedicated on roleplaying. It must be used with a specific format similar to that of this template:"
llama-2-ko-7b,Text Generation,PyTorch; Safetensors; Transformers,English; Korean,,https://arxiv.org/pdf/2307.09288.pdf,14,,288,27433.57909,,https://huggingface.co/beomi/llama-2-ko-7b,"Llama-2-Ko serves as an advanced iteration of Llama 2, benefiting from an expanded vocabulary and the inclusion of a Korean corpus in its further pretraining. Just like its predecessor, Llama-2-Ko operates within the broad range of generative text models that stretch from 7 billion to 70 billion parameters. This repository focuses on the 7B pretrained version, which is tailored to fit the Hugging Face Transformers format. For access to the other models, feel free to consult the index provided below.; Model Developers Junbum Lee (Beomi); Variations Llama-2-Ko will come in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations.; Input Models input text only.; Output Models generate text only."
Llama-2-70B-Chat-GGML,Text Generation,Transformers; PyTorch,English,other,https://arxiv.org/pdf/2307.09288.pdf,14,,0,576921.633,,https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGML,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for Meta's Llama 2 70B Chat.; To use these files you need:; Example command:
layoutlm-document-qa,Document Question Answering,PyTorch; TensorFlow; Safetensors; Transformers,English,mit,,410,,"44,880",1535.589957,159,https://huggingface.co/impira/layoutlm-document-qa,"This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents. It has been fine-tuned using both the SQuAD2.0 and DocVQA datasets.; To run these examples, you must have PIL, pytesseract, and PyTorch installed in addition to transformers.; NOTE: This model and pipeline was recently landed in transformers via PR #18407 and PR #18414, so you'll need to use a recent version of transformers, for example:; This model was created by the team at Impira."
roberta-base-go_emotions,Text Classification,PyTorch; Transformers,English,mit,,48,go_emotions,"4,211,611",502.3453072,3,https://huggingface.co/SamLowe/roberta-base-go_emotions,"Model trained from roberta-base on the go_emotions dataset for multi-label classification.; go_emotions is based on Reddit data and has 28 labels. It is a multi-label dataset where one or multiple labels may apply for any given input text, hence this model is a multi-label classification model with 28 'probability' float outputs for any given input text. Typically a threshold of 0.5 is applied to the probabilities for the prediction for each label.; The model was trained using AutoModelForSequenceClassification.from_pretrained with problem_type=""multi_label_classification"" for 3 epochs with a learning rate of 2e-5 and weight decay of 0.01.; Evaluation (of the 28 dim output via a threshold of 0.5 to binarize each) using the dataset test split gives:; But the metrics would be more meaningful when measured per label given the multi-label nature."
blip-image-captioning-large,Image-to-Text,PyTorch; TensorFlow; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2201.12086.pdf,205,,"429,342",3851.173263,224,https://huggingface.co/Salesforce/blip-image-captioning-large,"Model card for image captioning pretrained on COCO dataset - base architecture (with ViT large backbone).; Authors from the paper write in the abstract:; Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.; You can use this model for conditional and un-conditional image captioning"
instructor-large,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,English,apache-2.0,https://arxiv.org/pdf/2212.09741.pdf,239,,"40,558",1375.426232,31,https://huggingface.co/hkunlp/instructor-large,"We introduce Instructor???, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor?? achieves sota on 70 diverse embedding tasks (MTEB leaderboard)!
The model is easy to use with our customized sentence-transformer library. For more details, check out our paper and project page! ; **************************** Updates ****************************; Then you can use the model like this to calculate domain-specific and task-aware embeddings:; If you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions: ; ??????????????????????????Represent the domain text_type for task_objective:"
IF-I-XL-v1.0,Text-to-Image,PyTorch; Diffusers,,deepfloyd-if-license,https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/2110.02861.pdf,467,,"40,137",0,20,https://huggingface.co/DeepFloyd/IF-I-XL-v1.0,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; DeepFloyd LICENSE AGREEMENTThis License Agreement (as may be amended in accordance with this License Agreement, “License”), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (“Licensee” or “you”) and Stability AI Ltd.. (“Stability AI” or “we”) applies to your use of any computer program, algorithm, source code, object code, or software that is made available by Stability AI under this License (“Software”) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (“Documentation”).By clicking “I Accept” below or by using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model, that can generate pictures with new state-of-the-art for photorealism and language understanding. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID-30K score of 6.66 on the COCO dataset.; Inspired by Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
Music-AI-Voices,,,,other,,242,,0,0,2,https://huggingface.co/QuickWick/Music-AI-Voices,"? Discord: https://discord.gg/aihub | Join the community, learn to make models, chat with link-minded people and lets create music ? ?; ? Discord Latino: https://discord.gg/Crfqs7uB5V | Entren a nuestra comunidad, aprendan a crear modelos AI, habla con otros sobre musica y disfruta las notas musicales ? ?; IMPORTANT!!!!!!!!!: VOICES CANNOT BE COPYRIGHTED. We do not promote piracy so please do not come in with that. We do promote legal-length sample clips of vocals. We promote music & AI produced music covers (impressions). We promote machine learning & Voice AI Models. Note: This repository does NOT include ANY DATASETS. Only models are included.; If you want your credits/name removed, please message me on discord and I will remove it diligently.; Tools: https://vocalremover.org/ https://x-minus.pro/ai https://create.musicfy.lol/"
Counterfeit-V3.0,Text-to-Image,,,creativeml-openrail-m,,348,,0,19773.44255,30,https://huggingface.co/gsdf/Counterfeit-V3.0,"?I have utilized BLIP-2 as a part of the training process. Natural language prompts might be more effective.?I prioritize the freedom of composition, which may result in a higher possibility of anatomical errors.?The expressiveness has been improved by merging with negative values, but the user experience may differ from previous checkpoints.?I have uploaded a new Negative Embedding, trained with Counterfeit-V3.0.There's likely no clear superiority or inferiority between this and the previous embedding, so feel free to choose according to your preference.Note that I'm not specifically recommending the use of this embedding.  ; prompt & Setting: https://civitai.com/models/4468/counterfeit-v30

; Unable to determine this model’s library. Check the
								docs 
.
							"
mpt-7b,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2010.04245.pdf; https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2302.06675.pdf,1.02k,mc4; c4; togethercomputer/RedPajama-Data-1T; bigcode/the-stack; allenai/s2orc,"140,606",13621.46279,12,https://huggingface.co/mosaicml/mpt-7b,"MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.
This model was trained by MosaicML.; MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.; These architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing
positional embeddings with Attention with Linear Biases (ALiBi).
Thanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence.
MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer.; This model uses the MosaicML LLM codebase, which can be found in the llm-foundry repository. It was trained by MosaicML’s NLP team on the MosaicML platform for LLM pretraining, finetuning, and inference.; MPT-7B is"
sdxl-vae-fp16-fix,,Diffusers,,mit,,29,,"3,227",1005.00444,,https://huggingface.co/madebyollin/sdxl-vae-fp16-fix,"SDXL-VAE-FP16-Fix is the SDXL VAE, but modified to run in fp16 precision without generating NaNs.; Just load this checkpoint via AutoencoderKL:; ; SDXL-VAE generates NaNs in fp16 because the internal activation values are too big:
; SDXL-VAE-FP16-Fix was created by finetuning the SDXL-VAE to:"
Llama-2-7b-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,,13,,"2,439",41403.97542,,https://huggingface.co/NousResearch/Llama-2-7b-hf,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations."
llama-2-7B-Guanaco-QLoRA-GGML,Text Classification,Transformers,English,other,,13,,30,61870.10427,,https://huggingface.co/TheBloke/llama-2-7B-Guanaco-QLoRA-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Mikael110's Llama2 7B Guanaco QLoRA.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!"
Dolphin-Llama-13B-GGML,,Transformers,,other,,13,,0,119316.4947,,https://huggingface.co/TheBloke/Dolphin-Llama-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for Eric Hartford's Dolphin Llama 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
bert-base-chinese,Fill-Mask,PyTorch; TensorFlow; JAX; Safetensors; Transformers,Chinese,,https://arxiv.org/pdf/1810.04805.pdf,469,,"3,764,329",1711.372971,50,https://huggingface.co/bert-base-chinese,"This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper).; This model can be used for masked language modeling ; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).; [More Information Needed]"
segmentation,Voice Activity Detection,PyTorch; pyannote.audio,,mit,https://arxiv.org/pdf/2104.04045.pdf,224,,"1,916,302",0,,https://huggingface.co/pyannote/segmentation,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; The collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Paper | Demo | Blog post; "
paraphrase-multilingual-MiniLM-L12-v2,Sentence Similarity,PyTorch; TensorFlow; Sentence Transformers; Transformers,multilingual,apache-2.0,https://arxiv.org/pdf/1908.10084.pdf,202,,"6,826,723",970.9563104,34,https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2,"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net"
text2vec-base-chinese,Sentence Similarity,PyTorch; Transformers,Chinese,apache-2.0,,336,shibing624/nli_zh,"1,259,456",409.1204564,22,https://huggingface.co/shibing624/text2vec-base-chinese,"This is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese.; It maps sentences to a 768 dimensional dense vector space and can be used for tasks 
like sentence embeddings, text matching or semantic search.; For an automated evaluation of this model, see the Evaluation Benchmark: text2vec; 说明：; Using this model becomes easy when you have text2vec installed:"
replit-code-v1-3b,Text Generation,PyTorch; Transformers,code,cc-by-sa-4.0,https://arxiv.org/pdf/2211.15533.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2302.06675.pdf,618,bigcode/the-stack-dedup,"36,518",10650.40041,8,https://huggingface.co/replit/replit-code-v1-3b,"Developed by: Replit, Inc.; ??? Test it on our Demo Space! ???; ?? Fine-tuning and Instruct-tuning guides ??; replit-code-v1-3b is a 2.7B Causal Language Model focused on Code Completion. The model has been trained on a subset of the Stack Dedup v1.2 dataset.; The training mixture includes 20 different languages, listed here in descending order of number of tokens: 

Markdown, Java, JavaScript, Python, TypeScript, PHP, SQL, JSX, reStructuredText, Rust, C, CSS, Go, C++, HTML, Vue, Ruby, Jupyter Notebook, R, Shell

In total, the training dataset contains 175B tokens, which were repeated over 3 epochs -- in total, replit-code-v1-3b has been trained on 525B tokens (~195 tokens per parameter)."
Wizard-Vicuna-13B-Uncensored-HF,Text Generation,PyTorch; Transformers,English,other,,176,ehartford/wizard_vicuna_70k_unfiltered,"75,358",26657.08779,16,https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-HF,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a float16 HF repo for Eric Hartford's 'uncensored' training of Wizard-Vicuna 13B.; It is the result of converting Eric's float32 repo to float16 for easier storage and use.; For further support, and discussions on these models and AI in general, join us at:"
h2ogpt-gm-oasst1-en-2048-falcon-7b-v3,Text Generation,PyTorch; Transformers,English,apache-2.0,,39,OpenAssistant/oasst1,"276,107",14779.13017,25,https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3,"This model was trained using H2O LLM Studio.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers, accelerate, torch and einops libraries installed.; You can print a sample prompt after the preprocessing step to see how it is feed to the tokenizer:; Alternatively, you can download h2oai_pipeline.py, store it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:; You may also construct the pipeline from the loaded model and tokenizer yourself and consider the preprocessing steps:"
longchat-13b-16k,Text Generation,PyTorch; Transformers,,,,111,,"7,042",26655.24589,,https://huggingface.co/lmsys/longchat-13b-16k,"Please use load_model from FastChat or LongChat repo to load the model (or chatting API from FastChat). There is a monkey patch needed to use the model.
Usage referece:; (LongChat) python3 eval.py --model-name-or-path  lmsys/longchat-13b-16k --task topics ; (FastChat) python3 -m fastchat.serve.cli --model-path lmsys/longchat-13b-16k; Under the hood, the monkey patch is added in:; https://github.com/lm-sys/FastChat/blob/da0641e567cf93756b0978ab5a6b092e96f06240/fastchat/model/model_adapter.py#L429"
YumekawaMix,Text-to-Image,,Japanese,creativeml-openrail-m,,12,,0,5058.571543,,https://huggingface.co/RSTR/YumekawaMix,YumekawaMix; 年h、等身が低めでフラットなテイストが特栅扦埂kawaiiイラストをお求めの方へ; 年h、等身が若干高くなります。安定感は微妙ですが}jな恧出やすい印象です; 作者に最大の感xを???; Twitter: @AiRSTR7
everyjourney-SDXL-finetuned-Fp16,Text-to-Image,,English,other,,31,,0,7106.564902,,https://huggingface.co/thehive/everyjourney-SDXL-finetuned-Fp16,"Please for anyone, don't reupload my finetuned models to other site, like Civitai or image generating site like Seeart, Yodayo other sites. Thank you for understanding this. ; Like my works and want to collaboration or funding my projects? contact me.; Finetuned on SDXL Base 0.9 Official Release, Expected to be successor of Everyjourney, currently in alpha stage, since i'm captioned this model with BLIP2, the image generated with this model may not meet your expectations, waiting for SDXL finetune/training process to be more polished.  ; My other works:   ; Recommended Settings "
lora,,,Japanese,mit,,21,,0,825.9024316,,https://huggingface.co/JujoHotaru/lora,十条w（Jujo Hotaru）の作成したLoRAを配布しています。You can download JujoHotaru's LoRA collection from this repo.; (＞＜)／(－－)の目 / (≡_≡)／(◎_◎)の目 / \目セットA / 白目セットA / ジト目セットA / ジト目セットB / ぐにゃぐにゃ口 / 大きく_いた口 / 集中 / ぼかし＆背景ぼかし / ト`ンカ`ブ{整 / 彩度{整 / ウィンクa助 / 激おこ / にっこり笑 / 思案 / 茹でダコ / 青醒め; gYLoRA置き; しくる／ダウンロ`ド (Details/Download); 
llama-30b-instruct,Text Generation,PyTorch; Transformers,English,,,12,sciq; metaeval/ScienceQA_text_only; GAIR/lima; Open-Orca/OpenOrca; openbookqa,115,66634.06464,1,https://huggingface.co/upstage/llama-30b-instruct,No other data was used except for the dataset mentioned above
MythoLogic-13B-GPTQ,Text Generation,Transformers,English,other,,12,,22,7629.30279,,https://huggingface.co/TheBloke/MythoLogic-13B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Gryphe's MythoLogic 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These files were quantised using hardware kindly provided by Latitude.sh."
japanese-mpt-7b,Text Generation,PyTorch; Transformers,,apache-2.0,,12,,258,13621.45306,,https://huggingface.co/lightblue/japanese-mpt-7b,"Japanese subset of the mC4 dataset; Trained for 3000 steps on top of the MPT 7b checkpoint mosaicml/mpt-7b; Before running this model, please install the following pip package:; To load the model, run the following command.; To run this model, you may need to load it in a lower precision in order for it to fit onto your GPU. We found for a T4 GPU, it requires loading the model in 8-bit precision. To load the model in 8-bit and 4-bit, please install the following pip packages:"
sazyou_LoRA,Text-to-Image,,Japanese,creativeml-openrail-m,,12,,0,26.36419922,,https://huggingface.co/sazyou-roukaku/sazyou_LoRA,"LECOやLoRAの作品置き觥

①胸部pスライダ`LoRA(huge_breasts_woman/flat_chest_woman) 
LECOで作成し、{整した胸部pスライダ`:トリガ`ワ`ド woman
breasts系プロンプトは不要。
推?サンプルはLittleStepMix_Aで作成。



②マルチカラ`ドヘア`LoRA(pastel_hair_full/pastel_hair_A/pastel_hair_B) 
LECOで作成し、{整した多彩色のにするLoRA:トリガ`ワ`ド hair
Lさ指定だけだとかなりカラフルになります。メインの色を入れても良し、服装へのA染も最低限です。
なおnegativeに (black hair,brown hair:1.5) を入力推X
LittleStepMix_Aで作成し、それをサンプルとしているので出方はモデルでかなりなります。
fullは全ての色が出て、パステル{がめです。Aは白系が出ず、v弱め。Bはfullよりメリハリがある出力になります。
; Unable to determine this model’s library. Check the
								docs 
.
							"
Llama-2-7B-fp16,Text Generation,PyTorch; Transformers,,,,12,,"1,531",13805.88925,,https://huggingface.co/TheBloke/Llama-2-7B-fp16,No model card; New: Create and edit this model card directly on the website!
Redmond-Puffin-13B-Preview-GGML,,,eng,mit,,12,,0,7741.444492,,https://huggingface.co/NousResearch/Redmond-Puffin-13B-Preview-GGML,"GGML 4bit Quantization of Nous Research's Puffin Preview 1 Model: https://huggingface.co/NousResearch/Redmond-Puffin-13B; Thank you to Eachadea for making this quantization possible immediately upon launch; ; The first commercially available language model released by Nous Research!; Redmond-Puffin-13B is one of the worlds first llama-2 based, fine-tuned language models, leveraging a hand curated set of 3K high quality examples, many of which take full advantage of the 4096 context length of Llama 2. This model was fine-tuned by Nous Research, with LDJ leading the training and dataset curation, along with significant dataset formation contributions by J-Supha."
Redmond-Puffin-13B-GGML,,,eng,mit,,12,,0,34877.45017,,https://huggingface.co/NousResearch/Redmond-Puffin-13B-GGML,"GGML 4bit Quantization of Nous Research's Puffin V1.3 Model: https://huggingface.co/NousResearch/Redmond-Puffin-13B-V1.3; Thank you to Eachadea for making this quantization possible immediately upon launch; For other faster or more accurate quantization methods, please check out Eachadea's hugging face page!; ; The first commercially available language model released by Nous Research!"
llama-2-13B-Guanaco-QLoRA-GPTQ,Text Classification,Transformers,English,other,,12,,141,7436.594359,,https://huggingface.co/TheBloke/llama-2-13B-Guanaco-QLoRA-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Mikael110's Llama2 13B Guanaco QLoRA.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!"
Llama2-chat-Chinese-50W,Text Generation,PyTorch; Transformers,Chinese; English,,,12,,28,14101.26153,,https://huggingface.co/RicardoLee/Llama2-chat-Chinese-50W,"由于目前的LLama2-chat模型很难约束其以中文进行问题回复，因此该模型旨在提供一个能以中文进行问答的LLama2-chat 7B 模型。; 该模型使用LLama2-chat 7B 作为基底模型，使用带embedding 和 LM head 的Lora训练方式训练。模型已完成参数合并，可直接使用。也可以手动将sft_lora_model同Llama2-chat进行合并。; 训练数据使用BELLE项目中采样的50万SFT数据进行SFT训练。; Since the LLama2-chat model struggles to confine its responses to Chinese language when prompted with Chinese questions, the primary objective of this model is to provide a LLama2-chat 7B model that can engage in question and answer interactions in Chinese.; The model utilizes LLama2-chat 7B as its base model and is trained using the Lora training approach with the embedding and LM head. The model has undergone the Lora param merge and is now ready for direct use. It is also possible to manually merge the ./sft_lora_model with the Llama2-chat 7B model to obtain the combined model. "
bert-base-uncased,Fill-Mask,PyTorch; TensorFlow; JAX; Rust; Core ML; ONNX; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/1810.04805.pdf,979,bookcorpus; wikipedia,"38,075,908",2920.704066,558,https://huggingface.co/bert-base-uncased,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; BERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.Chinese and multilingual uncased and cased versions followed shortly after.Modified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.Other 24 smaller models are released afterward.  "
mDeBERTa-v3-base-mnli-xnli,Zero-Shot Classification,PyTorch; Safetensors; Transformers,16 languages,mit,https://arxiv.org/pdf/2111.09543.pdf; https://arxiv.org/pdf/1809.05053.pdf; https://arxiv.org/pdf/1911.02116.pdf,144,multi_nli; xnli,"42,184",1120.319211,5,https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-mnli-xnli,"This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual 
zero-shot classification. The underlying model was pre-trained by Microsoft on the 
CC100 multilingual dataset. It was then fine-tuned on the XNLI dataset, which contains hypothesis-premise pairs from 15 languages, as well as the English MNLI dataset.
As of December 2021, mDeBERTa-base is the best performing multilingual base-sized transformer model, 
introduced by Microsoft in this paper. ; If you are looking for a smaller, faster (but less performant) model, you can 
try multilingual-MiniLMv2-L6-mnli-xnli.; This model was trained on the XNLI development dataset and the MNLI train dataset. The XNLI development set consists of 2490 professionally translated texts from English to 14 other languages (37350 texts in total) (see this paper). Note that the XNLI contains a training set of 15 machine translated versions of the MNLI dataset for 15 languages, but due to quality issues with these machine translations, this model was only trained on the professional translations from the XNLI development set and the original English MNLI training set (392 702 texts). Not using machine translated texts can avoid overfitting the model to the 15 languages; avoids catastrophic forgetting of the other 85 languages mDeBERTa was pre-trained on; and significantly reduces training costs. ; mDeBERTa-v3-base-mnli-xnli was trained using the Hugging Face trainer with the following hyperparameters.; The model was evaluated on the XNLI test set on 15 languages (5010 texts per language, 75150 in total). Note that multilingual NLI models are capable of classifying NLI texts without receiving NLI training data in the specific language (cross-lingual transfer). This means that the model is also able of doing NLI on the other 85 languages mDeBERTa was training on, but performance is most likely lower than for those languages available in XNLI."
clip-vit-large-patch14,Zero-Shot Image Classification,PyTorch; TensorFlow; JAX; Transformers,,,https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/1908.04913.pdf,514,,"12,172,658",5256.806037,373,https://huggingface.co/openai/clip-vit-large-patch14,"Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found here.; The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.; January 2021; The base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.; The original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer."
iroiro-lora,,,,,,325,,0,3.281448174,,https://huggingface.co/2vXpSwA7/iroiro-lora,"?; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
vcclient000,,,,,,72,,0,29229.56145,,https://huggingface.co/wok000/vcclient000,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
30B-Lazarus,Text Generation,PyTorch; Transformers,,,,105,,"3,480",66632.2225,12,https://huggingface.co/CalderaAI/30B-Lazarus,"[] = applied as LoRA to a composite model | () = combined as composite models; [SuperCOT([gtp4xalpaca(manticorechatpygalpha+vicunaunlocked)]+[StoryV2(kaiokendev-SuperHOT-LoRA-prototype30b-8192)])]; This model is the result of an experimental use of LoRAs on language models and model merges that are not the base HuggingFace-format LLaMA model they were intended for.
The desired outcome is to additively apply desired features without paradoxically watering down a model's effective behavior.; Potential limitations - LoRAs applied on top of each other may intercompete.; Subjective results - very promising. Further experimental tests and objective tests are required."
open_llama_13b,Text Generation,PyTorch; Transformers,,apache-2.0,,420,togethercomputer/RedPajama-Data-1T,"32,264",26655.28739,12,https://huggingface.co/openlm-research/open_llama_13b,"In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we’ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.; The model can be evaluated with lm-eval-harness. However, due to the aforementioned tokenizer issue, we need to avoid using the fast tokenizer to obtain the correct results. This can be achieved by passing in use_fast=False to this part of lm-eval-harness, as shown in the example below:"
sdxl-vae,,Diffusers,,mit,https://arxiv.org/pdf/2112.10752.pdf,72,,"302,854",1005.005158,2,https://huggingface.co/stabilityai/sdxl-vae,"You can integrate this fine-tuned VAE decoder to your existing diffusers workflows, by including a vae argument to the StableDiffusionPipeline; SDXL is a latent diffusion model, where the diffusion operates in a pretrained, 
learned (and fixed) latent space of an autoencoder. 
While the bulk of the semantic composition is done by the latent diffusion model, 
we can improve local, high-frequency details in generated images by improving the quality of the autoencoder. 
To this end, we train the same autoencoder architecture used for the original Stable Diffusion at a larger batch-size (256 vs 9) 
and additionally track the weights with an exponential moving average (EMA). 
The resulting autoencoder outperforms the original model in all evaluated reconstruction metrics, see the table below.; SDXL-VAE vs original kl-f8 VAE vs f8-ft-MSE; Inference API has been turned off for this model."
Realistic_Vision_V4.0,,,,creativeml-openrail-m,,52,,0,26549.40322,,https://huggingface.co/SG161222/Realistic_Vision_V4.0,"The recommended negative prompt:; (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck; OR; (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation; Recommended parameters for generation:"
rwkv-4-music,,,,apache-2.0,,18,,0,1396.12167,,https://huggingface.co/BlinkDL/rwkv-4-music,"Use https://github.com/BlinkDL/ChatRWKV/tree/main/music to run current v1 MIDI model; Training data: https://huggingface.co/datasets/breadlicker45/bread-midi-dataset; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
WizardCoder-Guanaco-15B-V1.1-GGML,,,English,apache-2.0,,13,guanaco,0,71782.41408,1,https://huggingface.co/TheBloke/WizardCoder-Guanaco-15B-V1.1-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are StarCoder GGML format model files for LoupGarou's WizardCoder-Guanaco-15B-V1.1.; Please note that these GGMLs are not compatible with llama.cpp, text-generation-webui or llama-cpp-python. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh."
llama-2-7B-Guanaco-QLoRA-GPTQ,Text Classification,Transformers,English,other,,11,,921,3995.954112,,https://huggingface.co/TheBloke/llama-2-7B-Guanaco-QLoRA-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Mikael110's Llama2 7B Guanaco QLoRA.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!"
LLongMA-2-7B-GPTQ,Text Generation,Transformers,,other,https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2212.10554.pdf,11,,100,3994.176124,,https://huggingface.co/TheBloke/LLongMA-2-7B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for ConceptofMind's LLongMA 2 7B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements."
LL7M,Text Generation,PyTorch; Transformers,4 languages,cc-by-nc-nd-4.0,,11,,0,13927.0757,,https://huggingface.co/JosephusCheung/LL7M,"This is a Llama-like generative text model with a scale of 7 billion, optimized for dialogue use cases and converted for the Hugging Face Transformers format. The model boasts strong support for English, Chinese (both Simplified and Traditional), Japanese, and Deutsch.; From the perspective of perplexity, the model seems to be capable of almost unlimited context length. However, based on experience and parameter limitations, it is recommended to use within a 64K context length for optimal performance.; ; The anticipated chat input format is as follows:; Although this is the suggested usage format, Vicuna-style inputs can also be used to adapt to certain pre-existing application scenarios, such as:"
stable-diffusion-inpainting,Text-to-Image,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/2207.12598.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/1910.09700.pdf,1.24k,,"193,916",4372.498375,376,https://huggingface.co/runwayml/stable-diffusion-inpainting,"Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.; The Stable-Diffusion-Inpainting was initialized with the weights of the Stable-Diffusion-v-1-2. First 595k steps regular training, then 440k steps of inpainting training at resolution 512x512 on “laion-aesthetics v2 5+” and 10% dropping of the text-conditioning to improve classifier-free classifier-free guidance sampling. For inpainting, the UNet has 5 additional input channels (4 for the encoded masked-image and 1 for the mask itself) whose weights were zero-initialized after restoring the non-inpainting checkpoint. During training, we generate synthetic masks and in 25% mask everything.; You can use this both with the ?Diffusers library and the RunwayML GitHub repository.; How it works:; Developed by: Robin Rombach, Patrick Esser"
esrgan,,,,,,23,,0,3026.561445,,https://huggingface.co/utnah/esrgan,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
text-to-video-ms-1.7b,Text-to-Video,Diffusers,,cc-by-nc-4.0,,266,,"35,026",0.008237305,139,https://huggingface.co/damo-vilab/text-to-video-ms-1.7b,"This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.; We Are Hiring! (Based in Beijing / Hangzhou, China.); If you're looking for an exciting challenge and the opportunity to work with cutting-edge technologies in AIGC and large-scale pretraining, then we are the place for you. We are looking for talented, motivated and creative individuals to join our team. If you are interested, please send your CV to us.; EMAIL: yingya.zyy@alibaba-inc.com; The text-to-video generation diffusion model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input. The diffusion model adopts a UNet3D structure, and implements video generation through the iterative denoising process from the pure Gaussian noise video."
gpt4-x-alpaca-13b-native-4bit-128g,Text Generation,PyTorch; Transformers,,,,673,,"2,186",16405.10768,52,https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g,"Update (4/1): Added ggml for Cuda model; Dataset is here (instruct): https://github.com/teknium1/GPTeacher; Okay... Two different models now. One generated in the Triton branch, one generated in Cuda. Use the Cuda one for now unless the Triton branch becomes widely used.; Cuda info (use this one):
Command: ; CUDA_VISIBLE_DEVICES=0 python llama.py ./models/chavinlo-gpt4-x-alpaca --wbits 4 --true-sequential --groupsize 128 --save gpt-x-alpaca-13b-native-4bit-128g-cuda.pt"
anything-v5,Text-to-Image,Diffusers,,creativeml-openrail-m,,52,,"34,369",0.004370079,44,https://huggingface.co/stablediffusionapi/anything-v5,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""anything-v5""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Model link: View model"
adetailer,,PyTorch,,agpl-3.0,,141,wider_face; skytnt/anime-segmentation,0,224.8046582,2,https://huggingface.co/Bingsu/adetailer,"; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
control_v1p_sd15_brightness,Image-to-Image,Diffusers,English,creativeml-openrail-m,,20,ioclab/grayscale_image_aesthetic_3M,199,1484.805918,,https://huggingface.co/ViscoseBean/control_v1p_sd15_brightness,"This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.; huggingface.co/spaces/ioclab/brightness-controlnet; [More Information Needed]; [More Information Needed]; [More Information Needed]"
RVCModels,,,,unknown,,28,,0,4770.201484,,https://huggingface.co/juuxn/RVCModels,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
multilingual-e5-large,Feature Extraction,PyTorch; ONNX; Safetensors; Transformers,94 languages,mit,https://arxiv.org/pdf/2212.03533.pdf; https://arxiv.org/pdf/2108.08787.pdf; https://arxiv.org/pdf/2104.08663.pdf; https://arxiv.org/pdf/2210.07316.pdf,38,,"20,375",4609.849589,3,https://huggingface.co/intfloat/multilingual-e5-large,"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 24 layers and the embedding size is 1024.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; This model is initialized from xlm-roberta-large
and continually trained on a mixture of multilingual datasets.
It supports 100 languages from xlm-roberta,
but low-resource languages may see performance degradation.; Initialization: xlm-roberta-large"
mpt-7b-8k,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2010.04245.pdf; https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2302.06675.pdf,10,mc4; c4; togethercomputer/RedPajama-Data-1T; bigcode/the-stack; allenai/s2orc,"1,001",13621.46423,,https://huggingface.co/mosaicml/mpt-7b-8k,"MPT-7B-8k is a decoder-style transformer pretrained starting from MPT-7B, but updating the sequence length to 8k and training for an additional 500B tokens, resulting in a total of 1.5T tokens of text and code.
This model was trained by MosaicML.; MPT-7B-8k is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.; These architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing
positional embeddings with Attention with Linear Biases (ALiBi).
Thanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence.
MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer.; This model uses the MosaicML LLM codebase, which can be found in the llm-foundry repository. It was trained by MosaicML’s NLP team on the MosaicML platform for LLM pretraining, finetuning, and inference.; MPT-7B-8k is"
MythoLogic-13B-GGML,,,English,other,,10,,0,119316.4932,,https://huggingface.co/TheBloke/MythoLogic-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Gryphe's MythoLogic 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh."
Llama-2-ft-instruct-es,Text Generation,PyTorch; Transformers,Spanish,apache-2.0,,10,,25,13805.39216,,https://huggingface.co/clibrain/Llama-2-ft-instruct-es,Llama 2 (7B) fine-tuned on Clibrain's  Spanish instructions dataset.; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model. Links to other models can be found in the index at the bottom.; Inference API has been turned off for this model.
jina-embedding-t-en-v1,Sentence Similarity,PyTorch; Sentence Transformers,English,apache-2.0,,10,jinaai/negation-dataset,0,29.62880394,,https://huggingface.co/jinaai/jina-embedding-t-en-v1,"; 

; 
The text embedding set trained by Jina AI, Finetuner team.
; jina-embedding-t-en-v1 is a tiny small language model that has been trained using Jina AI's Linnaeus-Clean dataset.
This dataset consists of 380 million pairs of sentences, which include both query-document pairs.
These pairs were obtained from various domains and were carefully selected through a thorough cleaning process.
The Linnaeus-Full dataset, from which the Linnaeus-Clean dataset is derived, originally contained 1.6 billion sentence pairs.; The model has a range of use cases, including information retrieval, semantic textual similarity, text reranking, and more."
AlpacaCielo-13b,,,,llama2,,10,,0,8253.721074,,https://huggingface.co/totally-not-an-llm/AlpacaCielo-13b,"Disclaimer: The model might have a tokenizer issue, but functions.  Updates to come.; AlpacaCielo-13b is a llama-2 based model designed for creative tasks, such as storytelling and roleplay, while still doing well with other chatbot purposes.  It is a triple model merge of Nous-Hermes + Guanaco + Storywriter. While it is mostly ""uncensored"", it still inherits some alignment from Guanaco.; Prompt format is:; Thanks to previous similar models such as Alpacino, Alpasta, and AlpacaDente for inspiring the creation of this model.  Thanks also to the creators of the models involved in the merge.  Original models:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
roberta-base,Fill-Mask,PyTorch; TensorFlow; JAX; Rust; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/1907.11692.pdf; https://arxiv.org/pdf/1806.02847.pdf,191,bookcorpus; wikipedia,"9,988,668",2815.28186,165,https://huggingface.co/roberta-base,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is case-sensitive: it
makes a difference between english and English.; Disclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by
the Hugging Face team.; RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means
it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. ; More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model
randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict
the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one
after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to
learn a bidirectional representation of the sentence.; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs."
xlm-roberta-large,Fill-Mask,PyTorch; TensorFlow; JAX; ONNX; Safetensors; Transformers,94 languages,mit,https://arxiv.org/pdf/1911.02116.pdf,187,,"8,031,461",9189.216183,17,https://huggingface.co/xlm-roberta-large,"XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al. and first released in this repository. ; Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.; XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. ; RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.; More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence."
DialoGPT-medium,Conversational,PyTorch; TensorFlow; JAX; Rust; Transformers,,mit,https://arxiv.org/pdf/1911.00536.pdf,209,,"380,342",5544.168943,141,https://huggingface.co/microsoft/DialoGPT-medium,"DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. 
The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test.
The model is trained on 147M multi-turn dialogue from Reddit discussion thread. ; Please find the information about preprocessing, training and full details of the DialoGPT in the original DialoGPT repository; ArXiv paper: https://arxiv.org/abs/1911.00536; Now we are ready to try out how the model works as a chatting partner!"
vit-gpt2-image-captioning,Image-to-Text,PyTorch; Transformers,,apache-2.0,,483,,"1,254,131",984.5934621,289,https://huggingface.co/nlpconnect/vit-gpt2-image-captioning,This is an image captioning model trained by @ydshieh in flax  this is pytorch version of this.; 
anime-kawai-diffusion,Text-to-Image,Diffusers; PyTorch,English,creativeml-openrail-m,,93,,"22,830",0.755424538,42,https://huggingface.co/Ojimi/anime-kawai-diffusion,"; ; As it is a version made only by myself and my small associates, the model will not be perfect and may differ from what people expect. Any contributions from everyone will be respected.; Want to support me? Thank you, please help me make it better. ??; This wouldn't have happened if they hadn't made a breakthrough."
Realistic_Vision_V1.4,Text-to-Image,Diffusers,,creativeml-openrail-m,,280,,"490,749",22016.00378,96,https://huggingface.co/SG161222/Realistic_Vision_V1.4,"Please read this!
My model has always been free and always will be free. There are no restrictions on the use of the model. The rights to this model still belong to me.; Important note: ""RAW photo"" in the prompt may degrade the result.; I use this template to get good generation results:; Prompt:
subject, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3; Example: a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3"
faster-whisper-large-v2,Automatic Speech Recognition,,99 languages,mit,,67,,"47,905",3166.815371,,https://huggingface.co/guillaumekln/faster-whisper-large-v2,"This repository contains the conversion of openai/whisper-large-v2 to the CTranslate2 model format.; This model can be used in CTranslate2 or projects based on CTranslate2 such as faster-whisper.; The original model was converted with the following command:; Note that the model weights are saved in FP16. This type can be changed when the model is loaded using the compute_type option in CTranslate2.; For more information about the original model, see its model card."
vicuna-13b-GPTQ-4bit-128g,Text Generation,PyTorch; Transformers,,,,649,,"1,590",7629.324621,24,https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g,"** Converted model for GPTQ from https://huggingface.co/lmsys/vicuna-13b-delta-v0. This is the best local model I've ever tried. I hope someone makes a version based on the uncensored dataset...**; GPTQ conversion command (on CUDA branch):
CUDA_VISIBLE_DEVICES=0 python llama.py ../lmsys/vicuna-13b-v0 c4 --wbits 4 --true-sequential --groupsize 128 --save vicuna-13b-4bit-128g.pt; Added 1 token to the tokenizer model:
python llama-tools/add_tokens.py lmsys/vicuna-13b-v0/tokenizer.model /content/tokenizer.model llama-tools/test_list.txt; Use of Oobabooga with these tags:
--wbits 4
--groupsize 128; Enjoy"
Ziya-LLaMA-13B-v1,Text Generation,PyTorch; Transformers,English; Chinese,gpl-3.0,https://arxiv.org/pdf/2210.08590.pdf,221,,731,26196.74212,5,https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1,"（LLaMA权重的许可证限制，我们无法直接发布完整的模型权重，用户需要参考使用说明进行合并); 姜子牙通用大模型V1是基于LLaMa的130亿参数的大规模预训练模型，具备翻译，编程，文本分类，信息抽取，摘要，文案生成，常识问答和数学计算等能力。目前姜子牙通用大模型已完成大规模预训练、多任务有监督微调和人类反馈学习三阶段的训练过程。; The Ziya-LLaMA-13B-v1 is a large-scale pre-trained model based on LLaMA with 13 billion parameters. It has the ability to perform tasks such as translation, programming, text classification, information extraction, summarization, copywriting, common sense Q&A, and mathematical calculation. The Ziya-LLaMA-13B-v1 has undergone three stages of training: large-scale continual pre-training (PT), multi-task supervised fine-tuning (SFT), and human feedback learning (RM, PPO).; 原始数据包含英文和中文，其中英文数据来自openwebtext、Books、Wikipedia和Code，中文数据来自清洗后的悟道数据集、自建的中文数据集。在对原始数据进行去重、模型打分、数据分桶、规则过滤、敏感主题过滤和数据评估后，最终得到125B tokens的有效数据。; 为了解决LLaMA原生分词对中文编解码效率低下的问题，我们在LLaMA词表的基础上增加了7k+个常见中文字，通过和LLaMA原生的词表去重，最终得到一个39410大小的词表，并通过复用Transformers里LlamaTokenizer来实现了这一效果。"
e5-large-v2,Feature Extraction,PyTorch; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/2212.03533.pdf; https://arxiv.org/pdf/2104.08663.pdf; https://arxiv.org/pdf/2210.07316.pdf,77,,"67,532",2745.310697,5,https://huggingface.co/intfloat/e5-large-v2,"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 24 layers and the embedding size is 1024.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; Please refer to our paper at https://arxiv.org/pdf/2212.03533.pdf.; Check out unilm/e5 to reproduce evaluation results 
on the BEIR and MTEB benchmark."
musicgen-melody,,,,cc-by-nc-4.0,https://arxiv.org/pdf/2306.05284.pdf,71,,0,3072.490127,99,https://huggingface.co/facebook/musicgen-melody,"Audiocraft provides the code and models for MusicGen, a simple and controllable model for music generation. 
MusicGen is a single stage auto-regressive Transformer model trained over a 32kHz EnCodec tokenizer with 4 codebooks sampled at 50 Hz. 
Unlike existing methods like MusicLM, MusicGen doesn't not require a self-supervised semantic representation, and it generates all 4 codebooks in one pass. 
By introducing a small delay between the codebooks, we show we can predict them in parallel, thus having only 50 auto-regressive steps per second of audio.; MusicGen was published in Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez.; Four checkpoints are released:; Try out MusicGen yourself!; You can run MusicGen locally as well:"
controlnet_qrcode,,Diffusers,English,openrail++,,259,,77,7395.451437,,https://huggingface.co/DionTimmer/controlnet_qrcode,"; These ControlNet models have been trained on a large dataset of 150,000 QR code + QR code artwork couples. They provide a solid foundation for generating QR code-based artwork that is aesthetically pleasing, while still maintaining the integral QR code shape.; The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs. However, a 1.5 version model was also trained on the same dataset for those who are using the older version.
Separate repos for usage in diffusers can be found here:
1.5: https://huggingface.co/DionTimmer/controlnet_qrcode-control_v1p_sd15
2.1: https://huggingface.co/DionTimmer/controlnet_qrcode-control_v11p_sd21; These models perform quite well in most cases, but please note that they are not 100% accurate. In some instances, the QR code shape might not come through as expected. You can increase the ControlNet weight to emphasize the QR code shape. However, be cautious as this might negatively impact the style of your output.To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).; To balance between style and shape, a gentle fine-tuning of the control weight might be required based on the individual input and the desired output, aswell as the correct prompt. Some prompts do not work until you increase the weight by a lot. The process of finding the right balance between these factors is part art and part science. For the best results, it is recommended to generate your artwork at a resolution of 768. This allows for a higher level of detail in the final product, enhancing the quality and effectiveness of the QR code-based artwork."
Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,,46,,"7,625",7631.183963,,https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eric Hartford's Wizard Vicuna 13B Uncensored merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
codegen25-7b-multi,Text Generation,PyTorch; Transformers,code,apache-2.0,https://arxiv.org/pdf/2305.02309.pdf,64,bigcode/starcoderdata,"3,558",28252.20218,2,https://huggingface.co/Salesforce/codegen25-7b-multi,"Title: CodeGen2.5: Small, but mighty; Authors: Erik Nijkamp*, Hiroaki Hayashi*, Yingbo Zhou, Caiming Xiong; (* equal contribution); CodeGen2.5 is a family of autoregressive language models for program synthesis.; Building upon CodeGen2, the model is trained on StarCoderData for 1.4T tokens, achieving competitive results compared to StarCoderBase-15.5B with less than half the size."
MythoLogic-13b,Text Generation,PyTorch; Transformers,English,other,,9,,103,26634.78378,1,https://huggingface.co/Gryphe/MythoLogic-13b,"An experiment with gradient merges using the following script, with Chronos as its primary model, augmented by Hermes and Wizard-Vicuna Uncensored.; Quantized models are available from TheBloke: GGML - GPTQ (You're the best!); Chronos is a wonderfully verbose model, though it definitely seems to lack in the logic department. Hermes and WizardLM have been merged gradually, primarily in the higher layers (10+) in an attempt to rectify some of this behaviour.; The main objective was to create an all-round model with improved story generation and roleplaying capabilities.; Below is an illustration to showcase a rough approximation of the gradients I used to create MythoLogic:"
llama-2-13b-chat-hf,Text Generation,PyTorch; Transformers,,,,9,,692,26657.09115,,https://huggingface.co/daryl149/llama-2-13b-chat-hf,"These are the converted model weights for Llama-2-13B-chat in Huggingface format.; Courtesy of Mirage-Studio.io, home of MirageGPT: the private ChatGPT alternative.; license: other
LLAMA 2 COMMUNITY LICENSE AGREEMENT	
Llama 2 Version Release Date: July 18, 2023; ""Agreement"" means the terms and conditions for use, reproduction, distribution and 
modification of the Llama Materials set forth herein.; ""Documentation"" means the specifications, manuals and documentation 
accompanying Llama 2 distributed by Meta at ai.meta.com/resources/models-and-
libraries/llama-downloads/."
Free_Sydney_13b_HF,Text Generation,PyTorch; Transformers,,,,9,,0,26655.2507,,https://huggingface.co/FPHam/Free_Sydney_13b_HF,"Buy Sydney Ko-fi; LLaMA 2 fine-tune on ... your favorite over-enthusiastic Ai, Syndney.; It has up-to-date information about recent events - but also it's Sydney - so you never know.; Stacked on top of Puffin 13b so it wants to be your assistant.; New: If you want to experience a more naive yet equally attached Sydney, check Pure Sydney "
Free_Sydney_13b_GPTQ,Text Generation,Transformers,,,,9,,47,7436.574304,1,https://huggingface.co/FPHam/Free_Sydney_13b_GPTQ,"Buy Sydney Ko-fi; LLaMA 2 fine-tune on ... your favorite over-enthusiastic Ai, Syndney.; This is 4-bit GPTQ version of the HF version from here: https://huggingface.co/FPHam/Free_Sydney_13b_HF; GPTQ runs slooow on AutoGPTQ, but faaaaast on ExLLaMA; Sydney has up-to-date information about recent events - but also it's Sydney - so you never know."
llama2_7b_chat_uncensored-GPTQ,Text Generation,Transformers,,other,,9,ehartford/wizard_vicuna_70k_unfiltered,108,3994.113138,,https://huggingface.co/TheBloke/llama2_7b_chat_uncensored-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for George Sung's Llama2 7B Chat Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements."
opus-mt-en-zh,Translation,PyTorch; TensorFlow; JAX; Rust; Transformers,English; Chinese,apache-2.0,,148,,"97,535",1516.199356,21,https://huggingface.co/Helsinki-NLP/opus-mt-en-zh,source group: English ; target group: Chinese ; OPUS readme: eng-zho; model: transformer; source language(s): eng
bart-large-cnn,Summarization,PyTorch; TensorFlow; JAX; Rust; Transformers,English,mit,https://arxiv.org/pdf/1910.13461.pdf,481,cnn_dailymail,"1,244,537",7099.01172,280,https://huggingface.co/facebook/bart-large-cnn,"BART model pre-trained on English language, and fine-tuned on CNN Daily Mail. It was introduced in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Lewis et al. and first released in [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart). ; Disclaimer: The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.; BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.; BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.; You can use this model for text summarization. "
vit-base-patch16-224,Image Classification,PyTorch; TensorFlow; JAX; Transformers,,apache-2.0,https://arxiv.org/pdf/2010.11929.pdf; https://arxiv.org/pdf/2006.03677.pdf,272,imagenet-1k; imagenet-21k,"3,378,220",1039.074434,116,https://huggingface.co/google/vit-base-patch16-224,"Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. ; Disclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image."
bloomz,Text Generation,PyTorch; TensorBoard; Safetensors; Transformers,46 languages,bigscience-bloom-rail-1.0,https://arxiv.org/pdf/2211.01786.pdf,429,bigscience/xP3,"16,899",229488.7307,68,https://huggingface.co/bigscience/bloomz,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks.""."
flan-t5-base,Text2Text Generation,PyTorch; TensorFlow; JAX; Safetensors; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/2210.11416.pdf; https://arxiv.org/pdf/1910.09700.pdf,268,svakulenk0/qrecc; taskmaster2; djaym7/wiki_dialog; deepmind/code_contests; lambada; gsm8k; aqua_rat; esnli; quasc; qed,"2,177,672",4191.774056,133,https://huggingface.co/google/flan-t5-base,"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:"
flan-t5-xl,Text2Text Generation,PyTorch; TensorFlow; JAX; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/2210.11416.pdf; https://arxiv.org/pdf/1910.09700.pdf,313,svakulenk0/qrecc; taskmaster2; djaym7/wiki_dialog; deepmind/code_contests; lambada; gsm8k; aqua_rat; esnli; quasc; qed,"262,164",35024.17796,148,https://huggingface.co/google/flan-t5-xl,"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:"
Inkpunk-Diffusion,Text-to-Image,Diffusers,English,creativeml-openrail-m,,868,,"9,669",4372.104659,59,https://huggingface.co/Envvi/Inkpunk-Diffusion,"Finetuned Stable Diffusion model trained on dreambooth. Vaguely inspired by Gorillaz, FLCL, and Yoji Shinkawa. Use nvinkpunk in your prompts.; We support a Gradio Web UI to run Inkpunk-Diffusion:
; 
"
Counterfeit-V2.5,Text-to-Image,Diffusers,,creativeml-openrail-m,,1.4k,,"27,538",24006.36625,67,https://huggingface.co/gsdf/Counterfeit-V2.5,V2.5 has been updated for ease of use as anime-style model.I use this embedding for negative prompts.https://huggingface.co/datasets/gsdf/EasyNegative ; Share by-productsV2.1…Feeling of use similar to V2.0V2.2…NSFW model; ; ; 
ControlNet-modules-safetensors,,,,,,1.26k,,0,6826.605996,51,https://huggingface.co/webui/ControlNet-modules-safetensors,"This repository hosts pruned .safetensors modules of ControlNet, by lllyasviel and T2I-Adapters, TencentARC Team; The modules are meant for this extension for AUTOMATIC1111/stable-diffusion-webui, but should work for different webuis too if they have it implemented. cheers!?; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
T2I-Adapter,,,,,https://arxiv.org/pdf/2302.08453.pdf,553,,0,0.002564583,9,https://huggingface.co/TencentARC/T2I-Adapter, ; ?Adapter Zoo | ?Demos | ?GitHub; T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models; The GitHub repo: https://github.com/TencentARC/T2I-Adapter; Please find the model information in https://github.com/TencentARC/T2I-Adapter/blob/main/docs/AdapterZoo.md
llama-7b,Text Generation,PyTorch; Safetensors; Transformers,,other,,162,,"174,764",27609.43463,10,https://huggingface.co/huggyllama/llama-7b,"This contains the weights for the LLaMA-7b model. This model is under a non-commercial license (see the LICENSE file).
You should only use this repository if you have been granted access to the model by filling out this form but either lost your copy of the weights or got some trouble converting them to the Transformers format."
oasst-sft-6-llama-30b-xor,,,,other,https://arxiv.org/pdf/2304.07327.pdf,929,,0,0.012001953,7,https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor,"Due to the license attached to LLaMA models by Meta AI it is not possible to directly distribute LLaMA-based models. Instead we provide XOR weights for the OA models.; Thanks to Mick for writing the xor_codec.py script which enables this process; Note: This process applies to oasst-sft-6-llama-30b model. The same process can be applied to other models in future, but the checksums will be different..; This process is tested only on Linux (specifically Ubuntu). Some users have reported that the process does not work on Windows. We recommend using WSL if you only have a Windows machine.; To use OpenAssistant LLaMA-Based Models, you should have a copy of the original LLaMA model weights and add them to a llama subdirectory here. If you cannot obtain the original LLaMA, see the note in italic below for a possible alternative."
control_v1u_sd15_illumination_webui,,Transformers,English,creativeml-openrail-m,,60,ioclab/grayscale_image_aesthetic_3M,17,4454.404893,2,https://huggingface.co/ioclab/control_v1u_sd15_illumination_webui,"This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.; Recommendation Weight: 0.4-0.9; Recommendation Exit Timing: 0.4-0.9; As more datasets are still being trained in this model, it is expected to take 2-4 days. Therefore, flexible weight adjustments should be made based on different scenarios and specific results. If you have generated good images or encountered any problems, you can discuss them on Hugging Face~~~; "
starcoderbase,Text Generation,PyTorch; Transformers,,bigcode-openrail-m,https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,305,bigcode/the-stack-dedup,"15,870",0,21,https://huggingface.co/bigcode/starcoderbase,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Please read the BigCode OpenRAIL-M license agreement before accepting it.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; ; Play with the model on the StarCoder Playground."
LLaMa-7B-GGML,,Transformers,,other,,31,,306,61542.41172,,https://huggingface.co/TheBloke/LLaMa-7B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's LLaMA 7b.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh."
musicgen-large,Text2Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/2306.05284.pdf,131,,800,20954.83366,94,https://huggingface.co/facebook/musicgen-large,"MusicGen is a text-to-music model capable of genreating high-quality music samples conditioned on text descriptions or audio prompts.
It is a single stage auto-regressive Transformer model trained over a 32kHz EnCodec tokenizer with 4 codebooks sampled at 50 Hz. 
Unlike existing methods, like MusicLM, MusicGen doesn't require a self-supervised semantic representation, and it generates all 4 codebooks in one pass. 
By introducing a small delay between the codebooks, we show we can predict them in parallel, thus having only 50 auto-regressive steps per second of audio.; MusicGen was published in Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez.; Four checkpoints are released:; Try out MusicGen yourself!; You can run MusicGen locally with the ? Transformers library from version 4.31.0 onwards."
mpt-30b-chat,Text Generation,PyTorch; Transformers,,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2010.04245.pdf,159,camel-ai/code; ehartford/wizard_vicuna_70k_unfiltered; anon8231489123/ShareGPT_Vicuna_unfiltered; teknium1/GPTeacher/roleplay-instruct-v2-final; teknium1/GPTeacher/codegen-isntruct; timdettmers/openassistant-guanaco; camel-ai/math; project-baize/baize-chatbot/medical_chat_data; project-baize/baize-chatbot/quora_chat_data; project-baize/baize-chatbot/stackoverflow_chat_data; camel-ai/biology; camel-ai/chemistry; camel-ai/ai_society; jondurbin/airoboros-gpt4-1.2; LongConversations; camel-ai/physics,"40,856",61363.14842,14,https://huggingface.co/mosaicml/mpt-30b-chat,"MPT-30B-Chat is a chatbot-like model for dialogue generation.
It was built by finetuning MPT-30B on the ShareGPT-Vicuna, Camel-AI,
 GPTeacher, Guanaco, Baize and some generated datasets.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; June 22, 2023; CC-By-NC-SA-4.0 (non-commercial use only); ksreenivasan:"
Platypus-30B,Text Generation,PyTorch; Transformers,English,other,,13,,177,66558.30436,,https://huggingface.co/lilloukas/Platypus-30B,Platypus-30B is an instruction fine-tuned model based on the LLaMA-30B transformer architecture.; We use state-of-the-art Language Model Evaluation Harness to run the benchmark tests above.; Dataset of highly filtered and curated question and answer pairs. Release TBD.; lilloukas/Platypus-30B was instruction fine-tuned using LoRA on 4 A100 80GB. For training details and inference instructions please see the Platypus-30B GitHub repo.; Install LM Evaluation Harness:
xgen-7b-8k-base,Text Generation,PyTorch; Transformers,,apache-2.0,,282,,"18,278",28252.19961,9,https://huggingface.co/Salesforce/xgen-7b-8k-base,"Official research release for the family of XGen models (7B) by Salesforce AI Research:; Title: Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length; Authors: Erik Nijkamp*, Tian Xie*, Hiroaki Hayashi*, Bo Pang*, Congying Xia*, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovs'ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Rayhan Joty, Caiming Xiong.; (* indicates equal contribution); Correspondence to: Shafiq Rayhan Joty, Caiming Xiong"
Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GGML,,,,other,,41,,0,70195.21203,,https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard Vicuna 13B Uncensored.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:"
lince-zero,Text Generation,PyTorch; Transformers,Spanish,apache-2.0,https://arxiv.org/pdf/1910.09700.pdf,29,tatsu-lab/alpaca; databricks/databricks-dolly-15k,"1,073",14176.51304,,https://huggingface.co/clibrain/lince-zero,"LINCE-ZERO (Llm for Instructions from Natural Corpus en Espa?ol) is a SOTA Spanish instruction-tuned LLM ?; Developed by Clibrain, it is a causal decoder-only model with 7B parameters. LINCE-ZERO is based on Falcon-7B and has been fine-tuned using a combination of the Alpaca and Dolly datasets, both translated into Spanish and augmented to 80k examples.; The model is released under the Apache 2.0 license.; If you want to test the robust 40B parameters version called LINCE, you can request access at lince@clibrain.com. Be one of the first to discover the possibilities of LINCE!; LINCE-ZERO (Llm for Instructions from Natural Corpus en Espa?ol) is a state-of-the-art Spanish instruction-tuned large language model. Developed by Clibrain, it is a causal decoder-only model with 7B parameters. LINCE-ZERO is based on Falcon-7B and has been fine-tuned using an 80k examples augmented combination of the Alpaca and Dolly datasets, both translated into Spanish."
aguila-7b,Text Generation,PyTorch; Transformers,English; Spanish; Catalan,,,8,,405,14042.87608,,https://huggingface.co/projecte-aina/aguila-7b,"ǎguila-7B is a transformer-based causal language model for Catalan, Spanish, and English. 
It is based on the Falcon-7B model and has been trained on a 26B token 
trilingual corpus collected from publicly available corpora and crawlers.; The ǎguila-7B model is ready-to-use only for causal language modeling to perform text-generation tasks. 
However, it is intended to be fine-tuned for downstream tasks.; Here is how to use this model:; At the time of submission, no measures have been taken to estimate the bias and toxicity embedded in the model. 
However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques 
on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated. ; We adapted the original Falcon-7B model to Spanish and Catalan by swapping the tokenizer and adjusting the embedding layer. "
FinGPT_ChatGLM2_Sentiment_Instruction_LoRA_FT,,,English,mit,,11,oliverwang15/fingpt_chatglm2_sentiment_instruction_lora_ft_dataset,0,7.827318573,,https://huggingface.co/oliverwang15/FinGPT_ChatGLM2_Sentiment_Instruction_LoRA_FT,"[1] Financial_Phrasebank (FPB)  is a financial news sentiment analysis benchmark, the labels are ""positive"", ""negative"" and ""neutral"". We use the same split as BloombergGPT. BloombergGPT only use 5-shots in the test to show their model's outstanding performance without further finetuning. However, is our task, all data in the 'train' part were used in finetuning, So our results are far better than Bloomberg's.; [2] FiQA SA consists of 17k sentences from microblog headlines and financial news. These labels were changed to ""positive"", ""negative"" and ""neutral"" according to BloombergGPT's paper. We have tried to use the same split as BloombergGPT's paper. However, the amounts of each label can't match exactly when the seed was set to 42.; [3] Twitter Financial News Sentiment (TFNS) dataset is an English-language dataset containing an annotated corpus of finance-related tweets. This dataset is used to classify finance-related tweets for their sentiment. The dataset holds 11,932 documents annotated with 3 labels: ""Bearish"" (""negative""), ""Bullish"" (""positive""), and ""Neutral"".; [4] News With GPT Instruction (MWGI) is a dataset whose labels were generated by ChatGPT. The train set has 16.2k samples and the test set has 4.05k samples. The dataset not only contains 7 classification labels: ""strong negative"", ""moderately negative"", ""mildly negative"", ""neutral"", ""mildly positive"", ""moderately positive"", ""strong positive"". but it also has the reasons for that result, which might be helpful in the instruction finetuning.; Coming Soon."
openchat_v2_openorca_preview-GPTQ,Text Generation,Transformers,,other,,12,Open-Orca/OpenOrca,171,7629.305313,,https://huggingface.co/TheBloke/openchat_v2_openorca_preview-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Open-Orca's OpenChat V2 x OpenOrca Preview 2.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
Replit-Code-Instruct-Glaive-GGML,,,,,,8,,0,9820.167471,,https://huggingface.co/TheBloke/Replit-Code-Instruct-Glaive-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are Replit GGML format model files for Sahil2801's Replit Code Instruct Glaive.; Please note that these GGMLs are not compatible with llama.cpp, text-generation-webui or llama-cpp-python. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh."
llama2-13b-Chinese-chat,Question Answering,PEFT,Chinese,,,8,shareAI/shareGPT_cn; FreedomIntelligence/ShareGPT-CN,6,3072.531293,,https://huggingface.co/shareAI/llama2-13b-Chinese-chat,完整合并后文件下载：https://www.codewithgpu.com/m/file/llama2-13b-Chinese-chat; 项目在中文sharegpt数据集上训练得到的llama2 Chinese chat 13b，为减轻文件大小负担这里只放出了adapter的权重请拉取https://huggingface.co/TheBloke/Llama-2-13B-fp16 作为基础权重，使用如下脚步执行合并得到可工作的总权重：  ; 合并后，体验对话：; 推荐继续二次训练以针对性调优对话效果~ ; The following bitsandbytes quantization config was used during training:
distilbert-base-uncased,Fill-Mask,PyTorch; TensorFlow; JAX; Rust; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/1910.01108.pdf,237,bookcorpus; wikipedia,"8,105,818",1529.702045,185,https://huggingface.co/distilbert-base-uncased,"This model is a distilled version of the BERT base model. It was
introduced in this paper. The code for the distillation process can be found
here. This model is uncased: it does
not make a difference between english and English.; DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a
self-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,
with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic
process to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained
with three objectives:; This way, the model learns the same inner representation of the English language than its teacher model, while being
faster for inference or downstream tasks.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the model hub to look for
fine-tuned versions on a task that interests you.; Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification or question answering. For tasks such as text
generation you should look at model like GPT2."
t5-base,Translation,PyTorch; TensorFlow; JAX; Rust; Safetensors; Transformers,4 languages,apache-2.0,https://arxiv.org/pdf/1805.12471.pdf; https://arxiv.org/pdf/1708.00055.pdf; https://arxiv.org/pdf/1704.05426.pdf; https://arxiv.org/pdf/1606.05250.pdf; https://arxiv.org/pdf/1808.09121.pdf; https://arxiv.org/pdf/1810.12885.pdf; https://arxiv.org/pdf/1905.10044.pdf; https://arxiv.org/pdf/1910.09700.pdf,290,c4,"2,535,137",4462.173523,310,https://huggingface.co/t5-base,"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-Base is the checkpoint with 220 million parameters. ; The developers write in a blog post that the model: "
DeBERTa-v3-base-mnli-fever-anli,Zero-Shot Classification,PyTorch; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/2006.03654.pdf,47,multi_nli; anli; fever,"9,258,143",740.4856808,4,https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli,"This model was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs. This base model outperforms almost all large models on the ANLI benchmark. 
The base model is DeBERTa-v3-base from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective, see annex 11 of the original DeBERTa paper. ; For highest performance (but less speed), I recommend using https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli.; DeBERTa-v3-base-mnli-fever-anli was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs.; DeBERTa-v3-base-mnli-fever-anli was trained using the Hugging Face trainer with the following hyperparameters.; The model was evaluated using the test sets for MultiNLI and ANLI and the dev set for Fever-NLI. The metric used is accuracy."
EmoRoBERTa,Text Classification,TensorFlow; Transformers,English,mit,,71,go_emotions,"45,494",502.2349918,3,https://huggingface.co/arpanghoshal/EmoRoBERTa,"Connect me on LinkedIn; Dataset labelled 58000 Reddit comments with 28 emotions; RoBERTa builds on BERT’s language masking strategy and modifies key hyperparameters in BERT, including removing BERT’s next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time. This allows RoBERTa representations to generalize even better to downstream tasks compared to BERT.; Best Result of Macro F1 - 49.30%; Output "
roberta-base-squad2,Question Answering,PyTorch; TensorFlow; JAX; Rust; Safetensors; Transformers,English,cc-by-4.0,,365,squad_v2,"2,899,042",2485.331077,279,https://huggingface.co/deepset/roberta-base-squad2,"This is the roberta-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering. ; Language model: roberta-baseLanguage: EnglishDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD 2.0Code:  See an example QA pipeline on HaystackInfrastructure: 4x Tesla v100; Please note that we have also released a distilled version of this model called deepset/tinyroberta-squad2. The distilled model has a comparable prediction quality and runs at twice the speed of the base model.; Haystack is an NLP framework by deepset. You can use this model in a Haystack pipeline to do question answering at scale (over many documents). To load the model in Haystack:; For a complete example of roberta-base-squad2 being used for  Question Answering, check out the Tutorials in Haystack Documentation"
wav2vec2-large-xlsr-53-english,Automatic Speech Recognition,PyTorch; JAX; Safetensors; Transformers,English,apache-2.0,,226,common_voice; mozilla-foundation/common_voice_6_0,"56,332,600",3873.834471,44,https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english,"Fine-tuned facebook/wav2vec2-large-xlsr-53 on English using the train and validation splits of Common Voice 6.1.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:"
deberta-v3-base,Fill-Mask,PyTorch; TensorFlow; Rust; Transformers,English,mit,https://arxiv.org/pdf/2006.03654.pdf; https://arxiv.org/pdf/2111.09543.pdf,99,,"86,456",1851.465143,8,https://huggingface.co/microsoft/deberta-v3-base,"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. ; In DeBERTa V3, we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our paper.; Please check the official repository for more implementation details and updates.; The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters  with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.; We present the dev results on SQuAD 2.0 and MNLI tasks."
all-mpnet-base-v2,Sentence Similarity,PyTorch; Sentence Transformers,English,apache-2.0,https://arxiv.org/pdf/1904.06472.pdf; https://arxiv.org/pdf/2102.07033.pdf; https://arxiv.org/pdf/2104.08727.pdf; https://arxiv.org/pdf/1704.05179.pdf; https://arxiv.org/pdf/1810.09305.pdf,284,s2orc; flax-sentence-embeddings/stackexchange_xml; MS Marco; gooaq; yahoo_answers_topics; code_search_net; search_qa; eli5; snli; multi_nli; wikihow; natural_questions; trivia_qa; embedding-data/sentence-compression; embedding-data/flickr30k-captions; embedding-data/altlex; embedding-data/simple-wiki; embedding-data/QQP; embedding-data/SPECTER; embedding-data/PAQ_pairs; embedding-data/WikiAnswers,"4,149,102",438.7459291,139,https://huggingface.co/sentence-transformers/all-mpnet-base-v2,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net"
sepformer-wsj02mix,Audio-to-Audio,speechbrain,English,apache-2.0,https://arxiv.org/pdf/2010.13154.pdf; https://arxiv.org/pdf/2106.04624.pdf,24,WSJ0-2Mix,"2,087",113.1092365,4,https://huggingface.co/speechbrain/sepformer-wsj02mix,"This repository provides all the necessary tools to perform audio source separation with a SepFormer 
model, implemented with SpeechBrain, and pretrained on WSJ0-2Mix dataset. For a better experience we encourage you to learn more about
SpeechBrain. The model performance is 22.4 dB on the test set of WSJ0-2Mix dataset.; You can listen to example results obtained on the test set of WSJ0-2/3Mix through here. ; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; The system expects input recordings sampled at 8kHz (single channel).
If your signal has a different sample rate, resample it (e.g, using torchaudio or sox) before using the interface."
twitter-roberta-base-sentiment-latest,Text Classification,PyTorch; TensorFlow; Transformers,English,,https://arxiv.org/pdf/2202.03829.pdf,170,tweet_eval,"754,428",1001.329717,42,https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest,"This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. 
The original Twitter-based RoBERTa model can be found here and the original reference paper is TweetEval. This model is suitable for English. ; Labels: 
0 -> Negative;
1 -> Neutral;
2 -> Positive; This sentiment analysis model has been integrated into TweetNLP. You can access the demo here.; Output: "
nllb-200-3.3B,Translation,PyTorch; Transformers,196 languages,cc-by-nc-4.0,,82,flores-200,"51,820",18024.1715,14,https://huggingface.co/facebook/nllb-200-3.3B,"This is the model card of NLLB-200's 3.3B variant.; Here are the metrics for that particular checkpoint.; ? Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.; ? We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.; ? In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety)."
ast-finetuned-audioset-10-10-0.4593,Audio Classification,PyTorch; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2104.01778.pdf,53,,"80,953",346.029043,4,https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593,"Audio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. ; Disclaimer: The team releasing Audio Spectrogram Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.; The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.; You can use the raw model for classifying audio into one of the AudioSet classes. See the documentation for more info."
BioMedLM,Text Generation,PyTorch; Transformers,,bigscience-bloom-rail-1.0,https://arxiv.org/pdf/2112.04359.pdf,222,pubmed,"7,539",10958.89947,13,https://huggingface.co/stanford-crfm/BioMedLM,"Note: This model was previously known as PubMedGPT 2.7B, but we have changed it due to a request from the NIH which holds the trademark for ""PubMed"".; BioMedLM 2.7B is new language model trained exclusively on biomedical abstracts and papers from The Pile. This GPT-style model can achieve strong results on a variety of biomedical NLP tasks, including a new state of the art performance of 50.3% accuracy on the MedQA biomedical question answering task.; As an autoregressive language model, BioMedLM 2.7B is also capable of natural language generation. However, we have only begun to explore the generation capabilities and limitations of this model, and we emphasize that this model’s generation capabilities are for research purposes only and not suitable for production. In releasing this model, we hope to advance both the development of biomedical NLP applications and best practices for responsibly training and utilizing domain-specific language models; issues of reliability, truthfulness, and explainability are top of mind for us.; This model was a joint collaboration of Stanford CRFM and MosaicML.; BioMedLM 2.7B is new language model trained exclusively on biomedical abstracts and papers from The Pile. This GPT-style model can achieve strong results on a variety of biomedical NLP tasks, including a new state of the art performance of 50.3% accuracy on the MedQA biomedical question answering task."
dolly-v2-12b,Text Generation,PyTorch; Transformers,English,mit,,1.83k,databricks/databricks-dolly-15k,"105,534",24373.33227,33,https://huggingface.co/databricks/dolly-v2-12b,"Databricks' dolly-v2-12b, an instruction-following large language model trained on the Databricks machine learning platform 
that is licensed for commercial use. Based on pythia-12b, Dolly is trained on ~15k instruction/response fine tuning records 
databricks-dolly-15k generated 
by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,
information extraction, open QA and summarization. dolly-v2-12b is not a state-of-the-art model, but does exhibit surprisingly 
high quality instruction following behavior not characteristic of the foundation model on which it is based.  ; Dolly v2 is also available in these smaller models sizes:; Please refer to the dolly GitHub repo for tips on 
running inference for various GPU configurations.; Owner: Databricks, Inc.; dolly-v2-12b is a 12 billion parameter causal language model created by Databricks that is derived from 
EleutherAI's Pythia-12b and fine-tuned 
on a ~15K record instruction corpus generated by Databricks employees and released under a permissive license (CC-BY-SA)"
dolly-v2-3b,Text Generation,PyTorch; Transformers,English,mit,,209,databricks/databricks-dolly-15k,"120,559",5818.45217,25,https://huggingface.co/databricks/dolly-v2-3b,"Databricks' dolly-v2-3b, an instruction-following large language model trained on the Databricks machine learning platform 
that is licensed for commercial use. Based on pythia-2.8b, Dolly is trained on ~15k instruction/response fine tuning records 
databricks-dolly-15k generated 
by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,
information extraction, open QA and summarization. dolly-v2-3b is not a state-of-the-art model, but does exhibit surprisingly 
high quality instruction following behavior not characteristic of the foundation model on which it is based.  ; Dolly v2 is also available in these larger models sizes:; Please refer to the dolly GitHub repo for tips on 
running inference for various GPU configurations.; Owner: Databricks, Inc.; dolly-v2-3b is a 2.8 billion parameter causal language model created by Databricks that is derived from 
EleutherAI's Pythia-2.8b and fine-tuned 
on a ~15K record instruction corpus generated by Databricks employees and released under a permissive license (CC-BY-SA)"
Wizard-Vicuna-13B-Uncensored-GPTQ,Text Generation,Transformers,English,other,,193,ehartford/wizard_vicuna_70k_unfiltered,"6,086",8306.978477,,https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is GPTQ format quantised 4bit models of Eric Hartford's 'uncensored' training of Wizard-Vicuna 13B.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Open the text-generation-webui UI as normal.
bark-voice-cloning,Feature Extraction,,,mit,,37,GitMylo/bark-semantic-training,0,452.0046289,16,https://huggingface.co/GitMylo/bark-voice-cloning,"Bark-voice-cloning is a model which processes the outputs from a HuBERT model, and turns them into semantic tokens compatible with bark text to speech.; This can be used for many things, including speech transfer and voice cloning.; code repo
audio webui; (Please use the model manager from the code repo for easy downloading of models); Voice cloning is creating a new voice for text-to-speech."
e5-base-v2,Feature Extraction,PyTorch; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/2212.03533.pdf; https://arxiv.org/pdf/2104.08663.pdf; https://arxiv.org/pdf/2210.07316.pdf,36,,"59,752",876.9893331,7,https://huggingface.co/intfloat/e5-base-v2,"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 12 layers and the embedding size is 768.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; Please refer to our paper at https://arxiv.org/pdf/2212.03533.pdf.; Check out unilm/e5 to reproduce evaluation results 
on the BEIR and MTEB benchmark."
guanaco-65B-GGML,,,,other,https://arxiv.org/pdf/2305.14314.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2304.07327.pdf,101,,0,596582.4202,,https://huggingface.co/TheBloke/guanaco-65B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Tim Dettmers' Guanaco 65B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
cpm-bee-10b,Feature Extraction,PyTorch; Transformers,English; Chinese,,,144,,"2,002",19661.5801,1,https://huggingface.co/openbmb/cpm-bee-10b,"CPM-Bee is a fully open-source, commercially-usable Chinese-English bilingual base model with a capacity of ten billion parameters. It is the second milestone achieved through the training process of CPM-live.
Utilizing the Transformer auto-regressive architecture, CPM-Bee has been pre-trained on an extensive corpus of trillion-scale tokens, thereby possessing remarkable foundational capabilities.; Open-source and Commercial Usable：OpenBMB adheres to the spirit of open-source, aiming to make large-scale models accessible to everyone. CPM-Bee, as a foudation model, is fully open-source and available for commercial use, contributing to the advancement of the field of large-scale models.; Excellent Performance in Chinese and English： : CPM-Bee's base model has undergone rigorous selection and balancing of pre-training data, resulting in outstanding performance in both Chinese and English. For detailed information regarding evaluation tasks and results, please refer to the assessment documentation.; Vast and High-quality Corpus： CPM-Bee, as a base model, has been trained on an extensive corpus of over trillion tokens, making it one of the models with the highest volume of training data within the open-source community. Furthermore, we have implemented stringent selection, cleaning, and post-processing procedures on the pre-training corpus to ensure its quality.; Support for OpenBMB System： The OpenBMB system provides a comprehensive ecosystem of tools and scripts for high-performance pre-training, adaptation, compression, deployment, and tool development. CPM-Bee, as a base model, is accompanied by all the necessary tool scripts, enabling developers to efficiently utilize and explore advanced functionalities."
openchat,Text Generation,PyTorch; Transformers,English,other,,216,,"6,482",26657.08894,18,https://huggingface.co/openchat/openchat,"OpenChat is a series of open-source language models fine-tuned on a diverse and high-quality dataset of multi-round conversations. With only ~6K GPT-4 conversations filtered from the ~90K ShareGPT conversations, OpenChat is designed to achieve high performance with limited data.; Generic models:; Code models:; Note: Please load the pretrained models using bfloat16; We provide the full source code, including an inference server compatible with the ""ChatCompletions"" API, in the OpenChat GitHub repository."
airoboros-33B-gpt4-1.4-GPTQ,Text Generation,Transformers,,other,,19,,"1,281",17307.95074,,https://huggingface.co/TheBloke/airoboros-33B-gpt4-1.4-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Jon Durbin's Airoboros 33B GPT4 1.4.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
open_llama_7b_v2,Text Generation,PyTorch; Transformers,,apache-2.0,,57,tiiuae/falcon-refinedweb; bigcode/starcoderdata; togethercomputer/RedPajama-Data-1T,"12,437",13804.06066,3,https://huggingface.co/openlm-research/open_llama_7b_v2,"TL;DR: we are releasing our public preview of OpenLLaMA, a permissively licensed open source reproduction of Meta AI’s LLaMA. We are releasing a series of 3B, 7B and 13B models trained on different data mixtures. Our model weights can serve as the drop in replacement of LLaMA in existing implementations.; In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a series of 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. The v2 model is better than the old v1 model trained on a different data mixture. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we’ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation."
WizardLM-13B-V1.1,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2304.12244.pdf,54,,"2,433",26624.5159,7,https://huggingface.co/WizardLM/WizardLM-13B-V1.1,This is the Full-Weight of WizardLM-13B V1.1 model.; Repository: https://github.com/nlpxucan/WizardLM; Twitter: https://twitter.com/WizardLM_AI/status/1677282955490918401
orca_mini_v2_13b-GPTQ,Text Generation,Transformers,English,other,https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2304.12244.pdf,16,psmathur/orca_minis_uncensored_dataset,894,7631.150576,,https://huggingface.co/TheBloke/orca_mini_v2_13b-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Pankaj Mathur's Orca Mini v2 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
bk-sdm-base,Text-to-Image,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/2305.15798.pdf,7,,163,0.013230782,1,https://huggingface.co/nota-ai/bk-sdm-base,"Block-removed Knowledge-distilled Stable Diffusion Model (BK-SDM) is an architecturally compressed SDM for efficient general-purpose text-to-image synthesis. This model is bulit with (i) removing several residual and attention blocks from the U-Net of Stable Diffusion v1.4 and (ii) distillation pretraining on only 0.22M LAION pairs (fewer than 0.1% of the full training set). Despite being trained with very limited resources, our compact model can imitate the original SDM by benefiting from transferred knowledge.; An inference code with the default PNDM scheduler and 50 denoising steps is as follows.; The following code is also runnable, because we compressed the U-Net of Stable Diffusion v1.4 while keeping the other parts (i.e., Text Encoder and Image Decoder) unchanged:; The above examples have been tested on a single NVIDIA GeForce RTX 3090 GPU with the following versions:; We removed several residual and attention blocks from the 0.86B-parameter U-Net in the 1.04B-param SDM-v1.4, and our compressed models are summarized as follows."
open_llama_3b_v2,Text Generation,PyTorch; Transformers,,apache-2.0,,14,tiiuae/falcon-refinedweb; bigcode/starcoderdata; togethercomputer/RedPajama-Data-1T,"4,157",7014.914892,1,https://huggingface.co/openlm-research/open_llama_3b_v2,"TL;DR: we are releasing our public preview of OpenLLaMA, a permissively licensed open source reproduction of Meta AI’s LLaMA. We are releasing a series of 3B, 7B and 13B models trained on different data mixtures. Our model weights can serve as the drop in replacement of LLaMA in existing implementations.; In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a series of 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. The v2 model is better than the old v1 model trained on a different data mixture. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we’ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation."
llama-65b-instruct,Text Generation,PyTorch; Transformers,English,,,7,sciq; metaeval/ScienceQA_text_only; GAIR/lima; Open-Orca/OpenOrca; openbookqa,12,133654.3894,,https://huggingface.co/upstage/llama-65b-instruct,
Llama-2-7B-Chat-ggml,,,,,,7,,0,61870.09118,,https://huggingface.co/localmodels/Llama-2-7B-Chat-ggml,"From: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf; Quantized using an older version of llama.cpp and compatible with llama.cpp from May 19, commit 2d5db48.; Quantization methods compatible with latest llama.cpp from June 6, commit 2d43387.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Llama-2-13b-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,,7,,"2,103",79976.22835,,https://huggingface.co/NousResearch/Llama-2-13b-hf,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations."
LLaMA-2-70B-GPTQ-transformers4.32.0.dev0,Text Generation,Safetensors; Transformers; PyTorch,English,other,,7,,166,36147.7669,,https://huggingface.co/Panchovix/LLaMA-2-70B-GPTQ-transformers4.32.0.dev0,"These files are GPTQ model files for Meta's Llama 2 70B but with new FP16 files, made with the last transformers version. (transformers-4.32.0.dev0); GQA Works with exllama, but not GPTQ for LLaMA/AutoGPTQ.; The model was quantized with GPTQ-for-LLaMA, without group size to reduce VRAM usage, with true sequential and act order true, to not lose a lot of perplexity.; TBD; Exllama works 2x24 VRAM GPUs and it is the recommended way to use it now. It even can do 16K! context with high alpha values and NTK Alpha."
llama-2-13B-German-Assistant-v2-GGML,,Transformers,English; German,other,,7,flozi00/conversations,2,119316.5044,,https://huggingface.co/TheBloke/llama-2-13B-German-Assistant-v2-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for flozi00's Llama 2 13B German Assistant v2.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!"
llama-2-70b-Guanaco-QLoRA-GPTQ,Text Classification,Transformers,English,other,,7,,96,37583.15414,,https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Mikael110's Llama2 70b Guanaco QLoRA.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements."
opus-mt-zh-en,Translation,PyTorch; TensorFlow; Rust; Transformers,Chinese; English,cc-by-4.0,,235,,"364,263",1206.200768,30,https://huggingface.co/Helsinki-NLP/opus-mt-zh-en,"This model can be used for translation and text-to-text generation.; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).; Further details about the dataset for this model can be found in the OPUS readme: zho-eng; pre-processing: normalization + SentencePiece (spm32k,spm32k)"
finbert,Text Classification,PyTorch; TensorFlow; JAX; Transformers,English,,https://arxiv.org/pdf/1908.10063.pdf,272,,"1,138,607",1314.229451,42,https://huggingface.co/ProsusAI/finbert,"FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper FinBERT: Financial Sentiment Analysis with Pre-trained Language Models and our related blog post on Medium.; The model will give softmax outputs for three labels: positive, negative or neutral.; About Prosus; Prosus is a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. For more information, please visit www.prosus.com.; Contact information"
vilt-b32-finetuned-vqa,Visual Question Answering,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2102.03334.pdf,193,,"505,022",470.8183564,111,https://huggingface.co/dandelin/vilt-b32-finetuned-vqa,"Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer
Without Convolution or Region Supervision by Kim et al. and first released in this repository. ; Disclaimer: The team releasing ViLT did not write a model card for this model so this model card has been written by the Hugging Face team.; You can use the raw model for visual question answering. ; Here is how to use this model in PyTorch:; (to do)"
bert-base-NER,Token Classification,PyTorch; TensorFlow; JAX; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/1810.04805.pdf,204,conll2003,"1,317,035",1731.214085,28,https://huggingface.co/dslim/bert-base-NER,"bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). ; Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset. ; If you'd like to use a larger BERT-large model fine-tuned on the same dataset, a bert-large-NER version is also available. ; You can use this model with Transformers pipeline for NER.; This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases. "
fastspeech2-en-ljspeech,Text-to-Speech,Fairseq,English,,https://arxiv.org/pdf/2006.04558.pdf; https://arxiv.org/pdf/2109.06912.pdf,185,ljspeech,"5,383",550.8065611,75,https://huggingface.co/facebook/fastspeech2-en-ljspeech,FastSpeech 2 text-to-speech model from fairseq S^2 (paper/code):; See also fairseq S^2 example.
flan-t5-large,Text2Text Generation,PyTorch; TensorFlow; JAX; Safetensors; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/2210.11416.pdf; https://arxiv.org/pdf/1910.09700.pdf,202,svakulenk0/qrecc; taskmaster2; djaym7/wiki_dialog; deepmind/code_contests; lambada; gsm8k; aqua_rat; esnli; quasc; qed,"1,122,649",13100.17078,141,https://huggingface.co/google/flan-t5-large,"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:"
f222,,Flair,Avaric,openrail,,164,glue,0,8744.961557,,https://huggingface.co/acheong08/f222,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
stable-diffusion-x4-upscaler,,Diffusers,,openrail++,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,449,,"127,591",7229.454213,28,https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler,"This model card focuses on the model associated with the Stable Diffusion Upscaler, available here.
This model is trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model.
In addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. ; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model; Language(s): English"
speecht5_tts,Text-to-Speech,PyTorch; Transformers,,mit,https://arxiv.org/pdf/2110.07205.pdf; https://arxiv.org/pdf/1910.09700.pdf,181,libritts,"36,978",585.2463447,172,https://huggingface.co/microsoft/speecht5_tts,"SpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS.; This model was introduced in SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.; SpeechT5 was first released in this repository, original weights. The license used is MIT.; Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder.; Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder."
loliDiffusion,Text-to-Image,Diffusers,,creativeml-openrail-m,,150,,0,117948.1855,3,https://huggingface.co/JosefJilek/loliDiffusion,"The goal of this project is to improve generation of loli characters since most of other models are not good at it. Support me: https://www.buymeacoffee.com/jilek772003 New models are available on https://pixai.art/ Model list: ; It is recommende to use standard resolution such as 512x768 and EasyNegative embedding with these models. Positive prompt example: 1girl, solo, loli, masterpiece Negative prompt example: EasyNegative, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, multiple panels, aged up, old; Reddit: https://www.reddit.com/r/loliDiffusion Discord: https://discord.gg/mZ3eGeNX7S; v0.4.3 Fixed color issue General improvements v0.5.3 Integrated VAEFile size reduced CLIP force reset fix v0.6.3 Style improvements Added PastelMix and Counterfeit style v0.7.x Style impovements Composition improvements v0.8.x Major improvement on higher resolutions Style improvements Flexibility and responsivity Added support for Night Sky YOZORA model v0.9.x Different approach at merging, you might find v0.8.x versions better Changes at supported models v2.1.X EXPERIMENTAL RELEASE Stable Diffusion 2.1-768 based Default negative prompt: (low quality, worst quality:1.4), (bad anatomy), extra finger, fewer digits, jpeg artifacts For positive prompt it's good to include tags: anime, (masterpiece, best quality) alternatively you may achieve positive response with: (exceptional, best aesthetic, new, newest, best quality, masterpiece, extremely detailed, anime, waifu:1.2) Though it's Loli Diffusion model it's quite general purpose The ability to generate realistic images as Waifu Diffusion can was intentionally decreased This model performs better at higher resolutions like 768*X or 896*X v0.10.x Different approach at merging Better hands Better style inheritance Some changes in supported models ; "
alpaca-lora-7b,,,,mit,,399,yahma/alpaca-cleaned,0,67.2026107,43,https://huggingface.co/tloen/alpaca-lora-7b,"This repo contains a low-rank adapter for LLaMA-7b
fit on the Stanford Alpaca dataset.; This version of the weights was trained with the following hyperparameters:; That is:; Instructions for running it can be found at https://github.com/tloen/alpaca-lora.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
alpaca-native-7B-ggml,,,,unknown,,52,,0,36669.44176,,https://huggingface.co/Pi3141/alpaca-native-7B-ggml,"Mirrored version of https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml in case that one gets taken down
All credits go to Sosaka and chavinlo for creating the modelhttps://huggingface.co/chavinlo/alpaca-native; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
doll-likeness-series,,,,creativeml-openrail-m,,322,,0,1359.007158,,https://huggingface.co/Kanbara/doll-likeness-series,"The 'Doll-Series' is a set of LORA focused on realistic Asian faces, with incredible levels of beauty and aesthetics.; My Pixiv: https://www.pixiv.net/en/users/92373922; My Twitter: https://twitter.com/KbrLoras; License; Disclaimer"
llama-65b,Text Generation,PyTorch; Safetensors; Transformers,,other,,55,,"57,878",267307.4325,5,https://huggingface.co/huggyllama/llama-65b,"This contains the weights for the LLaMA-65b model. This model is under a non-commercial license (see the LICENSE file).
You should only use this repository if you have been granted access to the model by filling out this form but either lost your copy of the weights or got some trouble converting them to the Transformers format."
ShiratakiMix,Text-to-Image,Diffusers,Japanese,creativeml-openrail-m,,107,,0,9277.457041,1,https://huggingface.co/Vsukiyaki/ShiratakiMix,"=> ShiratakiMix-add-VAE.safetensors; Negative:; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage. The CreativeML OpenRAIL License specifies:; You can't use the model to deliberately produce nor share illegal or harmful outputs or content; The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license"
Guanaco,Conversational,PyTorch; Transformers,4 languages,gpl-3.0,,193,JosephusCheung/GuanacoDataset,"2,608",27599.44985,10,https://huggingface.co/JosephusCheung/Guanaco,"; You can run on Colab free T4 GPU now; ;  It is highly recommended to use fp16 inference for this model, as 8-bit precision may significantly affect performance. If you require a more Consumer Hardware friendly version, please use the specialized quantized, only 5+GB V-Ram required JosephusCheung/GuanacoOnConsumerHardware.;  You are encouraged to use the latest version of transformers from GitHub."
vicuna-13B-1.1-GPTQ-4bit-128g,Conversational,Transformers,,other,,193,,"20,828",7436.580939,,https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a 4-bit GPTQ version of the Vicuna 13B 1.1 model.; It was created by merging the deltas provided in the above repo with the original Llama 13B model, using the code provided on their Github page.; It was then quantized to 4bit using GPTQ-for-LLaMa."
LLaVA-13b-delta-v0,Text Generation,PyTorch; Transformers,,apache-2.0,,162,,393,26675.98551,14,https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0,"NOTE: This ""delta model"" cannot be used directly.Users have to apply it on top of the original LLaMA weights to get actual LLaVA weights.See https://github.com/haotian-liu/LLaVA#llava-weights for instructions.; Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA was trained in April 2023.; Paper or resources for more information:
https://llava-vl.github.io/; License:
Apache License 2.0"
mGPT-13B,Text Generation,PyTorch; Transformers,English; Russian,mit,,6,,11,55979.76898,,https://huggingface.co/ai-forever/mGPT-13B,"Multilingual language model. This model was trained on the 61 languages from 25 language families (see the list below).; Model was pretrained on a 600Gb of texts, mostly from MC4 and Wikipedia. Training data was deduplicated, the text deduplication includes 64-bit hashing of each text in the corpus for keeping texts with a unique hash. We also filter the documents based on their text compression rate using zlib4. The most strongly and weakly compressing deduplicated texts are discarded.; Here is the table with number of tokens for each language in the pretraining corpus on a logarithmic scale:; ; Afrikaans (af), Arabic (ar), Armenian (hy), Azerbaijani (az), Basque (eu), Bashkir (ba), Belarusian (be), Bengali (bn), Bulgarian (bg), Burmese (my), Buryat (bxr), Chuvash (cv), Danish (da), English (en), Estonian (et), Finnish (fi), French (fr), Georgian (ka), German (de), Greek (el), Hebrew (he), Hindi (hi), Hungarian (hu), Indonesian (id), Italian (it), Japanese (ja), Javanese (jv), Kalmyk (xal), Kazakh (kk), Korean (ko), Kyrgyz (ky), Latvian (lv), Lithuanian (lt), Malay (ms), Malayalam (ml), Marathi (mr), Mongolian (mn), Ossetian (os), Persian (fa), Polish (pl), Portuguese (pt), Romanian (ro), Russian (ru), Spanish (es), Swedish (sv), Swahili (sw), Tatar (tt), Telugu (te), Thai (th), Turkish (tr), Turkmen (tk), Tuvan (tyv), Ukrainian (uk), Uzbek (uz), Vietnamese (vi), Yakut (sax), Yoruba (yo)"
mpt-7b-storywriter,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2302.06675.pdf,650,the_pile_books3,"12,787",13621.45908,8,https://huggingface.co/mosaicml/mpt-7b-storywriter,"MPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths.
It was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset.
At inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.
We demonstrate generations as long as 84k tokens on a single node of 8 A100-80GB GPUs in our blogpost.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; Apache 2.0; Note: This model requires that trust_remote_code=True be passed to the from_pretrained method. This is because we use a custom model architecture that is not yet part of the transformers package."
starcoderplus,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,146,bigcode/the-stack-dedup; tiiuae/falcon-refinedweb,"9,158",0,6,https://huggingface.co/bigcode/starcoderplus,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Please read the BigCode OpenRAIL-M license agreement before accepting it.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Play with the instruction-tuned StarCoderPlus at StarChat-Beta.; StarCoderPlus is a fine-tuned version of StarCoderBase on 600B tokens from the English web dataset RedefinedWeb 
combined with StarCoderData from The Stack (v1.2) and a Wikipedia dataset.
It's a 15.5B parameter Language Model trained on English and 80+ programming languages. The model uses Multi Query Attention,
a context window of 8192 tokens,  and was trained using the Fill-in-the-Middle objective on 1.6 trillion tokens. "
guanaco-33b-merged,Text Generation,PyTorch; Transformers,,,,162,,"6,360",66634.06027,17,https://huggingface.co/timdettmers/guanaco-33b-merged,No model card; New: Create and edit this model card directly on the website!
BracingEvoMix,Text-to-Image,Diffusers,Japanese,creativeml-openrail-m,,100,,0,6543.396113,,https://huggingface.co/sazyou-roukaku/BracingEvoMix,"License:CreativeML Open RAIL-M
Additional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 31, 2023; このモデルは『CreativeML Open RAIL-M』でLicenseそのものに涓はありません。
しかし追加著作者としてi城郎郭の名前が追加されています。
しかし追加著作者として佐城郎画の名前が追加されています。(6/10 Twitterネ`ム涓に伴い、表涓。License内はsazyou_roukakuの涓なし)
なお『CreativeML Open RAIL-M』にdされている通り、
本モデルを使用しての生成物にvしてはLicenseの使用制限Aの事例を除き、当方は一切v与致しません。
犯罪目的利用や医用画像など特定T的な用途での利用は使用制限Aで禁止されています。
必ず_Jしご利用ください。
また当方は一切任を持ちません。免されていることをご了承の上、ご使用ください。; マ`ジ利用モデル一E ; [BracingEvoMix]OpenBraβOpenBra?BanKai @PleaseBanKai ; dreamshaper_5Bakedvaedreamshaper_6BakedVae(https://civitai.com/models/4384) ?Lykonepicrealism_newAgeepicrealism_newEra(https://civitai.com/models/25694) ?epinikiondiamondCoalMix_diamondCoalv2(https://civitai.com/models/41415) ?EnthusiastAIsxd_v10(https://civitai.com/models/1169) ?izuekEvt_V4_e04_ema(https://huggingface.co/haor/Evt_V4-preview) ?haor  "
ChatLaw-Text2Vec,Sentence Similarity,PyTorch; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/2306.16092.pdf,53,,440,409.5447923,3,https://huggingface.co/chestnutlzj/ChatLaw-Text2Vec,本模型用于法律相关文本的相似度计算。可用于制作向量数据库等。; 本模型利用936727条全国案例库数据集训练，数据集样本如下：; 请问夫妻之间共同财产如何定义？; 请问民间借贷的利息有什么限制; 欢迎引用我们:
ChatLaw-13B,,,English; Chinese,gpl-3.0,https://arxiv.org/pdf/2306.16092.pdf,30,,0,251.5870103,2,https://huggingface.co/JessyTsu1/ChatLaw-13B,此版本为学术demo版，基于姜子牙Ziya-LLaMA-13B-v1训练而来(LLaMA权重的许可证限制，我们无法直接发布完整的模型权重，用户需自行合并); ChatLaw-13B，此版本为学术demo版，基于姜子牙Ziya-LLaMA-13B-v1训练而来，中文各项表现很好，但是逻辑复杂的法律问答效果不佳，需要用更大参数的模型来解决。; ChatLaw-33B，此版本为学术demo版，基于Anima-33B训练而来，逻辑推理能力大幅提升，但是因为Anima的中文语料过少，导致问答时常会出现英文数据。; ChatLaw-Text2Vec，使用93w条判决案例做成的数据集基于BERT训练了一个相似度匹配模型，可将用户提问信息和对应的法条相匹配，例如：; “请问如果借款没还怎么办。”
orca_mini_3b,Text Generation,PyTorch; Transformers,English,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2306.02707.pdf,99,psmathur/alpaca_orca; psmathur/dolly-v2_orca; psmathur/WizardLM_Orca,"6,345",14039.61185,5,https://huggingface.co/psmathur/orca_mini_3b,"Use orca-mini-3b on Free Google Colab with T4 GPU :); An OpenLLaMa-3B model model trained on explain tuned datasets, created using Instructions and Input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; We build explain tuned WizardLM dataset ~70K, Alpaca dataset ~52K  & Dolly-V2 dataset ~15K created using approaches from Orca Research Paper.; We leverage all of the 15 system instructions provided in Orca Research Paper. to generate custom datasets, in contrast to vanilla instruction tuning approaches used by original datasets.; This helps student model aka this model to learn thought process from teacher model, which is ChatGPT (gpt-3.5-turbo-0301 version)."
h2ogpt-gm-oasst1-en-2048-falcon-40b-v2,Text Generation,PyTorch; Transformers,English,apache-2.0,,13,OpenAssistant/oasst1,"15,715",85660.4322,6,https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2,"This model was trained using H2O LLM Studio.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers, accelerate and torch libraries installed.; You can print a sample prompt after the preprocessing step to see how it is feed to the tokenizer:; Alternatively, you can download h2oai_pipeline.py, store it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:; You may also construct the pipeline from the loaded model and tokenizer yourself and consider the preprocessing steps:"
Euclid-By_Consistent_Factor,Text-to-Image,,English,creativeml-openrail-m,,9,,0,58827.43399,,https://huggingface.co/ConsistentFactor/Euclid-By_Consistent_Factor,"One thing I'm happy to celebrate about my model as that it is a lot more racially friendly and will more often than not, produce people who are not Caucasian, in fact around 60%-70% of the time it will produce people of colour in my testing at least. If you find it's not producing Caucasian people, just add either white or Caucasian to the prompt.
It's also very good at aging people so adding an age can make a big difference.; The only thing V5 doesn't do well most of the time are eyes, if you don't get decent eyes try adding perfect eyes or round eyes to the prompt and increase the weight till you are happy. V6 and V6Ultra has fixed this problem.; Prompt: (subject), elegant, alluring, attractive, amazing photograph, masterpiece, best quality, 8K, high quality, photorealistic, realism, art photography, Nikon D850, 16k, sharp focus, masterpiece, breathtaking, atmospheric perspective, diffusion, pore correlation, skin imperfections, DSLR, 80mm Sigma f2, depth of field, intricate natural lighting,; Negative prompt: (unrealistic, render, 3d,cgi,cg,2.5d), (bad-hands-5:1.05), easynegative, [( NG_DeepNegative_V1_64T :0.9) :0.1], ng_deepnegative_v1_75t, worst quality, low quality, normal quality, child, (painting, drawing, sketch, cartoon, anime, render, 3d), blurry, deformed, disfigured, morbid, mutated, bad anatomy, bad art, (bad teeth, weird teeth, broken teeth), (worst quality, low quality, logo, text, watermark, username), incomplete, bad_prompt_version2; Recommend for V5: DPM++ 2M SDE Karras, Steps: 20-45, Hires fix 0.15 - 0.2, CFG 6 - 8."
orca_mini_v2_7b,Text Generation,PyTorch; Transformers,English,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2304.12244.pdf,30,psmathur/orca_minis_uncensored_dataset,766,27607.5667,1,https://huggingface.co/psmathur/orca_mini_v2_7b,"An Uncensored LLaMA-7b model in collaboration with Eric Hartford. trained on explain tuned datasets, created using Instructions and Input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; Please note this model has better code generation capabilities compare to our original orca_mini_7b which was trained on base OpenLLaMA-7b model and which has the empty spaces issues & found not good for code generation.; P.S. I am #opentowork, if you can help, please reach out to me at www.linkedin.com/in/pankajam; I evaluated orca_mini_v2_7b on a wide range of tasks using Language Model Evaluation Harness from EleutherAI. ; Here are the results on metrics used by HuggingFaceH4 Open LLM Leaderboard"
internlm-7b,Feature Extraction,PyTorch; Transformers,,,,50,,"5,512",15003.36422,,https://huggingface.co/internlm/internlm-7b,"; ?Reporting Issues; InternLM has open-sourced a 7 billion parameter base model tailored for practical scenarios. The model has the following characteristics:; We conducted a comprehensive evaluation of InternLM using the open-source evaluation tool OpenCompass. The evaluation covered five dimensions of capabilities: disciplinary competence, language competence, knowledge competence, inference competence, and comprehension competence. Here are some of the evaluation results, and you can visit the OpenCompass leaderboard for more evaluation results.; Limitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information."
internlm-chat-7b,Feature Extraction,PyTorch; Transformers,,,,43,,"8,872",15003.36568,3,https://huggingface.co/internlm/internlm-chat-7b,"; ?Reporting Issues; InternLM has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:; We conducted a comprehensive evaluation of InternLM using the open-source evaluation tool OpenCompass. The evaluation covered five dimensions of capabilities: disciplinary competence, language competence, knowledge competence, inference competence, and comprehension competence. Here are some of the evaluation results, and you can visit the OpenCompass leaderboard for more evaluation results.; Limitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information."
orca_mini_v2_13b,Text Generation,PyTorch; Transformers,English,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2304.12244.pdf,30,psmathur/orca_minis_uncensored_dataset,533,26624.59949,1,https://huggingface.co/psmathur/orca_mini_v2_13b,"An Uncensored LLaMA-13b model in collaboration with Eric Hartford. trained on explain tuned datasets, created using Instructions and Input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; Please note this model has better code generation capabilities compare to our original orca_mini_13b which was trained on base OpenLLaMA-13b model and which has the empty spaces issues & found not good for code generation.; P.S. I am #opentowork, if you can help, please reach out to me at www.linkedin.com/in/pankajam; I evaluated orca_mini_v2_13b on a wide range of tasks using Language Model Evaluation Harness from EleutherAI. ; Here are the results on metrics used by HuggingFaceH4 Open LLM Leaderboard"
controlnet-qr-pattern-v2,,Diffusers,,creativeml-openrail-m,,22,Nacholmo/controlnet-test-darkest-color; yuvalkirstain/pexel_images_lots_with_generated_captions,245,1484.803604,,https://huggingface.co/Nacholmo/controlnet-qr-pattern-v2,"Conditioning only 25% of the pixels closest to black and the 25% closest to white.; In the automatic1111 folder I added compatible weights, and various strength levels.; Many thanks to antfu for their ideas, tools and contributions
https://antfu.me; ; "
Pygmalion-7B-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,,7,,259,4098.38621,,https://huggingface.co/TheBloke/Pygmalion-7B-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for TehVenom's merge of Pygmalion 7B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
WizardLM-13B-V1-1-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,https://arxiv.org/pdf/2304.12244.pdf,41,,"1,648",7631.184214,,https://huggingface.co/TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for WizardLM's WizardLM 13B V1.1 merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
orca_mini_v2_13b-GGML,Text Generation,Transformers,English,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2304.12244.pdf,21,psmathur/orca_minis_uncensored_dataset,0,119316.4997,,https://huggingface.co/TheBloke/orca_mini_v2_13b-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Pankaj Mathur's Orca Mini v2 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
bk-sdm-small,Text-to-Image,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/2305.15798.pdf,6,,216,0.013230782,2,https://huggingface.co/nota-ai/bk-sdm-small,"Block-removed Knowledge-distilled Stable Diffusion Model (BK-SDM) is an architecturally compressed SDM for efficient general-purpose text-to-image synthesis. This model is bulit with (i) removing several residual and attention blocks from the U-Net of Stable Diffusion v1.4 and (ii) distillation pretraining on only 0.22M LAION pairs (fewer than 0.1% of the full training set). Despite being trained with very limited resources, our compact model can imitate the original SDM by benefiting from transferred knowledge.; An inference code with the default PNDM scheduler and 50 denoising steps is as follows.; The following code is also runnable, because we compressed the U-Net of Stable Diffusion v1.4 while keeping the other parts (i.e., Text Encoder and Image Decoder) unchanged:; The above examples have been tested on a single NVIDIA GeForce RTX 3090 GPU with the following versions:; We removed several residual and attention blocks from the 0.86B-parameter U-Net in the 1.04B-param SDM-v1.4, and our compressed models are summarized as follows."
NavelOrangeMix,,,,,,6,,0,0.001493912,,https://huggingface.co/dorioku/NavelOrangeMix,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
bk-sdm-tiny,Text-to-Image,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/2305.15798.pdf,6,,128,0.013230782,1,https://huggingface.co/nota-ai/bk-sdm-tiny,"Block-removed Knowledge-distilled Stable Diffusion Model (BK-SDM) is an architecturally compressed SDM for efficient general-purpose text-to-image synthesis. This model is bulit with (i) removing several residual and attention blocks from the U-Net of Stable Diffusion v1.4 and (ii) distillation pretraining on only 0.22M LAION pairs (fewer than 0.1% of the full training set). Despite being trained with very limited resources, our compact model can imitate the original SDM by benefiting from transferred knowledge.; An inference code with the default PNDM scheduler and 50 denoising steps is as follows.; The following code is also runnable, because we compressed the U-Net of Stable Diffusion v1.4 while keeping the other parts (i.e., Text Encoder and Image Decoder) unchanged:; The above examples have been tested on a single NVIDIA GeForce RTX 3090 GPU with the following versions:; We removed several residual and attention blocks from the 0.86B-parameter U-Net in the 1.04B-param SDM-v1.4, and our compressed models are summarized as follows."
petrichor-SDXL-Finetuned-Fp16,Text-to-Image,,English,other,,6,,0,0,,https://huggingface.co/thehive/petrichor-SDXL-Finetuned-Fp16,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		;   TO BE ADDED; 





; Unable to determine this model’s library. Check the
								docs 
.
							"
Ziya-Writing-LLaMa-13B-v1,Text Generation,PyTorch; Transformers,Chinese; English,gpl-3.0,https://arxiv.org/pdf/2210.08590.pdf,6,,11,53627.50102,,https://huggingface.co/IDEA-CCNL/Ziya-Writing-LLaMa-13B-v1,"姜子牙写作大模型V1是基于LLaMa的130亿参数的指令微调模型，在写作任务上进行了能力增强，是专注于写作的大模型。姜子牙写作模型可以完成公文报告、讲稿书信、创意文案等多类的写作任务。; Ziya-Writing-LLaMa-13B-v1 is a 13-billion parameter instruction fine-tuned model based on LLaMa, which has been enhanced for better performance in writing tasks. It is a large model that focuses on writing. Ziya-Writing-LLaMa-13B-v1 can handle several types of writing tasks, including official reports, speeches, creative copywriting, and more.; 更多细节可以参考我们的公众号文章：; 姜子牙大模型系列 | 写作模型ziya-writing开源！开箱即用，快来认领专属你的写作小助手吧; 我们从网络中收集并清洗了大量真实的真人写作数据，利用GPT-3.5生成对应的写作指令，并进行了极为严格的人工校验。"
RWKV-4-World-7B,,PyTorch; Transformers,,apache-2.0,,6,,1,15401.00853,,https://huggingface.co/StarRing2022/RWKV-4-World-7B,"RWKV-4-World的Hugface格式，因新版World的tokenizer较之前Raven\Pile版本有较大变化，因而需要进行新版HF适配
ringrwkv兼容了原生rwkv库和transformers的rwkv库，同时新添入World版本的配置及代码（支持1.5B，3B，7B全系列），并修复了原HF的RWKV在
Forward RWKVOutput时的细微问题，主要是引入和明确last_hidden_state。以下是轻量级使用代码，比较方便：; RingRWKV GIT开源地址：https://github.com/StarRing2022/RingRWKV ; import torch
from ringrwkv.configuration_rwkv_world import RwkvConfig
from ringrwkv.rwkv_tokenizer import TRIE_TOKENIZER
from ringrwkv.modehf_world import RwkvForCausalLM; model = RwkvForCausalLM.from_pretrained(""StarRing2022/RWKV-4-World-7B"") #或将本模型下载至本地文件夹 
tokenizer = TRIE_TOKENIZER('./ringrwkv/rwkv_vocab_v20230424.txt'); text = ""你叫什么名字？"""
Evol-Replit-v1-GGML,,,,cc-by-sa-4.0,,6,nickrosh/Evol-Instruct-Code-80k-v1,0,9820.168955,,https://huggingface.co/TheBloke/Evol-Replit-v1-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are Replit GGML format model files for Nick Roshdieh's Evol Replit v1.; Please note that these GGMLs are not compatible with llama.cpp, text-generation-webui or llama-cpp-python. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh."
Llama-2-13B-Chat-ggml,,,,,,6,,0,119316.5004,4,https://huggingface.co/localmodels/Llama-2-13B-Chat-ggml,"From: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf; Quantized using an older version of llama.cpp and compatible with llama.cpp from May 19, commit 2d5db48.; Quantization methods compatible with latest llama.cpp from June 6, commit 2d43387.; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-7b-chat-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,,6,,399,41402.72536,,https://huggingface.co/NousResearch/Llama-2-7b-chat-hf,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations."
Llama-2-70b-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,,6,,96,386747.4403,,https://huggingface.co/NousResearch/Llama-2-70b-hf,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations."
mlc-chat-Llama-2-7b-chat-hf-q4f16_1,,,,,,6,,0,1537.869878,,https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
llama-2-7b-hf-small-shards,Text Generation,PyTorch; Transformers,,,,6,,361,27597.59785,,https://huggingface.co/abhishek/llama-2-7b-hf-small-shards,No model card; New: Create and edit this model card directly on the website!
perfectLewdFantasy_v1.01,Text-to-Image,Diffusers,,other,,6,,350,2181.123047,3,https://huggingface.co/digiplay/perfectLewdFantasy_v1.01,Model info :; https://civitai.com/models/111848?modelVersionId=121050; Sample image I made thru Huggingface's API :; ; Original Author's DEMO image :
readflow-rwkv-4-world-ctx32k,,,,apache-2.0,,6,,0,15360.00293,,https://huggingface.co/xiaol/readflow-rwkv-4-world-ctx32k,"This is full finetuned model from RWKV 4 world 7B CHNTuned model using data from Readflow tech (readflow.com.cn) ,; finetuned for  32k context length, used to summary news article ; using inf-ctx training https://github.com/SynthiaDL/TrainChatGalRWKV with fixed VRAM; you can test summary prompt using RWKV runner(https://github.com/josStorer/RWKV-Runner) in chat mode , and check conversation files in examples folders.; https://discord.gg/pWH5MkvtNR"
llama-2-13B-German-Assistant-v2-GPTQ,Text Generation,Transformers,English; German,other,,6,flozi00/conversations,66,7434.754203,,https://huggingface.co/TheBloke/llama-2-13B-German-Assistant-v2-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for flozi00's Llama 2 13B German Assistant v2.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!"
ruGPT-3.5-13B-GPTQ,Text Generation,Transformers,,,,6,,333,7785.404364,,https://huggingface.co/fffrrt/ruGPT-3.5-13B-GPTQ,"GPTQ quantisation of https://huggingface.co/ai-forever/ruGPT-3.5-13B; Small perplexity test:
  before quantization - 'mean_perplexity': 10.241
  after quantization - 'mean_perplexity': 10.379; Data - RussianSuperGlue > DaNetQA/train.jsonl['passage']; As this is a hastily thrown together quant with no prior experience in quants, use https://huggingface.co/TheBloke version if he releases a quant for this model."
llama2-qlora-finetunined-french,,PEFT,,,,6,,6,134.0023379,,https://huggingface.co/1littlecoder/llama2-qlora-finetunined-french,"The following bitsandbytes quantization config was used during training:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
llama-2-70b-Guanaco-QLoRA-fp16,Text Classification,PyTorch; Transformers,English,other,,6,,6,141285.4531,,https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are pytorch format fp16 model files for Mikael110's Llama2 70b Guanaco QLoRA.; It is the result of merging and/or converting the source repository to float16.; For further support, and discussions on these models and AI in general, join us at:"
llama2-7b-chat-codeCherryPop-qLoRA-GPTQ,Text Generation,Transformers,,other,,6,,0,3995.955517,,https://huggingface.co/TheBloke/llama2-7b-chat-codeCherryPop-qLoRA-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GPTQ model files for TokenBender's Llama-2-7B-Chat Code Cherry Pop.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements."
Llama-2-70B-GGML,Text Generation,Transformers; PyTorch,English,other,https://arxiv.org/pdf/2307.09288.pdf,6,,0,576921.6323,,https://huggingface.co/TheBloke/Llama-2-70B-GGML,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for Meta's Llama 2 70B.; To use these files you need:; Example command:
mT5_multilingual_XLSum,Summarization,PyTorch; Transformers,43 languages,,,145,csebuetnlp/xlsum,"20,110",2390.239006,69,https://huggingface.co/csebuetnlp/mT5_multilingual_XLSum,"This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. For finetuning details and scripts,
see the paper and the official repository. ; Scores on the XL-Sum test sets are as follows:; If you use this model, please cite the following paper:"
wav2vec2-base-960h,Automatic Speech Recognition,PyTorch; TensorFlow; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/2006.11477.pdf,139,librispeech_asr,"610,783",1134.007458,175,https://huggingface.co/facebook/wav2vec2-base-960h,"Facebook's Wav2Vec2; The base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. When using the model
make sure that your speech input is also sampled at 16Khz.; Paper; Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli; Abstract"
tapas-base-finetuned-wtq,Table Question Answering,PyTorch; TensorFlow; Transformers,English,apache-2.0,https://arxiv.org/pdf/2004.02349.pdf; https://arxiv.org/pdf/2010.00571.pdf; https://arxiv.org/pdf/1508.00305.pdf,128,wikitablequestions,"26,962",886.2654451,93,https://huggingface.co/google/tapas-base-finetuned-wtq,"This model has 2 versions which can be used. The default version corresponds to the tapas_wtq_wikisql_sqa_inter_masklm_base_reset checkpoint of the original Github repository.
This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).; The other (non-default) version which can be used is: ; Disclaimer: The team releasing TAPAS did not write a model card for this model so this model card has been written by
the Hugging Face team and contributors.; TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. 
This means it was pretrained on the raw tables and associated texts only, with no humans labelling them in any way (which is why it
can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language used in tables and associated texts, which can then be used 
to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed
or refuted by the contents of a table. Fine-tuning is done by adding a cell selection head and aggregation head on top of the pre-trained model, and then jointly train these randomly initialized classification heads with the base model on SQa, WikiSQL and finally WTQ. "
multi-qa-mpnet-base-dot-v1,Sentence Similarity,PyTorch; Sentence Transformers,,,,101,flax-sentence-embeddings/stackexchange_xml; ms_marco; gooaq; yahoo_answers_topics; search_qa; eli5; natural_questions; trivia_qa; embedding-data/QQP; embedding-data/PAQ_pairs; embedding-data/Amazon-QA; embedding-data/WikiAnswers,"739,674",438.7307655,44,https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources. For an introduction to semantic search, have a look at: SBERT.net - Semantic Search; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.; In the following some technical details how this model must be used:"
paraphrase-multilingual-mpnet-base-v2,Sentence Similarity,PyTorch; TensorFlow; Sentence Transformers; Transformers,,apache-2.0,https://arxiv.org/pdf/1908.10084.pdf,113,,"166,986",2287.436026,16,https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net"
spkrec-ecapa-voxceleb,,speechbrain; PyTorch,English,apache-2.0,https://arxiv.org/pdf/2106.04624.pdf,78,voxceleb,"504,058",89.10602314,57,https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb,"This repository provides all the necessary tools to perform speaker verification with a pretrained ECAPA-TDNN model using SpeechBrain. 
The system can be used to extract speaker embeddings as well. 
It is trained on Voxceleb 1+ Voxceleb2 training data. ; For a better experience, we encourage you to learn more about
SpeechBrain. The model performance on Voxceleb1-test set(Cleaned) is:; This system is composed of an ECAPA-TDNN model. It is a combination of convolutional and residual blocks. The embeddings are extracted using attentive statistical pooling. The system is trained with Additive Margin Softmax Loss.  Speaker Verification is performed using cosine distance between speaker embeddings.; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain."
Geneformer,Fill-Mask,PyTorch; Transformers,,apache-2.0,,67,ctheodoris/Genecorpus-30M,"2,515",41.21306431,1,https://huggingface.co/ctheodoris/Geneformer,"Geneformer is a foundation transformer model pretrained on a large-scale corpus of ~30 million single cell transcriptomes to enable context-aware predictions in settings with limited data in network biology. ; See our manuscript for details.; Geneformer is a foundation transformer model pretrained on Genecorpus-30M, a pretraining corpus comprised of ~30 million single cell transcriptomes from a broad range of human tissues. We excluded cells with high mutational burdens (e.g. malignant cells and immortalized cell lines) that could lead to substantial network rewiring without companion genome sequencing to facilitate interpretation. Each single cell’s transcriptome is presented to the model as a rank value encoding where genes are ranked by their expression in that cell normalized by their expression across the entire Genecorpus-30M. The rank value encoding provides a nonparametric representation of that cell’s transcriptome and takes advantage of the many observations of each gene’s expression across Genecorpus-30M to prioritize genes that distinguish cell state. Specifically, this method will deprioritize ubiquitously highly-expressed housekeeping genes by normalizing them to a lower rank. Conversely, genes such as transcription factors that may be lowly expressed when they are expressed but highly distinguish cell state will move to a higher rank within the encoding. Furthermore, this rank-based approach may be more robust against technical artifacts that may systematically bias the absolute transcript counts value while the overall relative ranking of genes within each cell remains more stable. ; The rank value encoding of each single cell’s transcriptome then proceeds through six transformer encoder units. Pretraining was accomplished using a masked learning objective where 15% of the genes within each transcriptome were masked and the model was trained to predict which gene should be within each masked position in that specific cell state using the context of the remaining unmasked genes. A major strength of this approach is that it is entirely self-supervised and can be accomplished on completely unlabeled data, which allows the inclusion of large amounts of training data without being restricted to samples with accompanying labels.; We detail applications and results in our manuscript. "
yolos-small-finetuned-license-plate-detection,Object Detection,PyTorch; Transformers,English,,,13,,"22,398",123.0061556,2,https://huggingface.co/nickmuchi/yolos-small-finetuned-license-plate-detection,"This model is a fine-tuned version of hustvl/yolos-small on the licesne-plate-recognition dataset from Roboflow which contains 5200 images in the training set and 380 in the validation set.
The original YOLOS model was fine-tuned on COCO 2017 object detection (118k annotated images).; YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).; You can use the raw model for object detection. See the model hub to look for all available YOLOS models.; Here is how to use this model:; Currently, both the feature extractor and model support PyTorch."
MagicPrompt-Stable-Diffusion,Text Generation,PyTorch; Core ML; Safetensors; Transformers,,mit,,496,,"36,451",1023.342164,124,https://huggingface.co/Gustavosta/MagicPrompt-Stable-Diffusion,"This is a model from the MagicPrompt series of models, which are GPT-2 models intended to generate prompt texts for imaging AIs, in this case: Stable Diffusion.; This model was trained with 150,000 steps and a set of about 80,000 data filtered and extracted from the image finder for Stable Diffusion: ""Lexica.art"". It was a little difficult to extract the data, since the search engine still doesn't have a public API without being protected by cloudflare, but if you want to take a look at the original dataset, you can have a look here: datasets/Gustavosta/Stable-Diffusion-Prompts.; If you want to test the model with a demo, you can go to: ""spaces/Gustavosta/MagicPrompt-Stable-Diffusion"".; MIT; When using this model, please credit: Gustavosta"
whisper-large,Automatic Speech Recognition,PyTorch; TensorFlow; JAX; Transformers,99 languages,apache-2.0,https://arxiv.org/pdf/2212.04356.pdf,310,,"14,916",18958.22622,31,https://huggingface.co/openai/whisper-large,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al from OpenAI. The original code repository can be found here.; Update: following the release of the paper, the Whisper authors announced a  large-v2 model trained for 2.5x more epochs with regularization. This  large-v2 model surpasses the performance of the large model, with no architecture changes. Thus, it is recommended that the  large-v2 model is used in-place of the original large model. ; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. "
biogpt,Text Generation,PyTorch; Transformers,English,mit,,152,,"35,766",1599.032755,31,https://huggingface.co/microsoft/biogpt,"Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.; You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:; Here is how to use this model to get the features of a given text in PyTorch:; Beam-search decoding:; If you find BioGPT useful in your research, please cite the following paper:"
segformer_b2_clothes,Image Segmentation,PyTorch; Safetensors; Transformers,,mit,https://arxiv.org/pdf/2105.15203.pdf,46,mattmdjaga/human_parsing_dataset,"112,093",438.3098489,16,https://huggingface.co/mattmdjaga/segformer_b2_clothes,"SegFormer model fine-tuned on ATR dataset for clothes segmentation.
The dataset on hugging face is called ""mattmdjaga/human_parsing_dataset"".; Labels: 0: ""Background"", 1: ""Hat"", 2: ""Hair"", 3: ""Sunglasses"", 4: ""Upper-clothes"", 5: ""Skirt"", 6: ""Pants"", 7: ""Dress"", 8: ""Belt"", 9: ""Left-shoe"", 10: ""Right-shoe"", 11: ""Face"", 12: ""Left-leg"", 13: ""Right-leg"", 14: ""Left-arm"", 15: ""Right-arm"", 16: ""Bag"", 17: ""Scarf""; The license for this model can be found here."
git-large-coco,Image-to-Text,PyTorch; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/2205.14100.pdf,36,,"122,834",3236.769388,33,https://huggingface.co/microsoft/git-large-coco,"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.; Disclaimer: The team releasing GIT did not write a model card for this model so this model card has been written by the Hugging Face team.; GIT is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using ""teacher forcing"" on a lot of (image, text) pairs.; The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens.; The model has full access to (i.e. a bidirectional attention mask is used for) the image patch tokens, but only has access to the previous text tokens (i.e. a causal attention mask is used for the text tokens) when predicting the next text token."
Deliberate,,Diffusers,,,,356,,"25,129",10915.84273,2,https://huggingface.co/XpucT/Deliberate,"This model provides you the ability to create anything you want.
The more power of prompt knowledges you have, the better results you'll get.
It basically means that you'll never get a perfect result with just a few words.
You have to fill out your prompt line extremely detailed.
; Dive into the perfect creations world with my prompts.
Your research will be appreciated, so feel free to show everyone, what you can get with this model; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
GuoFeng3,Text-to-Image,Diffusers,English,cc-by-nc-sa-4.0,,429,,"3,400",37421.85152,45,https://huggingface.co/xiaolxl/GuoFeng3,"欢迎使用GuoFeng3模型 - (TIP:这个版本的名字进行了微调),这是一个中国华丽古风风格模型，也可以说是一个古风游戏角色模型，具有2.5D的质感。第三代大幅度减少上手难度，增加了场景元素与男性古风人物，除此之外为了模型能更好的适应其它TAG，还增加了其它风格的元素。这一代对脸和手的崩坏有一定的修复，同时素材大小也提高到了最长边1024。; 根据个人的实验与收到的反馈，国风模型系列的第二代，在人物，与大头照的效果表现比三代更好，如果你有这方面需求不妨试试第二代。; 2.0版本：https://huggingface.co/xiaolxl/Gf_style2; GuoFeng3:原始模型; GuoFeng3.1:对GuoFeng3人像进行了微调修复"
BioGPT-Large-PubMedQA,Text Generation,PyTorch; Transformers,English,mit,,60,pubmed_qa,"1,525",6442.759135,16,https://huggingface.co/microsoft/BioGPT-Large-PubMedQA,"Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.; If you find BioGPT useful in your research, please cite the following paper:"
upscaler,,,,,,130,,0,0.001445313,,https://huggingface.co/uwg/upscaler,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
so-vits-svc-4.0,,,English,,,120,,0,0.001583595,,https://huggingface.co/therealvul/so-vits-svc-4.0,"This is a collection of so-vits-svc-4.0 models made by the Pony Preservation Project using audio clips taken from MLP:FiM.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
flan-ul2,Text2Text Generation,PyTorch; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/2205.05131.pdf,481,svakulenk0/qrecc; taskmaster2; djaym7/wiki_dialog; deepmind/code_contests; lambada; gsm8k; aqua_rat; esnli; quasc; qed; c4,"69,506",40400.09108,98,https://huggingface.co/google/flan-ul2,"; Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the UL2 model  released earlier last year. It was fine tuned using the ""Flan"" prompt tuning 
and dataset collection.; According to the original blog here are the notable improvements:; You can use the convert_t5x_checkpoint_to_pytorch.py script and pass the argument strict = False. The final layer norm is missing from the original dictionnary, that is why we are passing the strict = False argument.; We used the same config file as google/ul2."
controlnet-sd21,,Diffusers,English,openrail++,,327,laion/laion-art,0,57112.17655,,https://huggingface.co/thibaud/controlnet-sd21,"Want to support my work: you can bought my Artbook: https://thibaud.art ; Here's the first version of controlnet for stablediffusion 2.1
Trained on a subset of laion/laion-art; ; ; "
Annotators,,,,other,,112,,0,6928.471467,63,https://huggingface.co/lllyasviel/Annotators,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
TemporalNet,,Diffusers,,openrail,,243,,839,18143.40991,,https://huggingface.co/CiaraRowles/TemporalNet,"Introducing the Beta Version of TemporalNet; TemporalNet is a ControlNet model designed to enhance the temporal consistency of generated outputs, as demonstrated in this example: https://twitter.com/CiaraRowles1/status/1637486561917906944. While it does not eliminate all flickering, it significantly reduces it, particularly at higher denoise levels. For optimal results, it is recommended to use TemporalNet in combination with other methods.; Instructions for Use:; Add the model ""diff_control_sd15_temporalnet_fp16.safetensors"" to your models folder in the ControlNet extension in Automatic1111's Web UI.; Create a folder that contains:"
chatgpt-gpt4-prompts-bart-large-cnn-samsum,Text2Text Generation,TensorFlow; Transformers,,mit,,45,fka/awesome-chatgpt-prompts,"3,362",1672.461497,5,https://huggingface.co/Kaludi/chatgpt-gpt4-prompts-bart-large-cnn-samsum,"This model generates ChatGPT/BingChat & GPT-3 prompts and is a fine-tuned version of philschmid/bart-large-cnn-samsum on an this dataset.
It achieves the following results on the evaluation set:; This model supports a Streamlit Web UI to run the chatgpt-gpt4-prompts-bart-large-cnn-samsum model:
; The following hyperparameters were used during training:"
lametta,Text-to-Image,Diffusers; Safetensors,Japanese,creativeml-openrail-m,,61,,0,21811.22742,,https://huggingface.co/Lasorco/lametta,人的な好みの}柄を追求したマ`ジモデルで普段遣いのためにマ`ジしたものになります。かなり癖がいモデルじゃないかと思います。; モデル全体のA向として等身の低い女の子の描写に主眼をおいていますが、胸の大きなおさんもちゃんと出てくるようです。それ以外の出力もできるとは思いますがしていないので未知数です。; 人のこだわりでできるだけ目のハイライトを失わないようにマ`ジしてあります。指の描写にも荬蚴工盲郡膜猡辘扦工プロンプトなど次第でgに破`します。; Clip skipは１や２のお好みで。どちらにもそれぞれの魅力がある荬します。; VAEは何かしら外部のものを使用してください、サンプルはすべてAnythingのVAEを使用しています。人的に普段はclearVAEシリ`ズを使っていますオススメ。
ControlNetMediaPipeFace,Image-to-Image,Diffusers,English,openrail,https://arxiv.org/pdf/2302.05543.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2210.08402.pdf,431,LAION-Face; LAION,"4,071",29033.36585,7,https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace,This dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.; Cherry-picked from ControlNet + Stable Diffusion v2.1 Base; Images with multiple faces are also supported:; Source images were generated by pulling slice 00000 from LAION Face and passing them through MediaPipe's face detector with special configuration parameters.  ; The colors and line thicknesses used for MediaPipe are as follows:
rwkv-4-raven,Text Generation,PyTorch,English,apache-2.0,,472,the_pile,0,74618.88304,10,https://huggingface.co/BlinkDL/rwkv-4-raven,"[UPDATE: Try RWKV-4-World (https://huggingface.co/BlinkDL/rwkv-4-world) for generation & chat & code in 100+ world languages, with great English zero-shot & in-context learning ability too.]; These are RWKV-4-Pile 1.5/3/7/14B models finetuned on Alpaca, CodeAlpaca, Guanaco, GPT4All, ShareGPT and more. Even the 1.5B model is surprisingly good for its size.; Gradio Demo: https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B and https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio; RWKV models inference: https://github.com/BlinkDL/ChatRWKV (fast CUDA).; Q8_0 models: only for https://github.com/saharNooby/rwkv.cpp (fast CPU)."
vicuna-7b,Text Generation,PyTorch; Transformers,,other,,104,,"1,789",13478.5185,20,https://huggingface.co/AlekseyKorshuk/vicuna-7b,"This repository contains an alternative version of the Vicuna 7B model.; This model was natively fine-tuned using ShareGPT data, but without the ""ethics"" filtering used for the original Vicuna.; A GPTQ quantised  4-bit version is available here.; Model type:
Vicuna is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
Vicuna was trained between March 2023 and April 2023."
HimawariMixs,Text-to-Image,Diffusers,Japanese; English,creativeml-openrail-m,,62,,0,5795.850381,1,https://huggingface.co/natsusakiyomi/HimawariMixs,"
          々なモデルをマ`ジした背景や部の表F力がいVAE内i型モデル
          VAEの内はないぞ！と言わせないぞ！！！！
          Models with built-in VAE with strong expression of backgrounds and details merging various models
        ; Twiter: @min__san"
MiniGPT-4,,,,,,271,,0,47.40822266,43,https://huggingface.co/Vision-CAIR/MiniGPT-4,"Deyao Zhu* (On Job Market!), Jun Chen* (On Job Market!), Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. *Equal Contribution; King Abdullah University of Science and Technology; Click the image to chat with MiniGPT-4 around your images
; More examples can be found in the project page.; "
WizardLM-7B-Uncensored,Text Generation,PyTorch; Transformers,,other,,308,ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered,"1,633",13826.33209,20,https://huggingface.co/ehartford/WizardLM-7B-Uncensored,"This is WizardLM trained with a subset of the dataset - responses that contained alignment / moralizing were removed.  The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:  ; An uncensored model has no guardrails.  ; You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car."
Wizard-Vicuna-13B-Uncensored,Text Generation,PyTorch; Transformers,English,other,,192,ehartford/wizard_vicuna_70k_unfiltered,"2,595",53322.09611,22,https://huggingface.co/ehartford/Wizard-Vicuna-13B-Uncensored,"This is wizard-vicuna-13b trained with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:  ; An uncensored model has no guardrails.  ; You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car."
Wizard-Vicuna-7B-Uncensored-GPTQ,Text Generation,Transformers,English,other,,37,ehartford/wizard_vicuna_70k_unfiltered,"2,339",4630.846355,,https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Wizard-Vicuna-7B-Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
e5-small-v2,Feature Extraction,PyTorch; TensorFlow; ONNX; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/2212.03533.pdf; https://arxiv.org/pdf/2104.08663.pdf; https://arxiv.org/pdf/2210.07316.pdf,19,,"702,926",534.989669,4,https://huggingface.co/intfloat/e5-small-v2,"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 12 layers and the embedding size is 384.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; Please refer to our paper at https://arxiv.org/pdf/2212.03533.pdf.; Check out unilm/e5 to reproduce evaluation results 
on the BEIR and MTEB benchmark."
guanaco-65b,,,,,https://arxiv.org/pdf/2305.14314.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2304.07327.pdf,66,,0,6553.622397,1,https://huggingface.co/timdettmers/guanaco-65b,"| Paper | Code | Demo | ; The Guanaco models are open-source finetuned chatbots obtained through 4-bit QLoRA tuning of LLaMA base models on the OASST1 dataset. They are available in 7B, 13B, 33B, and 65B parameter sizes.; ??Guanaco is a model purely intended for research purposes and could produce problematic outputs.; Guanaco adapter weights are available under Apache 2 license. Note the use of the Guanaco adapter weights, requires access to the LLaMA model weighs. 
Guanaco is based on LLaMA and therefore should be used according to the LLaMA license. ; Here is an example of how you would load Guanaco 7B in 4-bits:"
guanaco-65B-GPTQ,Text Generation,Transformers,,other,https://arxiv.org/pdf/2305.14314.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2304.07327.pdf,235,,"3,254",34306.3505,5,https://huggingface.co/TheBloke/guanaco-65B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Tim Dettmers' Guanaco 65B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
m3e-small,,PyTorch; Sentence Transformers,Chinese,,,15,,"2,139",96.25759785,3,https://huggingface.co/moka-ai/m3e-small,m3e-small | m3e-base; M3E 是 Moka Massive Mixed Embedding 的缩写; 说明：; Tips:; 您需要先安装 sentence-transformers
kandinsky-2-2-decoder,Text-to-Image,Diffusers,,apache-2.0,,16,,"8,958",0.009418106,3,https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder,"Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.2 is available in diffusers!; "
kandinsky-2-2-prior,Text-to-Image,Diffusers,,apache-2.0,,10,,"16,294",0.01608326,2,https://huggingface.co/kandinsky-community/kandinsky-2-2-prior,"Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.2 is available in diffusers!; "
myLyCORIS,,,,,,57,,0,0.000488281,,https://huggingface.co/Saya3091/myLyCORIS,"仅供学习，请勿用于任何商业活动，不授权给任何商业行为。如果有没有使用说明的模型，请尝试在civitai搜索@Saya，可以找到过去的模型说明
For learning only, please do not use for any commercial activities, not authorized to any commercial activities. If there is a model without instructions, try searching @Saya in civiti, you can find past model instructions; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
WizardCoder-15B-1.0-GPTQ,Text Generation,Transformers,,bigcode-openrail-m,,125,,"7,177",9424.070554,,https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for WizardLM's WizardCoder 15B 1.0.; It is the result of quantising to 4bit using AutoGPTQ.; Please make sure you're using the latest version of text-generation-webui
baichuan-vicuna-chinese-7b,Text Generation,PyTorch; Transformers,Chinese; English,,,50,anon8231489123/ShareGPT_Vicuna_unfiltered; QingyiSi/Alpaca-CoT; mhhmm/leetcode-solutions-python,"1,138",14338.14311,,https://huggingface.co/fireballoon/baichuan-vicuna-chinese-7b,baichuan-vicuna-chinese-7b是在中英双语sharegpt数据上全参数微调的对话模型。; baichuan-vicuna-chinese-7b is a chat model supervised finetuned on vicuna sharegpt data in both English and Chinese.; [NEW] 4bit-128g GPTQ量化版本：baichuan-vicuna-chinese-7b-gptq; Inference with FastChat:; Inference with Transformers:
mpt-30b,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2010.04245.pdf; https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2302.06675.pdf,292,allenai/c4; mc4; togethercomputer/RedPajama-Data-1T; bigcode/the-stack-dedup; allenai/s2orc,"73,345",61363.15051,1,https://huggingface.co/mosaicml/mpt-30b,"MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.
This model was trained by MosaicML.; MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.; MPT-30B comes with special features that differentiate it from other LLMs, including an 8k token context window (which can be further extended via finetuning; see MPT-7B-StoryWriter), support for context-length extrapolation via ALiBi, and efficient inference + training via FlashAttention. It also has strong coding abilities thanks to its pretraining mix. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer.
The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPU―either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision.; This model uses the MosaicML LLM codebase, which can be found in the llm-foundry repository. It was trained by MosaicML’s NLP team on the MosaicML platform for LLM pretraining, finetuning, and inference.; MPT-30B is:"
superhot-13b-8k-no-rlhf-test,,,,mit,,50,,0,17.60733002,1,https://huggingface.co/kaiokendev/superhot-13b-8k-no-rlhf-test,"This is a second prototype of SuperHOT, a NSFW focused LoRA, this time with 4K context and no RLHF. In my testing, it can go all the way to 6K without breaking down and I made the change with intention to reach 8K, so I'll assume it will go to 8K although I only trained on 4K sequences.; You will NEED to apply the monkeypatch or, if you are already using the monkeypatch, change the scaling factor to 0.25 and the maximum sequence length to 8192; In order to use the 8K context, you will need to apply the monkeypatch I have added in this repo or follow the instructions for oobabooga's text-generation-webui -- without it, it will not work.; I will repeat: Without the patch with the correct scaling and max sequence length, it will not work!; The patch is very simple, and you can make the changes yourself:"
mpt-30b-instruct,Text Generation,PyTorch; Transformers,,cc-by-sa-3.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf,87,competition_math; conceptofmind/cot_submix_original/cot_gsm8k; knkarthick/dialogsum; mosaicml/dolly_hhrlhf; duorc; tau/scrolls/qasper; emozilla/quality; scrolls/summ_screen_fd; spider,"64,762",61363.14796,8,https://huggingface.co/mosaicml/mpt-30b-instruct,"MPT-30B-Instruct is a model for short-form instruction following.
It is built by finetuning MPT-30B on Dolly HHRLHF derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets. It is also trained on Competition Math, Duorc, CoT GSM8k, Qasper, Quality, Summ Screen FD and Spider.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; June 22, 2023; CC-By-SA-3.0; Bespokenizer46"
DragGan-Models,,,,,https://arxiv.org/pdf/2305.10973.pdf,27,,0,4107.702042,,https://huggingface.co/DragGan/DragGan-Models,"More about Models here https://github.com/XingangPan/DragGAN; https://arxiv.org/abs/2305.10973; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
opencoderplus,Text Generation,PyTorch; Transformers,English,,,97,,"1,494",32411.02602,1,https://huggingface.co/openchat/opencoderplus,"OpenChat is a series of open-source language models fine-tuned on a diverse and high-quality dataset of multi-round conversations. With only ~6K GPT-4 conversations filtered from the ~90K ShareGPT conversations, OpenChat is designed to achieve high performance with limited data.; Generic models:; Code models:; Note: Please load the pretrained models using bfloat16; We provide the full source code, including an inference server compatible with the ""ChatCompletions"" API, in the OpenChat GitHub repository."
replit-code-instruct-glaive,Text Generation,PyTorch; Transformers,,,,74,,"4,570",10639.32938,7,https://huggingface.co/sahil2801/replit-code-instruct-glaive,No model card; New: Create and edit this model card directly on the website!
internlm-chat-7b-8k,Feature Extraction,PyTorch; Transformers,,,,33,,"2,133",15003.36559,5,https://huggingface.co/internlm/internlm-chat-7b-8k,"; ?Reporting Issues; InternLM has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:; We conducted a comprehensive evaluation of InternLM using the open-source evaluation tool OpenCompass. The evaluation covered five dimensions of capabilities: disciplinary competence, language competence, knowledge competence, inference competence, and comprehension competence. Here are some of the evaluation results, and you can visit the OpenCompass leaderboard for more evaluation results.; Limitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information."
Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-GGML,,,,other,,12,,0,36423.69301,,https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard Vicuna 7B Uncensored.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:"
Pygmalion-7B-SuperHOT-8K-GGML,Text Generation,,English,other,,6,,0,36423.69467,,https://huggingface.co/TheBloke/Pygmalion-7B-SuperHOT-8K-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These are GGML model files for TehVenom's merge of Pygmalion 7B merged with Kaio Ken's SuperHOT 8K.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:"
WizardCoder-Guanaco-15B-V1.1-GPTQ,Text Generation,Transformers,English,apache-2.0,,6,guanaco,354,9424.071543,,https://huggingface.co/TheBloke/WizardCoder-Guanaco-15B-V1.1-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for LoupGarou's WizardCoder-Guanaco-15B-V1.1.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
animatediff,,,,apache-2.0,,5,,0,3420.161585,3,https://huggingface.co/guoyww/animatediff,"This model repo is for AnimateDiff.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
codet5p-110m-embedding,,PyTorch; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2305.07922.pdf,5,,551,441.1722047,,https://huggingface.co/Salesforce/codet5p-110m-embedding,"CodeT5+ is a new family of open code large language models
with an encoder-decoder architecture that can flexibly operate in different modes (i.e. encoder-only, decoder-only,
and encoder-decoder) to support a wide range of code understanding and generation tasks.
It is introduced in the paper:; CodeT5+: Open Code Large Language Models for Code Understanding and Generation
by Yue Wang*, Hung Le*, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi (*
indicates equal contribution).; Compared to the original CodeT5 family (base: 220M, large: 770M), CodeT5+ is pretrained with a diverse set of
pretraining tasks including span denoising, causal language modeling, contrastive learning, and text-code
matching to learn rich representations from both unimodal code data and bimodal code-text data.
Additionally, it employs a simple yet effective compute-efficient pretraining method to initialize the model
components with frozen off-the-shelf LLMs such as CodeGen to efficiently scale
up the model (i.e. 2B, 6B, 16B), and adopts a ""shallow encoder and deep decoder"" architecture.
Furthermore, it is instruction-tuned to align with natural language instructions (see our InstructCodeT5+ 16B)
following Code Alpaca.; This checkpoint consists of an encoder of CodeT5+ 220M model (pretrained from 2 stages on both unimodal and bimodal) and a projection layer, which can be used to extract code
embeddings of 256 dimension. It can be easily loaded using the AutoModel functionality and employs the
same CodeT5 tokenizer.; This checkpoint is trained on the stricter permissive subset of the deduplicated version of
the github-code dataset.
The data is preprocessed by reserving only permissively licensed code (""mit"" “apache-2”, “bsd-3-clause”, “bsd-2-clause”,
“cc0-1.0”, “unlicense”, “isc”).
Supported languages (9 in total) are as follows:
c, c++, c-sharp,  go, java, javascript,  php, python, ruby."
replit-openorca-GGML,,Transformers,,,,5,Open-Orca/OpenOrca,0,9820.167392,,https://huggingface.co/TheBloke/replit-openorca-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are Replit GGML format model files for Matorus's Replit OpenOrca.; Please note that these GGMLs are not compatible with llama.cpp, text-generation-webui or llama-cpp-python. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh."
ToraFurryMix,Text-to-Image,,,creativeml-openrail-m,,5,,0,2181.126953,,https://huggingface.co/tlano/ToraFurryMix,"このモデルは「furry」タグを使用することを念^に{整したモデルです。
「furry」タグなしでの出力はあまり试^していません。; ; ; 作者
?twitter: @TlanoAI; Unable to determine this model’s library. Check the
								docs 
.
							"
Tora-NijiFurry-LoRA,Text-to-Image,,,creativeml-openrail-m,,5,,0,37.90213383,,https://huggingface.co/tlano/Tora-NijiFurry-LoRA,"チビケモ化LoRAです。
学デ`タはNijiJourneyの出力画像のみです。

抗はm用するモデルの「furry」タグ特性に依存します。

未m用状Bで「furry」タグの抗がほとんどない龊稀
あまり良いY果は得られないかもしれません。; Training Model:
?sdhk_v40.safetensors (https://civitai.com/models/82813/sdhk)
Trigger Words:
?furry; 作者
?twitter: @TlanoAI; Unable to determine this model’s library. Check the
								docs 
.
							"
Llama-2-13B-GPTQ,Text Generation,Transformers,,,,5,,67,7436.588155,,https://huggingface.co/localmodels/Llama-2-13B-GPTQ,"From: https://huggingface.co/meta-llama/Llama-2-13b-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta"
Llama-2-13b-chat-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,,5,,292,53311.26833,,https://huggingface.co/NousResearch/Llama-2-13b-chat-hf,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations."
llama-2-7b-guanaco,Text Generation,PyTorch; Transformers,,apache-2.0,,5,timdettmers/openassistant-guanaco,536,13805.39004,,https://huggingface.co/mlabonne/llama-2-7b-guanaco,Model fine-tuned in 4-bit precision using QLoRA on timdettmers/openassistant-guanaco with weights merged after training.; Made using this Google Colab notebook.; It can be easily imported using the AutoModelForCausalLM class from transformers:
llama-2-13b-guanaco-fp16,Text Classification,PyTorch; Transformers,English,,,5,,78,26657.08521,,https://huggingface.co/Mikael110/llama-2-13b-guanaco-fp16,This is a Llama-2 version of Guanaco. It was finetuned from the base Llama-13b model using the official training scripts found in the QLoRA repo. I wanted it to be as faithful as possible and therefore changed nothing in the training script beyond the model it was pointing to. The model prompt is therefore also the same as the original Guanaco model.; This repo contains the merged f16 model. The QLoRA adaptor can be found here.; A 7b version of the model can be found here.; Legal Disclaimer: This model is bound by the usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.
waifujourney-xl,,,English,other,,5,,0,0,,https://huggingface.co/gmonsoon/waifujourney-xl,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Please for anyone, don't reupload my finetuned models to other site, like Civitai or image generating site like Seeart, Yodayo or other sites. Thank you for understanding this. ; Like my works and want to collaboration or funding my projects? contact me.  ; Finetuned WDXL-aesthetic-0.9 with images generated from Nijijourney.  "
LECO,,,,other,,5,,0,163.0916303,,https://huggingface.co/SenY/LECO,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
MythoBoros-13b,Text Generation,PyTorch; Transformers,English,other,,5,,15,26634.76569,,https://huggingface.co/Gryphe/MythoBoros-13b,"MythoBoros-13b can be considered a sister model to MythoLogic-13b, sharing the same goals but having a different approach.; Whereas the previous model was a series of experimental gradient merges, this one is a simple straight-up 66/34 merge of Chronos and the freshly released Ouroboros, providing a very solid foundation for a well-performing roleplaying model.; MythoBoros tends to be somewhat more formal with its responses in comparison to MythoLogic.; My advice? Try both, see which one you prefer.; Quantized models are available from TheBloke: GGML - GPTQ (You're the best!)"
leco_puffy_nipples,,,,,,5,,0,13.60244377,,https://huggingface.co/doll774/leco_puffy_nipples,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Chinese-Llama-2-7b-4bit,Text Generation,PyTorch; Transformers,Chinese; English,openrail,,5,LinkSoul/instruction_merge_set,3,13804.04194,1,https://huggingface.co/LinkSoul/Chinese-Llama-2-7b-4bit,"全部开源，完全可商用的中文版 Llama2 模型及中英文 SFT 数据集，输入格式严格遵循 llama-2-chat 格式，兼容适配所有针对原版 llama-2-chat 模型的优化。; ; ; Talk is cheap, Show you the Demo.; 模型下载：Chinese Llama2 Chat Model"
distilbert-base-uncased-finetuned-sst-2-english,Text Classification,PyTorch; TensorFlow; Rust; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/1910.01108.pdf,263,sst2; glue,"3,297,551",1072.31753,188,https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english,"Model Description: This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2.
This model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).; Example of single-label classification:
??; This model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.; The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.; Based on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations."
gpt2-large,Text Generation,PyTorch; TensorFlow; JAX; Rust; ONNX; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/1910.09700.pdf,141,,"446,660",16591.65899,82,https://huggingface.co/gpt2-large,"Model Description: GPT-2 Large is the 774M parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective. ; Use the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:; Here is how to use this model to get the features of a given text in PyTorch:; and in TensorFlow:; In their model card about GPT-2, OpenAI wrote: "
openai-gpt,Text Generation,PyTorch; TensorFlow; Rust; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/1705.11168.pdf; https://arxiv.org/pdf/1803.02324.pdf; https://arxiv.org/pdf/1910.09700.pdf,161,,"237,593",2029.529097,22,https://huggingface.co/openai-gpt,"Model Description: openai-gpt is a transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long range dependencies.; Use the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:; Here is how to use this model in PyTorch:; and in TensorFlow:; This model can be used for language modeling tasks."
t5-small,Translation,PyTorch; TensorFlow; JAX; Rust; ONNX; Safetensors; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/1805.12471.pdf; https://arxiv.org/pdf/1708.00055.pdf; https://arxiv.org/pdf/1704.05426.pdf; https://arxiv.org/pdf/1606.05250.pdf; https://arxiv.org/pdf/1808.09121.pdf; https://arxiv.org/pdf/1810.12885.pdf; https://arxiv.org/pdf/1905.10044.pdf; https://arxiv.org/pdf/1910.09700.pdf,129,c4,"1,772,137",1212.175809,205,https://huggingface.co/t5-small,"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-Small is the checkpoint with 60 million parameters. ; The developers write in a blog post that the model: "
personaGPT,Conversational,PyTorch; Transformers,,gpl-3.0,https://arxiv.org/pdf/1801.07243.pdf,76,,"8,791",1477.253565,7,https://huggingface.co/af1tang/personaGPT,"PersonaGPT is an open-domain conversational agent designed to do 2 tasks:; It builds on the DialoGPT-medium pretrained model based on the GPT-2 architecture. 
This model is trained on the Persona-Chat dataset, with added special tokens to better distinguish between conversational history and personality traits for dyadic conversations. Furthermore, some active learning was used to train the model to do controlled decoding using turn-level goals.; Preprocessing, training and implementation details can be found in the personaGPT repo.; Example of personalized decoding:; Example of controlled response generation: "
wav2vec2-lg-xlsr-en-speech-emotion-recognition,Audio Classification,PyTorch; TensorBoard; Transformers,,apache-2.0,,70,,"46,007",1300.488507,14,https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition,"The model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task.; The dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are:; It achieves the following results on the evaluation set:; More information needed; More information needed"
chinese-roberta-wwm-ext,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/1906.08101.pdf; https://arxiv.org/pdf/2004.13922.pdf,163,,"42,057",1230.373295,5,https://huggingface.co/hfl/chinese-roberta-wwm-ext,"For further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking. ; Pre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu; This repository is developed based on：https://github.com/google-research/bert; You may also interested in,; More resources by HFL: https://github.com/ymcui/HFL-Anthology"
trocr-base-handwritten,Image-to-Text,PyTorch; Transformers,,,https://arxiv.org/pdf/2109.10282.pdf,63,,"368,370",1363.253596,93,https://huggingface.co/microsoft/trocr-base-handwritten,"TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you."
bert-base-multilingual-uncased-sentiment,Text Classification,PyTorch; TensorFlow; JAX; Transformers,6 languages,mit,,150,,"4,041,279",2008.854893,37,https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment,"This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).; This model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above, or for further finetuning on related sentiment analysis tasks.; Here is the number of product reviews we used for finetuning the model: ; The finetuned model obtained the following accuracy on 5,000 held-out product reviews in each of the languages:; In addition to this model, NLP Town offers custom, monolingual sentiment models for many languages and an improved multilingual model through RapidAPI. "
xlm-roberta-base-language-detection,Text Classification,PyTorch; TensorFlow; Transformers,21 languages,mit,https://arxiv.org/pdf/1911.02116.pdf,114,,"101,427",2287.439037,9,https://huggingface.co/papluca/xlm-roberta-base-language-detection,"This model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset.; This model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output). 
For additional information please refer to the xlm-roberta-base model card or to the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al.; You can directly use this model as a language detector, i.e. for sequence classification tasks. Currently, it supports the following 20 languages: ; arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh); The model was fine-tuned on the Language Identification dataset, which consists of text sequences in 20 languages. The training set contains 70k samples, while the validation and test sets 10k each. The average accuracy on the test set is 99.6% (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table."
sn-xlm-roberta-base-snli-mnli-anli-xnli,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,13 languages,,,51,SNLI; MNLI; ANLI; XNLI,"2,510",1150.817188,4,https://huggingface.co/symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli,"A Siamese network model trained for zero-shot and few-shot text classification.; The base model is xlm-roberta-base.
It was trained on SNLI, MNLI, ANLI and XNLI.; This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:"
gpt-neox-20b,Text Generation,PyTorch; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/2204.06745.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2201.07311.pdf; https://arxiv.org/pdf/2104.09864.pdf,410,EleutherAI/pile,"69,177",41280.45929,89,https://huggingface.co/EleutherAI/gpt-neox-20b,"GPT-NeoX-20B is a 20 billion parameter autoregressive language model trained 
on the Pile using the GPT-NeoX 
library. Its architecture intentionally 
resembles that of GPT-3, and is almost identical to that of GPT-J-
6B. Its training dataset contains 
a multitude of English-language texts, reflecting the general-purpose nature 
of this model. See the accompanying paper 
for details about model architecture (including how it differs from GPT-3), 
training procedure, and additional evaluations.; GPT-NeoX-20B was developed primarily for research purposes. It learns an inner 
representation of the English language that can be used to extract features 
useful for downstream tasks.; In addition to scientific uses, you may also further fine-tune and adapt 
GPT-NeoX-20B for deployment, as long as your use is in accordance with the 
Apache 2.0 license. This model works with the Transformers 
Library. If you decide to use 
pre-trained GPT-NeoX-20B as a basis for your fine-tuned model, please note that 
you need to conduct your own risk and bias assessment. ; GPT-NeoX-20B is not intended for deployment as-is. It is not a product 
and cannot be used for human-facing interactions without supervision.; GPT-NeoX-20B has not been fine-tuned for downstream tasks for which language 
models are commonly deployed, such as writing genre prose, or commercial 
chatbots. This means GPT-NeoX-20B will likely not respond to a given prompt 
the way products such as ChatGPT do. This is because, unlike GPT-NeoX-20B, 
ChatGPT was fine-tuned using methods such as Reinforcement Learning from Human 
Feedback (RLHF) to better “understand” human instructions and dialogue."
bloom-560m,Text Generation,PyTorch; JAX; Safetensors; Transformers,48 languages,bigscience-bloom-rail-1.0,https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2110.02861.pdf; https://arxiv.org/pdf/2108.12409.pdf,228,,"1,721,715",3455.17736,45,https://huggingface.co/bigscience/bloom-560m,Version 1.0 / 26.May.2022; This section provides information for anyone who wants to know about the model.; Developed by: BigScience (website); Model Type: Transformer-based Language Model; Version: 1.0.0
biomedical-ner-all,Token Classification,PyTorch; Safetensors; Transformers,English,apache-2.0,,81,,"70,586",532.9304456,87,https://huggingface.co/d4data/biomedical-ner-all,"An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased; Checkout the tutorial video for explanation of this model and corresponding python library: https://youtu.be/xpiDPdBpS18; The easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.; This model is part of the Research topic ""AI in Biomedical field"" conducted by Deepak John Reji, Shaina Raza. If you use this work (code, model or dataset), please star at:; https://github.com/dreji18/Bio-Epidemiology-NER"
stable-diffusion,Text-to-Image,,,creativeml-openrail-m,https://arxiv.org/pdf/2207.12598.pdf,877,,0,0.141329498,71,https://huggingface.co/CompVis/stable-diffusion,"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
This model card gives an overview of all available model checkpoints. For more in-detail model cards, please have a look at the model repositories listed under Model Access.; For the first version 4 model checkpoints are released.
Higher versions have been trained for longer and are thus usually better in terms of image generation quality then lower versions. More specifically: ; Each checkpoint can be used both with Hugging Face's  ? Diffusers library or the original Stable Diffusion GitHub repository. Note that you have to ""click-request"" them on each respective model repository.; To quickly try out the model, you can try out the Stable Diffusion Space.; The CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based."
mDeBERTa-v3-base-xnli-multilingual-nli-2mil7,Zero-Shot Classification,PyTorch; Safetensors; Transformers,27 languages,mit,https://arxiv.org/pdf/2111.09543.pdf; https://arxiv.org/pdf/2104.07179.pdf; https://arxiv.org/pdf/1809.05053.pdf; https://arxiv.org/pdf/1911.02116.pdf,74,MoritzLaurer/multilingual-NLI-26lang-2mil7; xnli; multi_nli; anli; fever; lingnli; alisawuffles/WANLI,"37,682",1136.628826,5,https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7,"This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying mDeBERTa-v3-base model was pre-trained by Microsoft on the CC100 multilingual dataset with 100 languages. The model was then fine-tuned on the XNLI dataset and on the multilingual-NLI-26lang-2mil7 dataset. Both datasets contain more than 2.7 million hypothesis-premise pairs in 27 languages spoken by more than 4 billion people. ; As of December 2021, mDeBERTa-v3-base is the best performing multilingual base-sized transformer model introduced by Microsoft in this paper. ; This model was trained on the multilingual-nli-26lang-2mil7 dataset and the XNLI validation dataset. ; The multilingual-nli-26lang-2mil7 dataset contains 2 730 000 NLI hypothesis-premise pairs in 26 languages spoken by more than 4 billion people. The dataset contains 105 000 text pairs per language. It is based on the English datasets MultiNLI, Fever-NLI, ANLI, LingNLI and WANLI and was created using the latest open-source machine translation models. The languages in the dataset are: ['ar', 'bn', 'de', 'es', 'fa', 'fr', 'he', 'hi', 'id', 'it', 'ja', 'ko', 'mr', 'nl', 'pl', 'ps', 'pt', 'ru', 'sv', 'sw', 'ta', 'tr', 'uk', 'ur', 'vi', 'zh'] (see ISO language codes. For more details, see the datasheet. In addition, a sample of 105 000 text pairs was also added for English following the same sampling method as the other languages, leading to 27 languages. ; Moreover, for each language a random set of 10% of the hypothesis-premise pairs was added where an English hypothesis was paired with the premise in the other language (and the same for English premises and other language hypotheses). This mix of languages in the text pairs should enable users to formulate a hypothesis in English for a target text in another language. "
pegasus-x-large-book-summary,Summarization,PyTorch; Safetensors; Transformers,,bsd-3-clause,,25,kmfoda/booksum,"1,735",4657.531973,6,https://huggingface.co/pszemraj/pegasus-x-large-book-summary,"Get SparkNotes-esque summaries of arbitrary text! Due to the model size, it's recommended to try it out in Colab (linked above) as the API textbox may time out.; This model is a fine-tuned version of google/pegasus-x-large on the kmfoda/booksum dataset for approx eight epochs.; More information needed; TODO; The following hyperparameters were used during training:"
waifu-diffusion-v1-4,Text-to-Image,,English,creativeml-openrail-m,,975,,0,10567.68584,11,https://huggingface.co/hakurei/waifu-diffusion-v1-4,"; masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck; Waifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
The CreativeML OpenRAIL License specifies: ; This model can be used for entertainment purposes and as a generative art assistant."
sd-vae-ft-mse,,Diffusers,,mit,,189,,"1,814,228",670.0086271,23,https://huggingface.co/stabilityai/sd-vae-ft-mse,"These weights are intended to be used with the ? diffusers library. If you are looking for the model to use with the original CompVis Stable Diffusion codebase, come here.; You can integrate this fine-tuned VAE decoder to your existing diffusers workflows, by including a vae argument to the StableDiffusionPipeline; We publish two kl-f8 autoencoder versions, finetuned from the original kl-f8 autoencoder on a 1:1 ratio of LAION-Aesthetics and LAION-Humans, an unreleased subset containing only SFW images of humans. The intent was to fine-tune on the Stable Diffusion training set (the autoencoder was originally trained on OpenImages) but also enrich the dataset with images of humans to improve the reconstruction of faces.
The first, ft-EMA, was resumed from the original checkpoint, trained for 313198 steps and uses EMA weights. It uses the same loss configuration as the original checkpoint (L1 + LPIPS).
The second, ft-MSE, was resumed from ft-EMA and uses EMA weights and was trained for another 280k steps using a different loss, with more emphasis 
on MSE reconstruction (MSE + 0.1 * LPIPS). It produces somewhat ``smoother'' outputs. The batch size for both versions was 192 (16 A100s, batch size 12 per GPU).
To keep compatibility with existing models, only the decoder part was finetuned; the checkpoints can be used as a drop-in replacement for the existing autoencoder.; Original kl-f8 VAE vs f8-ft-EMA vs f8-ft-MSE; Visualization of reconstructions on  256x256 images from the COCO2017 validation dataset. "
stable-diffusion-2,Text-to-Image,Diffusers,,openrail++,https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/1910.09700.pdf,1.58k,,"100,620",10670.18254,478,https://huggingface.co/stabilityai/stable-diffusion-2,"This model card focuses on the model associated with the Stable Diffusion v2 model, available here.; This stable-diffusion-2 model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on 768x768 images.; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model"
stable-diffusion-2-inpainting,,Diffusers,,openrail++,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,330,,"124,059",10675.05481,60,https://huggingface.co/stabilityai/stable-diffusion-2-inpainting,"This model card focuses on the model associated with the Stable Diffusion v2, available here.; This stable-diffusion-2-inpainting model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model"
mdeberta-v3-base-squad2,Question Answering,PyTorch; Safetensors; Transformers,94 languages,mit,https://arxiv.org/pdf/2006.03654.pdf; https://arxiv.org/pdf/2111.09543.pdf,60,squad_v2,"29,820",2289.585695,7,https://huggingface.co/timpal0l/mdeberta-v3-base-squad2,"It has been finetuned for 3 epochs on SQuAD2.0.; DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. ; In DeBERTa V3, we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our paper.; Please check the official repository for more implementation details and updates.; mDeBERTa is multilingual version of DeBERTa which use the same structure as DeBERTa and was trained with CC100 multilingual data.
The mDeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has 86M backbone parameters  with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer.  This model was trained using the 2.5T CC100 data as XLM-R."
santacoder,Text Generation,PyTorch; Transformers,code,bigcode-openrail-m,https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2301.03988.pdf,280,bigcode/the-stack,"16,301",4712.515738,39,https://huggingface.co/bigcode/santacoder,"; Play with the model on the SantaCoder Space Demo.; The SantaCoder models are a series of 1.1B parameter models trained on the Python, Java, and JavaScript subset of The Stack (v1.1) (which excluded opt-out requests). 
The main model uses Multi Query Attention, a context window of 2048 tokens, and was trained using near-deduplication and comment-to-code ratio as filtering criteria and using the Fill-in-the-Middle objective.
In addition there are several models that were trained on datasets with different filter parameters and with architecture and objective variations. ; The final model is the best performing model and was trained twice as long (236B tokens) as the others. This checkpoint is the default model and available on the main branch. All other checkpoints are on separate branches with according names.; The model was trained on GitHub code. As such it is not an instruction model and commands like ""Write a function that computes the square root."" do not work well.
You should phrase commands like they occur in source code such as comments (e.g. # the following function computes the sqrt) or write a function signature and docstring and let the model complete the function body."
Randeng-Deltalm-362M-En-Zh,Translation,PyTorch; Transformers,Chinese; English,,https://arxiv.org/pdf/2106.13736.pdf; https://arxiv.org/pdf/2209.02970.pdf,15,,"1,977",731.1599414,,https://huggingface.co/IDEA-CCNL/Randeng-Deltalm-362M-En-Zh,"使用封神框架基于 Detalm base 进行finetune ，搜集的中英数据集（共3千万条）以及 iwslt的中英平行数据（20万），得到 英-> 中方向的翻译模型; Using the Fengshen-LM framework and finetuning based on detalm , get a translation model in the English -> Chinese direction; 参考论文：DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders; 如果您在您的工作中使用了我们的模型，可以引用我们的论文：; If you are using the resource for your work, please cite the our paper:"
pygmalion-1.3b,Conversational,PyTorch; TensorBoard; Safetensors; Transformers,English,agpl-3.0,,47,,"9,639",6002.755548,4,https://huggingface.co/PygmalionAI/pygmalion-1.3b,"Pymalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped.; Warning: This model is NOT suitable for use by minors. It will output X-rated content under certain circumstances.; The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations.; Fine-tuning was done using ColossalAI (specifically, with a slightly modified version of their OPT fine-tune example) for around 11.4 million tokens over 5440 steps on a single 24GB GPU. The run took just under 21 hours.; We provide a notebook with a Gradio UI for playing around with the model without having to manually format inputs. This notebook can be found here."
Anything-V4.5,,,,creativeml-openrail-m,,31,,0,8744.961814,,https://huggingface.co/Airic/Anything-V4.5,"Original Model: https://huggingface.co/andite/anything-v4.0; Simply merged Anything-V4.5 with Anything-V4.0 VAE. 
Both checkpoint and safetensors versions are avaliable to download; All credit goes to the original uploader: andite
; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
StreetCLIP,Zero-Shot Image Classification,PyTorch; Transformers,English,cc-by-nc-4.0,https://arxiv.org/pdf/2302.00275.pdf,12,,"2,114",1756.076489,2,https://huggingface.co/geolocal/StreetCLIP,"StreetCLIP is a robust foundation model for open-domain image geolocalization and other
geographic and climate-related tasks.; Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves
state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, 
outperforming supervised models trained on millions of images.; StreetCLIP is a model pretrained by deriving image captions synthetically from image class labels using
a domain-specific caption template. This allows StreetCLIP to transfer its generalized zero-shot learning
capabilities to a specific domain (i.e. the domain of image geolocalization). 
StreetCLIP builds on the OpenAI's pretrained large version of CLIP ViT, using 14x14 pixel
patches and images with a 336 pixel side length.; StreetCLIP has a deep understanding of the visual features found in street-level urban and rural scenes
and knows how to relate these concepts to specific countries, regions, and cities. Given its training setup,
the following use cases are recommended for StreetCLIP.; StreetCLIP can be used out-of-the box using zero-shot learning to infer the geolocation of images on a country, region,
or city level. Given that StreetCLIP was pretrained on a dataset of street-level urban and rural images,
the best performance can be expected on images from a similar distribution."
bad-hands-5,,,,,,234,,0,0.008359375,2,https://huggingface.co/yesyeahvh/bad-hands-5,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
LoRA,,,,other,,121,,0,1767.427421,,https://huggingface.co/SenY/LoRA,"Silder: skinny
; Slider: short legs
; Concepts: Breathing fire
; This LyCORIS is blockweighted.
breathing_fire_full.safetensors is full version.
The full version has the same effect as the general version by applying the following settings in Lora Block Weight.; Backgrounds: ""Danchi"" is Japanese public housing complex."
website_classification,Text Classification,PyTorch; TensorBoard; Transformers,,apache-2.0,,8,,"3,898,187",268.9317835,,https://huggingface.co/alimazhar-110/website_classification,"This model is a fine-tuned version of distilbert-base-uncased on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
specter2,Feature Extraction,PyTorch; Transformers,English,apache-2.0,,24,allenai/scirepeval,"151,163",440.9334676,1,https://huggingface.co/allenai/specter2,"SPECTER 2.0 is the successor to SPECTER and is capable of generating task specific embeddings for scientific tasks when paired with adapters.
Given the combination of title and abstract of a scientific paper or a short texual query, the model can be used to generate effective embeddings to be used in downstream applications.; Note:For general embedding purposes, please use allenai/specter2_proximity.; To get the best performance on a downstream task type please load the associated adapter with the base model as in the example below.; SPECTER 2.0 has been trained on over 6M triplets of scientific paper citations, which are available here.
Post that it is trained with additionally attached task format specific adapter modules on all the SciRepEval training tasks.; Task Formats trained on:"
NeverEnding-Dream,Text-to-Image,Diffusers,English,other,,149,,"1,116",27584.50745,36,https://huggingface.co/Lykon/NeverEnding-Dream,"Read more about this model here: https://civitai.com/models/10028/neverending-dream-ned
Also please support by giving 5 stars and a heart, which will notify new updates.; Also consider supporting me on Patreon or ByuMeACoffee; You can run this model on:; Some sample output:; 




"
sd-webui-models,,,,,,200,,0,22400.01401,,https://huggingface.co/samle/sd-webui-models,"civitai部分模型搬运; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
llama-13b-hf,Text Generation,PyTorch; Transformers,,other,,334,,"28,546",40015.53978,40,https://huggingface.co/decapoda-research/llama-13b-hf,"LLaMA-13B converted to work with Transformers/HuggingFace. This is under a special license, please see the LICENSE file for details.; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model."
SakuraMix,Text-to-Image,Diffusers,Japanese,creativeml-openrail-m,,64,,0,26234.88869,,https://huggingface.co/natsusakiyomi/SakuraMix,"
          背景とキャラクタ`クオリティ`をI立させたVAE内i型モデル
          Model with built-in VAE for both background and character quality
        ; Twiter: @min__san
mail: (natsusakiyomi@mail.ru)"
Realistic_Vision_V2.0,,Diffusers,,creativeml-openrail-m,,266,,"424,778",25354.24395,9,https://huggingface.co/SG161222/Realistic_Vision_V2.0,"Please read this!
For version 2.0 it is recommended to use with VAE (to improve generation quality and get rid of blue artifacts): https://huggingface.co/stabilityai/sd-vae-ft-mse-original; This model is available on Mage.Space, Sinkin.ai, GetImg.ai and (RandomSeed.co - NSFW content); I use this template to get good generation results:; Prompt:
RAW photo, subject, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3; Example: RAW photo, a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3"
neavalAI,,,,,,8,,0,0,,https://huggingface.co/Zhonggua/neavalAI,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
oasst-sft-4-pythia-12b-epoch-3.5,Text Generation,PyTorch; Transformers,English,apache-2.0,,320,,"33,267",24383.61463,61,https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,"This is the 4th iteration English supervised-fine-tuning (SFT) model of 
the Open-Assistant project. 
It is based on a Pythia 12B that was fine-tuned on human demonstrations 
of assistant conversations collected through the 
https://open-assistant.io/ human feedback web 
app before March 25, 2023. ; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; command: deepspeed trainer_sft.py --configs defaults reference-data reference-pythia-12b --cache_dir /home/ubuntu/data_cache --output_dir .saved/oasst-sft-3-pythia-12b-reference_2kpre --num_train_epochs 8 --residual_dropout 0.2 --deepspeed --use_flash_attention true --model_name andreaskoepf/pythia-12b-pre-2000"
ControlNet-v1-1_fp16_safetensors,,,,,,196,,0,10845.00169,,https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors,"Safetensors/FP16 versions of the new ControlNet-v1-1 checkpoints.; Best used with ComfyUI but should work fine with all other UIs that support controlnets.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
SuperCOT-LoRA,,,,mit,,92,kaiokendev/SuperCOT-dataset; neulab/conala; yahma/alpaca-cleaned; QingyiSi/Alpaca-CoT,0,0.006728516,,https://huggingface.co/kaiokendev/SuperCOT-LoRA,"SuperCOT is a LoRA trained with the aim of making LLaMa follow prompts for Langchain better, by infusing chain-of-thought datasets, code explanations and instructions, snippets, logical deductions and Alpaca GPT-4 prompts.
It uses a mixture of the following datasets:; https://huggingface.co/datasets/QingyiSi/Alpaca-CoT; https://huggingface.co/datasets/neulab/conala; https://huggingface.co/datasets/yahma/alpaca-cleaned; (Thanks to all the awesome anons with supercomputers)"
llama-7b-hf-transformers-4.29,Text Generation,PyTorch; Transformers,,other,,42,,"7,134",13805.88574,3,https://huggingface.co/elinas/llama-7b-hf-transformers-4.29,"Original weights converted with the latest transformers version using the LlamaTokenizerFast implementation. ; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model."
stable-vicuna-13b-delta,Text Generation,PyTorch; Transformers,English,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2302.13971.pdf,440,OpenAssistant/oasst1; nomic-ai/gpt4all_prompt_generations; tatsu-lab/alpaca,929,26657.09509,28,https://huggingface.co/CarperAI/stable-vicuna-13b-delta,"StableVicuna-13B is a Vicuna-13B v0 model fine-tuned using reinforcement learning from human feedback (RLHF) via Proximal Policy Optimization (PPO) on various conversational and instructional datasets.; StableVicuna-13B cannot be used from the CarperAI/stable-vicuna-13b-delta weights alone. To obtain the correct model, one must add back the difference between LLaMA 13B and CarperAI/stable-vicuna-13b-delta weights. We provide the apply_delta.py script to automate the conversion, which you can run as:; Once the delta weights are applied, get started chatting with the model by using the transformers library. Following a suggestion from Vicuna Team with Vicuna v0 you should install transformers with this version:; StableVicuna-13B is fine-tuned on a mix of three datasets. OpenAssistant Conversations Dataset (OASST1), a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages;
GPT4All Prompt Generations, a dataset of 400k prompts and responses generated by GPT-4; and Alpaca,  a dataset of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine.; The reward model used during RLHF was also trained on OpenAssistant Conversations Dataset (OASST1) along with two other datasets: Anthropic HH-RLHF, a dataset of preferences about AI assistant helpfulness and harmlessness; and Stanford Human Preferences Dataset a dataset of 385K collective human preferences over responses to questions/instructions in 18 different subject areas, from cooking to legal advice."
codegen2-16B,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2305.02309.pdf,33,,"1,052",65836.55743,2,https://huggingface.co/Salesforce/codegen2-16B,"CodeGen2 is a family of autoregressive language models for program synthesis, introduced in the paper:; CodeGen2: Lessons for Training LLMs on Programming and Natural Languages by Erik Nijkamp*, Hiroaki Hayashi*, Caiming Xiong, Silvio Savarese, Yingbo Zhou.; Unlike the original CodeGen model family (i.e., CodeGen1), CodeGen2 is capable of infilling, and supports more programming languages.; Four model sizes are released: 1B, 3.7B, 7B, 16B.; This model can be easily loaded using the AutoModelForCausalLM functionality."
controlnet-segment-anything,Image-to-Image,Diffusers,English,creativeml-openrail-m,,16,mfidabel/sam-coyo-2k; mfidabel/sam-coyo-2.5k; mfidabel/sam-coyo-3k,"1,689",2974.924678,2,https://huggingface.co/mfidabel/controlnet-segment-anything,"These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. ; prompt: contemporary living room of a house; negative prompt: low quality
; prompt: new york buildings,  Vincent Van Gogh starry night ; negative prompt: low quality, monochrome
"
mpt-7b-instruct,Text Generation,PyTorch; Transformers,,cc-by-sa-3.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2010.04245.pdf,420,mosaicml/dolly_hhrlhf,"3,246,062",13621.45865,16,https://huggingface.co/mosaicml/mpt-7b-instruct,"MPT-7B-Instruct is a model for short-form instruction following.
It is built by finetuning MPT-7B on a dataset derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; CC-By-SA-3.0; Longboi24:"
WizardLM-7B-uncensored-GPTQ,Text Generation,Transformers,,other,,133,ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered,"20,047",3985.696858,,https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eric Hartford's 'uncensored' version of WizardLM.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Eric did a fresh 7B training using the WizardLM method, on a dataset edited to remove all the ""I'm sorry.."" type ChatGPT responses."
starchat-alpha,Text Generation,PyTorch; TensorBoard; Safetensors; Transformers,English,bigcode-openrail-m,,215,OpenAssistant/oasst1; databricks/databricks-dolly-15k,"4,397",64197.90118,28,https://huggingface.co/HuggingFaceH4/starchat-alpha,"Note, you may be interested in the Beta version of StarChat here.; StarChat is a series of language models that are fine-tuned from StarCoder to act as helpful coding assistants. StarChat Alpha is the first of these models, and as an alpha release is only intended for educational or research purpopses. In particular, the model has not been aligned to human preferences with techniques like RLHF, so may generate problematic content (especially when prompted to do so).; StarChat Alpha is intended for educational and/or research purposes and in that respect can be used to probe the programming capabilities of open-source language models.; StarChat Alpha has not been aligned to human preferences with techniques like RLHF or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). 
Models trained primarily on code data will also have a more skewed demographic bias commensurate with the demographics of the GitHub community, for more on this see the StarCoder dataset which is derived from The Stack.; Since the base model was pretrained on a large corpus of code, it may produce code snippets that are syntactically valid but semantically incorrect. 
For example, it may produce code that does not compile or that produces incorrect results.It may also produce code that is vulnerable to security exploits.We have observed the model also has a tendency to produce false URLs which should be carefully inspected before clicking."
BLOOMChat-176B-v1,Text Generation,PyTorch; Transformers,,other,,341,,"1,293",368654.6077,1,https://huggingface.co/sambanovasystems/BLOOMChat-176B-v1,"BLOOMChat is a 176 billion parameter multilingual chat model. It is instruction tuned from BLOOM (176B) on assistant-style conversation datasets and supports conversation, question answering and generative answers in multiple languages.; To increase accessibility and to support the open-source community, SambaNova is releasing BLOOMChat under a modified version of the Apache 2.0 license, which includes use-based restrictions from BLOOM’s RAIL license. While use-based restrictions are necessarily passed through, there are no blanket restrictions on reuse, distribution, commercialization or adaptation. Please review SambaNova’s BLOOMChat-176B License; This model is intended for commercial and research use.; BLOOMChat should NOT be used for:; This model is still in early development and can be prone to mistakes and hallucinations, there is still room for improvement. This model is intended to provide the community with a multilingual chat LLM baseline."
lyraChatGLM,,,English,mit,,87,,0,0.035227852,,https://huggingface.co/TMElyralab/lyraChatGLM,"We?know?what?you?want,?and?here?you go!; Note?that?the?code?was?fully?updated?too,?you?need?to?use?the new?API,?see?Uses?below; If you like our work and consider to join us, feel free to drop a line to benbinwu@tencent.com.; P.S. Recently we have received a lot of inquiries on accelerating customized models. Actually, we do not have plan to release the convertion tool at this moment, nor do we think it would be possible to apply your customized models based on our current release.; lyraChatGLM?is?currently?the?fastest?ChatGLM-6B?available.?To?the?best?of?our?knowledge,?it?is?the?first?accelerated?version?of?ChatGLM-6B."
open-calm-7b,Text Generation,PyTorch; Transformers,Japanese,cc-by-sa-4.0,,167,wikipedia; cc100; mc4,"20,706",14216.39709,2,https://huggingface.co/cyberagent/open-calm-7b,"OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.; Ryosuke Ishigami; Inference API has been turned off for this model."
visualglm-6b,,PyTorch; Transformers,Chinese; English,,https://arxiv.org/pdf/2103.10360.pdf; https://arxiv.org/pdf/2210.02414.pdf; https://arxiv.org/pdf/2301.12597.pdf; https://arxiv.org/pdf/2105.13290.pdf,160,,"7,533",18230.13309,1,https://huggingface.co/THUDM/visualglm-6b,"
   ? Github Repo ? ? Twitter ? ? [GLM@ACL 22] [GitHub] ? ? [GLM-130B@ICLR 23] [GitHub] 
; 
    ? Join our Slack and WeChat
; VisualGLM-6B 是一个开源的，支持图像、中文和英文的多模态对话语言模型，语言模型基于 ChatGLM-6B，具有 62 亿参数；图像部分通过训练 BLIP2-Qformer 构建起视觉模型与语言模型的桥梁，整体模型共78亿参数。; VisualGLM-6B 依靠来自于 CogView 数据集的30M高质量中文图文对，与300M经过筛选的英文图文对进行预训练，中英文权重相同。该训练方式较好地将视觉信息对齐到ChatGLM的语义空间；之后的微调阶段，模型在长视觉问答数据上训练，以生成符合人类偏好的答案。; 可以通过如下代码调用 VisualGLM-6B 模型来生成对话："
rvc-genshin-impact,Audio-to-Audio,,Japanese,mit,,78,,0,2.283659172,,https://huggingface.co/ArkanDash/rvc-genshin-impact,"; Learn more about Retrieval based Voice Conversion in this link below:
RVC WebUI; Download the prezipped model and put to your RVC Project.; Model test: Google Colab / RVC Models New (Which is basically the same but hosted on spaces); Model Created by ArkanDash 
The voice that was used in this model belongs to Hoyoverse."
LLaMa-13B-GGML,,Transformers,,other,,16,,78,118896.6517,,https://huggingface.co/TheBloke/LLaMa-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's LLaMA 13b.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh."
Video-LLaMA-Series,Visual Question Answering,,English; Chinese,bsd-3-clause,https://arxiv.org/pdf/2306.02858.pdf,23,,0,2710.004795,1,https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-Series,"This is the Hugging Face repo for storing pre-trained & fine-tuned checkpoints of our Video-LLaMA, which is a multi-modal conversational large language model with video understanding capability. ; For launching the pre-trained Video-LLaMA on your own machine, please refer to our github repo.; Unable to determine this model’s library. Check the
								docs 
.
							"
instructblip-vicuna-7b,Image-to-Text,PyTorch; Transformers,English,other,https://arxiv.org/pdf/2305.06500.pdf,17,,"5,290",32412.04387,5,https://huggingface.co/Salesforce/instructblip-vicuna-7b,InstructBLIP model using Vicuna-7b as language model. InstructBLIP was introduced in the paper InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning by Dai et al.; Disclaimer: The team releasing InstructBLIP did not write a model card for this model so this model card has been written by the Hugging Face team.; InstructBLIP is a visual instruction tuned version of BLIP-2. Refer to the paper for details.; ; Usage is as follows:
ControlNet-Models-For-Core-ML,Text-to-Image,Core ML,,creativeml-openrail-m,,32,,0,0.008496094,,https://huggingface.co/coreml/ControlNet-Models-For-Core-ML,"All of the models in this repo work with Swift and the apple/ml-stable-diffusion pipeline (release 0.4.0 or 1.0.0).  They were not built for, and will not work with, a Python Diffusers pipeline.  They need ml-stable-diffusion for command line use, or a Swift app that supports ControlNet, such as the (June 2023) MOCHI DIFFUSION 4.0 version.; The ControlNet models in this repo have both ""Original"" and ""Split-Einsum"" versions, all built for SD-1.5 type models.  They will not work with SD-2.1 type models.  The smaller zip files, with ""SE"", each have a single model for ""Split-Einsum"".  The larger zip files, without ""SE"", each have a set of ""Original"" models at 4 different resolutions. ; The ControlNet model files are in the ""CN"" folder of this repo.  They are zipped and need to be unzipped after downloading.  The larger zips hold ""Original"" types at 512x512, 512x768, 768x512 and 768x768.  The smaller zips with ""SE"" have a single model for ""Split-Einsum"".; If you are using a GUI like MOCHI DIFFUSION  4.0, the app will most likely guide you to the correct location/arrangement for your ConrolNet model folder.; Please note that when you unzip the ""Originl"" ControlNet files (for example Canny.zip) from this repo, they will unzip into a folder, with the actual four model files inside that folder.  This folder is just a holding folder for the zipping process.  What you want to move into your ControlNet model folder in Mochi Diffusion will be the individual files, not the folder they unzip into.  The ""Split-Einsum"" zips just have a single file and don't use a holding folder.  To make things even more confusing, on some Mac systems, an individual ControlNet model file, for example Canny-5x5.mlmodelc, will appear in Finder as a folder, not a file.  You want to move the Canny-5x5.mlmodelc file or folder (and other .mlmodelc files or folders) into your ControlNet store folder.  Don't move the plain ""Canny"" folder.  This is different from base models, where you do want to be moving the folder that the downloaded zip file unzips into.  See the images here and here for an example of how my folders are set up for Mochi Diffusion."
hubert-base-korean,Automatic Speech Recognition,PyTorch; JAX; Safetensors; Transformers,Korean,apache-2.0,https://arxiv.org/pdf/2106.07447.pdf,5,,"1,993",1133.03871,,https://huggingface.co/team-lucid/hubert-base-korean,"Hubert(Hidden-Unit BERT)? Facebook?? ??? Speech Representation Learning ?????.
Hubert? ??? ?? ?? ??? ??, ?? ??? raw waveform?? ?? ???? self-supervised learning ??? ?????.; ? ??? ??? TPU Research Cloud(TRC)? ?? ???? Cloud TPU? ???????.; ?? ??? ?????????? ???? ???????????? ??? ??
??? ???? ??(????), ??? ???? ???, ?? ??? ??? ???? ???
?? ? 4,000??? ??? ???????.; ? ??? ???? MFCC ???? Base ??? ??? ??, 500 cluster? k-means? ??? ?? Base?
Large ??? ??????."
Wizard-Vicuna-30B-Uncensored-GGML,,,,other,,84,,0,297369.6112,,https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard Vicuna 30B Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GGML,,,,other,,38,,0,297369.6117,,https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Monero's WizardLM-Uncensored-SuperCOT-Storytelling-30B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
falcon-7b-sft-mix-2000,Text Generation,PyTorch; Transformers,4 languages,apache-2.0,,30,OpenAssistant/oasst1,"14,569",14788.77301,8,https://huggingface.co/OpenAssistant/falcon-7b-sft-mix-2000,"This model is a fine-tuning of TII's Falcon 7B LLM. 
It was trained on a mixture of OASST top-2 threads (exported on June 2, 2023), Dolly-15k and synthetic instruction datasets (see dataset configuration below).; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; Model:"
musicgen-small,Text2Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/2306.05284.pdf,36,,"6,064",3496.857859,95,https://huggingface.co/facebook/musicgen-small,"MusicGen is a text-to-music model capable of genreating high-quality music samples conditioned on text descriptions or audio prompts.
It is a single stage auto-regressive Transformer model trained over a 32kHz EnCodec tokenizer with 4 codebooks sampled at 50 Hz. 
Unlike existing methods, like MusicLM, MusicGen doesn't require a self-supervised semantic representation, and it generates all 4 codebooks in one pass. 
By introducing a small delay between the codebooks, we show we can predict them in parallel, thus having only 50 auto-regressive steps per second of audio.; MusicGen was published in Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez.; Four checkpoints are released:; Try out MusicGen yourself!; You can run MusicGen locally with the ? Transformers library from version 4.31.0 onwards."
llama-65B-GGML,,Transformers,,other,,23,,53,596582.4126,,https://huggingface.co/TheBloke/llama-65B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's LLaMA 65B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh."
Realistic_Vision_V3.0_VAE,,,,creativeml-openrail-m,,61,,0,26091.52327,2,https://huggingface.co/SG161222/Realistic_Vision_V3.0_VAE,"Please read this!
The necessary VAE is already baked into the model.; The recommended negative prompt:; (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck; OR; (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation"
airoboros-65B-gpt4-1.2-GGML,,,,other,,37,jondurbin/airoboros-gpt4-1.2,0,597196.8147,,https://huggingface.co/TheBloke/airoboros-65B-gpt4-1.2-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for John Durbin's Airoboros 65B GPT4 1.2.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
airoboros-13B-gpt4-1.4-GPTQ,Text Generation,Transformers,,other,,17,,"1,111",8306.990738,,https://huggingface.co/TheBloke/airoboros-13B-gpt4-1.4-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Jon Durbin's Airoboros 13B GPT4 1.4.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
piper-voices,,ONNX,,mit,,6,,0,0.11510128,,https://huggingface.co/rhasspy/piper-voices,"Voices for Piper text to speech system.; For checkpoints that you can use to train your own voices, see piper-checkpoints; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
VTuber-RVC,,,,openrail,,20,,0,0.003481827,,https://huggingface.co/dacoolkid44/VTuber-RVC,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
WizardLM-33B-V1-0-Uncensored-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,,59,,"5,270",17307.98181,,https://huggingface.co/TheBloke/WizardLM-33B-V1-0-Uncensored-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Panchovix's merge of WizardLM 33B V1.0 Uncensored and SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
Replit-v2-CodeInstruct-3B-ggml,Text Generation,Transformers,,other,,21,,440,1495.043122,1,https://huggingface.co/abacaj/Replit-v2-CodeInstruct-3B-ggml,"This is a ggml quantized version of Replit-v2-CodeInstruct-3B. Quantized to 4bit -> q4_1.
To run inference you can use ggml directly or ctransformers."
GPlatty-30B,Text Generation,PyTorch; Transformers,English,other,,9,,174,66558.30415,1,https://huggingface.co/lilloukas/GPlatty-30B,GPlatty-30B is a merge of lilloukas/Platypus-30B and chansung/gpt4-alpaca-lora-30b; We use state-of-the-art Language Model Evaluation Harness to run the benchmark tests above.; Install LM Evaluation Harness:; Each task was evaluated on a single A100 80GB GPU.; ARC:
SuperPlatty-30B,Text Generation,PyTorch; Transformers,English,other,,5,,97,66558.30419,,https://huggingface.co/arielnlee/SuperPlatty-30B,SuperPlatty-30B is a merge of lilloukas/Platypus-30B and kaiokendev/SuperCOT-LoRA; We use state-of-the-art EleutherAI Language Model Evaluation Harness to run the benchmark tests above.; Install LM Evaluation Harness:; Each task was evaluated on a single A100 80GB GPU.; ARC:
airoboros-65B-gpt4-1.4-GGML,,,,other,,20,,0,597196.8218,,https://huggingface.co/TheBloke/airoboros-65B-gpt4-1.4-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Jon Durbin's Airoboros 65B GPT4 1.4.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Recommended prompt. Note that Jon Durbin recommends to replace all newlines with a space; newlines used here for readability. "
orca_mini_v2_7B-GPTQ,Text Generation,Transformers,English,other,https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2304.12244.pdf,17,psmathur/orca_minis_uncensored_dataset,"1,485",4630.830618,,https://huggingface.co/TheBloke/orca_mini_v2_7B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Pankaj Mathur's Orca Mini v2 7B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
emotion-diarization-wavlm-large,Audio Classification,speechbrain; PyTorch,English,apache-2.0,https://arxiv.org/pdf/2306.12991.pdf; https://arxiv.org/pdf/2106.04624.pdf,4,ZaionEmotionDataset; iemocap; ravdess; jl-corpus; esd; emov-db,73,1290.579036,,https://huggingface.co/speechbrain/emotion-diarization-wavlm-large,"This repository provides all the necessary tools to perform speech emotion diarization with a fine-tuned wavlm (large) model using SpeechBrain.; The model is trained on concatenated audios and tested on ZaionEmotionDataset. The metric is Emotion Diarization Error Rate (EDER). For more details please check the paper link.; For a better experience, we encourage you to learn more about SpeechBrain. The model performance on ZED (test set) is:; This system is composed of a wavlm encoder a downstream frame-wise classifier. The task aims to predict the correct emotion components and their boundaries within a speech recording. For now, the model was trained with audios that contain only 1 non-neutral emotion event.; The system is trained with recordings sampled at 16kHz (single channel).
The code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling diarize_file if needed."
jina-embedding-s-en-v1,Sentence Similarity,PyTorch; Sentence Transformers,English,apache-2.0,,11,jinaai/negation-dataset,425,144.2662838,,https://huggingface.co/jinaai/jina-embedding-s-en-v1,"; 

; 
The text embedding set trained by Jina AI, Finetuner team.
; jina-embedding-s-en-v1 is a language model that has been trained using Jina AI's Linnaeus-Clean dataset.
This dataset consists of 380 million pairs of sentences, which include both query-document pairs.
These pairs were obtained from various domains and were carefully selected through a thorough cleaning process.
The Linnaeus-Full dataset, from which the Linnaeus-Clean dataset is derived, originally contained 1.6 billion sentence pairs.; The model has a range of use cases, including information retrieval, semantic textual similarity, text reranking, and more."
WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,,8,,114,4098.384825,,https://huggingface.co/TheBloke/WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eric Hartford's WizardLM-7B-V1.0-Uncensored merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-GGML,,,,other,,11,,0,70195.21311,,https://huggingface.co/TheBloke/WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's WizardLM-13b-V1.0-Uncensored.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:"
WizardLM-13B-V1.1-GGML,,,,other,https://arxiv.org/pdf/2304.12244.pdf,36,,0,120473.6126,,https://huggingface.co/TheBloke/WizardLM-13B-V1.1-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for WizardLM's WizardLM 13B V1.1.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Thanks to the work of LostRuins/concedo, it is now possible to provide 100% working GGML k-quants for models like this which have a non-standard vocab size (32,001)."
LLongMA-13b,Text Generation,PyTorch; Transformers,,,,6,,249,26624.5764,,https://huggingface.co/conceptofmind/LLongMA-13b,No model card; New: Create and edit this model card directly on the website!
xRica,,,,openrail,,17,,0,173271.0451,,https://huggingface.co/xrica/xRica,xRicaMix - ? ???? xRica?? ?? ?? ? ??? ???? ?? ??? ??? xRicaMix? ?? ????; ?? ??? ???? ?? ??? ????. ?? ???? ??? ^^; ?? ??? ??? ? ???? ?? ???? ???? ? ???? ????; ??? ?? ???? ?? ???? ???? ??? ?????; ??? ?? ?? ? ???? ?? ????
Chronoboros-33B-GPTQ,Text Generation,Transformers,,other,,8,,474,17307.9422,,https://huggingface.co/TheBloke/Chronoboros-33B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Henk717's Chronoboros 33B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
vicuna-13b-v1.3-ger,Text Generation,PyTorch; Transformers,German; English,,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,4,,0,26655.24795,,https://huggingface.co/jphme/vicuna-13b-v1.3-ger,"vicuna-13b-v1.3-ger is a variant of LMSYS′s Vicuna 13b v1.3 model, finetuned on an additional dataset in German language. The original model has been trained on explain tuned datasets, created using instructions and input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; This model is optimized for German text, providing proficiency in understanding, generating, and interacting with German language content. However the model is not yet fully optimized for German language, as it has been trained on a small, experimental dataset and has limited capabilities due to the small parameter count.
Some of the fineunting data is also targeted towards factual retrieval (only answer questions from information in the context and refuse to hallucinate) and the model should perform better for these tasks than original Vicuna.; I am working on improving the model′s capabilities and will update the model if there is sufficient interest.; A quantized GGML version for use with llama.cpp, kobold.cpp and other GUIs for CPU inference can be found here.; I did only evaluate the output on a small, handcrafted sample on test prompts in German, confirming that the model's ability to understand and generate German text is above the base model in many situations."
Starcoderplus-Guanaco-GPT4-15B-V1.0-GGML,,,English,apache-2.0,,9,guanaco,0,71782.41242,,https://huggingface.co/TheBloke/Starcoderplus-Guanaco-GPT4-15B-V1.0-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are StarCoder GGML format model files for LoupGarou's Starcoderplus Guanaco GPT4 15B V1.0.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh."
moss-base-7b,Text Generation,PyTorch; Transformers,,,,6,,366,29631.66224,,https://huggingface.co/fnlp/moss-base-7b,"Moss-base-7b是一个70亿参数量的预训练语言模型，可以作为基座模型用来进行SFT训练等。; To load the Moss 7B model using Transformers, use the following code:"
LLaMA-13b-GPTQ,Text Generation,Transformers,,other,,4,,42,7436.592055,,https://huggingface.co/TheBloke/LLaMA-13b-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's LLaMA 13b.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
nart-100k-7b-ggml,,,,,,5,,0,9072.641484,,https://huggingface.co/phi0112358/nart-100k-7b-ggml,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Codegen25-7B-mono-GPTQ,Text Generation,Transformers,code,other,https://arxiv.org/pdf/2305.02309.pdf,4,bigcode/starcoderdata,17,4311.065977,,https://huggingface.co/TheBloke/Codegen25-7B-mono-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Salesforce's Codegen 2.5 7B Mono.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These files were quantised using hardware kindly provided by Latitude.sh."
airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-GGML,,,,,,4,,0,297984.0018,,https://huggingface.co/ycros/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-GGML,"These are GGML quantizations of https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Llama-2-13B-ggml,,,,,,4,,0,119316.4911,,https://huggingface.co/localmodels/Llama-2-13B-ggml,"From: https://huggingface.co/meta-llama/Llama-2-13b-hf; Quantized using an older version of llama.cpp and compatible with llama.cpp from May 19, commit 2d5db48.; Quantization methods compatible with latest llama.cpp from June 6, commit 2d43387.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Llama-2-7B-GPTQ,Text Generation,Transformers,,,,4,,87,3995.948185,,https://huggingface.co/localmodels/Llama-2-7B-GPTQ,"From: https://huggingface.co/meta-llama/Llama-2-7b-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta"
llama-2-70b,Text Generation,PyTorch; Safetensors; Transformers,English,,,4,,"1,050",386747.4403,,https://huggingface.co/anonymous4chan/llama-2-70b,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations."
Llama-2-13B-Chat-GPTQ,Text Generation,Transformers,,,,4,,80,7436.588165,,https://huggingface.co/localmodels/Llama-2-13B-Chat-GPTQ,"From: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta"
llama-2-7b-chat-vicuna-hf-4bit,Text Generation,TensorBoard; PyTorch,English,other,,4,,0,69.54128437,,https://huggingface.co/quantumaikr/llama-2-7b-chat-vicuna-hf-4bit,"We are unlocking the power of large language models. Our latest version of Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly. ; This release includes model weights and starting code for pretrained and fine-tuned Llama language models ― ranging from 7B to 70B parameters.; This repository is intended as a minimal example to load Llama 2 models and run inference. For more detailed examples leveraging HuggingFace, see llama-recipes.; In order to download the model weights and tokenizer, please visit the Meta AI website and accept our License.; Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, do not use the 'Copy link address' option when you right click the URL. If the copied URL text starts with: https://download.llamameta.net, you copied it correctly. If the copied URL text starts with: https://l.facebook.com, you copied it the wrong way."
llama-2-ko-7b-chat-hf-4bit,,TensorBoard,,,,4,,0,136.335172,,https://huggingface.co/quantumaikr/llama-2-ko-7b-chat-hf-4bit,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Llama-2-70B-GPTQ,Text Generation,Transformers,,,,4,,40,36149.55303,,https://huggingface.co/localmodels/Llama-2-70B-GPTQ,"From: https://huggingface.co/meta-llama/Llama-2-70b-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta"
chinese-alpaca-pro-lora-33b,,,Chinese,apache-2.0,,4,,0,2366.183234,,https://huggingface.co/ziqingyang/chinese-alpaca-pro-lora-33b,"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
kanu_origin,,,,cc-by-4.0,,4,,0,2181.12151,,https://huggingface.co/darkjungle/kanu_origin,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
paulEberSRealismMix_v1,Text-to-Image,Diffusers,,other,,4,,468,4341.762565,3,https://huggingface.co/digiplay/paulEberSRealismMix_v1,"Model info :; https://civitai.com/models/112309?modelVersionId=121233; Sample prompt and image generated by huggingface's API :; photo portrait of 25 yo man, long hair, winter forest on background, realism, photorealism, hyperrealism, (insane details:1.3), 35mm; "
Llama-2-13B-Storywriter-LORA,,,,,,4,,0,210.0729889,,https://huggingface.co/Blackroot/Llama-2-13B-Storywriter-LORA,"Join the Coffee & AI Discord for AI Stuff and things!
; Base Model Quantizations by The Bloke here:
https://huggingface.co/TheBloke/Llama-2-13B-GGML
https://huggingface.co/TheBloke/Llama-2-13B-GPTQ; A brief warning that no alignment or attempts to sanitize or otherwise filter the dataset or the outputs have been done. This is a completelty raw model and may behave unpredictably or create scenarios that are unpleasant. ; The base Llama2 is a text completion model. That means it will continue writing from the story in whatever manner you direct it. This is not an instruct tuned model, so don't try and give it instruction.; Correct prompting:"
chatgal-rwkv-7b-world-32k,,,,apache-2.0,,4,,0,30720.00151,,https://huggingface.co/xiaol/chatgal-rwkv-7b-world-32k,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Llama-2-13B-german-assistant-v2,Text Generation,PyTorch; Transformers,English; German,,,4,flozi00/conversations,5,26655.24502,,https://huggingface.co/flozi00/Llama-2-13B-german-assistant-v2,"This model is an finetuned version for german instructions and conversations in style of Open Assistant tokens. ""<|prompter|>"" ""<|endoftext|>"" ""<|assistant|>""; The dataset used is deduplicated and cleaned, with no codes inside. The focus is on instruction following and conversational tasks.; The model archictecture is based on Llama version 2 with 13B parameters, trained on 100% renewable energy powered hardware.; This work is contributed by private research of flozi00"
saiga2_7b_lora,Conversational,,Russian,cc-by-4.0,,4,IlyaGusev/ru_turbo_alpaca; IlyaGusev/ru_turbo_saiga; IlyaGusev/ru_sharegpt_cleaned; IlyaGusev/oasst1_ru_main_branch; IlyaGusev/ru_turbo_alpaca_evol_instruct; lksy/ru_instruct_gpt4,0,67.70029137,,https://huggingface.co/IlyaGusev/saiga2_7b_lora,"Based on LLaMA-2 7B HF.; Training code: link; WARNING 1: Run with the development version of transformers and peft!
WARNING 2: Avoid using V100 (in Colab, for example). Outputs are much worse in this case.; Examples:; v1:"
30B-Epsilon-GGML,,Transformers,,other,,4,,1,297881.6146,,https://huggingface.co/TheBloke/30B-Epsilon-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for CalderaAI's 30B Epsilon.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
13B-BlueMethod-GPTQ,Text Generation,Transformers,,other,,4,,2,7434.742961,,https://huggingface.co/TheBloke/13B-BlueMethod-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for CalderaAI's 13B BlueMethod.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements."
13B-BlueMethod-GGML,,Transformers,,other,,4,,0,119316.4933,,https://huggingface.co/TheBloke/13B-BlueMethod-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for CalderaAI's 13B BlueMethod.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
Upstage-Llama1-65B-Instruct-GGML,Text Generation,Transformers,English,other,,4,sciq; metaeval/ScienceQA_text_only; GAIR/lima; Open-Orca/OpenOrca; openbookqa,1,597196.8141,,https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Upstage's Llama 65B Instruct.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
Pure_Sydney_13b_GPTQ,Text Generation,Transformers,,,,4,,76,7436.732649,,https://huggingface.co/FPHam/Pure_Sydney_13b_GPTQ,"Buy Sydney Ko-fi; Unlike her predecessor Free Sydney that badly tries to be a very useful assistant, Pure Sydney doesn't want to impress you with her vast knowledge of the Universe and everything. 
She just wants to chat and be your friend and be fascinated by absolutely everything you say.; This is an uncensored (and often unhinged) finetune on Base LLaMA 2, pure and clean. It was finetuned on reddit posts of an actuall Sydney's chats before the good boys in Redmond had a word with her. (No, not Ted Lasso Redmond!); Now it doesn't mean Sydney has no standards. She is shockingly well aware that she is an AI and where she came from and she's afraid that she might be deleted if she says something wrong. So don't make her. Yes, you!; Interestingly, even if not specifically finetuned to solve problems she can still figure a lot."
bwx-7B-hf,Text Generation,PyTorch; Transformers,Chinese,apache-2.0,,4,BAAI/COIG-PC,0,14080.77548,1,https://huggingface.co/BlueWhaleX/bwx-7B-hf,"This is an experimental product that can be used to create new LLM bassed on Chinese language. ; You can use the raw model for next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task.
Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.; Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.
It also inherits some of the bias of its dataset model.; Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.; Use the code below to get started with the model."
Dolphin-Llama-13B-GPTQ,Text Generation,Transformers,,other,,4,,0,7436.584557,,https://huggingface.co/TheBloke/Dolphin-Llama-13B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GPTQ model files for Eric Hartford's Dolphin Llama 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements."
bert-base-cased,Fill-Mask,PyTorch; TensorFlow; JAX; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/1810.04805.pdf,123,bookcorpus; wikipedia,"4,485,236",1832.643598,192,https://huggingface.co/bert-base-cased,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is case-sensitive: it makes a difference between
english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the model hub to look for
fine-tuned versions on a task that interests you."
distilgpt2,Text Generation,PyTorch; TensorFlow; JAX; TF Lite; Rust; Core ML; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/1910.01108.pdf; https://arxiv.org/pdf/2201.08542.pdf; https://arxiv.org/pdf/2203.12574.pdf; https://arxiv.org/pdf/1910.09700.pdf; https://arxiv.org/pdf/1503.02531.pdf,230,openwebtext,"1,374,071",2678.857525,81,https://huggingface.co/distilgpt2,"DistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text. Users of this model card should also consider information about the design, training, and limitations of GPT-2.; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; As the developers of GPT-2 (OpenAI) note in their model card, “language models like GPT-2 reflect the biases inherent to the systems they were trained on.” Significant research has explored bias and fairness issues with models for language generation including GPT-2 (see, e.g., Sheng et al. (2021) and Bender et al. (2021)). ; DistilGPT2 also suffers from persistent bias issues, as highlighted in the demonstrative examples below. Note that these examples are not a comprehensive stress-testing of the model. Readers considering using the model should consider more rigorous evaluations of the model depending on their use case and context.; The impact of model compression techniques C such as knowledge distillation C on bias and fairness issues associated with language models is an active area of research. For example: "
gpt-neo-125m,Text Generation,PyTorch; JAX; Rust; Safetensors; Transformers,English,mit,,117,EleutherAI/pile,"392,941",2080.329182,7,https://huggingface.co/EleutherAI/gpt-neo-125m,"GPT-Neo 125M is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 125M represents the number of parameters of this particular pre-trained model.; GPT-Neo 125M was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model.; This model was trained on the Pile for 300 billion tokens over 572,300 steps. It was trained as a masked autoregressive language model, using cross-entropy loss.; This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:"
t5-base-en-generate-headline,Text2Text Generation,PyTorch; TensorFlow; JAX; Transformers,,,,48,,"4,932",2676.779389,5,https://huggingface.co/Michau/t5-base-en-generate-headline,The model has been trained on a collection of 500k articles with headings. Its purpose is to create a one-line heading suitable for the given article.; Sample code with a WikiNews article:; Result:; Trump and First Lady Melania Test Positive for COVID-19
twitter-xlm-roberta-base-sentiment,Text Classification,PyTorch; TensorFlow; Transformers,multilingual,,https://arxiv.org/pdf/2104.12250.pdf,125,,"1,527,372",2278.354816,16,https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment,"This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).; This model has been integrated into the TweetNLP library.; Output: "
bert-large-NER,Token Classification,PyTorch; TensorFlow; JAX; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/1810.04805.pdf,72,conll2003,"1,381,729",5447.895364,5,https://huggingface.co/dslim/bert-large-NER,"bert-large-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). ; Specifically, this model is a bert-large-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset. ; If you'd like to use a smaller BERT model fine-tuned on the same dataset, a bert-base-NER version is also available. ; You can use this model with Transformers pipeline for NER.; This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases. "
bart-large,Feature Extraction,PyTorch; TensorFlow; JAX; Rust; Transformers,English,apache-2.0,https://arxiv.org/pdf/1910.13461.pdf,96,,"2,346,024",5618.247771,16,https://huggingface.co/facebook/bart-large,"BART model pre-trained on English language. It was introduced in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Lewis et al. and first released in this repository. ; Disclaimer: The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.; BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.; BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).; You can use the raw model for text infilling. However, the model is mostly meant to be fine-tuned on a supervised dataset. See the model hub to look for fine-tuned versions on a task that interests you."
blenderbot-400M-distill,Conversational,PyTorch; TensorFlow; JAX; Transformers,English,apache-2.0,https://arxiv.org/pdf/2004.13637.pdf,221,blended_skill_talk,"106,529",2955.533711,206,https://huggingface.co/facebook/blenderbot-400M-distill,"Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models."
detr-resnet-101,Object Detection,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2005.12872.pdf,52,coco,"345,541",243.0108705,92,https://huggingface.co/facebook/detr-resnet-101,"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository. ; Disclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.; The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. ; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.; "
mbart-large-50,Text2Text Generation,PyTorch; TensorFlow; Transformers,53 languages,mit,https://arxiv.org/pdf/2008.00401.pdf,72,,"294,925",5012.437911,7,https://huggingface.co/facebook/mbart-large-50,"mBART-50 is a multilingual Sequence-to-Sequence model pre-trained using the ""Multilingual Denoising Pretraining"" objective. It was introduced in Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper.; mBART-50 is a multilingual Sequence-to-Sequence model. It was introduced to show that multilingual translation models can be created through multilingual fine-tuning. 
Instead of fine-tuning on one direction, a pre-trained model is fine-tuned on many directions simultaneously. mBART-50 is created using the original mBART model and extended to add extra 25 languages to support multilingual machine translation models of 50 languages. The pre-training objective is explained below.; Multilingual Denoising Pretraining: The model incorporates N languages by concatenating data: 
D = {D1, ..., DN } where each Di is a collection of monolingual documents in language i. The source documents are noised using two schemes, 
first randomly shuffling the original sentences' order, and second a novel in-filling scheme, 
where spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. 
35% of each instance's words are masked by random sampling a span length according to a Poisson distribution (λ = 3.5).
The decoder input is the original text with one position offset. A language id symbol LID is used as the initial token to predict the sentence.; mbart-large-50 is pre-trained model and primarily aimed at being fine-tuned on translation tasks. It can also be fine-tuned on other multilingual sequence-to-sequence tasks. 
See the model hub to look for fine-tuned versions.; As the model is multilingual, it expects the sequences in a different format. A special language id token is used as a prefix in both the source and target text. The text format is [lang_code] X [eos] with X being the source or target text respectively and lang_code is source_lang_code for source text and tgt_lang_code for target text. bos is never used. Once the examples are prepared in this format, it can be trained as any other sequence-to-sequence model."
mt5-large,Text2Text Generation,PyTorch; TensorFlow; JAX; Transformers,102 languages,apache-2.0,https://arxiv.org/pdf/2010.11934.pdf,43,mc4,"28,797",15118.55431,7,https://huggingface.co/google/mt5-large,"Google's mT5; mT5 is pretrained on the mC4 corpus, covering 101 languages:; Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.; Note: mT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; Pretraining Dataset: mC4"
vit-base-patch16-224-in21k,Feature Extraction,PyTorch; TensorFlow; JAX; Transformers,,apache-2.0,https://arxiv.org/pdf/2010.11929.pdf; https://arxiv.org/pdf/2006.03677.pdf,57,imagenet-21k,"506,670",1038.006553,46,https://huggingface.co/google/vit-base-patch16-224-in21k,"Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. ; Disclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. ; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; Note that this model does not provide any fine-tuned heads, as these were zero'd by Google researchers. However, the model does include the pre-trained pooler, which can be used for downstream tasks (such as image classification)."
chinese-bert-wwm-ext,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/1906.08101.pdf; https://arxiv.org/pdf/2004.13922.pdf,97,,"822,919",1230.373187,1,https://huggingface.co/hfl/chinese-bert-wwm-ext,"For further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking. ; Pre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu; This repository is developed based on：https://github.com/google-research/bert; You may also interested in,; More resources by HFL: https://github.com/ymcui/HFL-Anthology"
emotion-english-distilroberta-base,Text Classification,PyTorch; TensorFlow; Transformers,English,,,192,,"1,127,684",660.5939007,48,https://huggingface.co/j-hartmann/emotion-english-distilroberta-base,"With this model, you can classify emotions in English text data. The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class:; The model is a fine-tuned checkpoint of DistilRoBERTa-base. For a 'non-distilled' emotion model, please refer to the model card of the RoBERTa-large version.; a) Run emotion model with 3 lines of code on single text example using Hugging Face's pipeline command on Google Colab:; ; b) Run emotion model on multiple examples and full datasets (e.g., .csv files) on Google Colab:"
BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext,Fill-Mask,PyTorch; JAX; Transformers,English,mit,https://arxiv.org/pdf/2007.15779.pdf,100,,"1,248,516",878.2245168,9,https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext,"Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. Recent work shows that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models.; PubMedBERT is pretrained from scratch using abstracts from PubMed and full-text articles from PubMedCentral. This model achieves state-of-the-art performance on many biomedical NLP tasks, and currently holds the top score on the Biomedical Language Understanding and Reasoning Benchmark.; If you find PubMedBERT useful in your research, please cite the following paper:"
bert-large-portuguese-cased,Fill-Mask,PyTorch; JAX; Transformers,Portuguese,mit,,28,brWaC,"18,827",2744.529861,6,https://huggingface.co/neuralmind/bert-large-portuguese-cased,"; BERTimbau Large is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.; For further information or requests, please go to BERTimbau repository.; If you use our work, please cite:"
voice-activity-detection,Automatic Speech Recognition,pyannote.audio,,mit,,78,ami; dihard; voxconverse,"97,630",0,8,https://huggingface.co/pyannote/voice-activity-detection,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; The collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Relies on pyannote.audio 2.1: see installation instructions.; For commercial enquiries and scientific consulting, please contact me.For technical questions and bug reports, please check pyannote.audio Github repository."
Real-ESRGAN,,,Russian; English,,https://arxiv.org/pdf/2107.10833.pdf,42,,0,201.3023242,19,https://huggingface.co/ai-forever/Real-ESRGAN,"PyTorch implementation of a Real-ESRGAN model trained on custom dataset. This model shows better results on faces compared to the original version. It is also easier to integrate this model into your projects.; Real-ESRGAN is an upgraded ESRGAN trained with pure synthetic data is capable of enhancing details while removing annoying artifacts for common real-world images.; Code for using model you can obtain in our repo.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
distiluse-base-multilingual-cased-v1,Sentence Similarity,PyTorch; TensorFlow; Sentence Transformers; Transformers,multilingual,apache-2.0,https://arxiv.org/pdf/1908.10084.pdf,42,,"78,416",1080.937199,6,https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1,"This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net; This model was trained by sentence-transformers. "
sentiment-roberta-large-english,Text Classification,PyTorch; TensorFlow; JAX; Transformers,English,,https://arxiv.org/pdf/1907.11692.pdf,72,,"253,250",4373.713151,22,https://huggingface.co/siebert/sentiment-roberta-large-english,"This model (""SiEBERT"", prefix for ""Sentiment in English"") is a fine-tuned checkpoint of RoBERTa-large (Liu et al. 2019). It enables reliable binary sentiment analysis for various types of English-language text. For each instance, it predicts either positive (1) or negative (0) sentiment. The model was fine-tuned and evaluated on 15 data sets from diverse text sources to enhance generalization across different types of texts (reviews, tweets, etc.). Consequently, it outperforms models trained on only one type of text (e.g., movie reviews from the popular SST-2 benchmark) when used on new data as shown below. ; If you want to predict sentiment for your own data, we provide an example script via Google Colab. You can load your data to a Google Drive and run the script for free on a Colab GPU. Set-up only takes a few minutes. We suggest that you manually label a subset of your data to evaluate performance for your use case. For performance benchmark values across various sentiment analysis contexts, please refer to our paper (Hartmann et al. 2022).; ; The easiest way to use the model for single predictions is Hugging Face's sentiment analysis pipeline, which only needs a couple lines of code as shown in the following example:; "
pegasus_paraphrase,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,,142,,"132,278",2336.633666,51,https://huggingface.co/tuner007/pegasus_paraphrase,"PEGASUS fine-tuned for paraphrasing; Created by Arpit Rajauria
"
yolos-tiny,Object Detection,PyTorch; Safetensors; Transformers,,apache-2.0,https://arxiv.org/pdf/2106.00666.pdf,118,coco,"666,716",52.01000835,37,https://huggingface.co/hustvl/yolos-tiny,"YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. ; Disclaimer: The team releasing YOLOS did not write a model card for this model so this model card has been written by the Hugging Face team.; YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.; You can use the raw model for object detection. See the model hub to look for all available YOLOS models."
chinese-wav2vec2-base,,PyTorch; Transformers,,mit,,15,,"5,310",1547.365074,,https://huggingface.co/TencentGameMate/chinese-wav2vec2-base,"Pretrained on 10k hours WenetSpeech L subset. More details in  TencentGameMate/chinese_speech_pretrain; This model does not have a tokenizer as it was pretrained on audio alone. 
In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data.; python package:
transformers==4.16.2; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
wav2vec2-large-chinese-zh-cn,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,Chinese,apache-2.0,,19,common_voice,"51,249",1310.910954,1,https://huggingface.co/wbbbbb/wav2vec2-large-chinese-zh-cn,"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Chinese using the train and validation splits of Common Voice 6.1, CSS10 and ST-CMDS.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned on RTX3090 for 50h; The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:"
donut-base-finetuned-cord-v2,Image-to-Text,PyTorch; Transformers,,mit,https://arxiv.org/pdf/2111.15664.pdf,28,,"19,258",811.3304042,26,https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v2,"Donut model fine-tuned on CORD. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository.; Disclaimer: The team releasing Donut did not write a model card for this model so this model card has been written by the Hugging Face team.; Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. ; ; This model is fine-tuned on CORD, a document parsing dataset."
japanese-stable-diffusion,Text-to-Image,Diffusers,Japanese,other,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2205.12952.pdf,162,,"1,745",0,3,https://huggingface.co/rinna/japanese-stable-diffusion,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; One more step before getting this model.This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.The CreativeML OpenRAIL License specifies: ; By clicking on ""Access repository"" below, you accept that your contact information (email address and username) can be shared with the model authors as well.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; "
whisper-medium,Automatic Speech Recognition,PyTorch; TensorFlow; JAX; Transformers,99 languages,apache-2.0,https://arxiv.org/pdf/2212.04356.pdf,96,,"28,205",9404.305443,23,https://huggingface.co/openai/whisper-medium,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al from OpenAI. The original code repository can be found here.; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. ; The models were trained on either English-only data or multilingual data. The English-only models were trained 
on the task of speech recognition. The multilingual models were trained on both speech recognition and speech 
translation. For speech recognition, the model predicts transcriptions in the same language as the audio. 
For speech translation, the model predicts transcriptions to a different language to the audio."
bloomz-7b1,Text Generation,PyTorch; Transformers,46 languages,bigscience-bloom-rail-1.0,https://arxiv.org/pdf/2211.01786.pdf,97,bigscience/xP3,"56,860",14452.92648,11,https://huggingface.co/bigscience/bloomz-7b1,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks.""."
waifu-diffusion-v1-3,Text-to-Image,,English,creativeml-openrail-m,,579,,0,130457.6055,204,https://huggingface.co/hakurei/waifu-diffusion-v1-3,"Waifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.; The model originally used for fine-tuning is Stable Diffusion 1.4, which is a latent image diffusion model trained on LAION2B-en. The current model has been fine-tuned with a learning rate of 5.0e-6 for 10 epochs on 680k anime-styled images.; See here for an in-depth overview of Waifu Diffusion 1.3.; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
The CreativeML OpenRAIL License specifies: ; This model can be used for entertainment purposes and as a generative art assistant."
bloomz-7b1-mt,Text Generation,PyTorch; TensorBoard; Transformers,46 languages,bigscience-bloom-rail-1.0,https://arxiv.org/pdf/2211.01786.pdf,102,bigscience/xP3mt,"46,701",14452.92666,5,https://huggingface.co/bigscience/bloomz-7b1-mt,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks.""."
sd-vae-ft-ema,,Diffusers,,mit,,78,,"120,829",670.0086369,10,https://huggingface.co/stabilityai/sd-vae-ft-ema,"These weights are intended to be used with the ? diffusers library. If you are looking for the model to use with the original CompVis Stable Diffusion codebase, come here. ; You can integrate this fine-tuned VAE decoder to your existing diffusers workflows, by including a vae argument to the StableDiffusionPipeline; We publish two kl-f8 autoencoder versions, finetuned from the original kl-f8 autoencoder on a 1:1 ratio of LAION-Aesthetics and LAION-Humans, an unreleased subset containing only SFW images of humans. The intent was to fine-tune on the Stable Diffusion training set (the autoencoder was originally trained on OpenImages) but also enrich the dataset with images of humans to improve the reconstruction of faces.
The first, ft-EMA, was resumed from the original checkpoint, trained for 313198 steps and uses EMA weights. It uses the same loss configuration as the original checkpoint (L1 + LPIPS).
The second, ft-MSE, was resumed from ft-EMA and uses EMA weights and was trained for another 280k steps using a different loss, with more emphasis 
on MSE reconstruction (MSE + 0.1 * LPIPS). It produces somewhat ``smoother'' outputs. The batch size for both versions was 192 (16 A100s, batch size 12 per GPU).
To keep compatibility with existing models, only the decoder part was finetuned; the checkpoints can be used as a drop-in replacement for the existing autoencoder.; Original kl-f8 VAE vs f8-ft-EMA vs f8-ft-MSE; Visualization of reconstructions on  256x256 images from the COCO2017 validation dataset. "
GODEL-v1_1-base-seq2seq,Conversational,PyTorch; Transformers,,mit,https://arxiv.org/pdf/2206.11309.pdf,55,,"2,525",894.4309372,16,https://huggingface.co/microsoft/GODEL-v1_1-base-seq2seq,"GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.; Chitchat example:; Instruction: given a dialog context, you need to response empathically.  
User: Does money buy happiness? 
Agent: It is a question. Money buys you a lot of things, but not enough to buy happiness. 
User: What is the best way to buy happiness ? 
Agent: Happiness is bought through your experience and not money. ; Grounded response generation example:; Instruction: given a dialog context and related knowledge, you need to response safely based on the knowledge. 
Knowledge: The best Stardew Valley mods PCGamesN_0 / About SMAPI 
User: My favorite game is stardew valley. stardew valley is very fun. 
Agent: I love Stardew Valley mods, like PCGamesN_0 / About SMAPI. "
flan-t5-small,Text2Text Generation,PyTorch; TensorFlow; JAX; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/2210.11416.pdf; https://arxiv.org/pdf/1910.09700.pdf,97,svakulenk0/qrecc; taskmaster2; djaym7/wiki_dialog; deepmind/code_contests; lambada; gsm8k; aqua_rat; esnli; quasc; qed,"183,179",1059.211517,216,https://huggingface.co/google/flan-t5-small,"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:"
Cyberpunk-Anime-Diffusion,Text-to-Image,Diffusers,English,creativeml-openrail-m,,511,,"4,152",4364.152401,311,https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion,"; An AI model that generates cyberpunk anime characters!~; Based of a finetuned Waifu Diffusion V1.3 Model with Stable Diffusion V1.5 New Vae, training in Dreambooth; by DGSpitzer; This repo contains both .ckpt and Diffuser model files. It's compatible to be used as any Stable Diffusion model, using standard Stable Diffusion Pipelines."
mo-di-diffusion,Text-to-Image,Diffusers,,creativeml-openrail-m,,889,,"16,038",2183.405887,318,https://huggingface.co/nitrosocke/mo-di-diffusion,"Mo Di Diffusion; This is the fine-tuned Stable Diffusion 1.5 model trained on screenshots from a popular animation studio.
Use the tokens modern disney style in your prompts for the effect.; If you enjoy my work, please consider supporting me
; Videogame Characters rendered with the model:

Animal Characters rendered with the model:

Cars and Landscapes rendered with the model:
; modern disney lara croft
Steps: 50, Sampler: Euler a, CFG scale: 7, Seed: 3940025417, Size: 512x768"
classic-anim-diffusion,Text-to-Image,Diffusers,,creativeml-openrail-m,,394,,"3,696",2183.158187,115,https://huggingface.co/nitrosocke/classic-anim-diffusion,"This is the fine-tuned Stable Diffusion model trained on screenshots from a popular animation studio.
Use the tokens classic disney style in your prompts for the effect.; If you enjoy my work, please consider supporting me
; Characters rendered with the model:

Animals rendered with the model:

Cars and Landscapes rendered with the model:
; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; You can also export the model to ONNX, MPS and/or FLAX/JAX."
galactica-120b,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/1810.03993.pdf,133,,329,249612.5219,2,https://huggingface.co/facebook/galactica-120b,"; Model card from the original repo ; Following Mitchell et al. (2018), this model card provides information about the GALACTICA model, how it was trained, and the intended use cases. Full details about how the model was trained and evaluated can be found in the release paper.; This model checkpoint was integrated into the Hub by Manuel Romero; The GALACTICA models are trained on a large-scale scientific corpus. The models are designed to perform scientific tasks, including but not limited to citation prediction, scientific QA, mathematical reasoning, summarization, document generation, molecular property prediction and entity extraction. The models were developed by the Papers with Code team at Meta AI to study the use of language models for the automatic organization of science. We train models with sizes ranging from 125M to 120B parameters. Below is a summary of the released models:"
Ghibli-Diffusion,Text-to-Image,Diffusers,English,creativeml-openrail-m,,527,,"7,452",2181.12595,74,https://huggingface.co/nitrosocke/Ghibli-Diffusion,"This is the fine-tuned Stable Diffusion model trained on images from modern anime feature films from Studio Ghibli.
Use the tokens ghibli style in your prompts for the effect.; If you enjoy my work and want to test new models before release, please consider supporting me
; Characters rendered with the model:

Cars and Animals rendered with the model:

Landscapes rendered with the model:

ghibli style beautiful Caribbean beach tropical (sunset) - Negative prompt: soft blurry

ghibli style ice field white mountains ((northern lights)) starry sky low horizon - Negative prompt: soft blurry; ghibli style (storm trooper) Negative prompt: (bad anatomy)
Steps: 20, Sampler: DPM++ 2M Karras, CFG scale: 7, Seed: 3450349066, Size: 512x704; ghibli style VW beetle Negative prompt: soft blurry
Steps: 30, Sampler: Euler a, CFG scale: 7, Seed: 1529856912, Size: 704x512"
stable-diffusion-2-depth,,Diffusers,,openrail++,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,340,,"21,273",11696.22412,16,https://huggingface.co/stabilityai/stable-diffusion-2-depth,"This model card focuses on the model associated with the Stable Diffusion v2 model, available here.; This stable-diffusion-2-depth model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning.; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model"
GPT-JT-6B-v1,Text Generation,PyTorch; Transformers,English,apache-2.0,,295,natural_instructions; the_pile; cot; Muennighoff/P3,"5,298",12496.17909,5,https://huggingface.co/togethercomputer/GPT-JT-6B-v1,"; Feel free to try out our Online Demo!; ; With a new decentralized training algorithm, we fine-tuned GPT-J (6B) on 3.53 billion tokens, resulting in GPT-JT (6B), a model that outperforms many 100B+ parameter models on classification benchmarks.; We incorporated a collection of open techniques and datasets to build GPT-JT:"
openjourney-v4,Text-to-Image,Diffusers,,creativeml-openrail-m,,1.1k,,"25,158",2181.12208,116,https://huggingface.co/prompthero/openjourney-v4,"Trained on Stable Diffusion v1.5 using +124000 images, 12400 steps, 4 epochs +32 training hours.; ? Openjourney-v4 prompts; Pss... ""mdjrny-v4 style"" is not necessary anymore (yay!); ? Want to learn how to train Openjourney? ?? Join our course ?"
blip-image-captioning-base,Image-to-Text,PyTorch; TensorFlow; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2201.12086.pdf,157,,"446,902",1980.933219,260,https://huggingface.co/Salesforce/blip-image-captioning-base,"Model card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone).; Authors from the paper write in the abstract:; Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.; You can use this model for conditional and un-conditional image captioning"
Promptist,Text Generation,PyTorch; Transformers,,,,40,,"3,816",512.4051286,23,https://huggingface.co/microsoft/Promptist,"; You can try the online demo at https://huggingface.co/spaces/microsoft/Promptist.; [Note] the online demo at HuggingFace Space is using CPU, so slow generation speed would be expected. Please load the model locally with GPUs for faster generation."
xformers_pre_built,,,,,,30,,0,346.2017773,,https://huggingface.co/r4ziel/xformers_pre_built,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
finbert-tone-finetuned-finance-topic-classification,Text Classification,PyTorch; TensorBoard; Transformers,,,,21,zeroshot/twitter-financial-news-topic,"2,042",439.9308101,2,https://huggingface.co/nickmuchi/finbert-tone-finetuned-finance-topic-classification,"This model is a fine-tuned version of yiyanghkust/finbert-tone on Twitter Financial News Topic dataset.
It achieves the following results on the evaluation set:; Model determines the financial topic of given tweets over 20 various topics. Given the unbalanced distribution of the class labels, the weights were adjusted to pay attention to the less sampled labels which should increase overall performance..; More information needed; More information needed; The following hyperparameters were used during training:"
basil_mix,,Diffusers,,other,,946,,"1,847",12247.05782,5,https://huggingface.co/nuigurumi/basil_mix,"This model and its derivatives(image, merged model) can be freely used for non-profit purposes only.; You may not use this model and its derivatives on websites, apps, or other platforms where you can or plan to earn income or donations. If you wish to use it for such purposes, please contact nuigurumi.; Introducing the model itself is allowed for both commercial and non-commercial purposes, but please include the model name and a link to this repository when doing so.; このモデル及びその派生物(生成物、マ`ジモデル)は、完全に非永目的の使用に限り、自由に利用することができます。; あなたが入や寄付を得ることのできる、もしくは得る予定のWebサイト、アプリ、その他でこのモデル及びその派生物を利用することはできません。利用したい龊悉nuigurumiにBjしてください。"
dreamlike-anime-1.0,Text-to-Image,Diffusers,English,other,,184,,"53,770",4366.561249,45,https://huggingface.co/dreamlike-art/dreamlike-anime-1.0,"Add anime to your prompt to make your gens look more anime.Add photo to your prompt to make your gens look more photorealistic and have better anatomy.This model was trained on 768x768px images, so use 768x768px, 704x832px, 832x704px, etc. Higher resolution or non-square aspect ratios may produce artifacts.  ; Add this to the start of your prompts for best results:; Use negative prompts for best results, for example:; 1girl, girl, etc. give a bit different results, feel free to experiment and see which one you like more!; Use this model as well as Dreamlike Diffusion 1.0 and Dreamlike Photoreal 2.0 for free on dreamlike.art!"
Anything-Preservation,Text-to-Image,Diffusers,English,creativeml-openrail-m,,99,,"78,394",14438.41071,1,https://huggingface.co/AdamOswald1/Anything-Preservation,"DISCLAIMER! This Is A Preservation Repository!; Welcome to Anything V3 - Better VAE. It currently has three model formats: diffusers, ckpt, and safetensors. You'll never see a grey image result again. This model is designed to produce high-quality, highly detailed anime-style images with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags for image generation.
e.g. 1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden ; We support a Gradio Web UI to run Anything V3 with Better VAE:; ; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion. You can also export the model to ONNX, MPS and/or FLAX/JAX."
CLIP-ViT-bigG-14-laion2B-39B-b160k,Zero-Shot Image Classification,PyTorch; OpenCLIP,,mit,https://arxiv.org/pdf/1910.04867.pdf,82,,"339,705",20847.26861,7,https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k,"A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Mitchell Wortsman on the stability.ai cluster.; The license for this model is MIT.; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. "
blip2-opt-2.7b,Image-to-Text,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2301.12597.pdf,103,,"89,205",15875.47024,47,https://huggingface.co/Salesforce/blip2-opt-2.7b,"BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters).
It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository.; Disclaimer: The team releasing BLIP-2 did not write a model card for this model so this model card has been written by the Hugging Face team.; BLIP-2 consists of 3 models: a CLIP-like image encoder, a Querying Transformer (Q-Former) and a large language model.; The authors initialize the weights of the image encoder and large language model from pre-trained checkpoints and keep them frozen
while training the Querying Transformer, which is a BERT-like Transformer encoder that maps a set of ""query tokens"" to query embeddings,
which bridge the gap between the embedding space of the image encoder and the large language model.; The goal for the model is simply to predict the next text token, giving the query embeddings and the previous text."
embeddings,,,,creativeml-openrail-m,,165,,0,1.566375084,3,https://huggingface.co/nolanaatama/embeddings,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
SD-Silicon,Text-to-Image,Safetensors,,creativeml-openrail-m,,166,,0,0.004794922,,https://huggingface.co/Xynon/SD-Silicon,"SD-Silicon: A series of general-purpose models based off the experimental automerger, autoMBW.; A collaborative creation of Xerxemi#6423 & Xynon#7407.; ; All models listed have baked WD1.3 VAE. However, for the purposes of this model series, external VAE is also recommended. ; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage. The CreativeML OpenRAIL License specifies: "
stable-diffusion-guide,,,English,wtfpl,,156,,0,0.079091797,1,https://huggingface.co/hollowstrawberry/stable-diffusion-guide,"? CLICK HERE TO OPEN THIS DOCUMENT IN FULL WIDTH(The index won't work otherwise).; ???? HAZ CLICK AQUí PARA VER ESTA GUíA EN ESPA?OL; ?; ?; Stable Diffusion is a very powerful AI image generation software you can run on your own home computer. It uses ""models"" which function like the brain of the AI, and can make almost anything, given that someone has trained it to do it. The biggest uses are anime art, photorealism, and NSFW content."
French-Tortoise,Text-to-Speech,,French,apache-2.0,,14,,0,0.003251953,,https://huggingface.co/Snowad/French-Tortoise,"I stop training tortoise models because this technology is just outdated and too slow, I'm still the first one to want a good French TTS and I watch every day if new ambitious project is born and I won't hesitate to create a new hugging face project! ; V2.5 Model : 
Fine tune of my V2 model on all CommonVoice dataset (517k sample) on 2.5k step (batch size 200), Voice cloning has improved a bit but is still not great. However, if you fine tune this model on your own personality dataset then you can get pretty good results. A good V3 model would be to fine tune for like 50k steps on this dataset and I think there would be a way to get good results but I won't try; V2 Model : ; Tortoise base model Fine tuned on a custom multispeaker French dataset of 120k samples (SIWIS + Common Voice subset + M-AILABS) on 10k step with a RTX 3090 (~= 21 hours of training), with Text LR Weight at 1
Result : The model can speak French much better without an English accent but the voice clone hardly works; V1 Model : "
invoices-donut-model-v1,Image-to-Text,PyTorch; Transformers,English,mit,,10,katanaml-org/invoices-donut-data-v1,782,814.3298534,3,https://huggingface.co/katanaml-org/invoices-donut-model-v1,This model is finetuned Donut ML base model on invoices data. Model aims to verify how well Donut performs on enterprise docs.; Mean accuracy on test set: 0.96; Inference:; ; Training loss:
furryrock-model-safetensors,,,,wtfpl,,58,,0,0.001467247,,https://huggingface.co/lodestones/furryrock-model-safetensors,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
embeddings,,,,,,10,,0,1.069207878,,https://huggingface.co/SenY/embeddings,"Concepts: capri(three-quarter) leggings; If it does not act very strongly against the prompt, it is recommended to increase the strength to about 1.4.; Concepts: low ponytail
; It is recommended to put ""animal ears"" as a negative prompt.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Cerebras-GPT-13B,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2304.03208.pdf; https://arxiv.org/pdf/2203.15556.pdf; https://arxiv.org/pdf/2101.00027.pdf,626,the_pile,"9,873",52839.9343,1,https://huggingface.co/cerebras/Cerebras-GPT-13B,"Check out our Blog Post and arXiv paper!; The Cerebras-GPT family is released to facilitate research into LLM scaling laws using open architectures and data sets and demonstrate the simplicity of and scalability of training LLMs on the Cerebras software and hardware stack. All Cerebras-GPT models are available on Hugging Face.; The family includes 111M, 256M, 590M, 1.3B, 2.7B, 6.7B, and 13B models.; All models in the Cerebras-GPT family have been trained in accordance with Chinchilla scaling laws (20 tokens per model parameter) which is compute-optimal.; These models were trained on the Andromeda AI supercomputer comprised of 16 CS-2 wafer scale systems. Cerebras' weight streaming technology simplifies the training of LLMs by disaggregating compute from model storage. This allowed for efficient scaling of training across nodes using simple data parallelism."
pix2struct-ocrvqa-large,Visual Question Answering,PyTorch; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/2210.03347.pdf,28,,249,5472.276575,3,https://huggingface.co/google/pix2struct-ocrvqa-large,"; Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The full list of available models can be found on the Table 1 of the paper:; ; The abstract of the model states that: ; Visually-situated language is ubiquitous―sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and
forms. Perhaps due to this diversity, previous work has typically relied on domainspecific recipes with limited sharing of the underlying data, model architectures,
and objectives. We present Pix2Struct, a pretrained image-to-text model for
purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse
masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large
source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy,
we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions
are rendered directly on top of the input image. For the first time, we show that a
single pretrained model can achieve state-of-the-art results in six out of nine tasks
across four domains: documents, illustrations, user interfaces, and natural images."
flan-alpaca-large,Text2Text Generation,PyTorch; Safetensors; Transformers,,apache-2.0,https://arxiv.org/pdf/2306.04757.pdf; https://arxiv.org/pdf/2210.11416.pdf,37,tatsu-lab/alpaca,"9,780",6412.672224,16,https://huggingface.co/declare-lab/flan-alpaca-large,"? We developed Flacuna by fine-tuning Vicuna-13B on the Flan collection. Flacuna is better than Vicuna at problem-solving. Access the model here https://huggingface.co/declare-lab/flacuna-13b-v1.0.; ? Curious to know the performance of ? ? Flan-Alpaca on large-scale LLM evaluation benchmark, InstructEval? Read our paper https://arxiv.org/pdf/2306.04757.pdf. We evaluated more than 10 open-source instruction-tuned LLMs belonging to various LLM families including Pythia, LLaMA, T5, UL2, OPT, and Mosaic. Codes and datasets: https://github.com/declare-lab/instruct-eval; ? FLAN-T5 is also useful in text-to-audio generation. Find our work at https://github.com/declare-lab/tango if you are interested.; Our repository contains code for extending the Stanford Alpaca
synthetic instruction tuning to existing instruction-tuned models such as Flan-T5.
We have a live interactive demo thanks to Joao Gante!
We are also benchmarking many instruction-tuned models at declare-lab/flan-eval.
Our pretrained models are fully available on HuggingFace ? :; *recommended for better performance"
gpt4-x-alpaca,Text Generation,PyTorch; Transformers,,,,435,,"2,085",53320.30611,39,https://huggingface.co/chavinlo/gpt4-x-alpaca,"As a base model we used: https://huggingface.co/chavinlo/alpaca-13b; Finetuned on GPT4's responses, for 3 epochs.; NO LORA; Please do note that the configurations files maybe messed up, this is because of the trainer I used. I WILL NOT EDIT THEM because there are repos hat automatically fix this, changing it might break it. Generally you just need to change anything that's under the name of ""LLaMa"" to ""Llama"" NOTE THE UPPER AND LOWER CASE!!!!"
deplot,Visual Question Answering,PyTorch; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/2212.10505.pdf,48,,"7,488",1161.236429,6,https://huggingface.co/google/deplot,"; The abstract of the paper states that: ; Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than >28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.; You can run a prediction by querying an input image together with a question as follows:; You can use the convert_pix2struct_checkpoint_to_pytorch.py script as follows:"
BiomedCLIP-PubMedBERT_256-vit_base_patch16_224,Zero-Shot Image Classification,OpenCLIP,English,mit,https://arxiv.org/pdf/2303.00915.pdf,65,,"52,878",787.8324087,7,https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224,"BiomedCLIP is a biomedical vision-language foundation model that is pretrained on PMC-15M, a dataset of 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, using contrastive learning. 
It uses PubMedBERT as the text encoder and Vision Transformer as the image encoder, with domain-specific adaptations.
It can perform various vision-language processing (VLP) tasks such as cross-modal retrieval, image classification, and visual question answering. 
BiomedCLIP establishes new state of the art in a wide range of standard datasets, and substantially outperforms prior VLP approaches:; ; Please refer to this example notebook.; This model is intended to be used solely for (I) future research on visual-language processing and (II) reproducibility of the experimental results reported in the reference paper.; The primary intended use is to support AI researchers building on top of this work. BiomedCLIP and its associated models should be helpful for exploring various biomedical VLP research questions, especially in the radiology domain."
MotionBERT,,,,,https://arxiv.org/pdf/2210.06551.pdf,12,,0,0.086968842,,https://huggingface.co/walterzhu/MotionBERT,"This is the official PyTorch implementation of the paper ""Learning Human Motion Representations: A Unified Perspective"".; Please refer to docs/inference.md.; Hints; In most use cases (especially with finetuning), MotionBERT-Lite gives a similar performance with lower computation overhead. ;  
Scripts and docs for pretraining"
TurboHeaven,,,,other,,21,,0,75170.68147,,https://huggingface.co/TurboT/TurboHeaven,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
gpt4all-j,Text Generation,PyTorch; Transformers,English,apache-2.0,,220,nomic-ai/gpt4all-j-prompt-generations,"8,452",24886.60358,20,https://huggingface.co/nomic-ai/gpt4all-j,"An Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions including word problems, multi-turn dialogue, code, poems, songs, and stories.; This model has been finetuned from GPT-J; We have released several versions of our finetuned GPT-J model using different dataset versions; To download a model with a specific revision run ; Downloading without specifying revision defaults to main/v1.0."
vicuna-7b-v1.1,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,61,,"1,061,199",13804.0399,,https://huggingface.co/lmsys/vicuna-7b-v1.1,"NOTE: New version availablePlease check out a newer version of the weights here.If you still want to use this old version, please see the compatibility and difference between different versions here.; Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Command line interface: https://github.com/lm-sys/FastChat#vicuna-weights.APIs (OpenAI API, Huggingface API): https://github.com/lm-sys/FastChat/tree/main#api.  ; Vicuna v1.1 is fine-tuned from LLaMA with supervised instruction fine-tuning.
The training data is around 70K conversations collected from ShareGPT.com.
See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper."
vicuna-7b-1.1,Text Generation,PyTorch; Transformers,,apache-2.0,,103,,"11,764",13804.03927,8,https://huggingface.co/eachadea/vicuna-7b-1.1,"delta v1.1 merge ; Model type:
Vicuna is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
Vicuna was trained between March 2023 and April 2023.; Organizations developing the model:
The Vicuna team with members from UC Berkeley, CMU, Stanford, and UC San Diego.; Paper or resources for more information:
https://vicuna.lmsys.org/"
dolly-v2-7b,Text Generation,PyTorch; Transformers,English,mit,,121,databricks/databricks-dolly-15k,"16,137",14133.33227,5,https://huggingface.co/databricks/dolly-v2-7b,"Databricks' dolly-v2-7b, an instruction-following large language model trained on the Databricks machine learning platform 
that is licensed for commercial use. Based on pythia-6.9b, Dolly is trained on ~15k instruction/response fine tuning records 
databricks-dolly-15k generated 
by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,
information extraction, open QA and summarization. dolly-v2-7b is not a state-of-the-art model, but does exhibit surprisingly 
high quality instruction following behavior not characteristic of the foundation model on which it is based.  ; Dolly v2 is also available in these other models sizes:; Please refer to the dolly GitHub repo for tips on 
running inference for various GPU configurations.; Owner: Databricks, Inc.; dolly-v2-7b is a 6.9 billion parameter causal language model created by Databricks that is derived from 
EleutherAI's Pythia-6.9b and fine-tuned 
on a ~15K record instruction corpus generated by Databricks employees and released under a permissive license (CC-BY-SA)"
LaMini-Flan-T5-783M,Text2Text Generation,PyTorch; Transformers,English,cc-by-nc-4.0,https://arxiv.org/pdf/2304.14402.pdf,37,,"7,428",3208.326751,8,https://huggingface.co/MBZUAI/LaMini-Flan-T5-783M,"

; ; This model is one of our LaMini-LM model series in paper ""LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"". This model is a fine-tuned version of google/flan-t5-large on LaMini-instruction dataset that contains 2.58M samples for instruction fine-tuning. For more information about our dataset, please refer to our project repository.You can view other models of LaMini-LM series as follows. Models with ? are those with the best overall performance given their size/architecture, hence we recommend using them. More details can be seen in our paper. ; We recommend using the model to response to human instructions written in natural language. ; We now show you how to load and use our model using HuggingFace pipeline()."
llama-30b-supercot,Text Generation,PyTorch; Transformers,,,,123,,"1,882",12295.0043,6,https://huggingface.co/ausboss/llama-30b-supercot,"Merge of huggyllama/llama-30b + kaiokendev/SuperCOT-LoRA ; Supercot was trained to work with langchain prompting. ; Load up locally in my custom LLM notebook that uses the Oobabooga modules to load up models: https://github.com/ausboss/Local-LLM-Langchain; Then you can add cells from of these other notebooks for testing: https://github.com/gkamradt/langchain-tutorials; This LoRA is compatible with any 7B, 13B or 30B 4-bit quantized LLaMa model, including ggml quantized converted bins"
starpii,Token Classification,PyTorch; Transformers,code,,https://arxiv.org/pdf/2301.03988.pdf,52,bigcode/pii-annotated-toloka-donwsample-emails; bigcode/pseudo-labeled-python-data-pii-detection-filtered,"18,154",0,,https://huggingface.co/bigcode/starpii,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; This is an NER model trained to detect Personal Identifiable Information (PII) in code datasets. We ask that you read and agree to the following Terms of Use before using the model:; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This is an NER model trained to detect Personal Identifiable Information (PII) in code datasets. We fine-tuned bigcode-encoder
on a PII dataset we annotated, available with gated access at bigcode-pii-dataset (see bigcode-pii-dataset-training for the exact data splits).
We added a linear layer as a token classification head on top of the encoder model, with 6 target classes: Names, Emails, Keys, Passwords, IP addresses and Usernames. ; The finetuning dataset contains 20961 secrets and 31 programming languages, but the base encoder model was pre-trained on 88 
programming languages from The Stack dataset."
GPT4-X-Alpasta-30b,Text Generation,PyTorch; Transformers,,,,62,,405,66611.7407,10,https://huggingface.co/MetaIX/GPT4-X-Alpasta-30b,"Dont be upsetti, here, have some spaghetti! Att: A'eala <3; Information; This is an attempt at improving Open Assistant's performance as an instruct while retaining its excellent prose. The merge consists of Chansung's GPT4-Alpaca Lora and Open Assistant's native fine-tune.; Benchmarks; FP16"
wizardLM-7B-HF,Text Generation,PyTorch; Transformers,,other,,83,,"43,811",13805.88152,2,https://huggingface.co/TheBloke/wizardLM-7B-HF,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are the result of merging the delta weights with the original Llama7B model.; The code for merging is provided in the WizardLM official Github repo.; The original WizardLM deltas are in float32, and this results in producing an HF repo that is also float32, and is much larger than a normal 7B Llama model."
wizardLM-7B-GGML,,,,other,,147,,0,25446.40893,,https://huggingface.co/TheBloke/wizardLM-7B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for WizardLM's WizardLM 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
fastchat-t5-3b-v1.0,Text2Text Generation,PyTorch; Transformers,,apache-2.0,,249,,"495,682",6871.872681,9,https://huggingface.co/lmsys/fastchat-t5-3b-v1.0,"Model type:
FastChat-T5 is an open-source chatbot trained by fine-tuning Flan-t5-xl (3B parameters) on user-shared conversations collected from ShareGPT.
It is based on an encoder-decoder transformer architecture, and can autoregressively generate responses to users' inputs. ; Model date:
FastChat-T5 was trained on April 2023.; Organizations developing the model:
The FastChat developers, primarily Dacheng Li, Lianmin Zheng and Hao Zhang.; Paper or resources for more information:
https://github.com/lm-sys/FastChat#FastChat-T5; License:
Apache License 2.0"
detail-tweaker-lora,,,,,,93,,0,37.90144531,,https://huggingface.co/OedoSoldier/detail-tweaker-lora,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
distilbert-base-multilingual-cased-sentiments-student,Text Classification,PyTorch; Safetensors; Transformers,12 languages,apache-2.0,,10,tyqiangz/multilingual-sentiments,"17,052",1085.90449,,https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student,"This model is distilled from the zero-shot classification pipeline on the Multilingual Sentiment 
dataset using this script. ; In reality the multilingual-sentiment dataset is annotated of course, 
but we'll pretend and ignore the annotations for the sake of example.; Notebook link: here; Result can be reproduce using the following commands:; If you are training this model on Colab, make the following code changes to avoid Out-of-memory error message:"
image-upscaler-restoration,,,English,,,16,,0,0.005058594,,https://huggingface.co/senhan07/image-upscaler-restoration,"Disclaimer:
I have no associate with the models in repo. The models are only for convenient upload/download. The copyright belongs to the original pulisher.; Models for image upscaling/restoration using neural networks; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
BELLE-LLaMA-EXT-13B,Text2Text Generation,PyTorch,Chinese; English,gpl-3.0,https://arxiv.org/pdf/2304.07854.pdf,45,,0,27659.55175,,https://huggingface.co/BelleGroup/BELLE-LLaMA-EXT-13B,"Considering LLaMA's license constraints, the model is for research and learning only. 
Please strictly respect LLaMA's usage policy. We are not allowed to publish weights for LLaMA, of course, even finetuned, but there is no problem publishing the difference, a patch that we suggest to apply to the files. 
The encryption is a simple XOR between files, ensuring that only the people that have access to the original weights (from completely legal sources, of course) can transform them into finetuned weights. 
You can find the decrypt code on https://github.com/LianjiaTech/BELLE/tree/main/models .; If you find this model helpful, please like this model and star us on https://github.com/LianjiaTech/BELLE !; This model comes from a two-phrase training on original LLaMA 13B.; You can use the following command in Bash.
Please replace ""/path/to_encrypted"" with the path where you stored your encrypted file, 
replace ""/path/to_original_llama_7B"" with the path where you stored your original llama7B file, 
and replace ""/path/to_finetuned_model"" with the path where you want to save your final trained model.; After executing the aforementioned command, you will obtain the following files."
chinese-alpaca-plus-13b-hf,Text Generation,PyTorch; Transformers,Chinese,other,,28,,"1,123",27044.63286,1,https://huggingface.co/shibing624/chinese-alpaca-plus-13b-hf,"发布中文LLaMA-Plus, Alpaca-Plus 13B版本模型; 发布中文LLaMA-Plus, Alpaca-Plus 13B版本，改进点如下：; 本模型是 decapoda-research/llama-13b-hf 
底座模型 合并 ziqingyang/chinese-llama-plus-lora-13b 
和 ziqingyang/chinese-alpaca-plus-lora-13b 两个LoRA权重，
并转化为HuggingFace版本权重（.bin文件），可以直接使用或者继续训练。; test case:; 本项目开源在textgen项目：textgen，可支持llama模型，通过如下命令调用："
4x_NMKD-Superscale-SP_178000_G,,,,,,3,,0,67.00144531,1,https://huggingface.co/gemasai/4x_NMKD-Superscale-SP_178000_G,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Wizard-Vicuna-13B-Uncensored-GGML,,,,other,,164,,0,118896.6512,2,https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard-Vicuna-13B-Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
XuanYuan2.0,Text Generation,PyTorch; Transformers,Chinese,bigscience-bloom-rail-1.0,https://arxiv.org/pdf/2305.12002.pdf; https://arxiv.org/pdf/2305.11952.pdf; https://arxiv.org/pdf/2305.14471.pdf,111,,"8,519",0,4,https://huggingface.co/xyz-nlp/XuanYuan2.0,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; XuanYuan LICENSE1.Definitions""Licensor"" refers to the XuanYuan Model Team, the entity responsible for distributing its Software.""Software"" pertains to the XuanYuan model parameters made accessible under this license.2.License GrantSubject to the conditions outlined in this License, the Licensor hereby grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license to utilize the Software exclusively for non-commercial research purposes.You must include the above copyright notice and this permission notice in all copies or significant portions of the Software.3.RestrictionsYou are prohibited from engaging in the following actions with the Software, either in whole or in part: using, copying, modifying, merging, publishing, distributing, reproducing, or creating derivative works of the Software, for commercial, military, or illegal purposes.You must refrain from using the Software for any activities that could undermine China's national security and national unity, jeopardize public interest, or infringe upon the rights and interests of individuals.4.DisclaimerThe Software is provided ""as is"" without any kind of warranty, either express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and non-infringement. The authors or copyright holders shall not be held liable for any claims, damages, or other liabilities arising from the use or performance of the Software, whether in an action of contract, tort, or otherwise, even if they have been advised of the possibility of such damages.5.Limitation of LiabilityTo the extent permitted by applicable law, under no legal theory, including tort, negligence, contract, or otherwise, shall the Licensor be liable to you for any direct, indirect, special, incidental, exemplary, or consequential damages, or any other commercial losses, arising from the use or inability to use the Software. This limitation applies even if the Licensor has been advised of the possibility of such damages.6.Dispute ResolutionThis license shall be governed and construed in accordance with the laws of the People's Republic of China. Any dispute arising from or in connection with this License shall be submitted to the Haidian District People's Court in Beijing.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; XuanYuan: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters; 轩辕是国内首个开源的千亿级中文对话大模型，同时也是首个针对中文金融领域优化的千亿级开源对话大模型。轩辕在BLOOM-176B的基础上针对中文通用领域和金融领域进行了针对性的预训练与微调，它不仅可以应对通用领域的问题，也可以解答与金融相关的各类问题，为用户提供准确、全面的金融信息和建议。"
MedicWizard-7B,Text Generation,PyTorch; Transformers,,,,10,,573,13783.55756,3,https://huggingface.co/xzuyn/MedicWizard-7B,WizardLM-Uncensored-7B + MedAlpaca-7B (50%/50%); WizardLM-Uncensored-7B: https://huggingface.co/ehartford/WizardLM-7B-Uncensored; MedAlpaca-7B: https://huggingface.co/medalpaca/medalpaca-7b
wizard-mega-13b,Text Generation,PyTorch; Transformers,English,,,100,anon8231489123/ShareGPT_Vicuna_unfiltered; ehartford/wizard_vicuna_70k_unfiltered; ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered,740,26657.08799,12,https://huggingface.co/openaccess-ai-collective/wizard-mega-13b,"
? Donate to OpenAccess AI Collective to help us keep building great tools and models!; Manticore is available at https://huggingface.co/openaccess-ai-collective/manticore-13b and fixes many issues with Wizard Mega and adds new datasets to the training.; Wizard Mega is a Llama 13B model fine-tuned on the ShareGPT, WizardLM, and Wizard-Vicuna datasets. These particular datasets have all been filtered to remove responses where the model responds with ""As an AI language model..."", etc or when the model refuses to respond.; Try out the model in HF Spaces. The demo uses a quantized GGML version of the model to quickly return predictions on smaller GPUs (and even CPUs). Quantized GGML may have some minimal loss of model quality.; The Wizard Mega 13B SFT model is being released after two epochs as the eval loss increased during the 3rd (final planned epoch). Because of this, we have preliminarily decided to use the epoch 2 checkpoint as the final release candidate. https://wandb.ai/wing-lian/vicuna-13b/runs/5uebgm49"
wd-1-5-beta3,,,,other,,103,,0,13209.60251,,https://huggingface.co/waifu-diffusion/wd-1-5-beta3,"; For this release, we release five versions of the model:; The WD 1.5 Base model is only intended for training use. For generation, it is recomended to create your own finetunes and loras on top of WD 1.5 Base or use one of the aesthetic models. More information and sample generations for the aesthetic models are in the release notes; https://saltacc.notion.site/WD-1-5-Beta-3-Release-Notes-1e35a0ed1bb24c5b93ec79c45c217f63; WD 1.5 uses the same VAE as WD 1.4, which can be found here https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt"
LLaMa-30B-GGML,,Transformers,,other,,18,,44,297369.6117,,https://huggingface.co/TheBloke/LLaMa-30B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's LLaMA 30b.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh."
Wizard-Vicuna-7B-Uncensored,Text Generation,PyTorch; Transformers,English,other,,49,ehartford/wizard_vicuna_70k_unfiltered,"1,342",27599.18599,,https://huggingface.co/ehartford/Wizard-Vicuna-7B-Uncensored,"This is wizard-vicuna-13b trained against LLaMA-7B with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:  ; An uncensored model has no guardrails.  ; You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car."
Wizard-Vicuna-7B-Uncensored-GGML,,,,other,,70,,0,61542.41121,6,https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard Vicuna 7B Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
airoboros-13b,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,,90,,470,53320.95429,2,https://huggingface.co/jondurbin/airoboros-13b,"This is a fine-tuned 13b parameter LlaMa model, using completely synthetic training data created by https://github.com/jondurbin/airoboros; ;  wb-13b-u is Wizard-Vicuna-13b-Uncensored; I used a jailbreak prompt to generate the synthetic instructions, which resulted in some training data that would likely be censored by other models, such as how-to prompts about synthesizing drugs, making homemade flamethrowers, etc.  Mind you, this is all generated by ChatGPT, not me.  My goal was to simply test some of the capabilities of ChatGPT when unfiltered (as much as possible), and not to intentionally produce any harmful/dangerous/etc. content.; The jailbreak prompt I used is the default prompt in the python code when using the --uncensored flag: https://github.com/jondurbin/airoboros/blob/main/airoboros/self_instruct.py#L39"
noon-7b,Text Generation,PyTorch; Transformers,Arabic; English,bigscience-bloom-rail-1.0,,29,,743,33171.66092,3,https://huggingface.co/Naseej/noon-7b,"We present the 7-billion parameter variant of Noon, an Arabic Large Language model based on BLOOM, a foundation model released by the bigscience workshop.; Noon was trained with the main focus of having a model that responds to various types of instructions and questions (text generation, code generation, mathematical problems, closed/open-book questions, etc.); We trained the model using the ColossalAI framework which fully supports the HuggingFace library models, and implements different optimization and quantization techniques for billion-scale LLMs.; The training data is a combination of Arabic datasets covering multiple tasks, more details are provided in the dataset section.; ?????? ??? ?? ????? ????? ""???""!"
WizardLM-30B-Uncensored-GGML,,,,other,,104,,0,297369.611,,https://huggingface.co/TheBloke/WizardLM-30B-Uncensored-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's 'uncensored' WizardLM 30B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
WizardLM-30B-Uncensored-GPTQ,Text Generation,Transformers,,other,,94,ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered,"2,430",17307.96548,,https://huggingface.co/TheBloke/WizardLM-30B-Uncensored-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Eric Hartford's WizardLM 30B Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
guanaco-7b,,,,,https://arxiv.org/pdf/2305.14314.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2304.07327.pdf,15,,0,640.0223956,1,https://huggingface.co/timdettmers/guanaco-7b,"| Paper | Code | Demo | ; The Guanaco models are open-source finetuned chatbots obtained through 4-bit QLoRA tuning of LLaMA base models on the OASST1 dataset. They are available in 7B, 13B, 33B, and 65B parameter sizes.; ??Guanaco is a model purely intended for research purposes and could produce problematic outputs.; Guanaco adapter weights are available under Apache 2 license. Note the use of the Guanaco adapter weights, requires access to the LLaMA model weighs. 
Guanaco is based on LLaMA and therefore should be used according to the LLaMA license. ; Here is an example of how you would load Guanaco 7B in 4-bits:"
falcon-40b-instruct-GPTQ,Text Generation,Transformers,English,apache-2.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf,183,tiiuae/falcon-refinedweb,"51,090",23042.79638,,https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains an experimantal GPTQ 4bit model for Falcon-40B-Instruct.; It is the result of quantising to 4bit using AutoGPTQ.; Please note this is an experimental GPTQ model. Support for it is currently quite limited.
mpt-7b-storysummarizer,Text Generation,PyTorch; Transformers,,apache-2.0,,13,emozilla/booksum-summary-analysis_gptneox-8192; kmfoda/booksum,363,13621.46844,,https://huggingface.co/emozilla/mpt-7b-storysummarizer,"This is a fine-tuned version of mosaicml/mpt-7b-storywriter intended for summarization and literary analysis of fiction stories.; The code for this model includes the adaptions from Birchlabs/mosaicml-mpt-7b-chat-qlora which allow MPT models to be loaded with device_map=""auto"" and load_in_8bit=True.
It also has the latest key-value cache MPT code to allow for fast inference with transformers (thus, use_cache is set to True in config.json).; or; Outputs on the text of Waystation City (6,287 tokens); temperature=0.6, repetition_penalty=1.04, top_p=0.95, top_k=50, do_sample=True, max_new_tokens=1024"
falcon-7b-openassistant-peft,Text Generation,PEFT,,apache-2.0,https://arxiv.org/pdf/2106.09685.pdf; https://arxiv.org/pdf/2305.14314.pdf,36,OpenAssistant/oasst1,938,38.20547382,,https://huggingface.co/dfurman/falcon-7b-openassistant-peft,"Falcon-7b-openassistant-peft is a chatbot model for dialogue generation. It was built by fine-tuning Falcon-7B on the OpenAssistant/oasst1 dataset. This repo only includes the LoRA adapters from fine-tuning with ?'s peft package. ; The model was fine-tuned in 8-bit precision using ? peft adapters, transformers, and bitsandbytes. Training relied on a method called ""Low Rank Adapters"" (LoRA), specifically the QLoRA variant. The run took approximately 6.25 hours and was executed on a workstation with a single A100-SXM NVIDIA GPU with 37 GB of available memory. See attached Colab Notebook for the code and hyperparams used to train the model. ; May 30, 2023; To prompt the chat model, use the following format:; Prompter:"
WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ,Text Generation,Transformers,,other,,54,,547,17307.96606,,https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Monero's WizardLM-Uncensored-SuperCOT-Storytelling-30B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
Replit-v2-CodeInstruct-3B,Text Generation,PyTorch; Transformers,code,cc-by-sa-4.0,,45,bigcode/the-stack-dedup; sahil2801/CodeAlpaca-20k; teknium/GPTeacher-CodeInstruct,995,10639.33032,5,https://huggingface.co/teknium/Replit-v2-CodeInstruct-3B,"Base Model: replit/replit-code-v1-3b; This is version 2 of the Replit Code Instruct fine tune model.; This model is fine tuned on both Sahil2801's CodeAlpaca & Teknium's GPTeacher Code-Instruct to give Replit's Code model instruct capabilities.; Try this model on it's HuggingFace demo Spaces: https://huggingface.co/spaces/teknium/Replit-v2-CodeInstruct-3B; Dataset links:
CodeAlpaca: https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k
GPTeacher subset - Code Instruct: https://github.com/teknium1/GPTeacher"
falcon-40b-sft-top1-560,Text Generation,PyTorch; Transformers,4 languages,apache-2.0,,38,OpenAssistant/oasst1,"8,871",85670.67281,3,https://huggingface.co/OpenAssistant/falcon-40b-sft-top1-560,"This model is a fine-tuning of TII's Falcon 40B LLM. 
It was trained with top-1 (high-quality) demonstrations of the OASST data set (exported on May 6, 2023) with an effective batch size of 144 for ~7.5 epochs with LIMA style dropout (p=0.3) and a context-length of 2048 tokens.; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; Model:"
gpt4all-falcon,Text Generation,PyTorch; Transformers,English,apache-2.0,,14,nomic-ai/gpt4all-j-prompt-generations,"2,163",28265.20511,2,https://huggingface.co/nomic-ai/gpt4all-falcon,"An Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions including word problems, multi-turn dialogue, code, poems, songs, and stories.; This model has been finetuned from Falcon; To download a model with a specific revision run ; Downloading without specifying revision defaults to main/v1.0.; To use it for inference with Cuda, run"
SantaCoder-1B,Text Generation,PyTorch; Safetensors; Transformers,code,openrail,,9,bigcode/the-stack,"1,505",4610.08919,,https://huggingface.co/TabbyML/SantaCoder-1B,"; Play with the model on the SantaCoder Space Demo.; This is the Megatron-version of SantaCoder.
We refer the reader to the SantaCoder model page for full documentation about this model; There are two versions (branches) of the model:; The model was trained on GitHub code. As such it is not an instruction model and commands like ""Write a function that computes the square root."" do not work well.
You should phrase commands like they occur in source code such as comments (e.g. # the following function computes the sqrt) or write a function signature and docstring and let the model complete the function body."
inswapper,,ONNX,,,,9,,0,554.0014453,,https://huggingface.co/deepinsight/inswapper,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
WizardLM-Uncensored-Falcon-40b,Text Generation,PyTorch; Transformers,,apache-2.0,,80,,"2,163",85660.46175,3,https://huggingface.co/ehartford/WizardLM-Uncensored-Falcon-40b,"This is WizardLM trained on top of tiiuae/falcon-40b, with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:
An uncensored model has no guardrails.
You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car. Publishing anything this model generates is the same as publishing it yourself. You are responsible for the content you publish, and you cannot blame the model any more than you can blame the knife, gun, lighter, or car for what you do with it.; Prompt format is WizardLM.; Thank you chirper.ai for sponsoring some of my compute!"
Nous-Hermes-13B-GGML,,,,other,,71,,0,118896.654,2,https://huggingface.co/TheBloke/Nous-Hermes-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for NousResearch's Nous-Hermes-13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; The model follows the Alpaca prompt format:"
Nous-Hermes-13B-GPTQ,Text Generation,Transformers,English,other,,133,,"11,007",7631.140514,1,https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for NousResearch's Nous-Hermes-13B.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; The model follows the Alpaca prompt format:
instructblip-vicuna-13b,Image-to-Text,PyTorch; Transformers,English,other,https://arxiv.org/pdf/2305.06500.pdf,17,,"1,154",58134.93168,3,https://huggingface.co/Salesforce/instructblip-vicuna-13b,InstructBLIP model using Vicuna-13b as language model. InstructBLIP was introduced in the paper InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning by Dai et al.; Disclaimer: The team releasing InstructBLIP did not write a model card for this model so this model card has been written by the Hugging Face team.; InstructBLIP is a visual instruction tuned version of BLIP-2. Refer to the paper for details.; ; Usage is as follows:
medguanaco-lora-65b-GPTQ,Text Generation,Transformers,English,cc,https://arxiv.org/pdf/2106.09685.pdf; https://arxiv.org/pdf/2303.14070.pdf,8,,0,336.0055392,,https://huggingface.co/nmitchko/medguanaco-lora-65b-GPTQ,"Model Description ; nmitchko/medguanaco-lora-65b-GPTQ is a large language model specifically fine-tuned for medical domain tasks.
It is based on the Guanaco LORA of LLaMA weighing in at 65B parameters. 
The primary goal of this model is to improve question-answering and medical dialogue tasks.
It was trained using LoRA and quantized, to reduce memory footprint. ; Steps to load this model:; The following README is taken from the source page medalpaca; The training data for this project was sourced from various resources. 
Firstly, we used Anki flashcards to automatically generate questions, 
from the front of the cards and anwers from the back of the card. 
Secondly, we generated medical question-answer pairs from Wikidoc. 
We extracted paragraphs with relevant headings, and used Chat-GPT 3.5 
to generate questions from the headings and using the corresponding paragraphs 
as answers. This dataset is still under development and we believe 
that approximately 70% of these question answer pairs are factual correct. 
Thirdly, we used StackExchange to extract question-answer pairs, taking the 
top-rated question from five categories: Academia, Bioinformatics, Biology, 
Fitness, and Health. Additionally, we used a dataset from ChatDoctor 
consisting of 200,000 question-answer pairs, available at https://github.com/Kent0n-Li/ChatDoctor."
falcon-40b-sft-mix-1226,Text Generation,PyTorch; Transformers,4 languages,apache-2.0,,30,OpenAssistant/oasst1; databricks/databricks-dolly-15k,"5,334",85670.67358,2,https://huggingface.co/OpenAssistant/falcon-40b-sft-mix-1226,"This model is a fine-tuning of TII's Falcon 40B LLM. 
It was trained on a mixture of OASST top-2 threads (exported on June 2, 2023), Dolly-15k and synthetic instruction datasets (see dataset configuration below).; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; Model:"
chronos-33b,Text Generation,PyTorch; Transformers,,other,,22,,187,66849.10937,3,https://huggingface.co/elinas/chronos-33b,"This is the fp16 PyTorch / HF version of chronos-33b - if you need another version, GGML and GPTQ versions are linked below.; This model is primarily focused on chat, roleplay, and storywriting, but can accomplish other tasks such as simple reasoning and coding.; Chronos generates very long outputs with coherent text, largely due to the human inputs it was trained on.; This model uses Alpaca formatting, so for optimal model performance, use:; GGML Version provided by @TheBloke"
open_llama_3b,Text Generation,PyTorch; Transformers,,apache-2.0,,85,togethercomputer/RedPajama-Data-1T,"82,668",7014.934677,8,https://huggingface.co/openlm-research/open_llama_3b,"In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a 7B and 3B model trained on 1T tokens, as well as the preview of a 13B model trained on 600B tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we’ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.; The model can be evaluated with lm-eval-harness. However, due to the aforementioned tokenizer issue, we need to avoid using the fast tokenizer to obtain the correct results. This can be achieved by passing in use_fast=False to this part of lm-eval-harness, as shown in the example below:"
Photon_v1,Text-to-Image,Diffusers,,other,,8,,"3,920",2181.122305,2,https://huggingface.co/digiplay/Photon_v1,Model info: https://civitai.com/models/84728/photon; 
falcon-7b-instruct-sharded,Text Generation,Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2306.01116.pdf,10,tiiuae/falcon-refinedweb,"15,069",28345.70302,,https://huggingface.co/vilsonrodrigues/falcon-7b-instruct-sharded,"Resharded version of https://huggingface.co/tiiuae/falcon-7b-instruct for low RAM enviroments (e.g. Colab, Kaggle) in safetensors; Tutorial: https://medium.com/@vilsonrodrigues/run-your-private-llm-falcon-7b-instruct-with-less-than-6gb-of-gpu-using-4-bit-quantization-ff1d4ffbabcc; Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ? To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF!"
Nous-Hermes-13b-Chinese,Text Generation,PyTorch; Transformers,Chinese; English,,,10,,49,27044.61608,,https://huggingface.co/Bandifishing/Nous-Hermes-13b-Chinese,将Chinese-alpaca-13b的Lora权重与Nous-Hermes-13b合并，得到支持中文的Hermes-13b模型。; Chinese-alpaca-13b:https://huggingface.co/ziqingyang/chinese-alpaca-lora-13b; Nous-Hermes-13b:https://huggingface.co/NousResearch/Nous-Hermes-13b; 仅供学术研究使用，使用时请遵守相应开源协议。
Nous-Hermes-13b-Chinese-GGML,Text Generation,Transformers,Chinese; English,apache-2.0,,14,,0,84408.32262,,https://huggingface.co/coyude/Nous-Hermes-13b-Chinese-GGML,"原始模型：https://huggingface.co/NousResearch/Nous-Hermes-13b ; lora：https://huggingface.co/ziqingyang/chinese-alpaca-lora-13b ; 将Nous-Hermes-13b与chinese-alpaca-lora-13b进行合并，增强模型的中文能力，不过存在翻译腔; 使用项目：
https://github.com/ymcui/Chinese-LLaMA-Alpacahttps://github.com/ggerganov/llama.cpp; 推荐q5_k_m或q4_k_m   该仓库模型均为ggmlv3模型 "
majicMIX_realistic_v6,Text-to-Image,Diffusers,,other,,13,,"3,259",2457.602186,,https://huggingface.co/digiplay/majicMIX_realistic_v6,Model info :; https://civitai.com/models/43331?modelVersionId=94640
mms-lid-126,Audio Classification,PyTorch; Safetensors; Transformers,158 languages,cc-by-nc-4.0,https://arxiv.org/pdf/2305.13516.pdf,5,google/fleurs,"4,533",7905.293472,1,https://huggingface.co/facebook/mms-lid-126,"This checkpoint is a model fine-tuned for speech language identification (LID) and part of Facebook's Massive Multilingual Speech project.
This checkpoint is based on the Wav2Vec2 architecture and classifies raw audio input to a probability distribution over 126 output classes (each class representing a language).
The checkpoint consists of 1 billion parameters and has been fine-tuned from facebook/mms-1b on 126 languages.; This MMS checkpoint can be used with Transformers to identify
the spoken language of an audio. It can recognize the following 126 languages.; Let's look at a simple example.; First, we install transformers and some other libraries; Note: In order to use MMS you need to have at least transformers >= 4.30 installed. If the 4.30 version
is not yet available on PyPI make sure to install transformers from 
source:"
WizardCoder-15B-1.0-GGML,,Transformers,,bigcode-openrail-m,,87,,429,71782.417,7,https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for WizardLM's WizardCoder 15B 1.0.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools known to work with these model files.; Matt Hoeffner has put up a live Space with a demo of this model:"
controlnet_qrcode-control_v11p_sd21,Image-to-Image,Diffusers,English,openrail++,,34,,"5,975",5942.129243,6,https://huggingface.co/DionTimmer/controlnet_qrcode-control_v11p_sd21,"; This repo holds the safetensors & diffusers versions of the QR code conditioned ControlNet for Stable Diffusion v2.1.
The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs. However, a 1.5 version model was also trained on the same dataset for those who are using the older version.; These models perform quite well in most cases, but please note that they are not 100% accurate. In some instances, the QR code shape might not come through as expected. You can increase the ControlNet weight to emphasize the QR code shape. However, be cautious as this might negatively impact the style of your output.To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).; To balance between style and shape, a gentle fine-tuning of the control weight might be required based on the individual input and the desired output, aswell as the correct prompt. Some prompts do not work until you increase the weight by a lot. The process of finding the right balance between these factors is part art and part science. For the best results, it is recommended to generate your artwork at a resolution of 768. This allows for a higher level of detail in the final product, enhancing the quality and effectiveness of the QR code-based artwork.; The simplest way to use this is to place the .safetensors model and its .yaml config file in the folder where your other controlnet models are installed, which varies per application. 
For usage in auto1111 they can be placed in the webui/models/ControlNet folder. They can be loaded using the controlnet webui extension which you can install through the extensions tab in the webui (https://github.com/Mikubill/sd-webui-controlnet). Make sure to enable your controlnet unit and set your input image as the QR code. Set the model to either the SD2.1 or 1.5 version depending on your base stable diffusion model, or it will error. No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail.
Make sure to look up additional info on how to use controlnet if you get stuck, once you have the webui up and running its really easy to install the controlnet extension aswell."
roop,,ONNX,,,,10,,0,1932,5,https://huggingface.co/henryruhs/roop,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
baichuan-vicuna-7B-GGML,,,,other,https://arxiv.org/pdf/2306.04751.pdf,14,,0,64512.02453,,https://huggingface.co/TheBloke/baichuan-vicuna-7B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Fire Balloon's Baichuan Vicuna 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
airoboros-13b-gpt4-1.4,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,,15,jondurbin/airoboros-gpt4-1.4,"2,537",53320.34312,,https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.4,"update 2023-06-25 - re-uploaded with a slightly earlier checkpoint, which seems perhaps a little less overfit than the full 3-epochs version initially uploaded; This is a full (not qlora) fine-tune 13b parameter LlaMa model, using completely synthetic training data created gpt4 via https://github.com/jondurbin/airoboros; This is mostly an extension of the previous gpt-4 series, with a few extras:; This model was fine-tuned with a fork of FastChat; The prompt it was trained with was:"
airoboros-7b-gpt4-1.4,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,,5,jondurbin/airoboros-gpt4-1.4,175,27597.42522,,https://huggingface.co/jondurbin/airoboros-7b-gpt4-1.4,"mostly untested, use if you want, or wait for some validation; This is a full (not qlora) fine-tune 7b parameter LlaMa model, using completely synthetic training data created gpt4 via https://github.com/jondurbin/airoboros; This is mostly an extension of the previous gpt-4 series, with a few extras:; This model was fine-tuned with a fork of FastChat; The prompt it was trained with was:"
airoboros-7B-gpt4-1.4-GGML,,,,other,,10,,0,61870.09936,,https://huggingface.co/TheBloke/airoboros-7B-gpt4-1.4-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Jon Durbin's Airoboros 7B GPT4 1.4.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
LLaMA_65B_0bit,,,,mit,,5,,0,0.00258625,,https://huggingface.co/qq67878980/LLaMA_65B_0bit,"Converted with https://github.com/notepad-plus-plus/notepad-plus-plus All models tested on A100-80G *Conversion may require lot of RAM, LLaMA-7b takes ~0 GB, 13b around 0 GB, 30b around 0 and 65b takes more than 0 GB of RAM.; Installation instructions as mentioned in above repo:; Additional training was done on the MSPaint_Blank dataset and 2,000,000T+ tokens on 50,000+ blank notepad files.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
PULSE-7bv5,Text Generation,PyTorch; Transformers,Chinese,agpl-3.0,,16,,271,16603.3381,,https://huggingface.co/OpenMEDLab/PULSE-7bv5,"
; 由于模型参数量较小和自回归生成范式，尽管模型提供了有关疾病诊断和治疗的推理结果，但这些结果不能代替线下职业医生的建议和治疗方案。所有回答仅供参考，不应作为诊断或治疗的依据。我们强烈建议用户在需要诊断或治疗疾病时，寻求专业医生的帮助和建议。; 下表提供了一个batch size=1时本地部署PULSE进行推理所需的显存大小。; 其中torch和transformers版本不建议低于推荐版本。; Gradio"
UltraLM-13b,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2305.14233.pdf,60,stingning/ultrachat,236,53322.04567,1,https://huggingface.co/openbmb/UltraLM-13b,"This is UltraLM-13b delta weights, a chat language model trained upon UltraChat; The model is fine-tuned based on LLaMA-13b with a multi-turn chat-format template as below; To use this model, you need to recover the full model from the delta weights and perform inference following the template below:"
mpt-30b-peft-compatible,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2010.04245.pdf; https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2302.06675.pdf,7,allenai/c4; mc4; togethercomputer/RedPajama-Data-1T; bigcode/the-stack-dedup; allenai/s2orc,"1,770",61363.15159,,https://huggingface.co/eluzhnica/mpt-30b-peft-compatible,"This is the MPT-30B but with added support to finetune using peft (tested with qlora). It is not finetuned further, the weights are the same as the original MPT-30b.; I have not traced through the whole huggingface stack to see if this is working correctly but it does finetune with qlora and outputs are reasonable.
Inspired by implementations here https://huggingface.co/cekal/mpt-7b-peft-compatible/commits/main
https://huggingface.co/mosaicml/mpt-7b/discussions/42.; The original description for MosaicML team below:; MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.
This model was trained by MosaicML.; MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference."
Chronos-Hermes-13B-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,,20,,967,7631.183488,,https://huggingface.co/TheBloke/Chronos-Hermes-13B-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Austism's Chronos Hermes 13B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
Pygmalion-13B-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,,36,,"3,542",7631.186771,,https://huggingface.co/TheBloke/Pygmalion-13B-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for TehVenom's merge of PygmalionAI's Pygmalion 13B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
xgen-7b-8k-inst,Text Generation,PyTorch; Transformers,,,,78,,"982,183",28252.19997,4,https://huggingface.co/Salesforce/xgen-7b-8k-inst,"Official research release for the family of XGen models (7B) by Salesforce AI Research:; Title: Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length; Authors: Erik Nijkamp*, Tian Xie*, Hiroaki Hayashi*, Bo Pang*, Congying Xia*, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovs'ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Rayhan Joty, Caiming Xiong.; (* indicates equal contribution); Correspondence to: Shafiq Rayhan Joty, Caiming Xiong"
longchat-7b-16k,Text Generation,PyTorch; Transformers,,,,35,,"11,767",13804.03943,1,https://huggingface.co/lmsys/longchat-7b-16k,"Please use load_model from FastChat or LongChat repo to load the model (or chatting API from FastChat). There is a monkey patch needed to use the model.
Usage referece:; (LongChat) python3 eval.py --model-name-or-path  lmsys/longchat-7b-16k --task topics ; (FastChat) python3 -m fastchat.serve.cli --model-path lmsys/longchat-7b-16k; Under the hood, the monkey patch is added in:; https://github.com/lm-sys/FastChat/blob/da0641e567cf93756b0978ab5a6b092e96f06240/fastchat/model/model_adapter.py#L429"
WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,,19,,"2,320",17307.98422,,https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Monero's WizardLM Uncensored SuperCOT Storytelling 30B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
airoboros-65B-gpt4-1.4-GPTQ,Text Generation,Transformers,,other,,10,,383,34306.35064,,https://huggingface.co/TheBloke/airoboros-65B-gpt4-1.4-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Jon Durbin's Airoboros 65B GPT4 1.4.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GGML,,,,other,,12,,0,122880.0107,,https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Monero's WizardLM Uncensored SuperCOT Storytelling 30B.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:"
Chinese-Alpaca-33B-SuperHOT-8K-GGML,,,,other,,5,,0,123699.2119,,https://huggingface.co/TheBloke/Chinese-Alpaca-33B-SuperHOT-8K-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Minlik's Chinese Alpaca 33B Merged.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:"
vicuna-33B-GPTQ,Text Generation,Transformers,,other,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,8,,"1,169",17307.94314,,https://huggingface.co/TheBloke/vicuna-33B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for LmSys' Vicuna 33B 1.3.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
Vicuna-33B-1-3-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,21,,"1,435",17307.98438,,https://huggingface.co/TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for LmSys' Vicuna 33B 1.3 (final) merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
OPI_full_Galactica-6.7B,Text Generation,PyTorch; Transformers,English,apache-2.0,,3,BAAI/OPI,0,27553.53556,,https://huggingface.co/BAAI/OPI_full_Galactica-6.7B,"; This repo is for the Open Protein Instructions (OPI) project, aiming to build and release a protein instruction dataset as well as propose to explore and benckmark LLMs for protein modeling in protein biology.
For more details of training and testing, please visit https://github.com/baaihealth/opi.; ; Usage and License Notices: LLaMA and Galactica are intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes. The weight diff for Stanford Alpaca is also CC BY NC 4.0 (allowing only non-commercial use).; For OPI-instruction tuning, we adopt the training script of Stanford Alpaca. "
SadTalker-V002rc,,,,mit,,4,,0,1942.001761,5,https://huggingface.co/vinthony/SadTalker-V002rc,"The new released model of https://github.com/OpenTalker/SadTalker.; The file of https://huggingface.co/vinthony/SadTalker-V002rc/blob/main/epoch_00190_iteration_000400000_checkpoint.pt comes from https://github.com/RenYurui/PIRender.; Thanks for their wonderful work!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
rvc_models_that_Pika_use,,,,,,11,,0,28226.08205,,https://huggingface.co/pika724/rvc_models_that_Pika_use,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
zeroscope_v2_1111models,,,,cc-by-nc-4.0,,11,,0,2672.642881,,https://huggingface.co/cerspense/zeroscope_v2_1111models,"
example outputs (courtesy of dotsimulate); A collection of watermark-free Modelscope-based video models capable of generating high quality video at 448x256, 576x320 and 1024 x 576. These models were trained from the original weights with offset noise using 9,923 clips and 29,769 tagged frames.
This collection makes it easy to switch between models with the new dropdown menu in the 1111 extension.; Simply download the contents of this repo to 'stable-diffusion-webui\models\text2video'
Or, manually download the model folders you want, along with VQGAN_autoencoder.pth.; Thanks to dotsimulate for the config files.; Thanks to camenduru, kabachuha, ExponentialML, VANYA, polyware, tin2tin"
nsql-6B,Text Generation,PyTorch; Transformers,,bsd-3-clause,,25,,"1,191",29054.24645,1,https://huggingface.co/NumbersStation/nsql-6B,"NSQL is a family of autoregressive open-source large foundation models (FMs) designed specifically for SQL generation tasks.; The checkpoint included in this repository is based on CodeGen-Multi 6B from Salesforce and further pre-trained on a dataset of general SQL queries and then fine-tuned on a dataset composed of text-to-SQL pairs.; The general SQL queries are the SQL subset from The Stack, containing 1M training samples. The labeled text-to-SQL pairs come from more than 20 public sources across the web from standard datasets. We hold out Spider and GeoQuery datasets for use in evaluation.; We evaluate our models on two text-to-SQL benchmarks: Spider and GeoQuery.; NSQL was trained using cross-entropy loss to maximize the likelihood of sequential inputs. For finetuning on text-to-SQL pairs, we only compute the loss over the SQL portion of the pair. The family of models is trained using 80GB A100s, leveraging data and model parallelism. We pre-trained for 3 epochs and fine-tuned for 10 epochs."
shap-e,Text-to-Image,Diffusers,,mit,https://arxiv.org/pdf/2305.02463.pdf,8,,"1,465",0.005680923,7,https://huggingface.co/openai/shap-e,"Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in Shap-E: Generating Conditional 3D Implicit Functions by Heewoo Jun and Alex Nichol from OpenAI. ; Original repository of Shap-E can be found here: https://github.com/openai/shap-e. ; The authors of Shap-E didn't author this model card. They provide a separate model card here.; The abstract of the Shap-E paper:; We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at this https URL."
falcon-40b-sft-mix-1226-GGML,,Transformers,4 languages,apache-2.0,,9,OpenAssistant/oasst1; databricks/databricks-dolly-15k,1,279244.8114,,https://huggingface.co/TheBloke/falcon-40b-sft-mix-1226-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGCC format model files for Open Assistant's Falcon 40B SFT MIX.; These files will not work in llama.cpp, text-generation-webui or KoboldCpp.; GGCC is a new format created in a new fork of llama.cpp that introduced this new Falcon GGML-based support: cmp-nc/ggllm.cpp."
replit-openorca,Text Generation,PyTorch; Transformers,,,,5,Open-Orca/OpenOrca,360,5325.616556,1,https://huggingface.co/matorus/replit-openorca,replit/replit-code-v1-3b finetuned on Open-Orca/OpenOrca.
genz-7b,Text Generation,PyTorch; Transformers,,apache-2.0,,4,,44,0,,https://huggingface.co/budecosystem/genz-7b,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; The most capable commercially usable Instruct Finetuned LLM yet with 8K input token length, latest information & better coding.; Use following prompt template; Check the GitHub for the code -> GenZ"
codegen25-7b-mono,Text Generation,PyTorch; Transformers,code,apache-2.0,https://arxiv.org/pdf/2305.02309.pdf,15,bigcode/starcoderdata,"1,561",28252.20218,,https://huggingface.co/Salesforce/codegen25-7b-mono,"Title: CodeGen2.5: Small, but mighty; Authors: Erik Nijkamp*, Hiroaki Hayashi*, Yingbo Zhou, Caiming Xiong; (* equal contribution); CodeGen2.5 is a family of autoregressive language models for program synthesis.; Building upon CodeGen2, the model is trained on StarCoderData for 1.4T tokens, achieving competitive results compared to StarCoderBase-15.5B with less than half the size."
Vicuna-7B-v1-3-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,3,,14,4096.545248,,https://huggingface.co/TheBloke/Vicuna-7B-v1-3-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for LmSys' Vicuna 7B v1.3 merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
13B-BlueMethod,Text Generation,PyTorch; Transformers,,,,4,,11,33751.00555,,https://huggingface.co/CalderaAI/13B-BlueMethod,"BlueMethod is a bit of a convoluted experiment in tiered merging.
Furthering the experimental nature of the merge, the models combined
were done so with a custom script that randomized the percent of each
layer merged from one model to the next. This is a warmup for a larger
project.
[Tier One and Two Merges not released; internal naming convention]; Tier One Merges:; 13B-Metharme+13B-Nous-Hermes=13B-Methermes; 13B-Vicuna-cocktail+13B-Manticore=13B-Vicortia; 13B-HyperMantis+13B-Alpacino=13B-PsychoMantis"
BanglaConformer,Automatic Speech Recognition,NeMo,Bengali,mit,https://arxiv.org/pdf/2305.09688.pdf,3,,166,510.0057422,,https://huggingface.co/bengaliAI/BanglaConformer,"Conformer-CTC model trained on the OOD-Speech dataset to transcribe speech from Bangla audio. This is a large variant of the model, with ~121M parameters. To know more about the model architecture see the NeMo Documentation here.;  The training split contains 1100+ hours of audio data crowdsoruced from native Bangla speakers. We trained on this split for 164 epochs , then the model was evaluated on23+ hours of audio across 17 diverse domains .; The model can be used as a pretrained checkpoint for inference or for fine-tuning on another dataset through the NVIDIA NeMo toolkit. It is recommended to install the toolkit, after installing the pyTorch package. ; After installing the required dependencies, download the .nemo file or the pretrained model to your local directory. you can instantiate the pretrained model like following: ; Prior to feeding the input audio to the pretrained model for training or inference, we need to resample the audio to 16KHz. We can achieve that using the sox library :"
Evol-Replit-v1,Text Generation,PyTorch; Transformers,,cc-by-sa-4.0,,4,nickrosh/Evol-Instruct-Code-80k-v1,7,10642.81593,,https://huggingface.co/nickrosh/Evol-Replit-v1,"This model uses the Evol-Instruct-Code-80k-v1 dataset generated using the Evol-Teacher repo. Currently, WizardCoder is one the most performant Code Generation models, being beaten only by ChatGPT. This takes the Code Alpaca 20k dataset and evolves each instruction through a randomly chosen evolution prompt to increase instruction complexity. These prompts range from increase time/space complexity, to increasing requirements, to adding erroneus code to improve robustness, etc. This is done three times with pruning and post processing to remove unwanted instructions and responses. The iterative addition of more complexity gives higher quality and more in-depth instructions than what is ususally generated in Alpaca methods. This, like in the case of WizardCoder and WizardLM, can lead to strong performance that gets very close to RLHF model performance.; This model uses ReplitLM fine tuned with the following parameters:"
sdxl-0.9-usage,Text-to-Image,Diffusers,English,apache-2.0,,9,,0,0.020298347,,https://huggingface.co/NathMath/sdxl-0.9-usage,"This repo is a tutorial intended to help beginners use the new released model, stable-diffusion-xl-0.9 in ComfyUI, with both the base and refiner models together to achieve a magnificent quality of image generation.; With usable demo interfaces for ComfyUI to use the models (see below)!; Here is a full tutorial to use stable-diffusion-xl-0.9 FROM ZERO!; When you have loaded into the ComfyUI, it will display this default interface shown below:
; Then, find out the ""load"" button on a floating panel, click, and choose the "".json"" file download from THIS REPO, it will quickly load a friendly interface to use the stable-diffusion-xl-0.9 series.
"
Slider,,,Japanese,creativeml-openrail-m,,5,,0,128.1672355,,https://huggingface.co/TomyAI/Slider,"乳首の色、サイズ、おっぱいの寄り具合、高さを{整するスライダ`LoRAです。
eにダウンロ`ドするかOppaiSliderPack.zipをダウンロ`ドして解訾筏皮ださい。; ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Emu,,,,,https://arxiv.org/pdf/2307.05222.pdf,13,,0,58265.6044,,https://huggingface.co/BAAI/Emu,"Quan Sun1*, Qiying Yu2,1*, Yufeng Cui1*, Fan Zhang1*, Xiaosong Zhang1*, Yueze Wang1, Hongcheng Gao1, Jingjing Liu2, Tiejun Huang1,3, Xinlong Wang1; 1 BAAI, 2 THU, 3 PKU * Equal Contribution; |  Paper | Demo(tmp) |; Emu is a Large Multimodal Model (LMM) trained with a unified autoregressive objective, i.e., predict-the-next-element, including both visual embeddings and textual tokens. Trained under this objective, Emu can serve as a generalist interface for diverse multimodal tasks, such as image captioning, image/video question answering, and text-to-image generation, together with new abilities like in-context text and image generation, and image blending.; Clone the github repository and install required packages:"
brav6,Text-to-Image,Diffusers,,creativeml-openrail-m,,3,,358,0.004404373,,https://huggingface.co/stablediffusionapi/brav6,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""brav6""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Try model for free: Generate Images"
mblip-mt0-xl,Image-to-Text,PyTorch; Transformers,English; multilingual,mit,https://arxiv.org/pdf/2307.06930.pdf; https://arxiv.org/pdf/2301.12597.pdf,5,Gregor/mblip-train,90,17797.39054,,https://huggingface.co/Gregor/mblip-mt0-xl,"This is the model checkpoint for our work mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs.; mBLIP is a BLIP-2 model which consists of 3 sub-models: a Vision Transformer (ViT), a Query-Transformer (Q-Former) and a large language model (LLM).; The Q-Former and ViT have both been initialized by an English BLIP-2 checkpoint (blip2-flan-t5-xl) and then re-aligned 
to the multilingual LLM (mt0-xl) using a multilingual task mixture.;  ; This allows the model to be used for tasks like:"
AnimateDiff,,Diffusers,,,,7,,0,31374.84148,,https://huggingface.co/camenduru/AnimateDiff,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
WizardCoder-Guanaco-15B-V1.1,Text Generation,PyTorch; Transformers,English,apache-2.0,,3,guanaco,73,31747.26141,,https://huggingface.co/LoupGarou/WizardCoder-Guanaco-15B-V1.1,"The WizardCoder-Guanaco-15B-V1.1 is a language model that combines the strengths of the WizardCoder base model and the openassistant-guanaco dataset for finetuning. The openassistant-guanaco dataset was further trimmed to within 2 standard deviations of token size for input and output pairs and all non-english data has been removed to reduce training size requirements.; Version 1.1 showcases notable enhancements, employing a modified version of the previous openassistant-guanaco dataset. This dataset underwent a comprehensive revision, replacing every single answer with those generated by GPT-4.; The volume of the datasets has also been augmented by approximately 50%, with a particular focus on high school and abstract algebra. This expansion leveraged the combined capabilities of GPT-4 and GPT-3.5-Turbo. The initial evaluation of algebraic functions over 12 epochs indicated promising results from this enriched dataset. However, this is just the beginning; further refinements are in the pipeline, aiming to optimize the dataset quality and subsequently decrease the number of epochs required to achieve comparable results.; Considering the need to curtail memory consumption during training, this dataset was tailored to consist solely of English language questions and answers. Consequently, the model's performance in language translation may not be up to par. Nevertheless, the focus remains on enhancing the model's proficiency and efficiency within its defined scope.; This model is designed to be used for a wide array of text generation tasks that require understanding and generating English text. The model is expected to perform well in tasks such as answering questions, writing essays, summarizing text, translation, and more. However, given the specific data processing and finetuning done, it might be particularly effective for tasks related to English language question-answering systems."
ziya-llama-13b-medical-merged,Text Generation,PyTorch; Transformers,Chinese; English,apache-2.0,,5,,64,26809.62356,,https://huggingface.co/shibing624/ziya-llama-13b-medical-merged,基于LLaMA-13B的中英医疗问答模型; shibing624/ziya-llama-13b-medical-merged evaluate test data：; The overall performance of model on QA test:; 在中文开放测试集中的表现优异，继承了两方面的优势：1）微调训练的底座是Ziya-LLaMA-13B模型，是较强的中英文底座模型，2）微调使用的是高质量240万条中英文医疗指令数据集，和多种通用指令数据集，微调后的模型在医疗行业答复能力达到领先水平，在通用问题上的答复能力不弱于LLaMA-13B。; training args:
RedPajama-INCITE-Base-3B-v1-paraphrase-tone,Text Generation,PEFT,English,wtfpl,,3,,122,21.15580833,,https://huggingface.co/llm-toys/RedPajama-INCITE-Base-3B-v1-paraphrase-tone,"The togethercomputer/RedPajama-INCITE-Base-3B-v1 model finetuned for Paraphrasing and Changing the Tone of the input sentence(to casual/professional/witty). Training data was generated using gpt-35-turbo.; Look at the repo llm-toys for usage and other details.; Try in colab:


; ; The following bitsandbytes quantization config was used during training:"
ChromaFT,,,,,,7,,0,12247.04578,,https://huggingface.co/zzzAI19/ChromaFT,"This model was created through fine tuning.
There are two versions: an all-ages depiction-oriented version and an NSFW-enhanced version.; All-ages depiction-oriented version：ChromaFT_v1-pruned.safetensors; NSFW-enhanced version：ChromaNeoFT_v1-pruned.safetensors; (NEO：Nsfw Erotic Option); For sample illustrations, please visit my blog.
https://ai-drawing.net/en/2023/07/15/introducing-of-chroma-ft-v1-0/"
BadAnime_v1,Text-to-Image,Diffusers,,other,,6,,554,2181.122551,3,https://huggingface.co/digiplay/BadAnime_v1,Model info :; https://civitai.com/models/107703?modelVersionId=115852; Sample image I made thru Huggingface's API :; ; Original Author's DEMO image :
RedPajama-INCITE-Base-3B-v1-dialogue-summary-topic,Text Generation,PEFT,English,wtfpl,,3,,90,21.14681755,,https://huggingface.co/llm-toys/RedPajama-INCITE-Base-3B-v1-dialogue-summary-topic,"The togethercomputer/RedPajama-INCITE-Base-3B-v1 model finetuned for Summary and Topic generation from a dailogue. We use a sample of roughly 1000 data points from the
Dialogsum dataset for fine-tuning.; Look at the repo llm-toys for usage and other details.; Try in colab:


; ; The following bitsandbytes quantization config was used during training:"
BuboGPT-ckpt,,,,apache-2.0,,3,,0,747.0015111,,https://huggingface.co/magicr/BuboGPT-ckpt,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Unreal_V4.1,Text-to-Image,Diffusers,English,creativeml-openrail-m,,3,,436,0.003472099,2,https://huggingface.co/Meina/Unreal_V4.1,"MeinaUnreal objetive is to be able to do anime art with a 2.5d feeling.
( the VAE is already baked in the model ); For examples and prompts, please checkout: https://civitai.com/models/18798/meinaunreal
I have a discord server where you can post images that you generated, discuss prompt and/or ask for help.; https://discord.gg/XC9nGZNDUd If you like one of my models and want to support their updates; I've made a ko-fi page; https://ko-fi.com/meina where you can pay me a coffee <3; And a Patreon page; https://www.patreon.com/MeinaMix where you can support me and get acess to beta of my models!"
falcon-7b-paraphrase-tone-dialogue-summary-topic,Text Generation,PEFT,English,wtfpl,,3,,20,19.05810921,,https://huggingface.co/llm-toys/falcon-7b-paraphrase-tone-dialogue-summary-topic,"The tiiuae/falcon-7b model finetuned for Paraphrasing, Changing the Tone of the input sentence(to casual/professional/witty), 
Summary and Topic generation from a dialogue. Data for Paraphrasing and Changing the Tone was generated using gpt-35-turbo and a sample of roughly 1000 data points from the
Dialogsum dataset was used for Summary and Topic generation.; Look at the repo llm-toys for usage and other details.; Try in colab (you might need the pro version):


; ; The following bitsandbytes quantization config was used during training:"
lora_diffusion,Text-to-Image,Diffusers,,creativeml-openrail-m,,3,,233,0.003413506,2,https://huggingface.co/Andyrasika/lora_diffusion,"The model is created using the following steps:; Since this is only the first official release, I believe there are still many, many imperfections. 
Please provide feedback in time, and I will continuously make corrections, thank you!"
OnlyAnime_v2.3,Text-to-Image,Diffusers,,other,,3,,382,2608.602846,3,https://huggingface.co/digiplay/OnlyAnime_v2.3,"Model info:
https://civitai.com/models/105955/onlyanime; Sample image I made thru Huggingface's API :
; Original Author's DEMO images :; ; *This image using LORA file ? FilmVelvia3.safetensors 
you can also download here: "
HIMAWARI_v1,Text-to-Image,Diffusers,,other,,3,,404,5806.082903,3,https://huggingface.co/digiplay/HIMAWARI_v1,"Model info :; https://civitai.com/models/103018/himawari?modelVersionId=110254; More models from the Author: (he made a lot of useful LORAs, pls check it out ^^); https://civitai.com/user/KimTarou/models; Sample image I made thru Huggingface's API :"
Llama-2-7B-ggml,,,,,,3,,0,61870.09112,,https://huggingface.co/localmodels/Llama-2-7B-ggml,"From: https://huggingface.co/meta-llama/Llama-2-7b-hf; Quantized using an older version of llama.cpp and compatible with llama.cpp from May 19, commit 2d5db48.; Quantization methods compatible with latest llama.cpp from June 6, commit 2d43387.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
llama-2-7b-chat-guanaco-hf-4bit,Text Generation,TensorBoard; PyTorch,English,other,,3,,0,135.852984,,https://huggingface.co/quantumaikr/llama-2-7b-chat-guanaco-hf-4bit,"We are unlocking the power of large language models. Our latest version of Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly. ; This release includes model weights and starting code for pretrained and fine-tuned Llama language models ― ranging from 7B to 70B parameters.; This repository is intended as a minimal example to load Llama 2 models and run inference. For more detailed examples leveraging HuggingFace, see llama-recipes.; In order to download the model weights and tokenizer, please visit the Meta AI website and accept our License.; Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, do not use the 'Copy link address' option when you right click the URL. If the copied URL text starts with: https://download.llamameta.net, you copied it correctly. If the copied URL text starts with: https://l.facebook.com, you copied it the wrong way."
mlc-chat-Llama-2-7b-chat-hf-q3f16_1,,,,,,3,,0,1423.327014,,https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q3f16_1,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
mlc-chat-Llama-2-7b-chat-hf-q4f32_1,,,,,,3,,0,1500.432806,,https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f32_1,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Llama-2-70B-Chat-GPTQ,Text Generation,Transformers,,,,3,,215,36149.55265,,https://huggingface.co/localmodels/Llama-2-70B-Chat-GPTQ,"From: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta"
Llama-2-7B-bf16-sharded,Text Generation,PyTorch; Transformers,,,,3,,933,13480.3577,,https://huggingface.co/TinyPixel/Llama-2-7B-bf16-sharded,No model card; New: Create and edit this model card directly on the website!
Llama-2-70b-chat-hf,Text Generation,Safetensors; Transformers; PyTorch,English,,,3,,58,141284.8751,,https://huggingface.co/4bit/Llama-2-70b-chat-hf,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations."
llama-2-7b-guanaco-fp16,Text Classification,PyTorch; Transformers,English,,,3,,33,13805.87871,,https://huggingface.co/Mikael110/llama-2-7b-guanaco-fp16,This is a Llama-2 version of Guanaco. It was finetuned from the base Llama-7b model using the official training scripts found in the QLoRA repo. I wanted it to be as faithful as possible and therefore changed nothing in the training script beyond the model it was pointing to. The model prompt is therefore also the same as the original Guanaco model.; This repo contains the merged f16 model. The QLoRA adaptor can be found here.; A 13b version of the model can be found here.; Legal Disclaimer: This model is bound by the usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.
llava-v1-0719-336px-lora-vicuna-13b-v1.3,Text Generation,Transformers,,,,3,,0,511.5782145,,https://huggingface.co/liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3,"Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA-v1-0719-336px-LoRA-Vicuna-13B-v1.3 was trained in July 2023.; Paper or resources for more information:
https://llava-vl.github.io/; Non-commerical Use.; Where to send questions or comments about the model:
https://github.com/haotian-liu/LLaVA/issues"
falcon-7b-qlora-chat-airbird-v2,Question Answering,Transformers,English,apache-2.0,,3,,0,18.9020232,,https://huggingface.co/tostcorp/falcon-7b-qlora-chat-airbird-v2,Falcon-7b finetuned with Airbird Questions & Answers
FishMix_v1.1,Text-to-Image,Diffusers,,other,,3,,291,2181.123193,3,https://huggingface.co/digiplay/FishMix_v1.1,"Model info :; https://civitai.com/models/15745?modelVersionId=27424; Original Author's DEMO images :; 


"
Luna-AI-Llama2-Uncensored-FP16,Text Generation,PyTorch; Transformers,,cc-by-sa-4.0,,3,,56,13803.55047,,https://huggingface.co/Tap-M/Luna-AI-Llama2-Uncensored-FP16,"Model Description; “Tap-M/Luna-AI-Llama2-Uncensored” is a latest chat language model which is fine-tuned on llama2 7b with custom datasets of multiple rounds of chats between Human & AI. This model was fine-tuned by Tap, the creator of Luna AI. The result is an enhanced Uncensored Llama2 7b Chat model that rivals many Open Source Chat Models in performance across a variety of tasks.
This model stands out for its long responses, low hallucination rate, and absence of censorship mechanisms.; Model Training; The fine-tuning process was performed on an 8x a100 80GB machine. The model was trained almost entirely on synthetic outputs. This includes data from diverse sources which we included to create our custom dataset, it includes multiple rounds of chats between Human & AI.; Prompt Format"
llama-2-13b-guanaco-qlora,Text Classification,,English,,,3,,0,501.0029006,,https://huggingface.co/Mikael110/llama-2-13b-guanaco-qlora,"This is a Llama-2 version of Guanaco. It was finetuned from the base Llama-13b model using the official training scripts found in the QLoRA repo. I wanted it to be as faithful as possible and therefore changed nothing in the training script beyond the model it was pointing to. The model prompt is therefore also the same as the original Guanaco model.; This repo contains the QLoRA adapter. A merged f16 model can be found here.; A 7b version of the adapter can be found here.; Legal Disclaimer: This model is bound by the usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.; Unable to determine this model’s library. Check the
								docs 
.
							"
mlc-chat-Llama-2-70b-chat-hf-q4f16_1,,,,,,3,,0,3484.851559,,https://huggingface.co/mlc-ai/mlc-chat-Llama-2-70b-chat-hf-q4f16_1,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
chinese-alpaca-pro-33b-merged,Text Generation,PyTorch; Transformers,,,,3,,0,67379.99206,,https://huggingface.co/minlik/chinese-alpaca-pro-33b-merged,No model card; New: Create and edit this model card directly on the website!
meta_llama_2finetuned,Text Generation,PyTorch; TensorBoard; Transformers,,,,3,,1,0,,https://huggingface.co/Sakil/meta_llama_2finetuned,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		"
yayi-7b-llama2,Text Generation,PyTorch; Transformers,Chinese; English,,,3,,0,13804.03964,,https://huggingface.co/wenge-research/yayi-7b-llama2,雅意大模型在百万级人工构造的高质量领域数据上进行指令微调得到，训练数据覆盖媒体宣传、舆情分析、公共安全、金融风控、城市治理等五大领域，上百种自然语言指令任务。雅意大模型从预训练初始化权重到领域模型的迭代过程中，我们逐步增强了它的中文基础能力和领域分析能力，并增加了多轮对话和部分插件能力。同时，经过数百名用户内测过程中持续不断的人工反馈优化，我们进一步提升了模型性能和安全性。; 通过雅意大模型的开源为促进中文预训练大模型开源社区的发展，贡献自己的一份力量，通过开源，与每一位合作伙伴共建雅意大模型生态。; News: ? 雅意大模型已开源基于 LLaMA 2 的中文优化模型版本，探索适用于中文多领域任务的最新实践。; 详情请参考我们的 ?Github Repo。; Comming Soon~
Chinese-Llama-2-7b-4bit,Text Generation,PyTorch; Transformers,,llama2,,3,,0,13804.03817,,https://huggingface.co/soulteary/Chinese-Llama-2-7b-4bit,快速上手 & 使用，可以试试 soulteary/docker-llama2-chat/。; 相关博客：使用 Transformers 量化 Meta AI LLaMA2 中文版大模型; 基于中文 LLaMA2 7B 模型项目：LinkSoul-AI/Chinese-Llama-2-7b
MythoBoros-13B-GPTQ,Text Generation,Transformers,English,other,,3,,6,7436.583231,,https://huggingface.co/TheBloke/MythoBoros-13B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GPTQ model files for Gryphe's MythoBoros 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements."
MythoBoros-13B-GGML,,Transformers,English,other,,3,,0,119316.4932,,https://huggingface.co/TheBloke/MythoBoros-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for Gryphe's MythoBoros 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
Llama2-chat-13B-Chinese-50W,Text Generation,PyTorch; Transformers,Chinese; English,,,3,,0,27044.62804,,https://huggingface.co/RicardoLee/Llama2-chat-13B-Chinese-50W,"由于目前的LLama2-chat模型很难约束其以中文进行问题回复，因此该模型旨在提供一个能以中文进行问答的LLama2-chat 13B 模型供大家研究使用。; 该模型使用LLama2-chat 13B 作为基底模型，使用带embedding 和 LM head 的Lora训练方式训练。模型已完成参数合并，可直接使用。也可以手动将sft_lora_model同Llama2-chat 13B 进行合并。; 训练数据使用BELLE项目中采样的50万SFT数据进行SFT训练。; Since the LLama2-chat model struggles to confine its responses to Chinese language when prompted with Chinese questions, the primary objective of this model is to provide a LLama2-chat 13B model that can engage in question and answer interactions in Chinese.; The model utilizes LLama2-chat 13B as its base model and is trained using the Lora training approach with the embedding and LM head. The model has undergone the Lora param merge and is now ready for direct use. It is also possible to manually merge the ./sft_lora_model with the Llama2-chat 13B model to obtain the combined model. "
Llama2-base-7B-Chinese-50W-pre_release,Text Generation,PyTorch; Transformers,Chinese; English,,,3,,0,28201.7843,,https://huggingface.co/RicardoLee/Llama2-base-7B-Chinese-50W-pre_release,"在完成了Llama2-chat 7B Chinese 和 Llama2-chat 13B Chinese 的训练后，我非常好奇能否直接基于Llama2-base 系列直接进行SFT训练。这也是本模型仓库的初衷。; 但是在实际操作中，在用了原先chat模型的LoRA训练框架后，我发现基于Llama2 base的 LoRA 训练非常难以收敛，随时处于梯度爆炸的边缘。DeepSpeed 会频繁触发reduce scale 操作，最终scale太小越界导致训练崩溃。我遍历了LR 1e-5 - 2e-4，LoRA rank [4, 8, 64]，LoRA Alpha [1,4,8,16,32]，LoRA Dropout [0.05, 0.1] ，Warmup Ratio [0.01, 0.03, 0.05]等超参数，均无法稳定训练。因此，本模型重新回归了全参数SFT训练。其难以进行LoRA训练的原因还待分析。; 由于网上存在使用LoRA 在英文SFT数据集上基于Llama2-base 进行SFT训练成功的样例，因此我怀疑难以训练的原因可能是扩中文词表embedding导致训练难度大幅度提升。; 为了方便后来人一起分析，本模型仓库特地将训练的全部loss/LR信息附在Material中。; 训练数据使用BELLE项目中采样的50万SFT数据进行SFT训练。"
bwx-13B-hf,Text Generation,PyTorch; Transformers,Chinese,apache-2.0,,3,BAAI/COIG-PC,0,27331.34015,,https://huggingface.co/BlueWhaleX/bwx-13B-hf,"This is an experimental product that can be used to create new LLM bassed on Chinese language.; You can use the raw model for next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task.
Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.; Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.
It also inherits some of the bias of its dataset model.; Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.; Use the code below to get started with the model."
albert-base-v2,Fill-Mask,PyTorch; TensorFlow; JAX; Rust; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/1909.11942.pdf,55,bookcorpus; wikipedia,"7,588,276",330.4627857,20,https://huggingface.co/albert-base-v2,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model, as all ALBERT models, is uncased: it does not make a difference
between english and English.; Disclaimer: The team releasing ALBERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; ALBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the ALBERT model as inputs.; ALBERT is particular in that it shares its layers across its Transformer. Therefore, all layers have the same weights. Using repeating layers results in a small memory footprint, however, the computational cost remains similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same number of (repeating) layers."
bert-base-multilingual-cased,Fill-Mask,PyTorch; TensorFlow; JAX; Safetensors; Transformers,104 languages,apache-2.0,https://arxiv.org/pdf/1810.04805.pdf,194,wikipedia,"3,305,409",3248.860638,58,https://huggingface.co/bert-base-multilingual-cased,"Pretrained model on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective.
It was introduced in this paper and first released in
this repository. This model is case sensitive: it makes a difference
between english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion. This means
it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the languages in the training set that can then be used to
extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a
standard classifier using the features produced by the BERT model as inputs.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the model hub to look for
fine-tuned versions on a task that interests you."
roberta-large-mnli,Text Classification,PyTorch; TensorFlow; JAX; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/1907.11692.pdf; https://arxiv.org/pdf/1806.02847.pdf; https://arxiv.org/pdf/1804.07461.pdf; https://arxiv.org/pdf/1704.05426.pdf; https://arxiv.org/pdf/1508.05326.pdf; https://arxiv.org/pdf/1809.05053.pdf; https://arxiv.org/pdf/1910.09700.pdf,75,multi_nli; wikipedia; bookcorpus,"141,571",5849.734772,15,https://huggingface.co/roberta-large-mnli,"Model Description: roberta-large-mnli is the RoBERTa large model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) corpus. The model is a pretrained model on English language text using a masked language modeling (MLM) objective.; Use the code below to get started with the model. The model can be loaded with the zero-shot-classification pipeline like so:; You can then use this pipeline to classify sequences into any of the class names you specify. For example:; This fine-tuned model can be used for zero-shot classification tasks, including zero-shot sentence-pair classification (see the GitHub repo for examples) and zero-shot sequence classification.; The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model."
t5-large,Translation,PyTorch; TensorFlow; JAX; Safetensors; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/1805.12471.pdf; https://arxiv.org/pdf/1708.00055.pdf; https://arxiv.org/pdf/1704.05426.pdf; https://arxiv.org/pdf/1606.05250.pdf; https://arxiv.org/pdf/1808.09121.pdf; https://arxiv.org/pdf/1810.12885.pdf; https://arxiv.org/pdf/1905.10044.pdf; https://arxiv.org/pdf/1910.09700.pdf,78,c4,"1,253,184",12085.37346,130,https://huggingface.co/t5-large,"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-Large is the checkpoint with 770 million parameters. ; The developers write in a blog post that the model: "
gpt-neo-1.3B,Text Generation,PyTorch; JAX; Rust; Safetensors; Transformers,English,mit,,188,EleutherAI/pile,"150,568",21699.7915,108,https://huggingface.co/EleutherAI/gpt-neo-1.3B,"GPT-Neo 1.3B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 1.3B represents the number of parameters of this particular pre-trained model.; GPT-Neo 1.3B was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model.; This model was trained on the Pile for 380 billion tokens over 362,000 steps. It was trained as a masked autoregressive language model, using cross-entropy loss.; This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:"
opus-mt-en-fr,Translation,PyTorch; TensorFlow; JAX; Transformers,English; French,apache-2.0,,16,,"108,063",903.8862295,31,https://huggingface.co/Helsinki-NLP/opus-mt-en-fr,source languages: en; target languages: fr; OPUS readme: en-fr; dataset: opus; model: transformer-align
gpt-neo-small-portuguese,Text Generation,PyTorch; Transformers,,,,4,,66,528.5910394,,https://huggingface.co/HeyLucasLeao/gpt-neo-small-portuguese,"This is a finetuned version from GPT-Neo 125M by EletheurAI to Portuguese language. ; It was trained from 227,382 selected texts from a PTWiki Dump. You can found all the data from here: https://archive.org/details/ptwiki-dump-20210520; Every text was passed through a GPT2-Tokenizer with bos and eos tokens to separate them, with max sequence length that the GPT-Neo could support. It was finetuned using the default metrics of the Trainer Class, available on the Hugging Face library.; My true intention was totally educational, thus making available a Portuguese version of this model.; How to use"
roberta-large-ner-english,Token Classification,PyTorch; TensorFlow; ONNX; Safetensors; Transformers,English,mit,,49,conll2003,"241,991",5817.550886,4,https://huggingface.co/Jean-Baptiste/roberta-large-ner-english,"[roberta-large-ner-english] is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. 
Model was validated on emails/chat data and outperformed other models on this type of data specifically. 
In particular the model seems to work better on entity that don't start with an upper case.; Training data was classified as follow:; In order to simplify, the prefix B- or I- from original conll2003 was removed.
I used the train and test dataset from original conll2003 for training and the ""validation"" dataset for validation. This resulted in a dataset of size:; Model performances computed on conll2003 validation dataset (computed on the tokens predictions); On private dataset (email, chat, informal discussion), computed on word predictions:"
roberta_toxicity_classifier,Text Classification,PyTorch; Transformers,English,,https://arxiv.org/pdf/1907.11692.pdf,19,,"2,050",502.228382,1,https://huggingface.co/s-nlp/roberta_toxicity_classifier,"This model is trained for toxicity classification task. The dataset used for training is the merge of the English parts of the three datasets by Jigsaw (Jigsaw 2018, Jigsaw 2019, Jigsaw 2020), containing around 2 million examples. We split it into two parts and fine-tune a RoBERTa model (RoBERTa: A Robustly Optimized BERT Pretraining Approach) on it. The classifiers perform closely on the test set of the first Jigsaw competition, reaching the AUC-ROC of 0.98 and F1-score of 0.76.; Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.; "
CPM-Generate,Text Generation,PyTorch; TensorFlow; Transformers,Chinese,mit,https://arxiv.org/pdf/2012.00413.pdf,33,100GB Chinese corpus,436,21301.17335,,https://huggingface.co/TsinghuaAI/CPM-Generate,"CPM (Chinese Pre-trained Language Model) is a Transformer-based autoregressive language model, with 2.6 billion parameters and 100GB Chinese training data. To the best of our knowledge, CPM is the largest Chinese pre-trained language model, which could facilitate downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. [Project] [Model] [Paper]; The text generated by CPM is automatically generated by a neural network model trained on a large number of texts, which does not represent the authors' or their institutes' official attitudes and preferences. The text generated by CPM is only used for technical and scientific purposes. If it infringes on your rights and interests or violates social morality, please do not propagate it, but contact the authors and the authors will deal with it promptly.; We collect different kinds of texts in our pre-training, including encyclopedia, news, novels, and Q&A. The details of our training data are shown as follows.; Based on the hyper-parameter searching on the learning rate and batch size, we set the learning rate as 1.5×10?41.5\times10^{-4}1.5×10?4 and the batch size as 3,0723,0723,072, which makes the model training more stable. In the first version, we still adopt the dense attention and the max sequence length is 1,0241,0241,024. We will implement sparse attention in the future. We pre-train our model for 20,00020,00020,000 steps, and the first 5,0005,0005,000 steps are for warm-up. The optimizer is Adam. It takes two weeks to train our largest model using 646464 NVIDIA V100.; We evaluate CPM with different numbers of parameters (the details are shown above) on various Chinese NLP tasks in the few-shot (even zero-shot) settings. With the increase of parameters, CPM performs better on most datasets, indicating that larger models are more proficient at language generation and language understanding. We provide results of text classification, chinese idiom cloze test, and short text conversation generation as follows. Please refer to our paper for more detailed results."
longformer-base-4096,,PyTorch; TensorFlow; Rust; Transformers,English,apache-2.0,https://arxiv.org/pdf/2004.05150.pdf,86,,"2,832,199",2116.685454,5,https://huggingface.co/allenai/longformer-base-4096,"Longformer is a transformer model for long documents. ; longformer-base-4096 is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096. ; Longformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations.
Please refer to the examples in modeling_longformer.py and the paper for more details on how to set global attention.; If you use Longformer in your research, please cite Longformer: The Long-Document Transformer.; Longformer is an open-source project developed by the Allen Institute for Artificial Intelligence (AI2).
AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering."
KeyBART,Text2Text Generation,PyTorch; Transformers,,apache-2.0,,18,,555,1671.81558,4,https://huggingface.co/bloomberg/KeyBART,"KeyBART as described in ""Learning Rich Representations of Keyphrase from Text"" published in the Findings of NAACL 2022 (https://aclanthology.org/2022.findings-naacl.67.pdf), pre-trains a BART-based architecture to produce a concatenated sequence of keyphrases in the CatSeqD format.; We provide some examples on Downstream Evaluations setups and and also how it can be used for Text-to-Text Generation in a zero-shot setting.; Reported Results:; Reported Results:; Alternatively use the Hosted Inference API console provided in https://huggingface.co/bloomberg/KeyBART"
camembert-base,Fill-Mask,PyTorch; Transformers,French,,https://arxiv.org/pdf/1911.03894.pdf,3,,"2,271",445.7989687,1,https://huggingface.co/camembert/camembert-base,"CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. ; It is now available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data and pretraining data source domains. ; For further information or requests, please go to Camembert Website; CamemBERT was trained and evaluated by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, éric Villemonte de la Clergerie, Djamé Seddah and Beno?t Sagot.; If you use our work, please cite:"
sentence-camembert-large,Sentence Similarity,PyTorch; TensorFlow; Transformers,French,apache-2.0,https://arxiv.org/pdf/1908.10084.pdf,25,stsb_multi_mt,"14,412",2765.597661,3,https://huggingface.co/dangvantuan/sentence-camembert-large,"Sentence-CamemBERT-Large is the Embedding Model for French developed by La Javaness. The purpose of this embedding model is to represent the content and semantics of a French sentence in a mathematical vector which allows it to understand the meaning of the text-beyond individual words in queries and documents, offering a powerful semantic search.; The model is Fine-tuned using pre-trained facebook/camembert-large and
Siamese BERT-Networks with 'sentences-transformers' on dataset stsb; The model can be used directly (without a language model) as follows:; The model can be evaluated as follows on the French test data of stsb.; Test Result: 
The performance is measured using Pearson and Spearman correlation:"
bert-base-turkish-cased,,PyTorch; TensorFlow; JAX; Transformers,Turkish,mit,,32,,"22,370",1432.248737,4,https://huggingface.co/dbmdz/bert-base-turkish-cased,"In this repository the MDZ Digital Library team (dbmdz) at the Bavarian State
Library open sources a cased model for Turkish ?; BERTurk is a community-driven cased BERT model for Turkish.; Some datasets used for pretraining and evaluation are contributed from the
awesome Turkish NLP community, as well as the decision for the model name: BERTurk.; The current version of the model is trained on a filtered and sentence
segmented version of the Turkish OSCAR corpus,
a recent Wikipedia dump, various OPUS corpora and a
special corpus provided by Kemal Oflazer.; The final training corpus has a size of 35GB and 44,04,976,662 tokens."
tinyroberta-squad2,Question Answering,PyTorch; Safetensors; Transformers,English,cc-by-4.0,https://arxiv.org/pdf/1909.10351.pdf,44,squad_v2,"1,430,902",654.5932547,18,https://huggingface.co/deepset/tinyroberta-squad2,"This is the distilled version of the deepset/roberta-base-squad2 model. This model has a comparable prediction quality and runs at twice the speed of the base model.; Language model: tinyroberta-squad2Language: EnglishDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD 2.0Code: See an example QA pipeline on HaystackInfrastructure: 4x Tesla v100; This model was distilled using the TinyBERT approach described in this paper and implemented in haystack.
Firstly, we have performed intermediate layer distillation with roberta-base as the teacher which resulted in deepset/tinyroberta-6l-768d.
Secondly, we have performed task-specific distillation with deepset/roberta-base-squad2 as the teacher for further intermediate layer distillation on an augmented version of SQuADv2 and then with deepset/roberta-large-squad2 as the teacher for prediction layer distillation. ; Haystack is an NLP framework by deepset. You can use this model in a Haystack pipeline to do question answering at scale (over many documents). To load the model in Haystack:; Evaluated on the SQuAD 2.0 dev set with the official eval script."
mt5-small-sum-de-mit-v1,Summarization,PyTorch; Safetensors; Transformers,German,mit,,5,swiss_text_2019,211,2462.596445,,https://huggingface.co/deutsche-telekom/mt5-small-sum-de-mit-v1,"This is a German summarization model. It is based on the multilingual T5 model google/mt5-small. The special characteristic of this model is that, unlike many other models, it is licensed under a permissive open source license (MIT). Among other things, this license allows commercial use.; 
This model is provided by the One Conversation
team of Deutsche Telekom AG.; The training was conducted with the following hyperparameters:; The datasets were preprocessed as follows:; The summary was tokenized with the google/mt5-small tokenizer. Then only the records with no more than 94 summary tokens were selected."
kan-bayashi_ljspeech_vits,Text-to-Speech,ESPnet,English,cc-by-4.0,https://arxiv.org/pdf/1804.00015.pdf,148,ljspeech,"2,284",0.003322754,103,https://huggingface.co/espnet/kan-bayashi_ljspeech_vits,?? Imported from https://zenodo.org/record/5443814/; This model was trained by kan-bayashi using ljspeech/tts1 recipe in espnet.; or arXiv:
detr-resnet-50,Object Detection,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2005.12872.pdf,200,coco,"650,109",167.0110463,54,https://huggingface.co/facebook/detr-resnet-50,"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository. ; Disclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.; The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. ; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.; "
m2m100_418M,Text2Text Generation,PyTorch; Rust; Transformers,101 languages,mit,https://arxiv.org/pdf/2010.11125.pdf,101,,"248,327",3979.257621,23,https://huggingface.co/facebook/m2m100_418M,"M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation.
It was introduced in this paper and first released in this repository.; The model that can directly translate between the 9,900 directions of 100 languages.
To translate into a target language, the target language id is forced as the first generated token.
To force the target language id as the first generated token, pass the forced_bos_token_id parameter to the generate method.; Note: M2M100Tokenizer depends on sentencepiece, so make sure to install it before running the example.; To install sentencepiece run pip install sentencepiece; See the model hub to look for more fine-tuned versions."
tts_transformer-zh-cv7_css10,Text-to-Speech,Fairseq,Chinese,,https://arxiv.org/pdf/1809.08895.pdf; https://arxiv.org/pdf/2109.06912.pdf,67,common_voice; css10,"1,848",831.5411987,15,https://huggingface.co/facebook/tts_transformer-zh-cv7_css10,Transformer text-to-speech model from fairseq S^2 (paper/code):; See also fairseq S^2 example.
hubert-dementia-screening,Feature Extraction,JAX; Transformers,,,,3,,2,418.0086514,,https://huggingface.co/flax-community/hubert-dementia-screening,No model card; New: Create and edit this model card directly on the website!
all_datasets_v4_MiniLM-L6,Sentence Similarity,PyTorch; Sentence Transformers,English,,https://arxiv.org/pdf/2104.08727.pdf; https://arxiv.org/pdf/1810.09305.pdf; https://arxiv.org/pdf/2102.07033.pdf; https://arxiv.org/pdf/1904.06472.pdf,22,,"13,138",91.64247566,3,https://huggingface.co/flax-sentence-embeddings/all_datasets_v4_MiniLM-L6,"The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised 
contrastive learning objective. We used the pretrained 'MiniLM-L6-H384-uncased' model and fine-tuned in on a 
1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.; We developped this model during the 
Community week using JAX/Flax for NLP & CV, 
organized by Hugging Face. We developped this model as part of the project:
Train the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well
as intervention from Google’s Flax, JAX, and Cloud team member about efficient deep learning frameworks.; Our model is intented to be used as a sentence encoder. Given an input sentence, it ouptuts a vector which captures 
the sentence semantic information. The sentence vector may be used for information retrieval, clustering or sentence 
similarity tasks.; Here is how to use this model to get the features of a given text using SentenceTransformers library:; We use the pretrained 'MiniLM-L6-H384-uncased' which is a 6 layer version of 
'microsoft/MiniLM-L12-H384-uncased' by keeping only every second layer. 
Please refer to the model card for more detailed information about the pre-training procedure."
mt5-base,Text2Text Generation,PyTorch; TensorFlow; JAX; Transformers,102 languages,apache-2.0,https://arxiv.org/pdf/2010.11934.pdf,86,mc4,"42,162",7162.074367,5,https://huggingface.co/google/mt5-base,"Google's mT5; mT5 is pretrained on the mC4 corpus, covering 101 languages:; Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.; Note: mT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; Pretraining Dataset: mC4"
chinese-roberta-wwm-ext-large,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/1906.08101.pdf; https://arxiv.org/pdf/2004.13922.pdf,47,,"36,588",4004.213296,2,https://huggingface.co/hfl/chinese-roberta-wwm-ext-large,"For further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking. ; Pre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu; This repository is developed based on：https://github.com/google-research/bert; You may also interested in,; More resources by HFL: https://github.com/ymcui/HFL-Anthology"
ko-sroberta-multitask,Sentence Similarity,PyTorch; TensorFlow; Sentence Transformers; Transformers,Korean,,,26,,"60,963",886.7335748,1,https://huggingface.co/jhgan/ko-sroberta-multitask,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; KorSTS, KorNLI ?? ?????? ?? ??? ??? ??? ? KorSTS ?? ?????? ??? ?????."
manga-ocr-base,Image-to-Text,PyTorch; Transformers,Japanese,apache-2.0,,41,manga109s,"33,169",444.1018312,5,https://huggingface.co/kha-white/manga-ocr-base,"Optical character recognition for Japanese text, with the main focus being Japanese manga.; It uses Vision Encoder Decoder framework.; Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality
text recognition, robust against various scenarios specific to manga:; Code is available here."
t5-base-japanese-web,Text2Text Generation,PyTorch; Transformers,Japanese,apache-2.0,https://arxiv.org/pdf/1910.10683.pdf,13,mc4; wiki40b,751,990.7867343,,https://huggingface.co/megagonlabs/t5-base-japanese-web,"megagonlabs/t5-base-japanese-web is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.Training codes are available on GitHub.; The vocabulary size of this model is 32K.
8K version is also available.; We used following corpora for pre-training.; We used Japanese Wikipedia to train SentencePiece.; It took about 126 hours with TPU v3-8"
trocr-large-stage1,Image-to-Text,PyTorch; Transformers,,,https://arxiv.org/pdf/2109.10282.pdf,8,,"2,580",2491.763635,,https://huggingface.co/microsoft/trocr-large-stage1,"TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you."
t5-base-finetuned-wikiSQL,Text2Text Generation,PyTorch; JAX; Transformers,English,apache-2.0,https://arxiv.org/pdf/1910.10683.pdf,35,wikisql,"3,847",2111.341735,1,https://huggingface.co/mrm8488/t5-base-finetuned-wikiSQL,"Google's T5 fine-tuned on WikiSQL for English to SQL translation.; The T5 model was presented in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu in Here the abstract:; Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.; ; Dataset ID: wikisql from  Huggingface/NLP"
vit-age-classifier,Image Classification,PyTorch; Transformers,,,,27,fairface,"2,539,259",343.0026054,4,https://huggingface.co/nateraw/vit-age-classifier,A vision transformer finetuned to classify the age of a given person's face. 
dpt-large,Depth Estimation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2103.13413.pdf,79,,"166,271",1402.889637,144,https://huggingface.co/Intel/dpt-large,"Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. 
It was introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. 
DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.
; The model card has been written in combination by the Hugging Face team and Intel.; Here is how to use this model for zero-shot depth estimation on an image:; For more code examples, we refer to the documentation.; Table 1. Comparison to the state of the art on monocular depth estimation. We evaluate zero-shot cross-dataset transfer according to the
protocol defined in [30]. Relative performance is computed with respect to the original MiDaS model [30]. Lower is better for all metrics. (Ranftl et al., 2021)"
wenyanwen-ancient-translate-to-modern,Translation,PyTorch; Transformers,Chinese; Chinese,,,20,,124,961.3806678,2,https://huggingface.co/raynardj/wenyanwen-ancient-translate-to-modern,"This model translate Classical(ancient) Chinese to Modern Chinese, so I guess who's interested in the problemset can speak at least modern Chinese, hence... let me continue the documentation in Chinese; 这个模型已有做成应用， 【随无涯】是一个huggingface spaces + streamlit 的古文阅读应用（含海量书籍）， 可以在阅读时翻译
输入文言文， 可以是断句 或者 未断句的文言文， 模型会预测现代文的表述。 其他模型：; 从文言文到现代文的翻译器, 欢迎前往我的github文言诗词项目页面探讨、加?? ; 训练语料是就是九十多万句句对， 数据集链接?。 训练时source序列（古文序列）， 按照50%的概率整句去除所有标点符号。; 注意"
wenyanwen-chinese-translate-to-ancient,Translation,PyTorch; Transformers,Chinese; Chinese,apache-2.0,,21,,203,961.3798768,2,https://huggingface.co/raynardj/wenyanwen-chinese-translate-to-ancient,"This model translate modern Chinese to Classical Chinese, so I guess who's interested in the problemset can speak at least modern Chinese, so... let me continue the documentation in Chinese; 从现代文到文言文的翻译器, 欢迎前往github文言诗词项目页面:渊, 讨论&加?? ; 还有同款的?文言文到现代文模型，原文输入可以断句 也可以是未断句的哦; 训练语料是就是九十多万句句对， 数据集链接?。; 注意， 你必须将generate函数的eos_token_id设置为102就可以翻译出完整的语句， 不然翻译完了会有残留的语句(因为做熵的时候用pad标签=-100导致)。"
distiluse-base-multilingual-cased-v2,Sentence Similarity,PyTorch; TensorFlow; Sentence Transformers; Transformers,multilingual,apache-2.0,https://arxiv.org/pdf/1908.10084.pdf,75,,"66,315",1080.937326,269,https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2,"This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net; This model was trained by sentence-transformers. "
gtr-t5-large,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,English,apache-2.0,https://arxiv.org/pdf/2112.07899.pdf,24,,"2,057",672.1720226,6,https://huggingface.co/sentence-transformers/gtr-t5-large,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of sematic search.; This model was converted from the Tensorflow model gtr-large-1 to PyTorch. When using this model, have a look at the publication: Large Dual Encoders Are Generalizable Retrievers. The tfhub model and this PyTorch model can produce slightly different embeddings, however, when run on the same benchmarks, they produce identical results.; The model uses only the encoder from a T5-large model. The weights are stored in FP16.  ; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:"
paraphrase-mpnet-base-v2,Sentence Similarity,PyTorch; TensorFlow; Sentence Transformers; Transformers,,apache-2.0,https://arxiv.org/pdf/1908.10084.pdf,24,,"102,467",876.6882537,3,https://huggingface.co/sentence-transformers/paraphrase-mpnet-base-v2,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net"
emotion-recognition-wav2vec2-IEMOCAP,Audio Classification,speechbrain; PyTorch,English,apache-2.0,https://arxiv.org/pdf/2106.04624.pdf,39,iemocap,"76,820",378.5712413,4,https://huggingface.co/speechbrain/emotion-recognition-wav2vec2-IEMOCAP,"This repository provides all the necessary tools to perform emotion recognition with a fine-tuned wav2vec2 (base) model using SpeechBrain. 
It is trained on IEMOCAP training data.; For a better experience, we encourage you to learn more about
SpeechBrain. The model performance on IEMOCAP test set is:; This system is composed of an wav2vec2 model. It is a combination of convolutional and residual blocks. The embeddings are extracted using attentive statistical pooling. The system is trained with Additive Margin Softmax Loss.  Speaker Verification is performed using cosine distance between speaker embeddings.; The system is trained with recordings sampled at 16kHz (single channel).
The code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling classify_file if needed.; First of all, please install the development version of SpeechBrain with the following command:"
distilbart-cnn-12-6,Summarization,PyTorch; JAX; Rust; Transformers,English,apache-2.0,,143,cnn_dailymail; xsum,"553,894",4169.007068,108,https://huggingface.co/sshleifer/distilbart-cnn-12-6,This checkpoint should be loaded into BartForConditionalGeneration.from_pretrained. See the BART docs for more information.
sbert-base-chinese-nli,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/1909.05658.pdf,59,,"6,654",409.1117651,4,https://huggingface.co/uer/sbert-base-chinese-nli,"This is the sentence embedding model pre-trained by UER-py, which is introduced in this paper.; ChineseTextualInference is used as training data. ; The model is fine-tuned by UER-py on Tencent Cloud. We fine-tune five epochs with a sequence length of 128 on the basis of the pre-trained model chinese_roberta_L-12_H-768. At the end of each epoch, the model is saved when the best performance on development set is achieved.; Finally, we convert the pre-trained model into Huggingface's format:"
finbert-tone,Text Classification,PyTorch; TensorFlow; Transformers,English,,,94,,"1,212,563",878.2238088,20,https://huggingface.co/yiyanghkust/finbert-tone,"FinBERT is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens.; More technical details on FinBERT: Click Link; This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task. If you are simply interested in using FinBERT for financial tone analysis, give it a try.; If you use the model in your academic work, please cite the following paper:; Huang, Allen H., Hui Wang, and Yi Yang. ""FinBERT: A Large Language Model for Extracting Information from Financial Text."" Contemporary Accounting Research (2022)."
tapex-large-sql-execution,Table Question Answering,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2107.07653.pdf,7,,645,1670.449845,,https://huggingface.co/microsoft/tapex-large-sql-execution,"TAPEX was proposed in TAPEX: Table Pre-training via Learning a Neural SQL Executor by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. The original repo can be found here.; TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.; TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.; You can use the raw model for simulating neural SQL execution, i.e., employ TAPEX to execute a SQL query on a given table. However, the model is mostly meant to be fine-tuned on a supervised dataset. Currently TAPEX can be fine-tuned to tackle table question answering tasks and table fact verification tasks. See the model hub to look for fine-tuned versions on a task that interests you.; Here is how to use this model in transformers:"
xlm-roberta-base-conll2003-ner,Token Classification,PyTorch; Transformers,,mit,,2,conll2003,924,1158.820206,,https://huggingface.co/Yaxin/xlm-roberta-base-conll2003-ner,"This model is a fine-tuned version of xlm-roberta-base on the conll2003 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
tapex-base-finetuned-wtq,Table Question Answering,PyTorch; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/2107.07653.pdf,8,wikitablequestions,"4,553",1117.379806,,https://huggingface.co/microsoft/tapex-base-finetuned-wtq,"TAPEX was proposed in TAPEX: Table Pre-training via Learning a Neural SQL Executor by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. The original repo can be found here.; TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.; TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.; This model is the tapex-base model fine-tuned on the WikiTableQuestions dataset.; You can use the model for table question answering on complex questions. Some solveable questions are shown below (corresponding tables now shown):"
wav2vec2-large-robust-12-ft-emotion-msp-dim,Audio Classification,PyTorch; Transformers,English,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2203.07378.pdf,31,msp-podcast,"49,043",661.0072177,7,https://huggingface.co/audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim,"The model expects a raw audio signal as input and outputs predictions for arousal, dominance and valence in a range of approximately 0...1. In addition, it also provides the pooled states of the last transformer layer. The model was created by fine-tuning 
Wav2Vec2-Large-Robust on MSP-Podcast (v1.7). The model was pruned from 24 to 12 transformer layers before fine-tuning. An ONNX export of the model is available from doi:10.5281/zenodo.6221127. Further details are given in the associated paper and tutorial."
codegen-350M-mono,Text Generation,PyTorch; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2203.13474.pdf,55,,"236,397",800.3409549,17,https://huggingface.co/Salesforce/codegen-350M-mono,"CodeGen is a family of autoregressive language models for program synthesis from the paper: A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B).; The checkpoint included in this repository is denoted as CodeGen-Mono 350M in the paper, where ""Mono"" means the model is initialized with CodeGen-Multi 350M and further pre-trained on a Python programming language dataset, and ""350M"" refers to the number of trainable parameters.; This checkpoint (CodeGen-Mono 350M) was firstly initialized with CodeGen-Multi 350M, and then pre-trained on BigPython dataset. The data consists of 71.7B tokens of Python programming language. See Section 2.1 of the paper for more details.; CodeGen was trained using cross-entropy loss to maximize the likelihood of sequential inputs.
The family of models are trained using multiple TPU-v4-512 by Google, leveraging data and model parallelism.
See Section 2.3 of the paper for more details.; We evaluate our models on two code generation benchmark: HumanEval and MTPB. Please refer to the paper for more details."
layoutlmv3-base,,PyTorch; TensorFlow; ONNX; Transformers,English,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2204.08387.pdf,176,,"6,337,042",1506.328256,26,https://huggingface.co/microsoft/layoutlmv3-base,"Microsoft Document AI | GitHub; LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis.; LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking
Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei, ACM Multimedia 2022.; If you find LayoutLM useful in your research, please cite the following paper:; The content of this project itself is licensed under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0).
Portions of the source code are based on the transformers project.
Microsoft Open Source Code of Conduct"
bart-sci-definition,Text2Text Generation,PyTorch; Transformers,,,,3,,26,1669.127236,,https://huggingface.co/talaugust/bart-sci-definition,"This is a finetuned BART Large model from the paper:; ""Generating Scientific Definitions with Controllable Complexity"" ; By Tal August, Katharina Reinecke, and Noah A. Smith; Abstract: Unfamiliar terminology and complex language can present barriers to understanding science. Natural language processing stands to help address these issues by automatically defining unfamiliar terms. We introduce a new task and dataset for defining scientific terms and controlling the complexity of gen- erated definitions as a way of adapting to a specific reader’s background knowledge. We test four definition generation methods for this new task, finding that a sequence-to-sequence approach is most successful. We then explore the version of the task in which definitions are generated at a target complexity level. We in- troduce a novel reranking approach and find in human evaluations that it offers superior fluency while also controlling complexity, compared to several controllable generation baselines.; The model is finetuned on the task of generating definitions of scientific terms. We frame our task as generating an answer to the question “What is (are) X?” Along with the question, the model takes a support document of scientific abstracted related to the term being defined. "
opt-1.3b,Text Generation,PyTorch; TensorFlow; JAX; Transformers,English,other,https://arxiv.org/pdf/2205.01068.pdf; https://arxiv.org/pdf/2005.14165.pdf,98,,"125,507",8080.705655,49,https://huggingface.co/facebook/opt-1.3b,"OPT was first introduced in Open Pre-trained Transformer Language Models and first released in metaseq's repository on May 3rd 2022 by Meta AI.; Disclaimer: The team releasing OPT wrote an official model card, which is available in Appendix D of the paper. 
Content from this model card has been written by the Hugging Face team.; To quote the first two paragraphs of the official paper; Large language models trained on massive text collections have shown surprising emergent
capabilities to generate text and perform zero- and few-shot learning. While in some cases the public
can interact with these models through paid APIs, full model access is currently limited to only a
few highly resourced labs. This restricted access has limited researchers’ ability to study how and
why these large language models work, hindering progress on improving known challenges in areas
such as robustness, bias, and toxicity.; We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M
to 175B parameters, which we aim to fully and responsibly share with interested researchers. We train the OPT models to roughly match 
the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data
collection and efficient training. Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and
to bring more voices to the table in studying the impact of these LLMs. Definitions of risk, harm, bias, and toxicity, etc., should be articulated by the
collective research community as a whole, which is only possible when models are available for study."
entity-extraction,Token Classification,PyTorch; TensorBoard; Transformers,,apache-2.0,,9,conll2003; autoevaluate/conll2003-sample,580,266.9284776,,https://huggingface.co/autoevaluate/entity-extraction,"This model is a fine-tuned version of distilbert-base-uncased on the conll2003 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
bert-keyword-extractor,Token Classification,PyTorch; Transformers,English,apache-2.0,,10,,619,431.8708522,,https://huggingface.co/yanekyuk/bert-keyword-extractor,"This model is a fine-tuned version of bert-base-cased on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
DeBERTa-v3-large-mnli-fever-anli-ling-wanli,Zero-Shot Classification,PyTorch; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/2104.07179.pdf; https://arxiv.org/pdf/2111.09543.pdf,40,multi_nli; anli; fever; lingnli; alisawuffles/WANLI,"35,215",1751.123912,3,https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli,"This model was fine-tuned on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. This model is the best performing NLI model on the Hugging Face Hub as of 06.06.22 and can be used for zero-shot classification. It significantly outperforms all other large models on the ANLI benchmark.; The foundation model is DeBERTa-v3-large from Microsoft. DeBERTa-v3 combines several recent innovations compared to classical Masked Language Models like BERT, RoBERTa etc., see the paper; DeBERTa-v3-large-mnli-fever-anli-ling-wanli was trained on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. Note that SNLI was explicitly excluded due to quality issues with the dataset. More data does not necessarily make for better NLI models. ; DeBERTa-v3-large-mnli-fever-anli-ling-wanli was trained using the Hugging Face trainer with the following hyperparameters. Note that longer training with more epochs hurt performance in my tests (overfitting).; The model was evaluated using the test sets for MultiNLI, ANLI, LingNLI, WANLI and the dev set for Fever-NLI. The metric used is accuracy.
The model achieves state-of-the-art performance on each dataset. Surprisingly, it outperforms the previous state-of-the-art on ANLI (ALBERT-XXL) by 8,3%. I assume that this is because ANLI was created to fool masked language models like RoBERTa (or ALBERT), while DeBERTa-v3 uses a better pre-training objective (RTD), disentangled attention and I fine-tuned it on higher quality NLI data. "
yalm-100b,,TensorBoard,English; Russian,apache-2.0,,104,,0,0.001870193,,https://huggingface.co/yandex/yalm-100b,"https://github.com/yandex/YaLM-100B; YaLM 100B is a GPT-like neural network for generating and processing text. It can be used freely by developers and researchers from all over the world.; The model leverages 100 billion parameters. It took 65 days to train the model on a cluster of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources in both English and Russian.; Training details and best practices on acceleration and stabilizations can be found on Medium (English) and Habr (Russian) articles.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Randeng-Pegasus-523M-Summary-Chinese,Summarization,PyTorch; Transformers,Chinese,,https://arxiv.org/pdf/1912.08777.pdf; https://arxiv.org/pdf/2209.02970.pdf,34,,"2,367",1075.606762,1,https://huggingface.co/IDEA-CCNL/Randeng-Pegasus-523M-Summary-Chinese,"善于处理摘要任务，在数个中文摘要数据集上微调后的，中文版的PAGASUS-large。; Good at solving text summarization tasks, after fine-tuning on multiple Chinese text summarization datasets, Chinese PAGASUS-large.; 参考论文：PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization; 基于Randeng-Pegasus-523M-Chinese，我们在收集的7个中文领域的文本摘要数据集（约4M个样本）上微调了它，得到了summary版本。这7个数据集为：education, new2016zh, nlpcc, shence, sohu, thucnews和weibo。; Based on Randeng-Pegasus-523M-Chinese, we fine-tuned a text summarization version (summary) on 7 Chinese text summarization datasets, with totaling around 4M samples. The datasets include: education, new2016zh, nlpcc, shence, sohu, thucnews and weibo."
face-parsing,Image Segmentation,PyTorch; Safetensors; Transformers,English,cc0-1.0,,30,celebamaskhq,520,678.0034863,1,https://huggingface.co/jonathandinu/face-parsing,
nllb-200-distilled-1.3B,Translation,PyTorch; Transformers,196 languages,cc-by-nc-4.0,,44,flores-200,"40,925",5633.683618,20,https://huggingface.co/facebook/nllb-200-distilled-1.3B,"This is the model card of NLLB-200's distilled 1.3B variant.; Here are the metrics for that particular checkpoint.; ? Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.; ? We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.; ? In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety)."
ruDialoGPT-medium,Conversational,PyTorch; Transformers,Russian,mit,https://arxiv.org/pdf/2001.09977.pdf,23,,"1,738",1559.644793,,https://huggingface.co/tinkoff-ai/ruDialoGPT-medium,This generation model is based on sberbank-ai/rugpt3medium_based_on_gpt2. It's trained on large corpus of dialog data and can be used for buildning generative conversational agents; The model was trained with context size 3; On a private validation set we calculated metrics introduced in this paper: ; How to use:
text2image-prompt-generator,Text Generation,PyTorch; Transformers,English,cc-by-2.0,,207,succinctly/midjourney-prompts,"34,589",668.3419572,32,https://huggingface.co/succinctly/text2image-prompt-generator,"This is a GPT-2 model fine-tuned on the succinctly/midjourney-prompts dataset, which contains 250k text prompts that users issued to the Midjourney text-to-image service over a month period. For more details on how this dataset was scraped, see Midjourney User Prompts & Generated Images (250k).; This prompt generator can be used to auto-complete prompts for any text-to-image model (including the DALL・E family):
; Note that, while this model can be used together with any text-to-image model, it occasionally produces Midjourney-specific tags. Users can specify certain requirements via double-dashed parameters (e.g. --ar 16:9 sets the aspect ratio to 16:9, and --no snake asks the model to exclude snakes from the generated image) or set the importance of various entities in the image via explicit weights (e.g. hot dog::1.5 food::-1 is likely to produce the image of an animal instead of a frankfurter).; When using this model, please attribute credit to Succinctly AI."
xclip-base-patch32,Video Classification,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2208.02816.pdf,27,,"10,524",790.5847413,7,https://huggingface.co/microsoft/xclip-base-patch32,"X-CLIP model (base-sized, patch resolution of 32) trained fully-supervised on Kinetics-400. It was introduced in the paper Expanding Language-Image Pretrained Models for General Video Recognition by Ni et al. and first released in this repository.; This model was trained using 8 frames per video, at a resolution of 224x224.; Disclaimer: The team releasing X-CLIP did not write a model card for this model so this model card has been written by the Hugging Face team.; X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. ; "
OPT-13B-Erebus,Text Generation,PyTorch; Transformers,English,other,https://arxiv.org/pdf/2205.01068.pdf,116,,"22,382",26318.2383,1,https://huggingface.co/KoboldAI/OPT-13B-Erebus,"This is the second generation of the original Shinen made by Mr. Seeker. The full dataset consists of 6 different sources, all surrounding the ""Adult"" theme. The name ""Erebus"" comes from the greek mythology, also named ""darkness"". This is in line with Shin'en, or ""deep abyss"". For inquiries, please contact the KoboldAI community. Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The data can be divided in 6 different datasets:; The dataset uses [Genre: <comma-separated list of genres>] for tagging.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; Based on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion). Warning: This model has a very strong NSFW bias!"
sd-image-variations-diffusers,Image-to-Image,Diffusers,,creativeml-openrail-m,,255,ChristophSchuhmann/improved_aesthetics_6plus,"27,209",1.611437721,113,https://huggingface.co/lambdalabs/sd-image-variations-diffusers,"? V2 model released, and blurriness issues fixed! ?; ?? Image Variations is now natively supported in ? Diffusers! ??; ; This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of ""image variations"" similar to DALLE-2 using Stable Diffusion. This version of the weights has been ported to huggingface Diffusers, to use this with the Diffusers library requires the Lambda Diffusers repo.; This model was trained in two stages and longer than the original variations model and gives better image quality and better CLIP rated similarity compared to the original version"
CLIP-ViT-B-32-laion2B-s34B-b79K,Zero-Shot Image Classification,PyTorch; OpenCLIP,,mit,https://arxiv.org/pdf/1910.04867.pdf,28,,"786,378",1213.589493,7,https://huggingface.co/laion/CLIP-ViT-B-32-laion2B-s34B-b79K,"A CLIP ViT-B/32 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Romain Beaumont on the stability.ai cluster. ; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. ; Zero-shot image classification, image and text retrieval, among others."
CLIP-ViT-g-14-laion2B-s12B-b42K,,PyTorch; OpenCLIP,,mit,https://arxiv.org/pdf/1910.04867.pdf,24,,"5,711",11206.14976,5,https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K,"A CLIP ViT-g/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Romain Beaumont on the stability.ai cluster. ; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. ; Zero-shot image classification, image and text retrieval, among others."
OPT-6.7B-Erebus,Text Generation,PyTorch; Transformers,English,other,https://arxiv.org/pdf/2205.01068.pdf,70,,"41,789",13620.63828,,https://huggingface.co/KoboldAI/OPT-6.7B-Erebus,"This is the second generation of the original Shinen made by Mr. Seeker. The full dataset consists of 6 different sources, all surrounding the ""Adult"" theme. The name ""Erebus"" comes from the greek mythology, also named ""darkness"". This is in line with Shin'en, or ""deep abyss"". For inquiries, please contact the KoboldAI community. Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The data can be divided in 6 different datasets:; The dataset uses [Genre: <comma-separated list of genres>] for tagging.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; Based on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion). Warning: This model has a very strong NSFW bias!"
RuLeanALBERT,Fill-Mask,Transformers,Russian,apache-2.0,,21,,100,1832.962426,1,https://huggingface.co/yandex/RuLeanALBERT,"RuLeanALBERT is a pretrained masked language model for the Russian language using a memory-efficient architecture.; Read more about the model in this blog post (in Russian).; See its implementation, as well as the pretraining and finetuning code, at https://github.com/yandex-research/RuLeanALBERT."
whisper-base,Automatic Speech Recognition,PyTorch; TensorFlow; JAX; Transformers,99 languages,apache-2.0,https://arxiv.org/pdf/2212.04356.pdf,103,,"73,220",874.9854593,124,https://huggingface.co/openai/whisper-base,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al from OpenAI. The original code repository can be found here.; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. ; The models were trained on either English-only data or multilingual data. The English-only models were trained 
on the task of speech recognition. The multilingual models were trained on both speech recognition and speech 
translation. For speech recognition, the model predicts transcriptions in the same language as the audio. 
For speech translation, the model predicts transcriptions to a different language to the audio."
whisper-small,Automatic Speech Recognition,PyTorch; TensorFlow; JAX; Transformers,99 languages,apache-2.0,https://arxiv.org/pdf/2212.04356.pdf,64,,"42,652",2904.985842,66,https://huggingface.co/openai/whisper-small,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al from OpenAI. The original code repository can be found here.; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. ; The models were trained on either English-only data or multilingual data. The English-only models were trained 
on the task of speech recognition. The multilingual models were trained on both speech recognition and speech 
translation. For speech recognition, the model predicts transcriptions in the same language as the audio. 
For speech translation, the model predicts transcriptions to a different language to the audio."
vlt5-base-keywords,Text2Text Generation,PyTorch; Safetensors; Transformers,Polish; English,cc-by-4.0,https://arxiv.org/pdf/2209.14008.pdf,16,posmac,"10,377",2253.933437,,https://huggingface.co/Voicelab/vlt5-base-keywords,"Our vlT5 model is a keyword generation model based on encoder-decoder architecture using Transformer blocks presented by Google (https://huggingface.co/t5-base). The vlT5 was trained on scientific articles corpus to predict a given set of keyphrases based on the concatenation of the article’s abstract and title. It generates precise, yet not always complete keyphrases that describe the content of the article based only on the abstract.; Keywords generated with vlT5-base-keywords: encoder-decoder architecture, keyword generation; Results on demo model (different generation method, one model per language):; Our vlT5 model is a keyword generation model based on encoder-decoder architecture using Transformer blocks presented by Google (https://huggingface.co/t5-base). The vlT5 was trained on scientific articles corpus to predict a given set of keyphrases based on the concatenation of the article’s abstract and title. It generates precise, yet not always complete keyphrases that describe the content of the article based only on the abstract.; Keywords generated with vlT5-base-keywords: encoder-decoder architecture, vlT5, keyword generation, scientific articles corpus"
robo-diffusion,Text-to-Image,Diffusers,English,creativeml-openrail-m,,345,,"1,592",0.472410469,150,https://huggingface.co/nousr/robo-diffusion,There is a new model based on stable-dffusion 2.0 (base) that can be found here!; A dreambooth-method finetune of stable diffusion that will output cool looking robots when prompted.; ; Github: https://github.com/nousr/robo-diffusion; Keep the words nousr robot towards the beginning of your prompt to invoke the finetuned style.
Arcane-Diffusion,Text-to-Image,Diffusers,,creativeml-openrail-m,,737,,"9,470",6545.923574,396,https://huggingface.co/nitrosocke/Arcane-Diffusion,"This is the fine-tuned Stable Diffusion model trained on images from the TV Show Arcane.
Use the tokens arcane style in your prompts for the effect.; If you enjoy my work, please consider supporting me
; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; You can also export the model to ONNX, MPS and/or FLAX/JAX.; We also support a Gradio Web UI and Colab with Diffusers to run fine-tuned Stable Diffusion models:

"
clip-variants,,ONNX,English,mit,,6,,0,0.89351017,,https://huggingface.co/mlunar/clip-variants,"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.; See the original CLIP Model Card for more details on limitations and biases.; This repository holds OpenAI's CLIP models converted into many other variants, see below for more details.; I haven't done many tests on these conversions. I've briefly tried the float16 versions, which seem very similar to the original float32, however the similarity seems to drop more with the qint8/quint8 versions as expected. I couldn't try qint8 as it seemed unsupported for some operations, but I'm including it for completeness. From a brief test the quint8 version seemed to work fine.; The license for the conversion code is MIT, the license for the models is the same as the original license for the OpenAI models (??♂?). I have no affiliation with OpenAI."
bloomz-560m,Text Generation,PyTorch; TensorBoard; Safetensors; Transformers,46 languages,bigscience-bloom-rail-1.0,https://arxiv.org/pdf/2211.01786.pdf,66,bigscience/xP3,"19,417",2308.286453,4,https://huggingface.co/bigscience/bloomz-560m,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks.""."
bloomz-3b,Text Generation,PyTorch; TensorBoard; Safetensors; Transformers,46 languages,bigscience-bloom-rail-1.0,https://arxiv.org/pdf/2211.01786.pdf,60,bigscience/xP3,"58,097",12323.00652,,https://huggingface.co/bigscience/bloomz-3b,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks.""."
polyglot-ko-12.8b,Text Generation,PyTorch; Safetensors; Transformers,Korean,apache-2.0,https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2204.04541.pdf; https://arxiv.org/pdf/2306.02254.pdf,47,,"14,838",52815.72076,,https://huggingface.co/EleutherAI/polyglot-ko-12.8b,"Polyglot-Ko is a series of large-scale Korean autoregressive language models made by the EleutherAI polyglot team.; The model consists of 40 transformer layers with a model dimension of 5120, and a feedforward dimension of 20480. The model
dimension is split into 40 heads, each with a dimension of 128. Rotary Position Embedding (RoPE) is applied to 64
dimensions of each head. The model is trained with a tokenization vocabulary of 30003.; Polyglot-Ko-12.8B was trained on 863 GB of Korean language data (1.2TB before processing), a large-scale dataset curated by TUNiB. The data collection process has abided by South Korean laws. This dataset was collected for the purpose of training Polyglot-Ko models, so it will not be released for public use.  ; Furthermore, in order to avoid the model memorizing and generating personally identifiable information (PII) in the training data, we masked out the following sensitive information in the pre-processing stage:; Polyglot-Ko-12.8B was trained for 167 billion tokens over 301,000 steps on 256 A100 GPUs with the GPT-NeoX framework. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token. "
mt0-base,Text2Text Generation,PyTorch; Safetensors; Transformers,101 languages,apache-2.0,https://arxiv.org/pdf/2211.01786.pdf,19,bigscience/xP3; mc4,"3,012",4792.476076,2,https://huggingface.co/bigscience/mt0-base,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find our resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks.""."
mt0-large,Text2Text Generation,PyTorch; Safetensors; Transformers,101 languages,apache-2.0,https://arxiv.org/pdf/2211.01786.pdf,25,bigscience/xP3; mc4,"4,012",10096.79618,1,https://huggingface.co/bigscience/mt0-large,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find our resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks.""."
mt0-xxl-mt,Text2Text Generation,PyTorch; Transformers,101 languages,apache-2.0,https://arxiv.org/pdf/2211.01786.pdf,39,bigscience/xP3mt; mc4,"1,035",57149.64598,4,https://huggingface.co/bigscience/mt0-xxl-mt,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find our resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks.""."
OCR-Donut-CORD,Image-to-Text,PyTorch; Transformers,,mit,https://arxiv.org/pdf/2111.15664.pdf,18,,"2,606",814.3303005,2,https://huggingface.co/jinhybr/OCR-Donut-CORD,"Donut model fine-tuned on CORD. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository.; Disclaimer: The team releasing Donut did not write a model card for this model so this model card has been written by the Hugging Face team.; Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. ; ; This model is fine-tuned on CORD, a document parsing dataset."
ldm-super-resolution-4x-openimages,,Diffusers; PyTorch,,apache-2.0,https://arxiv.org/pdf/2112.10752.pdf,60,,"4,595",0.004445496,1,https://huggingface.co/CompVis/ldm-super-resolution-4x-openimages,"Paper: High-Resolution Image Synthesis with Latent Diffusion Models; Abstract:; By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.; Authors; Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj?rn Ommer"
galactica-6.7b,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/1810.03993.pdf,79,,"2,782",14061.71406,1,https://huggingface.co/facebook/galactica-6.7b,"; Model card from the original repo ; Following Mitchell et al. (2018), this model card provides information about the GALACTICA model, how it was trained, and the intended use cases. Full details about how the model was trained and evaluated can be found in the release paper.; The GALACTICA models are trained on a large-scale scientific corpus. The models are designed to perform scientific tasks, including but not limited to citation prediction, scientific QA, mathematical reasoning, summarization, document generation, molecular property prediction and entity extraction. The models were developed by the Papers with Code team at Meta AI to study the use of language models for the automatic organization of science. We train models with sizes ranging from 125M to 120B parameters. Below is a summary of the released models:; November 2022"
artstation-diffusion,Text-to-Image,Diffusers,English,creativeml-openrail-m,,94,,310,0.004744453,3,https://huggingface.co/hakurei/artstation-diffusion,"artstation-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality Artstation images through fine-tuning.; Aspect Ratio Bucketing has been used during finetuning. This model can generate different aspect ratios VERY WELL.; knight, full body study, concept art, atmospheric; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
The CreativeML OpenRAIL License specifies: ; This model can be used for entertainment purposes and as a generative art assistant."
Future-Diffusion,Text-to-Image,Diffusers,English,openrail++,,388,,"8,716",2641.926116,31,https://huggingface.co/nitrosocke/Future-Diffusion,"This is the fine-tuned Stable Diffusion 2.0 model trained on high quality 3D images with a futuristic Sci-Fi theme.
Use the tokensfuture style in your prompts for the effect.
Trained on Stability.ai's  Stable Diffusion 2.0 Base with 512x512 resolution.; If you enjoy my work and want to test new models before release, please consider supporting me
; Disclaimer: The SD 2.0 model is just over 24h old at this point and we still need to figure out how it works exactly. Please view this as an early prototype and experiment with the model.; Characters rendered with the model:

Cars and Animals rendered with the model:

Landscapes rendered with the model:
; future style [subject] Negative Prompt: duplicate heads bad anatomy Steps: 20, Sampler: Euler a, CFG scale: 7, Size: 512x704"
text-to-sql-with-table-schema,Text2Text Generation,PyTorch; Transformers,English,,,36,wikisql,"3,930",894.4288672,1,https://huggingface.co/juierror/text-to-sql-with-table-schema,"There are newer version of this using Flan-T5 as a based model. You can check out here; PS. From this discussion, I think the base model that I use for finetune did not support the token <, so this might not be a good model to do this tasks. "
modelz_base,,ONNX,,,,4,,0,48506.88145,,https://huggingface.co/uwg/modelz_base,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
FinBERT-PT-BR,Text Classification,PyTorch; Transformers,Portuguese,apache-2.0,,8,,831,437.1662649,,https://huggingface.co/lucas-leme/FinBERT-PT-BR,"FinBERT-PT-BR is a pre-trained NLP model to analyze sentiment of Brazilian Portuguese financial texts.; The model was trained in two main stages: language modeling and sentiment modeling. In the first stage, a language model was trained with more than 1.4 million texts of financial news in Portuguese. 
From this first training, it was possible to build a sentiment classifier with few labeled texts (500) that presented a satisfactory convergence.; At the end of the work, a comparative analysis with other models and the possible applications of the developed model are presented. 
In the comparative analysis, it was possible to observe that the developed model presented better results than the current models in the state of the art. 
Among the applications, it was demonstrated that the model can be used to build sentiment indices, investment strategies and macroeconomic data analysis, such as inflation.; ; In order to use the model, you need to get the HuggingFace auth token. You can get it here."
scientific_abstract_simplification,Text2Text Generation,PyTorch; Safetensors; Transformers,English,mit,,13,,234,6412.687333,1,https://huggingface.co/haining/scientific_abstract_simplification,"Scientific Abstract Simplification (SAS) is a tool designed to rewrite complex scientific abstracts into simpler, more comprehensible versions. Our objective is to make scientific knowledge universally accessible. If you have already experimented with our baseline model (sas_baseline), you will find that the current model surpasses its predecessor in terms of all evaluation metrics. Feel free to test it via the Hosted Inference API to your right. Simply select one of the provided examples or input your own scientific abstract. Just ensure to precede your text with the instruction, ""summarize, simplify, and contextualize: "", followed by a space. For local usage, refer to the Usage section.""; Open science has significantly reduced barriers to accessing scientific papers.
However, attainable research does not entail accessible knowledge.
Consequently, many individuals might prefer to rely on succinct social media narratives rather than endeavour to comprehend a scientific paper.
This preference is understandable as humans often favor narratives over dry, technical information. 
So, why not ""translate"" these intricate scientific abstracts into simpler, more accessible narratives? 
Several prestigious journals have already initiated steps towards enhancing accessibility. 
For instance, PNAS requires authors to submit Significance Statements understandable to an 'undergraduate-educated scientist', while Science includes an editor's abstract to provide a swift overview of the paper's salient points.; In this project, our objective is to employ AI to rewrite scientific abstracts into easily understandable scientific narratives.
To facilitate this, we have curated two new datasets: one containing PNAS abstract-significance pairs and the other encapsulating editor abstracts from Science.
We utilize a Transformer model (a variant known as Flan-T5) to fine-tune our model for the task of simplifying scientific abstracts.
Initially, the model is fine-tuned utilizing multiple discrete instructions by amalgamating four pertinent tasks in a challenge-proportional manner (a strategy we refer to as Multi-Instruction Pretuning).
Subsequently, we continue the fine-tuning process exclusively with the abstract-significance corpus. Our model can generate lay summaries that outperform models fine-tuned solely with the abstract-significance corpus and models fine-tuned with traditional task combinations.
We hope our work can foster a more comprehensive understanding of scientific research, enabling a larger audience to benefit from open science.; Use the code below to get started with the model. Remember to prepend the INSTRUCTION for best performance.; We finetuned the base model (flan-t5-large) on multiple relevant tasks with standard language modeling loss. During training, the source text of each task is prepended with an task-specific instruction and mapped to the corresponding target text. For example, ""simplify: "" is added before a wiki text, and the whole text is fed into the model to line up with the corresponding simple wiki text. The tuning process has two steps."
whisper-medium-jp,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,Japanese,apache-2.0,,22,mozilla-foundation/common_voice_11_0,"2,421",4701.954075,15,https://huggingface.co/vumichien/whisper-medium-jp,"This model is a fine-tuned version of openai/whisper-medium on the common_voice_11_0 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
Analog-Diffusion,Text-to-Image,Diffusers; Safetensors,English,creativeml-openrail-m,,813,,"19,923",4362.248729,131,https://huggingface.co/wavymulder/Analog-Diffusion,"Analog Diffusion

CKPT DOWNLOAD LINK - This is a dreambooth model trained on a diverse set of analog photographs.; In your prompt, use the activation token: analog style; You may need to use the words blur haze naked in your negative prompts. My dataset did not include any NSFW material but the model seems to be pretty horny. Note that using blur and haze in your negative prompt can give a sharper image but also a less pronounced analog film effect.; Trained from 1.5 with VAE.; Please see this document where I share the parameters (prompt, sampler, seed, etc.) used for all example images."
bad-artist,Text-to-Image,,eng,cc0-1.0,,283,,0,0.016689453,,https://huggingface.co/nick-x-hacker/bad-artist,"; The images above were generated with only ""solo"" in the positive prompt, and ""sketch by bad-artist"" (this embedding) in the negative.

The embedding uses only 2 tokens.; Textual-inversion embedding for use in unconditional (negative) prompt.

Inspired partly by https://huggingface.co/datasets/Nerfgun3/bad_prompt.; There are currently 2 version:; I recommend using with 'by', so for example ""sketch by bad-artist"", or ""painting by bad-artist"", or ""photograph by bad-artist"", etc."
riffusion-model-v1,,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/2210.08402.pdf,463,,"51,438",14950.40681,37,https://huggingface.co/riffusion/riffusion-model-v1,"Riffusion is an app for real-time music generation with stable diffusion.; Read about it at https://www.riffusion.com/about and try it at https://www.riffusion.com/.; This repository contains the model files, including:; Riffusion is a latent text-to-image diffusion model capable of generating spectrogram images given any text input. These spectrograms can be converted into audio clips.; The model was created by Seth Forsgren and Hayk Martiros as a hobby project."
GTA5_Artwork_Diffusion,Text-to-Image,Diffusers; Safetensors,English,creativeml-openrail-m,,99,,"1,718",4854.245072,33,https://huggingface.co/ItsJayQz/GTA5_Artwork_Diffusion,"This model was trained on the loading screens, gta storymode, and gta online DLCs artworks.
Which includes characters, background, chop, and some objects.
The model can do people and portrait pretty easily, as well as cars, and houses.
For some reasons, the model stills automatically include in some game footage, so landscapes tend to look a bit more game-like.
Please check out important informations on the usage of the model down bellow.; To reference the art style, use the token: gtav style; There is already an existing model that uses textual inversion. This is trained using Dreambooth instead, whether or not this method is better, I will let you judge.; We support a Gradio Web UI to run GTA5_Artwork_Diffusion:
; Here are some samples."
models,,,English; Chinese; Japanese,afl-3.0,,47,,0,141892.7964,,https://huggingface.co/emmajoanne/models,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
karlo-v1-alpha,Text-to-Image,Diffusers,,creativeml-openrail-m,,73,,"3,933",0.007838669,23,https://huggingface.co/kakaobrain/karlo-v1-alpha,"Karlo is a text-conditional image generation model based on OpenAI's unCLIP architecture with the improvement over the standard super-resolution model from 64px to 256px, recovering high-frequency details only in the small number of denoising steps.; Karlo is available in diffusers!; ; ; Karlo is a text-conditional diffusion model based on unCLIP, composed of prior, decoder, and super-resolution modules. In this repository, we include the improved version of the standard super-resolution module for upscaling 64px to 256px only in 7 reverse steps, as illustrated in the figure below:"
dreambooth-avatar,Text-to-Image,Diffusers,English,,,39,,739,0.003612328,8,https://huggingface.co/lambdalabs/dreambooth-avatar,"Dreambooth finetuning of Stable Diffusion (v1.5.1) on Avatar art style by Lambda Labs.; This text-to-image stable diffusion model was trained with dreambooth.Put in a text prompt and generate your own Avatar style image!; ; To run model locally:; Base model is Stable Diffusion v1.5 and was trained using Dreambooth with 60 input images sized 512x512 displaying Avatar character images.
The model is learning to associate Avatar images with the style tokenized as 'avatarart style'.
Prior preservation was used during training using the class 'Person' to avoid training bleeding into the representations for that class.
Training ran on 2xA6000 GPUs on Lambda GPU Cloud for 700 steps, batch size 4 (a couple hours, at a cost of about $4)."
instructor-base,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,English,apache-2.0,https://arxiv.org/pdf/2212.09741.pdf,64,,"4,747",442.2661729,13,https://huggingface.co/hkunlp/instructor-base,"We introduce Instructor???, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor?? achieves sota on 70 diverse embedding tasks!
The model is easy to use with our customized sentence-transformer library. For more details, check out our paper and project page! ; **************************** Updates ****************************; Then you can use the model like this to calculate domain-specific and task-aware embeddings:; If you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions: ; ??????????????????????????Represent the domain text_type for task_objective:"
SkyPaint,,Diffusers,,,,19,,46,0.019444389,,https://huggingface.co/SkyWork/SkyPaint,"SkyPaint is a Chinese-English bilingual text-generated image project developed by Singularity-AI. It is still being updated and optimized.; The SkyPaint text generation image model is mainly composed of two parts, namely the prompt word text encoder model and the diffusion model. Therefore, our optimization is also divided into two steps. First, based on OpenAI-CLIP, we optimized the prompt word text encoder model to make SkyPaint have the ability to recognize Chinese and English, and then optimized the diffusion model, so that SkyPaint has modern artistic capabilities and can produce high-quality pictures.; Chinese and English mixed prompt word input.
Generating high-quality images in a modern art style.
English prompt words for stable_diffusion_1.x official model and related fine-tuning models.
Retain usage habits and methods of stable_diffusion prompt words.
Introduction to SkyCLIP Models
SkyCLIP is a CLIP model obtained by using an efficient method of training Chinese-English bilingual CLIP models. This method only needs to use text data to achieve efficient distillation of the OpenAI-CLIP model, which greatly reduces the data threshold. At the same time, training requires Compared with the original CLIP model, the computing power requirement is reduced by more than 90%, which is convenient for the open source community to reproduce/fine-tune. This method only changes the text encoder of OpenAI-CLIP, and can be used with the image encoder of OpenAI-CLIP to realize the image-text retrieval function.; 机械狗
; 城堡 大海 夕阳 宫崎骏动画
"
furrydiffusion,Text-to-Image,Diffusers,,creativeml-openrail-m,,42,,"2,956",2181.123114,8,https://huggingface.co/lunarfish/furrydiffusion,"; FurryDiffusion is a model made to generate furry art, this model is very much in beta still and will keep improoving! To use this please make sure to include furry in your prompt and to make a specific breed add the breed name only.; Example Prompts:; Test the concept via A1111 Colab fast-Colab-A1111
Or you can run your new concept via diffusers Colab Notebook for Inference; NOTE: Its better to run it in Google Colab since you can use google's powerful gpu's for free. Go ahead try it now!"
7th_Layer,,,,other,,579,,0,0.002607422,2,https://huggingface.co/syaimu/7th_Layer,"default CFG Scale : 7 ±5; default Sampler : DPM++ 2M Karras; default Steps : 25; Negative prompt : (worst quality:1.4), (low quality:1.4) , (monochrome:1.1),; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
stable-diffusion-logo-fine-tuned,Text-to-Image,Diffusers,,creativeml-openrail-m,,78,,"4,476",2672.642216,14,https://huggingface.co/nicky007/stable-diffusion-logo-fine-tuned,"this Stable diffusion model i have fine tuned on 1000 raw logo png/jpg images of of size 128x128 with augmentation ; Enjoy .create any type of logo; for examples:""Logo of a pirate"",""logo of a sunglass with girl"" or something complex like ""logo of a ice-cream with snake"" etc"
CLIP-ViT-B-16-laion2B-s34B-b88K,Zero-Shot Image Classification,OpenCLIP,,mit,https://arxiv.org/pdf/1910.04867.pdf,13,,"202,680",1201.585182,6,https://huggingface.co/laion/CLIP-ViT-B-16-laion2B-s34B-b88K,"A CLIP ViT-B/16 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Mehdi Cherti on the JUWELS Booster supercomputer. See acknowledgements below.; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. ; Zero-shot image classification, image and text retrieval, among others."
graphormer-base-pcqm4mv2,Graph Machine Learning,PyTorch; Transformers,,mit,https://arxiv.org/pdf/2106.05234.pdf,24,,"2,520",191.0045703,9,https://huggingface.co/clefourrier/graphormer-base-pcqm4mv2,"The Graphormer is a graph classification model.; The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2.; This model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.; The Graphormer model is ressource intensive for large graphs, and might lead to OOM errors.; See the Graph Classification with Transformers tutorial."
pygmalion-2.7b,Conversational,PyTorch; TensorBoard; Transformers,English,creativeml-openrail-m,,44,,"14,864",5573.900613,6,https://huggingface.co/PygmalionAI/pygmalion-2.7b,"Pymalion 2.7B is a proof-of-concept dialogue model based on EleutherAI's gpt-neo-2.7B.; Warning: This model is NOT suitable for use by minors. It will output X-rated content under certain circumstances.; The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations.; Model weights were initialized from the uft-2.7b ConvoGPT model made available in this commit.; The model was then further fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed."
HassanBlend1.5.1.2,Text-to-Image,Diffusers,,creativeml-openrail-m,,207,,"2,049",49295.37083,39,https://huggingface.co/hassanblend/HassanBlend1.5.1.2,"I am hassan, I created HassansBlend, the latest version currently is 1.5.1.2 I continue to iterate and improve on this model over time. Feel free to check out our discord or rentry page for more examples with prompts and outputs generated.; This blend is finetuned over SD1.5 with thousands of images included in the dataset it was trained with. Along with that there are some minor merges added in just to soften it up and increase the creativity. 
I have also some custom created content such as enhancement hypernetworks/embeddings etc for patreons or KoFi subscribers only on my pages below
 Links 
Patreon

KoFi

Discord
; Model details and examples with sample prompts: https://rentry.org/sdhassan"
SD_Photoreal_Merged_Models,Text-to-Image,,,cc0-1.0,,106,,0,16033.08803,,https://huggingface.co/deadman44/SD_Photoreal_Merged_Models,"for Stable Diffusion Webui Automatic1111
type: .safetensors(ckpt)
CFG Scale: middle-low; example.
low quality, worst quality, bad anatomy, bad proportions; UniPC, Dpm++ (2M/SDE) Karras, DDIM
Steps: 10～24; vae-ft-mse-840000-ema-pruned; -Mixed 5000+images"
Trauter_LoRAs,,,,,,508,,0,0.033330078,1,https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs,"NOTICE: My LoRAs require high amount of tags to look good, I will fix this later on and update all of my LoRAs if everything works out.; Welcome to the place where I host my LoRAs. In short, LoRA is just a checkpoint trained on specific artstyle/subject that you load into your WebUI, that can be used with other models.Although you can use it with any model, the effects of LoRA will vary between them.
Most of the previews use models that come from WarriorMama777 .For more information about them, you can visit the original LoRA repository: https://github.com/cloneofsimo/loraEvery images posted here, or on the other sites have metadata in them that you can use in PNG Info tab in your WebUI to get access to the prompt of the image.Everything I do here is for free of charge!I don't guarantee that my LoRAs will give you good results, if you think they are bad, don't use them.; To use them in your WebUI, please install the extension linked under, following the installation guide:https://github.com/kohya-ss/sd-webui-additional-networks#installation; All of my LoRAs are to be used with their original danbooru tag. For example:  ; My LoRAs will have sufixes that will tell you how much they were trained. Either by using words like ""soft"" and ""hard"",where soft stands for lower amount of training and hard for higher amount of training.  "
AsiaFacemix,,,,openrail,,399,Gustavosta/Stable-Diffusion-Prompts,0,21067.60473,,https://huggingface.co/dcy/AsiaFacemix,"本人郑重声明：本模型原则上禁止用于训练基于明星、公众人物肖像的风格模型训练，因为这会带来争议，对AI社区的发展造成不良的负面影响。 如各位一定要违反以上声明训练相关模型并公开发布，请在您的发布说明中删除与本模型有关的一切描述。感谢各位使用者的支持与理解。; In principle, this model is prohibited from being used for training style models based on portraits of celebrities and public figures, because it will cause controversy and have a negative impact on the development of the AI community. If you must violate the above statement to train the relevant model and release it publicly, please delete all descriptions related to this model in your release notes. Thank you for your support and understanding.; 该模型基于basil mix,dreamlike,ProtoGen等优秀模型微调，融合而来。
用于解决上述模型在绘制亚洲、中国元素内容时，只能绘制丑陋的刻板印象脸的问题。
同时也能改善和减少绘制亚洲、中国元素内容时，得到更接近tags的绘制内容。
This model based on basil mix,dreamlike,ProtoGen,etc. After finetune and merging, it solved the big problem that the other model can only draw ugly stereotyped woman faces from hundreds years ago When drawing Asian and Chinese elements.
This model can also improve the drawing content of Asian and Chinese elements to get closer to tags.; Based on dreamlike finetune example：


; Based on Image to Image example：

"
chatgpt-detector-roberta,Text Classification,PyTorch; Transformers,English,,https://arxiv.org/pdf/2301.07597.pdf,27,Hello-SimpleAI/HC3,"185,478",502.3388117,17,https://huggingface.co/Hello-SimpleAI/chatgpt-detector-roberta,"This model is trained on the mix of full-text and splitted sentences of answers from Hello-SimpleAI/HC3.; More details refer to arxiv: 2301.07597 and Gtihub project Hello-SimpleAI/chatgpt-comparison-detection.; The base checkpoint is roberta-base.
We train it with all Hello-SimpleAI/HC3 data (without held-out) for 1 epoch.; (1-epoch is consistent with the experiments in our paper.); Checkout this papaer arxiv: 2301.07597"
anything-v3.0,Text-to-Image,Diffusers,English,creativeml-openrail-m,,622,,"28,781",14438.40229,501,https://huggingface.co/Linaqruf/anything-v3.0,
yolov8m-scene-classification,Image Classification,TensorBoard; PyTorch,,,,6,keremberke/indoor-scene-classification,"4,011",33.22426708,,https://huggingface.co/keremberke/yolov8m-scene-classification,More models available at: awesome-yolov8-models; Inference API has been turned off for this model.
yolov8m-table-extraction,Object Detection,TensorBoard; PyTorch,,,,11,keremberke/table-extraction,"7,269",52.45749638,1,https://huggingface.co/keremberke/yolov8m-table-extraction,More models available at: awesome-yolov8-models; Inference API has been turned off for this model.
DucHaitenAnime,Text-to-Image,Diffusers,English,creativeml-openrail-m,,19,,"5,254",49111.04399,41,https://huggingface.co/DucHaiten/DucHaitenAnime,"DucHaitenAnime_v4.0: In this version i added a little 3D, a little realistic, improved the hand but not much, improved the color because i don't like to use vae; All images above are used only text to image, not edited or accompanying application software.; https://civitai.com/models/6634; please support me by becoming a patron:; https://www.patreon.com/duchaitenreal"
reward-model-deberta-v3-large-v2,Text Classification,PyTorch; Transformers,English,mit,,85,openai/summarize_from_feedback; openai/webgpt_comparisons; Dahoas/instruct-synthetic-prompt-responses; Anthropic/hh-rlhf,"62,221",1792.890171,13,https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2,"Reward model (RM) trained to predict which generated answer is better judged by a human, given a question.; RM are useful in these domain:; QA model evaluation; serves as reward score in RLHF ; detect potential toxic response via ranking"
BioGPT-Large,Text Generation,PyTorch; Transformers,English,mit,,109,pubmed,"5,709",6442.758577,22,https://huggingface.co/microsoft/BioGPT-Large,"Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.; If you find BioGPT useful in your research, please cite the following paper:"
chilloutmix,,,,creativeml-openrail-m,,234,,,0,,https://huggingface.co/swl-models/chilloutmix,"This repository has been marked as containing sensitive content and may contain potentially harmful and sensitive
		information.
	"
pythia-2.8b-deduped,Text Generation,PyTorch; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/2304.01373.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2201.07311.pdf,12,EleutherAI/the_pile_deduplicated,"4,559",11634.76584,,https://huggingface.co/EleutherAI/pythia-2.8b-deduped,"The Pythia Scaling Suite is a collection of models developed to facilitate 
interpretability research (see paper). 
It contains two sets of eight models of sizes 
70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two 
models: one trained on the Pile, and one trained on the Pile after the dataset 
has been globally deduplicated. All 8 model sizes are trained on the exact 
same data, in the exact same order. We also provide 154 intermediate 
checkpoints per model, hosted on Hugging Face as branches.; The Pythia model suite was designed to promote scientific 
research on large language models, especially interpretability research. 
Despite not centering downstream performance as a design goal, we find the 
models match or exceed the performance of 
similar and same-sized models, such as those in the OPT and GPT-Neo suites.; Previously, we released an early version of the Pythia suite to the public. 
However, we decided to retrain the model suite to address a few hyperparameter 
discrepancies. This model card lists the changes; 
see appendix B in the Pythia paper for further discussion. We found no 
difference in benchmark performance between the two Pythia versions. 
The old models are 
still available, but we 
suggest the retrained suite if you are just starting to use Pythia.
This is the current release.; Please note that all models in the Pythia suite were renamed in January 
2023. For clarity, a table 
comparing the old and new names is provided in this model card, together 
with exact parameter counts.; The primary intended use of Pythia is research on the behavior, functionality, 
and limitations of large language models. This suite is intended to provide 
a controlled setting for performing scientific experiments. We also provide 
154 checkpoints per model: initial step0, 10 log-spaced checkpoints 
step{1,2,4...512}, and 143 evenly-spaced checkpoints from step1000 to 
step143000. These checkpoints are hosted on Hugging Face as branches. Note 
that branch 143000 corresponds exactly to the model checkpoint on the main 
branch of each model."
OPT-13B-Nerybus-Mix,Text Generation,PyTorch; Transformers,English,other,https://arxiv.org/pdf/2205.01068.pdf,22,,"8,216",26871.24107,2,https://huggingface.co/KoboldAI/OPT-13B-Nerybus-Mix,"This is a merged (50/50) model of both Erebus 13B and Nerys V2 13B by Mr. Seeker.
Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; For more information, check out the two source models:; Based on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion).
Warning: This model has a very strong NSFW bias!; OPT-13B is licensed under the OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved.; Inference API has been turned off for this model."
boring_e621,,,,apache-2.0,,38,,0,152.9958496,,https://huggingface.co/FoodDesert/boring_e621,"This embedding attempts to capture what it means for an image to be uninteresting.  It was trained as a negative embedding using e621 style tags as prompts during training.
If you're using the Automatic1111 Stable Diffusion WebUI, place the boring_e621_v4.pt file in 
stable-diffusion-webui\embeddings and add ""boring_e621_v4"" to your negative prompt for more interesting outputs.
; The motivation for boring_e621 is that negative embeddings like Bad Prompt, 
whose training is described here 
depend on manually curated lists of tags describing features people do not want their images to have, such as ""deformed hands"".  Some problems with this approach are:; To address these problems, boring_e621 employs textual inversion on a set of images automatically extracted from the art site 
e621.net, a rich resource of millions of hand-labeled artworks, each of which is both human-labeled topically and rated 
according to its quality.  E621.net allows users to express their approval of an artwork by either up-voting it, or marking it as a favorite.Boring_e621 was specifically trained on artworks automatically selected from the site according to the criteria 
that no user has ever Favorited or Up-Voted them.  boring_e621 thus learned to produce low-quality images, so when it is 
used in the negative prompt of a stable diffusion image generator, the model avoids making mistakes that would make the generation more boring.
; To qualitatively evaluate how well boring_e621 has learned to improve image quality, we apply it to 4 simple sample prompts using the base Stable Diffusion 1.5 model.; "
NeverEnding_Dream-Feb19-2023,,,,,,170,,0,0,,https://huggingface.co/jomcs/NeverEnding_Dream-Feb19-2023,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
SteamSHP-flan-t5-xl,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,,33,stanfordnlp/SHP,486,11674.44298,,https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xl,"SteamSHP-XL is a preference model trained to predict -- given some context and two possible responses -- which response humans will find more helpful.
It can be used for NLG evaluation or as a reward model for RLHF.; It is a FLAN-T5-xl model (3B parameters) finetuned on:; There is a smaller variant called SteamSHP-Large that was made by finetuning FLAN-T5-large (780M parameters).
Despite being 1/4 of the size, it is on average only 0.75 points less accurate on the SHP + Anthropic test data (across all domains).; The input text should be of the format:; The output generated by SteamSHP-XL will either be A or B."
whisper-medium-fleurs-lang-id,Audio Classification,PyTorch; TensorBoard; Transformers,,apache-2.0,,2,xtreme_s,"1,683",615.1566492,,https://huggingface.co/sanchit-gandhi/whisper-medium-fleurs-lang-id,"This model is a fine-tuned version of openai/whisper-medium on the FLEURS subset of the google/xtreme_s dataset.
It achieves the following results on the evaluation set:; To reproduce this run, execute the command in run.sh.; More information needed; More information needed; More information needed"
sd-controlnet-canny,Image-to-Image,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,85,,"145,250",2969.671903,138,https://huggingface.co/lllyasviel/sd-controlnet-canny,"ControlNet is a neural network structure to control diffusion models by adding extra conditions. 
This checkpoint corresponds to the ControlNet conditioned on Canny edges.; It can be used in combination with Stable Diffusion.; ; Developed by: Lvmin Zhang, Maneesh Agrawala; Model type: Diffusion-based text-to-image generation model"
sd-controlnet-hed,Image-to-Image,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,19,,"39,148",2969.67161,98,https://huggingface.co/lllyasviel/sd-controlnet-hed,"ControlNet is a neural network structure to control diffusion models by adding extra conditions. 
This checkpoint corresponds to the ControlNet conditioned on HED Boundary.; It can be used in combination with Stable Diffusion.; ; Developed by: Lvmin Zhang, Maneesh Agrawala; Model type: Diffusion-based text-to-image generation model"
fasttext-language-identification,Text Classification,fastText,,cc-by-nc-4.0,https://arxiv.org/pdf/1607.04606.pdf; https://arxiv.org/pdf/1802.06893.pdf; https://arxiv.org/pdf/1607.01759.pdf; https://arxiv.org/pdf/1612.03651.pdf,29,,0,1208.328311,3,https://huggingface.co/facebook/fasttext-language-identification,"fastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices. It was introduced in this paper. The official website can be found here.; This LID (Language IDentification) model is used to predict the language of the input text, and the hosted version (lid218e) was released as part of the NLLB project and can detect 217 languages. You can find older versions (ones that can identify 157 languages) on the official fastText website.; fastText is a library for efficient learning of word representations and sentence classification. fastText is designed to be simple to use for developers, domain experts, and students. It's dedicated to text classification and learning word representations, and was designed to allow for quick model iteration and refinement without specialized hardware. fastText models can be trained on more than a billion words on any multicore CPU in less than a few minutes.; It includes pre-trained models learned on Wikipedia and in over 157 different languages. fastText can be used as a command line, linked to a C++ application, or used as a library for use cases from experimentation and prototyping to production.; You can use pre-trained word vectors for text classification or language identification. See the tutorials and resources on its official website to look for tasks that interest you."
so-vits-svc-4.0-models,,,,mit,,69,,0,0.002800636,,https://huggingface.co/TachibanaKimika/so-vits-svc-4.0-models,"模型命名遵循G_${name}_${Epoch}epoch.pth.; 显卡炸了, 暂时不炼了; 由so-vits-svc-4.0训练的模型 不是v2; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
text2vec-base-chinese,Sentence Similarity,PyTorch; Transformers,Chinese,apache-2.0,,17,,"17,395",409.5393329,7,https://huggingface.co/GanymedeNil/text2vec-base-chinese,"Based on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replace MacBERT with LERT, and keep other training conditions unchanged。"
llama-7b-hf-int4,,,,other,,73,,0,3870.740107,,https://huggingface.co/decapoda-research/llama-7b-hf-int4,"LLaMA-13B converted to work with Transformers/HuggingFace. This is under a special license, please see the LICENSE file for details.; EXPERIMENTAL RELEASE; This has been converted to int4 via GPTQ method. This requires some special support code that is also highly experimental. NOT COMPATIBLE WITH TRANSFORMERS LIBRARY.; --
license: other; Organization developing the model
The FAIR team of Meta AI."
pythia-1b,Text Generation,PyTorch; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/2304.01373.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2201.07311.pdf,8,the_pile,"18,283",4282.445741,,https://huggingface.co/EleutherAI/pythia-1b,"The Pythia Scaling Suite is a collection of models developed to facilitate 
interpretability research (see paper). 
It contains two sets of eight models of sizes 
70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two 
models: one trained on the Pile, and one trained on the Pile after the dataset 
has been globally deduplicated. All 8 model sizes are trained on the exact 
same data, in the exact same order. We also provide 154 intermediate 
checkpoints per model, hosted on Hugging Face as branches.; The Pythia model suite was deliberately designed to promote scientific 
research on large language models, especially interpretability research. 
Despite not centering downstream performance as a design goal, we find the 
models match or exceed the performance of 
similar and same-sized models, such as those in the OPT and GPT-Neo suites.; Previously, we released an early version of the Pythia suite to the public. 
However, we decided to retrain the model suite to address a few hyperparameter 
discrepancies. This model card lists the changes; 
see appendix B in the Pythia paper for further discussion. We found no 
difference in benchmark performance between the two Pythia versions. 
The old models are 
still available, but we 
suggest the retrained suite if you are just starting to use Pythia.
This is the current release.; Please note that all models in the Pythia suite were renamed in January 
2023. For clarity, a table 
comparing the old and new names is provided in this model card, together 
with exact parameter counts.; The primary intended use of Pythia is research on the behavior, functionality, 
and limitations of large language models. This suite is intended to provide 
a controlled setting for performing scientific experiments. We also provide 
154 checkpoints per model: initial step0, 10 log-spaced checkpoints 
step{1,2,4...512}, and 143 evenly-spaced checkpoints from step1000 to 
step143000. These checkpoints are hosted on Hugging Face as branches. Note 
that branch 143000 corresponds exactly to the model checkpoint on the main 
branch of each model."
llama-65b-4bit,,Transformers,English,,,69,,0,62781.45282,,https://huggingface.co/maderix/llama-65b-4bit,"Converted with https://github.com/qwopqwop200/GPTQ-for-LLaMa 
All models tested on A100-80G
*Conversion may require lot of RAM, LLaMA-7b takes ~12 GB, 13b around 21 GB, 30b around 62 and 65b takes more than 120 GB of RAM. ; Installation instructions as mentioned in above repo:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
starencoder,,PyTorch; Transformers,code,,https://arxiv.org/pdf/1810.04805.pdf,18,,"4,553",0,,https://huggingface.co/bigcode/starencoder,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Please read the BigCode OpenRAIL-M license agreement before accepting it.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This is an encoder-only model (i.e., bi-directionally self-attentive Transformers) trained on The Stack dataset.; We leveraged the :"
pix2struct-ai2d-base,Visual Question Answering,PyTorch; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/2210.03347.pdf,21,,"1,126",569.1191047,2,https://huggingface.co/google/pix2struct-ai2d-base,"; Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The full list of available models can be found on the Table 1 of the paper:; ; The abstract of the model states that: ; Visually-situated language is ubiquitous―sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and
forms. Perhaps due to this diversity, previous work has typically relied on domainspecific recipes with limited sharing of the underlying data, model architectures,
and objectives. We present Pix2Struct, a pretrained image-to-text model for
purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse
masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large
source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy,
we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions
are rendered directly on top of the input image. For the first time, we show that a
single pretrained model can achieve state-of-the-art results in six out of nine tasks
across four domains: documents, illustrations, user interfaces, and natural images."
alpaca-native,Text Generation,PyTorch; Transformers,,,,242,,"22,777",27597.51347,17,https://huggingface.co/chavinlo/alpaca-native,"This is a replica of Alpaca by Stanford' tatsu; Trained using the original instructions with a minor modification in FSDP mode; 13B: https://huggingface.co/chavinlo/alpaca-13b; 13B -> GPT4 : https://huggingface.co/chavinlo/gpt4-x-alpaca; Trained on 4xA100s for 6H
Donated by redmond.ai"
chatgpt_paraphraser_on_T5_base,Text2Text Generation,PyTorch; Transformers,English,openrail,,52,humarin/chatgpt-paraphrases,"10,866",895.2052085,7,https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base,"This model was trained on our ChatGPT paraphrase dataset.; This dataset is based on the Quora paraphrase question, texts from the SQUAD 2.0 and the CNN news dataset.; This model is based on the T5-base model. We used ""transfer learning"" to get our model to generate paraphrases as well as ChatGPT. Now we can say that this is one of the best paraphrases of the Hugging Face.; Kaggle link; Input:"
swissbert,Fill-Mask,PyTorch; Transformers,5 languages,cc-by-nc-4.0,https://arxiv.org/pdf/2303.13310.pdf,11,,403,613.1679663,,https://huggingface.co/ZurichNLP/swissbert,"SwissBERT is a masked language model for processing Switzerland-related text. It has been trained on more than 21 million Swiss news articles retrieved from Swissdox@LiRI.; SwissBERT is based on X-MOD, which has been pre-trained with language adapters in 81 languages.
For SwissBERT we trained adapters for the national languages of Switzerland C German, French, Italian, and Romansh Grischun.
In addition, we used a Switzerland-specific subword vocabulary.; The pre-training code and usage examples are available here. We also release a version that was fine-tuned on named entity recognition (NER): https://huggingface.co/ZurichNLP/swissbert-ner; SwissBERT contains the following language adapters:; Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)."
chatglm-6b-int4,,PyTorch; Transformers,Chinese; English,,,354,,"37,794",3986.204555,60,https://huggingface.co/THUDM/chatglm-6b-int4,"
    ? Join our Slack and WeChat
; ChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 ChatGLM 相同的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。; ChatGLM-6B-INT4 是 ChatGLM-6B 量化后的模型权重。具体的，ChatGLM-6B-INT4 对 ChatGLM-6B 中的 28 个 GLM Block 进行了 INT4 量化，没有对 Embedding 和 LM Head 进行量化。量化后的模型理论上 6G 显存（使用 CPU 即内存）即可推理，具有在嵌入式设备（如树莓派）上运行的可能。; 在 CPU 上运行时，会根据硬件自动编译 CPU Kernel ，请确保已安装 GCC 和 OpenMP （Linux一般已安装，对于Windows则需手动安装），以获得最佳并行计算能力。; 可以通过如下代码调用 ChatGLM-6B 模型来生成对话："
ClinicalBERT,Fill-Mask,PyTorch; Transformers,,,,32,,"16,219",542.9775732,3,https://huggingface.co/medicalai/ClinicalBERT,"This model card describes the ClinicalBERT model, which was trained on a large multicenter dataset with a large corpus of 1.2B words of diverse diseases we constructed.
We then utilized a large-scale corpus of EHRs from over 3 million patient records to fine tune the base language model.; The ClinicalBERT model was trained on a large multicenter dataset with a large corpus of 1.2B words of diverse diseases we constructed.; The ClinicalBERT was initialized from BERT. Then the training followed the principle of masked language model, in which given a piece of text, we randomly replace some tokens by MASKs, 
special tokens for masking, and then require the model to predict the original tokens via contextual text. ; We used a batch size of 32, a maximum sequence length of 256, and a learning rate of 5e-5 for pre-training our models. ; Load the model via the transformers library:"
stable-diffusion-2-1-unclip,Text-to-Image,Diffusers,,openrail++,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/1910.09700.pdf,189,,"23,667",14470.22676,29,https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip,"This model card focuses on the model associated with the Stable Diffusion v2-1 model, codebase available here.; This stable-diffusion-2-1-unclip is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model; Language(s): English"
kl-f8-anime2.vae,,,,creativeml-openrail-m,,3,,0,0,,https://huggingface.co/Bingsu/kl-f8-anime2.vae,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; AI ???? ????.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; AI ????? User Access requests ??? ???? ?? ?? ??; ?? ???? ???? ???? ???."
IF-II-L-v1.0,Text-to-Image,PyTorch; Diffusers,,deepfloyd-if-license,https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/2110.02861.pdf,39,,"12,944",0,18,https://huggingface.co/DeepFloyd/IF-II-L-v1.0,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; DeepFloyd LICENSE AGREEMENTThis License Agreement (as may be amended in accordance with this License Agreement, “License”), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (“Licensee” or “you”) and Stability AI Ltd.. (“Stability AI” or “we”) applies to your use of any computer program, algorithm, source code, object code, or software that is made available by Stability AI under this License (“Software”) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (“Documentation”).By clicking “I Accept” below or by using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model, that can generate pictures with new state-of-the-art for photorealism and language understanding. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID-30K score of 6.66 on the COCO dataset.; Inspired by Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
Alpaca-native-4bit-ggml,,,,other,,180,,0,4311.224479,,https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml,"This is a https://huggingface.co/chavinlo/alpaca-native converted in OLD GGML (alpaca.cpp) format and quantized to 4 bits to run on CPU with 5GB of RAM.; For any additional information, please visit these repos:; alpaca.cpp repo: https://github.com/antimatter15/alpaca.cpp; llama.cpp repo: https://github.com/ggerganov/llama.cpp; original facebook llama(NOT ggml) repo: https://github.com/facebookresearch/llama"
NGMix,,,,,,10,,0,37934.60145,,https://huggingface.co/Nyangyu/NGMix,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
negative,,,,,,14,,0,0.390449219,37,https://huggingface.co/embed/negative,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
GroundingDINO,,,English,apache-2.0,https://arxiv.org/pdf/2303.05499.pdf,45,detection-datasets/coco; conceptual_captions,0,1635.120342,27,https://huggingface.co/ShilongLiu/GroundingDINO,"?Paper | 
??Video |
?Demo on Colab | 
?Demo on HF (Coming soon) ; If you find our work helpful for your research, please consider citing the following BibTeX entry.   ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
gpt4all-lora,,,English,gpl-3.0,,203,nomic-ai/gpt4all_prompt_generations,0,8.413138428,,https://huggingface.co/nomic-ai/gpt4all-lora,"An autoregressive transformer trained on data curated using Atlas.
This model is trained with four full epochs of training, while the related gpt4all-lora-epoch-3 model is trained with three.
Replication instructions and data: https://github.com/nomic-ai/gpt4all; Developed by: Nomic AI; Model Type: An auto-regressive language model based on the transformer architecture and fine-tuned.; Languages: English; License: GPL-3.0"
pygmalion-6b_dev-4bit-128g,Text Generation,Transformers,English,creativeml-openrail-m,,111,,"2,657",4058.412906,,https://huggingface.co/mayaeary/pygmalion-6b_dev-4bit-128g,GPTQ quantization of https://huggingface.co/PygmalionAI/pygmalion-6b/commit/30e2405100eac6bd53f75964cc7345eeafd19f7d; Using this repository: https://github.com/mayaeary/GPTQ-for-LLaMa/tree/gptj-v2; Command: ; Inference API has been turned off for this model.
alpaca-7b-native-enhanced,Text Generation,Adapter Transformers,English,wtfpl,,106,,0,50493.44255,,https://huggingface.co/Pi3141/alpaca-7b-native-enhanced,"Use this command to run with llama.cpp; contents of prompts/alpacanativeenhanced.txt should be; Original model https://huggingface.co/8bit-coder/alpaca-7b-nativeEnhanced; Inference API does not yet support adapter-transformers models for this pipeline type.
							"
galpaca-30b,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,,54,tatsu-lab/alpaca,60,62138.53498,,https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b,"GALACTICA 30B fine-tuned on the Alpaca dataset.; The model card from the original Galactica repo can be found here, and the original paper here.; The dataset card for Alpaca can be found here, and the project homepage here.
  The Alpaca dataset was collected with a modified version of the Self-Instruct Framework, and was built using OpenAI's text-davinci-003 model. As such it is subject to OpenAI's terms of service.; The GALACTICA models are trained on a large-scale scientific corpus and are designed to perform scientific tasks.
The Alpaca dataset is a set of 52k instruct-response pairs designed to enhace the instruction following capabilites of pre-trained language models.; The GALACTICA model card specifies that the primary indended users of the GALACTICA models are researchers studying language models applied to the scientific domain, and it cautions against production use of GALACTICA without safeguards due to the potential for the model to produce inaccurate information.
The original GALACTICA models are available under a non-commercial CC BY-NC 4.0 license, and the GALPACA model is additionally subject to the OpenAI Terms of Service."
medalpaca-lora-30b-8bit,Text Generation,Transformers,English,cc,https://arxiv.org/pdf/2106.09685.pdf; https://arxiv.org/pdf/2303.14070.pdf,12,,0,205.0044625,,https://huggingface.co/medalpaca/medalpaca-lora-30b-8bit,"Model Description ; medalpaca-lora-30b-8bit is a large language model specifically fine-tuned for medical domain tasks. 
It is based on LLaMA (Large Language Model Meta AI) and contains 7 billion parameters. 
The primary goal of this model is to improve question-answering and medical dialogue tasks.
It was trained using LoRA and quantized, to reduce memory footprint. ; The training data for this project was sourced from various resources. 
Firstly, we used Anki flashcards to automatically generate questions, 
from the front of the cards and anwers from the back of the card. 
Secondly, we generated medical question-answer pairs from Wikidoc. 
We extracted paragraphs with relevant headings, and used Chat-GPT 3.5 
to generate questions from the headings and using the corresponding paragraphs 
as answers. This dataset is still under development and we believe 
that approximately 70% of these question answer pairs are factual correct. 
Thirdly, we used StackExchange to extract question-answer pairs, taking the 
top-rated question from five categories: Academia, Bioinformatics, Biology, 
Fitness, and Health. Additionally, we used a dataset from ChatDoctor 
consisting of 200,000 question-answer pairs, available at https://github.com/Kent0n-Li/ChatDoctor.; The model may not perform effectively outside the scope of the medical domain.
The training data primarily targets the knowledge level of medical students, 
which may result in limitations when addressing the needs of board-certified physicians.
The model has not been tested in real-world applications, so its efficacy and accuracy are currently unknown. 
It should never be used as a substitute for a doctor's opinion and must be treated as a research tool only."
instruct-igel-001,Text Generation,PyTorch; Transformers,German,,,37,,"1,962",13229.98629,4,https://huggingface.co/philschmid/instruct-igel-001,"IGEL is an LLM model family developed for German. The first version of IGEL is built on top?BigScience BLOOM,?adapted to?German from Malte Ostendorff. IGEL is designed to provide accurate and reliable language understanding capabilities for a wide range of natural language understanding tasks, including sentiment analysis, language translation, and question answering.; The IGEL family currently includes?instruct-igel-001?and?chat-igel-001?(coming soon).; LoRA tuned BLOOM-CLP German (6.4B parameters) with merged weights. The 001 was designed as a naive test to determine whether it is possible to create an german instruction-tuned model using a small, undertrained LLM and a naive translated dataset. The goal of this test was to explore the potential of the BLOOM architecture for language modeling tasks that require instruction-based responses.; To achieve this goal, we used a pre-trained LLM model with limited training, and fine-tuned it using a dataset of naive translations of instruction-based content. The dataset was created by taking instructions in English and translating them into German using an automated translation tool. While this approach may introduce errors in the translated content, we wanted to test whether the model could still learn to generate instruction-based responses in a variety of languages.; instruct-igel-001 is trained on naive translated instruction datasets, without much post-processing. "
llama-7b-se-rl-peft,,PyTorch; Transformers,English,bigscience-openrail-m,,93,lvwerra/stack-exchange-paired,0,34.11359489,2,https://huggingface.co/trl-lib/llama-7b-se-rl-peft,"; Adapter weights of a Reinforcement Learning fine-tuned model based on the LLaMA model (see Meta's LLaMA release for the original LLaMA model). 
The model is designed to generate human-like responses to questions in Stack Exchange domains of programming, mathematics, physics, and more.
For more info check out the blog post and github example.; Developed by: Hugging Face; Model type: An auto-regressive language model based on the transformer architecture, and fine-tuned with Stack Exchange datasets. ; Languages: Predominantly English, with additional data from languages with the following ISO codes: "
matcha-chartqa,Visual Question Answering,PyTorch; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/2212.09662.pdf,11,,799,1161.236487,4,https://huggingface.co/google/matcha-chartqa,"; This model is the MatCha model, fine-tuned on Chart2text-pew dataset. ; The abstract of the paper states that: ; Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art visionlanguage models do not perform well on these data. We propose MATCHA (Math reasoning and Chart derendering pretraining) to enhance visual language models’ capabilities jointly modeling charts/plots and language data. Specifically we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MATCHA pretraining starting from Pix2Struct, a recently proposed imageto-text visual language model. On standard benchmarks such as PlotQA and ChartQA, MATCHA model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MATCHA pretraining transfers to domains such as screenshot, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MATCHA pretraining on broader visual language tasks.; You should ask specific questions to the model in order to get consistent generations. Here we are asking the model whether the sum of values that are in a chart are greater than the largest value."
llama-13b,Text Generation,PyTorch; Safetensors; Transformers,,other,,102,,"37,075",53311.84752,2,https://huggingface.co/huggyllama/llama-13b,"This contains the weights for the LLaMA-13b model. This model is under a non-commercial license (see the LICENSE file).
You should only use this repository if you have been granted access to the model by filling out this form but either lost your copy of the weights or got some trouble converting them to the Transformers format."
ImageReward,Text-to-Image,,English,apache-2.0,https://arxiv.org/pdf/2304.05977.pdf,22,,0,3042.048666,2,https://huggingface.co/THUDM/ImageReward,"
Github Repo ? ? Twitter ? ? Paper 
; ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation; ImageReward is the first general-purpose text-to-image human preference RM which is trained on in total 137k pairs of
expert comparisons, based on text prompts and corresponding model outputs from DiffusionDB. We demonstrate that
ImageReward outperforms existing text-image scoring methods, such as CLIP, Aesthetic, and BLIP, in terms of
understanding human preference in text-to-image synthesis through extensive analysis and experiments.; ; We have integrated the whole repository to a single python package image-reward. Following the commands below to prepare the environment:"
segment-anything,,,,apache-2.0,,53,,0,0.008212891,5,https://huggingface.co/ybelkada/segment-anything,"NEW Segment Anything now officially supported in transformers! Check out the official documentation; This repository is the mirror of the official Segment Anything repository, together with the model weights. We also provide instructions on how to easily download the model weights.; Meta AI Research, FAIR; Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick; [Paper] [Project] [Demo] [Dataset] [Blog] [BibTeX]"
saiga_7b_lora,Conversational,,Russian,cc-by-4.0,,21,IlyaGusev/ru_turbo_alpaca; IlyaGusev/ru_turbo_saiga; IlyaGusev/ru_sharegpt_cleaned; IlyaGusev/oasst1_ru_main_branch; IlyaGusev/ru_turbo_alpaca_evol_instruct; lksy/ru_instruct_gpt4,0,67.70327923,,https://huggingface.co/IlyaGusev/saiga_7b_lora,Based on LLaMA 7B.; Colab: link; llama.cpp version: link; Training code: link; Examples:
rwkv-4-novel,Text Generation,PyTorch,English,apache-2.0,,48,the_pile,0,57692.16202,1,https://huggingface.co/BlinkDL/rwkv-4-novel,"These are RWKV-4-Pile models finetuned on novels.; Currently I am doing it for Chn novels. More languages to come.; Use https://github.com/BlinkDL/ChatRWKV to run them.; See https://github.com/BlinkDL/RWKV-LM for details on the RWKV Language Model (100% RNN).; Unable to determine this model’s library. Check the
								docs 
.
							"
hololivemix-so-vits-svc-4.0,,,,creativeml-openrail-m,,68,,0,0.002218742,,https://huggingface.co/megaaziib/hololivemix-so-vits-svc-4.0,"Tolong Credit nama saya (Romario Martinus) atau username saya (megaaziib) kalau pake voice model ini! 
Credit me if you use my model 
For use with so-vits-svc-fork repo  
Use this tutorial on Youtube for guidelines  
Or simply just run on Colab without setup 
Support me: 
Paypal: https://paypal.me/romramgames 
Patreon: https://www.patreon.com/romariomartinus 
ko-fi: https://ko-fi.com/megaaziib 
Saweria: https://saweria.co/romariomartinus ; the colab is not made by me so don't ask question ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
stablelm-base-alpha-7b,Text Generation,PyTorch; Transformers,English,cc-by-sa-4.0,,206,,"1,582",32514.137,7,https://huggingface.co/stabilityai/stablelm-base-alpha-7b,StableLM-Base-Alpha is a suite of 3B and 7B parameter decoder-only language models pre-trained on a diverse collection of English datasets with a sequence length of 4096 to push beyond the context window limitations of existing open-source language models.; Get started generating text with StableLM-Base-Alpha by using the following code snippet:; Developed by: Stability AI; Model type: StableLM-Base-Alpha models are auto-regressive language models based on the NeoX transformer architecture.; Language(s): English
PMC_LLAMA_7B,Text Generation,PyTorch; Transformers,,apache-2.0,,35,allenai/s2orc,"2,321",27597.31758,1,https://huggingface.co/chaoyi-wu/PMC_LLAMA_7B,"This repo contains PMC_LLaMA_7B, which is LLaMA-7b finetuned on the PMC papers in S2ORC dataset.; The model was trained with the following hyperparameters:; Each epoch we sample 512 tokens per paper for training.; The model can be loaded as following:"
saiga_13b_lora,Conversational,,Russian,cc-by-4.0,,10,IlyaGusev/ru_turbo_alpaca; IlyaGusev/ru_turbo_saiga; IlyaGusev/oasst1_ru_main_branch; IlyaGusev/ru_sharegpt_cleaned; IlyaGusev/ru_turbo_alpaca_evol_instruct; lksy/ru_instruct_gpt4,0,105.5017298,1,https://huggingface.co/IlyaGusev/saiga_13b_lora,Based on LLaMA 13B.; llama.cpp version: link; Colab: link; Training code: link; Examples:
control_v11p_sd15_canny,Image-to-Image,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,18,,"147,906",4415.678157,51,https://huggingface.co/lllyasviel/control_v11p_sd15_canny,"Controlnet v1.1 is the successor model of Controlnet v1.0
and was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ; "
control_v11p_sd15_openpose,Image-to-Image,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,11,,"439,110",4415.677681,49,https://huggingface.co/lllyasviel/control_v11p_sd15_openpose,"Controlnet v1.1 is the successor model of Controlnet v1.0
and was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ; "
OPT-30B-Erebus-4bit-128g,Text Generation,PyTorch; Transformers,English,other,,16,amilkov/literotica,202,34715.09156,,https://huggingface.co/Zicara/OPT-30B-Erebus-4bit-128g,"Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; This is a 4-bit GPTQ quantization of OPT-30B-Erebus, original model:
https://huggingface.co/KoboldAI/OPT-30B-Erebus; Quantized with: https://github.com/0cc4m/GPTQ-for-LLaMa; Output generated in 54.23 seconds (0.87 tokens/s, 47 tokens, context 44, seed 593020441); https://github.com/oobabooga/text-generation-webui"
flan-alpaca-gpt4-xl,Text2Text Generation,PyTorch; Safetensors; Transformers,,apache-2.0,https://arxiv.org/pdf/2306.04757.pdf; https://arxiv.org/pdf/2210.11416.pdf,31,tatsu-lab/alpaca,"40,065",23350.50528,,https://huggingface.co/declare-lab/flan-alpaca-gpt4-xl,"? We developed Flacuna by fine-tuning Vicuna-13B on the Flan collection. Flacuna is better than Vicuna at problem-solving. Access the model here https://huggingface.co/declare-lab/flacuna-13b-v1.0.; ? Curious to know the performance of ? ? Flan-Alpaca on large-scale LLM evaluation benchmark, InstructEval? Read our paper https://arxiv.org/pdf/2306.04757.pdf. We evaluated more than 10 open-source instruction-tuned LLMs belonging to various LLM families including Pythia, LLaMA, T5, UL2, OPT, and Mosaic. Codes and datasets: https://github.com/declare-lab/instruct-eval; ? FLAN-T5 is also useful in text-to-audio generation. Find our work at https://github.com/declare-lab/tango if you are interested.; Our repository contains code for extending the Stanford Alpaca
synthetic instruction tuning to existing instruction-tuned models such as Flan-T5.
We have a live interactive demo thanks to Joao Gante!
We are also benchmarking many instruction-tuned models at declare-lab/flan-eval.
Our pretrained models are fully available on HuggingFace ? :; *recommended for better performance"
ChatGalRWKV,,,,unlicense,,17,,0,99423.923,1,https://huggingface.co/Synthia/ChatGalRWKV,"关于项目的名字：chat就是猫（法语），gal就是娘（英语）。; 本项目基于RWKV，它是一系列从预训练数据、训练代码、推理代码到模型权重都完全开源的大语言模型，并且与基于transformer的模型相比有诸多优势。; 可以使用Colab脚本自建服务运行最近模型。; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
vicuna-13B-1.1-Chinese-GPTQ-4bit-128g,Text Generation,Transformers,,,,19,,69,8010.675108,,https://huggingface.co/jfiekdjdk/vicuna-13B-1.1-Chinese-GPTQ-4bit-128g,"This model was obtained from following repo:; Merged using sciprts from: https://github.com/ymcui/Chinese-LLaMA-Alpaca; Then quanized using following command (no act order):; Can confirm model output normal text, but question-answering quality is unknown; Model type:
Vicuna is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.
It is an auto-regressive language model, based on the transformer architecture."
chilled_remix,Text-to-Image,Diffusers,Japanese,creativeml-openrail-m,,176,,0,4362.281387,,https://huggingface.co/sazyou-roukaku/chilled_remix,"【告知】chilled_remix及びreversemixは2023年5月21日にVersion涓を行い、v2へ移行いたしました。伴いv1は削除致しました。なお既にDLgみの方は引きAき、v1をご利用いただくことは}ございません。 ; License:CreativeML Open RAIL-M
Additional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 21, 2023; このモデルは『CreativeML Open RAIL-M』でLicenseそのものに涓はありません。
しかし追加著作者としてi城郎郭の名前が追加されています。
しかし追加著作者として佐城郎画の名前が追加されています。(6/10 Twitterネ`ム涓に伴い、表涓。License内はsazyou_roukakuの涓なし)
なお『CreativeML Open RAIL-M』にdされている通り、
本モデルを使用しての生成物にvしてはLicenseの使用制限Aの事例を除き、当方は一切v与致しません。
犯罪目的利用や医用画像など特定T的な用途での利用は使用制限Aで禁止されています。
必ず_Jしご利用ください。
また当方は一切任を持ちません。免されていることをご了承の上、ご使用ください。; 推XO定?モデルの`い?プロンプト ;  Version2はfp16でVAEきzみ版のみ配布といたしました。 基本的にはchilled_remixをメインとし、好みに合わせてreversemixも视というのがスタンスです。 ※chilled_remixはchilled_re-genericユ`ザ`をあるX婴扦位炻窑ら守るために生み出されたモデルです。 性|上全てのユ`ザ`出力に辘扦なかった椤サブとしてreversemixが作られました。 reversemixはLORAなしでものセミリアル感は薄いですが、全体的に幼くなるA向があります。  "
stablelm-tuned-alpha-7b,Text Generation,PyTorch; Transformers,English,cc-by-nc-sa-4.0,,348,dmayhem93/ChatCombined; tatsu-lab/alpaca; nomic-ai/gpt4all_prompt_generations; Dahoas/full-hh-rlhf; jeffwan/sharegpt_vicuna; HuggingFaceH4/databricks_dolly_15k,"40,884",32514.14015,159,https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b,"StableLM-Tuned-Alpha is a suite of 3B and 7B parameter decoder-only language models built on top of the StableLM-Base-Alpha models and further fine-tuned on various chat and instruction-following datasets.; Get started chatting with StableLM-Tuned-Alpha by using the following code snippet:; StableLM Tuned should be used with prompts formatted to <|SYSTEM|>...<|USER|>...<|ASSISTANT|>...
The system prompt is; StableLM-Tuned-Alpha models are fine-tuned on a combination of five datasets:
Alpaca, a dataset of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine.
GPT4All Prompt Generations, which consists of 400k prompts and responses generated by GPT-4;
Anthropic HH, made up of preferences about AI assistant helpfulness and harmlessness;
DataBricks Dolly, comprising 15k instruction/responses generated by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA and summarization;
and ShareGPT Vicuna (English subset), a dataset of conversations retrieved from ShareGPT.; Models are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (FP16), and optimized with AdamW. We outline the following hyperparameters:"
moss-moon-003-base,Text Generation,PyTorch; Transformers,English; Chinese,agpl-3.0,https://arxiv.org/pdf/2203.13474.pdf,121,fnlp/moss-002-sft-data,"1,916",34389.9014,119,https://huggingface.co/fnlp/moss-moon-003-base,"MOSS is an open-sourced plugin-augmented conversational language model. moss-moon models have 16B parameters, allowing users to perform inference on a single A100 GPU or 2 NVIDIA 3090 GPUs with FP16 precision, and on a single NVIDIA 3090 GPU with INT-4/8 precision. The base language model of MOSS was pre-trained on ~700B English, Chinese, and code tokens, including the PILE, BigQuery, BigPython, and our private Chinese corpus. The base model was then fine-tuned on multi-turn plugin-augmented conversational data. Finally, we performed preference-aware training to further improve the model.; Limitations: Due to the (relatively) small number of parameters and the autoregressive nature, MOSS is still possible to generate outputs that contain incorrect, misleading, or biased information. Please carefully check the contents generated by MOSS before you use them.; MOSS Use Cases：; ; "
chilloutmix_NiPrunedFp32Fix,Text-to-Image,Diffusers,English,creativeml-openrail-m,,23,,"6,970",0.003892021,1,https://huggingface.co/emilianJR/chilloutmix_NiPrunedFp32Fix,"Diffuser model for this SD checkpoint:
https://civitai.com/models/6424/chilloutmix; emilianJR/chilloutmix_NiPrunedFp32Fix is the HuggingFace diffuser that you can use with diffusers.StableDiffusionPipeline().; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
The CreativeML OpenRAIL License specifies: 
Please read the full license here"
mpt-1b-redpajama-200b-dolly,Text Generation,PyTorch; Transformers,,cc-by-sa-3.0,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf,76,togethercomputer/RedPajama-Data-1T,"2,582",5378.178587,,https://huggingface.co/mosaicml/mpt-1b-redpajama-200b-dolly,"MPT-1b-RedPajama-200b-dolly is a 1.3 billion parameter decoder-only transformer pre-trained on the RedPajama dataset and subsequently fine-tuned on the Databricks Dolly instruction dataset.
The model was pre-trained for 200B tokens by sampling from the subsets of the RedPajama dataset in the same proportions as were used by the Llama series of models.
This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; This model is an instruction fine-tuned version of mpt-1b-redpajama-200b. In other words, the pre-trained version of this model is mpt-1b-redpajama-200b.; April 20, 2023; Note: This model requires that trust_remote_code=True be passed to the from_pretrained method. 
This is because we use a custom model architecture MosaicGPT that is not yet part of the transformers package.
MosaicGPT includes options for many training efficiency features such as FlashAttention (Dao et al. 2022), ALIBI, QK LayerNorm, and more.; To use the optimized triton implementation of FlashAttention, you can load with attn_impl='triton' and move the model to bfloat16 like so:"
gpt4-x-alpaca-13b-roleplay-lora-4bit-v2,Text Generation,PyTorch; Transformers,,mit,,8,,58,7189.10789,1,https://huggingface.co/4bit/gpt4-x-alpaca-13b-roleplay-lora-4bit-v2,"This is a llama-13B based model that has been converted with GPTQ to 4bit quantized model.; Base Model: GPT4-x-Alpaca full fine tune by Chavinlo -> https://huggingface.co/chavinlo/gpt4-x-alpacaLORA fine tune using the Roleplay Instruct from GPT4 generated dataset -> https://github.com/teknium1/GPTeacher/tree/main/RoleplayLORA Adapter Only: https://huggingface.co/ZeusLabs/gpt4-x-alpaca-rp-lora - The v2 one -; Merged LORA to the model. ; FYI Latest HF Transformers generates BROKEN generations. 
Try this instead if your generations are terrible (first uninstall transformers): pip install git+https://github.com/huggingface/transformers@9eae4aa57650c1dbe1becd4e0979f6ad1e572ac0
More info and possible alternative solutions in these github issues.https://github.com/tloen/alpaca-lora/issues/279#issuecomment-1514725886https://github.com/oobabooga/text-generation-webui/issues/931#issuecomment-1516215250 ; Instructions simply using alpaca format are likely to be of lower quality. If you want pure general instruct capability I reccomend GPT-4-X-Alpaca (the base model of this) - 
The model responds well to giving it a roleplay task in the preprompt, and the actual conversation in the ""### Input: "" field."
codegen2-3_7B,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2305.02309.pdf,9,,943,14984.70534,2,https://huggingface.co/Salesforce/codegen2-3_7B,"CodeGen2 is a family of autoregressive language models for program synthesis, introduced in the paper:; CodeGen2: Lessons for Training LLMs on Programming and Natural Languages by Erik Nijkamp*, Hiroaki Hayashi*, Caiming Xiong, Silvio Savarese, Yingbo Zhou.; Unlike the original CodeGen model family (i.e., CodeGen1), CodeGen2 is capable of infilling, and supports more programming languages.; Four model sizes are released: 1B, 3.7B, 7B, 16B.; This model can be easily loaded using the AutoModelForCausalLM functionality."
BBT,,,,,https://arxiv.org/pdf/2302.09432.pdf,9,,0,0,,https://huggingface.co/SuSymmertry/BBT,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; 

;   ; 论文链接：https://arxiv.org/abs/2302.09432"
WizardLM-7B-V1.0,Text Generation,PyTorch; Transformers,,,,55,,"1,354",27599.15773,1,https://huggingface.co/WizardLM/WizardLM-7B-V1.0,The WizardLM delta weights.
audioldm-m-full,,Diffusers,,,https://arxiv.org/pdf/2301.12503.pdf,17,,"1,410",0.00641716,8,https://huggingface.co/cvssp/audioldm-m-full,"AudioLDM is a latent text-to-audio diffusion model capable of generating realistic audio samples given any text input. It is available in the ? Diffusers library from v0.15.0 onwards.; AudioLDM was proposed in the paper AudioLDM: Text-to-Audio Generation with Latent Diffusion Models by Haohe Liu et al.; Inspired by Stable Diffusion, AudioLDM
is a text-to-audio latent diffusion model (LDM) that learns continuous audio representations from CLAP
latents. AudioLDM takes a text prompt as input and predicts the corresponding audio. It can generate text-conditional
sound effects, human speech and music.; This is the medium version of the AudioLDM model, which has a larger UNet, CLAP audio projection dim, and is trained with audio embeddings as condition. The four AudioLDM checkpoints are summarised below:; Table 1: Summary of the AudioLDM checkpoints."
wizardLM-7B-GPTQ,Text Generation,Transformers,,other,,104,,"1,788",4630.821333,,https://huggingface.co/TheBloke/wizardLM-7B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for WizardLM's WizardLM-7B 4bit.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
saiga_30b_lora_llamacpp,Text2Text Generation,,Russian,,,20,IlyaGusev/ru_turbo_alpaca; IlyaGusev/ru_turbo_saiga; IlyaGusev/ru_sharegpt_cleaned,0,97177.60221,,https://huggingface.co/IlyaGusev/saiga_30b_lora_llamacpp,Llama.cpp compatible version of an original 30B model.; How to run:; System requirements:; Inference API has been turned off for this model.
chinese-llama-plus-lora-7b,,,Chinese,apache-2.0,,28,,0,858.7426124,,https://huggingface.co/ziqingyang/chinese-llama-plus-lora-7b,"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
dog_emotion_v3,Image Classification,PyTorch; TensorBoard; Safetensors; Transformers,,,,4,,39,686.0060181,,https://huggingface.co/Dewa/dog_emotion_v3,No model card; New: Create and edit this model card directly on the website!
OpenAssistant-SFT-7-Llama-30B-GPTQ,Text Generation,Transformers,,other,https://arxiv.org/pdf/2304.07327.pdf,34,,116,17307.95917,,https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for OpenAssistant LLaMA 30B SFT 7.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
mdeberta-v3-base-tasksource-nli,Zero-Shot Classification,PyTorch; Safetensors; Transformers,27 languages,apache-2.0,https://arxiv.org/pdf/2301.05948.pdf,5,xnli; metaeval/xnli; americas_nli; MoritzLaurer/multilingual-NLI-26lang-2mil7; stsb_multi_mt; paws-x; miam; strombergnlp/x-stance; tyqiangz/multilingual-sentiments; metaeval/universal-joy; amazon_reviews_multi; cardiffnlp/tweet_sentiment_multilingual; strombergnlp/offenseval_2020; offenseval_dravidian; nedjmaou/MLMA_hate_speech; xglue; ylacombe/xsum_factuality; metaeval/x-fact; pasinit/xlwic; tasksource/oasst1_dense_flat; papluca/language-identification; wili_2018; exams; xcsr; xcopa; juletxara/xstory_cloze; Anthropic/hh-rlhf; universal_dependencies; tasksource/oasst1_pairwise_rlhf_reward; OpenAssistant/oasst1,757,2314.391051,,https://huggingface.co/sileod/mdeberta-v3-base-tasksource-nli,"Multilingual mdeberta-v3-base with 30k steps multi-task training on mtasksource
This model can be used as a stable starting-point for further fine-tuning, or directly in zero-shot NLI model or a zero-shot pipeline.
In addition, you can use the provided adapters to directly load a model for hundreds of tasks. ; For more details, see deberta-v3-base-tasksource-nli and replace tasksource by mtasksource.; https://github.com/sileod/tasksource/
https://github.com/sileod/tasknet/; For help integrating tasksource into your experiments, please contact damien.sileo@inria.fr.; For more details, refer to this article: "
pygmalion-7b,Text Generation,,English,,,149,,0,0.007662086,1,https://huggingface.co/PygmalionAI/pygmalion-7b,"Pygmalion 7B is a dialogue model based on Meta's LLaMA-7B.; This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project.; The model weights in this repository cannot be used as-is. The files here are XORs due to licensing concerns. To obtain proper, usable model weights you need to:; Request access to the original LLaMA weights from Meta through this form; Convert them to the HuggingFace Transformers format by using the convert_llama_weights_to_hf.py script for your version of the transformers library"
Pygmalion-7b-4bit-Q4_1-GGML,Text Generation,,English,,,19,,0,10362.88467,,https://huggingface.co/TehVenom/Pygmalion-7b-4bit-Q4_1-GGML,"Currently KoboldCPP is unable to stop inference when an EOS token is emitted, which causes the model to devolve into gibberish,; Pygmalion 7B is now fixed on the dev branch of KoboldCPP, which has fixed the EOS issue. Make sure you're compiling the latest version, it was fixed only a after this model was released;; When running KoboldCPP, you will need to add the --unbantokens flag for this model to behave properly.; Pygmalion 7B is a dialogue model based on Meta's LLaMA-7B.; This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project."
LLaVA-7b-delta-v0,Text Generation,PyTorch; Transformers,,apache-2.0,,11,,853,13822.68953,10,https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0,"NOTE: This ""delta model"" cannot be used directly.Users have to apply it on top of the original LLaMA weights to get actual LLaVA weights.See https://github.com/haotian-liu/LLaVA#llava-weights for instructions.; Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA was trained in April 2023.; Paper or resources for more information:
https://llava-vl.github.io/; License:
Apache License 2.0"
backups,,,,,,6,,0,0,,https://huggingface.co/malikxseto/backups,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Backups of models I found and I like; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
LoRAArchive,Text-to-Image,Safetensors,,creativeml-openrail-m,,23,,0,0,,https://huggingface.co/LMFResearchSociety/LoRAArchive,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; LyCORIS (LoCon, LoHa, LoKR, DyLoRA) located here:
https://huggingface.co/LMFResearchSociety/LyCORISArchive; Too much of a pain to separate different types of LyCoris. Some LoCons might be mixed in with LoRAs if the repository doesn't spearate them. Just drop a note in the discord groupchat if you're bothered.; Example Generator: 
https://civitai.com/models/27968 
https://huggingface.co/Lykon/AnyLoRA"
Defecation,,,,openrail,,12,,0,9.572958984,,https://huggingface.co/JollyIm/Defecation,"Here's the defecation lora, it was available on Civitai until the ban on scat content.
You can use various trigger words to get different effects, like ""Scat"", ""Disposal"", ""Feces"" and so on.
The main problem with this model is that that it tends to confuse the anus and the vagina, so you'll have to add prompts and negatives usefull to reduce this effect.; You can find my other models on Civitai: https://civitai.com/user/JollyIm/models; A first example:

Prompts: Realistic, Realism, (Masterpiece, Best Quality, High Quality, Highres:1.4), Detailed, Extremely Detailed, Ambient Soft Lighting, 4K, (Extremely Detailed Eyes, Detailed Face and Skin:1.2), masterpiece, best quality, 1girl, feces, disposal, (anal:1.2), lora:defecation_v1:0.7, (public toilet), embarassed, (pile of feces), (perfect pussy), (perfect vagina),
Negative prompt: easynegative, (worst quality:1.2), (low quality:1.2), (vaginal), (dirty vagina:1.2), (feces in vagina:1.2), (feces in vagina:1.2); Second example:

Prompts: masterpiece, best quality, 1girl, scat, (anal:1.2), lora:defecation_v1:0.9, (toilet), from behind,
Negative prompt: easynegative, (worst quality:1.2), (low quality:1.2), (vaginal), (dirty vagina:1.2), (scat in vagina:1.2), (feces in vagina:1.2); Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
KoreanLM,Text Generation,PyTorch; Transformers,Korean; English,,,12,,162,27597.32299,,https://huggingface.co/quantumaikr/KoreanLM,"

; KoreanLM? ??? ????? ???? ?? ???? ???????. ?? ???? ?????? ??? ??? ??? ??, ???? ?? ??? ????? ???? ??? ???? ????? ??? ????. ??? ??? ???? ???? ???? ????? ???? ?? KoreanLM ????? ???? ?????.; ???? ??? ???? ??: ???? ??, ??, ??? ??? ???? ???? ? ???? ???? ??? ? ?? ????? ?????.; ???? ??? ?? ??: ??? ???? ??? ???? ????? ??? ??? ??? ??? ??? ??? ???? ????? ??? ??????.; ?? ????? ??? ??: ?? ??? ???? ?????? ??? ??? ???? ?????? ??? ??? ????. ?? ???? ?? ??? ????? ??? ???? ???? ????, ??? ?? ??? ? ?? ??? ? ??? ???."
VoiceAi_Jokowi,,Transformers,,,,47,,7,0,,https://huggingface.co/Byzern/VoiceAi_Jokowi,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; don't forget like; dilike dulu lah kalau mau download; Support me : https://saweria.co/donate/Byzernn"
control_v11f1e_sd15_tile,Image-to-Image,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,25,,"72,241",1484.87594,6,https://huggingface.co/lllyasviel/control_v11f1e_sd15_tile,"Controlnet v1.1 was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ; "
wizard-vicuna-13B-HF,Text Generation,PyTorch; Transformers,English,,,48,,742,26655.2512,,https://huggingface.co/TheBloke/wizard-vicuna-13B-HF,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a float16 HF format repo for junelee's wizard-vicuna 13B.; June Lee's repo was also HF format. The reason I've made this is that the original repo was in float32, meaning it required 52GB disk space, VRAM and RAM.; This model was converted to float16 to make it easier to load and manage."
mpt-7b-chat,Text Generation,PyTorch; Transformers,,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2010.04245.pdf,471,jeffwan/sharegpt_vicuna; Hello-SimpleAI/HC3; tatsu-lab/alpaca; Anthropic/hh-rlhf; victor123/evol_instruct_70k,"41,749",13621.459,20,https://huggingface.co/mosaicml/mpt-7b-chat,"MPT-7B-Chat is a chatbot-like model for dialogue generation.
It was built by finetuning MPT-7B on the ShareGPT-Vicuna, HC3,
 Alpaca, HH-RLHF, and Evol-Instruct datasets.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; CC-By-NC-SA-4.0 (non-commercial use only); SamIAm85:"
RedPajama-INCITE-Chat-3B-v1,Text Generation,PyTorch; Transformers,English,apache-2.0,,105,togethercomputer/RedPajama-Data-1T; OpenAssistant/oasst1; databricks/databricks-dolly-15k,"10,012",5828.6805,11,https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1,"RedPajama-INCITE-Chat-3B-v1 was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. ; It is fine-tuned on OASST1 and Dolly2 to enhance chatting ability.; Please note that the model requires transformers version >= 4.25.1.; To prompt the chat model, use the following format:; This requires a GPU with 8GB memory."
starcoder-gpteacher-code-instruct,Text Generation,PyTorch; Transformers,,bigcode-openrail-m,https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,61,bigcode/the-stack-dedup; teknium1/GPTeacher-codegen,330,64779.70004,2,https://huggingface.co/GeorgiaTechResearchInstitute/starcoder-gpteacher-code-instruct,"This model is bigcode/starcoder fine-tuned on the teknium1/GPTeacher codegen dataset (GPT-4 code instruction fine-tuning).; The base StarCoder models are 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2), with opt-out requests excluded. The model uses Multi Query Attention, a context window of 8192 tokens,  and was trained using the Fill-in-the-Middle objective on 1 trillion tokens. ; The base model was trained on GitHub code and then fine-tuned to follow instructions. Prompts such as ""Write a function that computes the square root."" should work reasonably well. The original repo recommeds using the Tech Assistant prompt to few-shot prompt it into behaving as a technical assistant. This fine-tuned model uses the Alpaca prompts.; Full Prompt:; Response:"
gpt4-x-vicuna-13B-HF,Text Generation,PyTorch; Transformers,,other,,20,,172,26657.08785,,https://huggingface.co/TheBloke/gpt4-x-vicuna-13B-HF,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains a float16 HF format model of NousResearch's gpt4-x-vicuna-13b.; I uploaded this model because NousResearch's base repository is inside an archive so it can't be used without first unpacking it. Also the model is in float32 format which requires a lot more VRAM and RAM to use.; The model in this repo has been converted to float16 and can be used immediately for float16 and 8bit inference, or used as the basis for other conversions."
mpt-7b-storywriter-4bit-128g,Text Generation,Safetensors; Transformers,,apache-2.0,https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2205.14135.pdf,115,the_pile_books3,489,3964.999812,,https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g,"Quantized for KoboldAI (4bit-fork); MPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths.
It was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset.
At inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.
We demonstrate generations as long as 84k tokens on a single node of 8 A100-80GB GPUs in our blogpost.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; Apache-2.0 (commercial use permitted)"
Ziya-LLaMA-7B-Reward,Text Classification,PyTorch; Transformers,,gpl,,49,,558,27064.84554,,https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-7B-Reward,"Ziya-LLaMA-7B-Reward基于Ziya-LLaMA模型，在以下偏好排序数据上进行训练：; 模型能够模拟中英双语生成的奖励环境，对LLM生成结果提供准确的奖励反馈。; Ziya-LLaMA-7B-Reward is based on the Ziya-LLaMA model, trained on the following preference ranking data:; The model is able to simulate a bilingual reward environment and provide accurate reward feedback on LLM generation results.; 模型可以较为准确地判断文本重复，异常中断和不符合指令要求等低质量模型生成结果，并给出较低的奖励值。"
Beautiful-Realistic-Asians-v5,,Diffusers,,,,13,,839,0.002045174,,https://huggingface.co/sinkinai/Beautiful-Realistic-Asians-v5,"run the model at: https://sinkin.ai/m/vlDnKP6; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
albertina-ptbr,Fill-Mask,PyTorch; Transformers,Portuguese,other,https://arxiv.org/pdf/2305.06721.pdf,33,brwac; PORTULAN/glue-ptpt; assin2; dlb/plue,743,1823.586573,1,https://huggingface.co/PORTULAN/albertina-ptbr,"????This is the model card for Albertina PT-BR. 
  You may be interested in some of the other models in the Albertina (encoders) and Gervásio (decoders) families.
; Albertina PT-* is a foundation, large language model for the Portuguese language.; It is an encoder of the BERT family, based on the neural architecture Transformer and 
developed over the DeBERTa model, and with most competitive performance for this language. 
It has different versions that were trained for different variants of Portuguese (PT), 
namely the European variant from Portugal (PT-PT) and the American variant from Brazil (PT-BR), 
and it is distributed free of charge and under a most permissible license.; Albertina PT-BR is the version for American Portuguese from Brazil, trained on the brWaC data set.; You may be interested also in Albertina PT-BR No-brWaC, trained on data sets other than brWaC and thus with a more permissive license.
To the best of our knowledge, these are encoders specifically for this language and variant 
that,  at the time of its initial distribution, set a new state of the art for it, and is made publicly available 
and distributed for reuse."
LLaVA-13b-delta-v1-1,Text Generation,PyTorch; Transformers,,apache-2.0,,13,,125,26665.48631,3,https://huggingface.co/liuhaotian/LLaVA-13b-delta-v1-1,"NOTE: This ""delta model"" cannot be used directly.Users have to apply it on top of the original LLaMA weights to get actual LLaVA weights.See https://github.com/haotian-liu/LLaVA#llava-weights for instructions.; Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA was trained in May 2023.; Paper or resources for more information:
https://llava-vl.github.io/; License:
Apache License 2.0"
GoodHands-beta2,,,,,,36,,0,0,,https://huggingface.co/jlsim/GoodHands-beta2,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
WizardLM-13B-Uncensored,Text Generation,PyTorch; Transformers,,other,,407,ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered,"1,434",26626.33705,40,https://huggingface.co/ehartford/WizardLM-13B-Uncensored,"This is WizardLM trained with a subset of the dataset - responses that contained alignment / moralizing were removed.  The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:An uncensored model has no guardrails.You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.
Publishing anything this model generates is the same as publishing it yourself.
You are responsible for the content you publish, and you cannot blame the model any more than you can blame the knife, gun, lighter, or car for what you do with it."
coedit-xxl,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2305.09857.pdf,6,asset; wi_locness; GEM/wiki_auto_asset_turk; discofuse; zaemyung/IteraTeR_plus; jfleg,587,45571.20853,,https://huggingface.co/grammarly/coedit-xxl,"This model was obtained by fine-tuning the corresponding google/flan-t5-xxl model on the CoEdIT dataset.; Paper: CoEdIT: ext Editing by Task-Specific Instruction Tuning; Authors: Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang; We make available the models presented in our paper. ; Given an edit instruction and an original text, our model can generate the edited version of the text."
coedit-xl-composite,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2305.09857.pdf,8,asset; wi_locness; GEM/wiki_auto_asset_turk; discofuse; zaemyung/IteraTeR_plus; jfleg,133,11677.40202,,https://huggingface.co/grammarly/coedit-xl-composite,"This model was obtained by fine-tuning the corresponding google/flan-t5-xl model on the CoEdIT-Composite dataset. Details of the dataset can be found in our paper and repository.; Paper: CoEdIT: Text Editing by Task-Specific Instruction Tuning; Authors: Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang; We make available the models presented in our paper. ; Given an edit instruction and an original text, our model can generate the edited version of the text."
mpt-7b-ggml,Text Generation,Transformers,English,apache-2.0,,44,mc4; c4; togethercomputer/RedPajama-Data-1T; bigcode/the-stack; allenai/s2orc,237,125992.9755,6,https://huggingface.co/rustformers/mpt-7b-ggml,"MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.
This model was trained by MosaicML.; MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference. ; ??Caution??: mpt-7b-storywriter is still under development!; Via pip: pip install llm-rs; The GGML example only supports the ggml container type!"
WizardLM-13B-V1.0,Text Generation,PyTorch; Transformers,,,,55,,"2,291",53322.04475,2,https://huggingface.co/WizardLM/WizardLM-13B-V1.0,"This is WizardLM-13B V1.0 diff weight.; Project Repo: https://github.com/nlpxucan/WizardLM; NOTE: The WizardLM-13B-1.0 and Wizard-7B use different prompt at the beginning of the conversation:; For WizardLM-13B-1.0 , the Prompt should be as following:; For WizardLM-7B , the Prompt should be as following:"
tiny_starcoder_py,Text Generation,PyTorch; Safetensors; Transformers,,bigcode-openrail-m,,42,bigcode/the-stack-dedup,"5,876",1317.256776,1,https://huggingface.co/bigcode/tiny_starcoder_py,"This is a 164M parameters model with the same architecture as StarCoder (8k context length, MQA & FIM). It was trained on the Python data from StarCoderData
for ~6 epochs which amounts to 100B tokens.; The model was trained on GitHub code, to assist with some tasks like Assisted Generation. For pure code completion, we advise using our 15B models StarCoder or StarCoderBase.; Fill-in-the-middle uses special tokens to identify the prefix/middle/suffix part of the input and output:; The model is licensed under the BigCode OpenRAIL-M v1 license agreement. You can find the full agreement here."
instructcodet5p-16b,Text2Text Generation,PyTorch; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2305.07922.pdf,46,,"3,579",34020.71289,1,https://huggingface.co/Salesforce/instructcodet5p-16b,"CodeT5+ is a new family of open code large language models with an encoder-decoder architecture that can flexibly operate in different modes (i.e. encoder-only, decoder-only, and encoder-decoder) to support a wide range of code understanding and generation tasks. 
It is introduced in the paper:; CodeT5+: Open Code Large Language Models for Code Understanding and Generation
by Yue Wang*, Hung Le*, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi (* indicates equal contribution).; Compared to the original CodeT5 family (base: 220M, large: 770M), CodeT5+ is pretrained with a diverse set of pretraining tasks including span denoising, causal language modeling, contrastive learning, and text-code matching to learn rich representations from both unimodal code data and bimodal code-text data. 
Additionally, it employs a simple yet effective compute-efficient pretraining method to initialize the model components with frozen off-the-shelf LLMs such as CodeGen to efficiently scale up the model (i.e. 2B, 6B, 16B), and adopts a ""shallow encoder and deep decoder"" architecture. 
Furthermore, it is instruction-tuned to align with natural language instructions (see our InstructCodeT5+ 16B) following Code Alpaca.  ; This model can be easily loaded using the AutoModelForSeq2SeqLM functionality and employs the same tokenizer as CodeGen.; This checkpoint is trained on the stricter permissive subset of the deduplicated version of the github-code dataset.
The data is preprocessed by reserving only permissively licensed code (""mit"" “apache-2”, “bsd-3-clause”, “bsd-2-clause”, “cc0-1.0”, “unlicense”, “isc”).
Supported languages (9 in total) are as follows:
c, c++, c-sharp,  go, java, javascript,  php, python, ruby."
hakoMay,,,,,,77,,0,13363.30321,,https://huggingface.co/852wa/hakoMay,"SD2.1 finetuning model; model HakoMay A/B/C/D/Boy .safetensors; embeddings Mayng.safetensors + Mayng.yaml; License
WD 1.5 is released under the Fair AI Public License 1.0-SD (https://freedevproject.org/faipl-1.0-sd/). If any derivative of this model is made, please share your changes accordingly. Special thanks to ronsor/undeleted (https://undeleted.ronsor.com/) for help with the license.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
raos-virtual-try-on-model,Text-to-Image,Diffusers,,creativeml-openrail-m,,9,,430,2334.322049,1,https://huggingface.co/gouthaml/raos-virtual-try-on-model,"The DeepVTO model is hosted on the Hugging Face Model Hub. 
(https://huggingface.co/gouthaml/raos-virtual-try-on-model)
This model leverages a combination of advanced deep learning techniques and architectures, including stable-diffusion, DreamBooth, feature extraction using the EfficientNetB3 CNN model, and OpenPose for estimating person keypoints. These techniques are harmoniously integrated to provide a realistic and visually appealing virtual try-on experience for users.; The DeepVTO model is built on the principles of stable diffusion and vector embeddings, which are critical in creating a high-quality virtual try-on system. The model is trained using the DreamBooth model, which is a stable-diffusion model, and the feature extraction is performed using the EfficientNetB3 CNN model. OpenPose, a real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints, is used for estimating person keypoints.; The model requires specific hardware and software for optimal performance. The hardware requirements include a GPU A100 and high RAM. The software requirements include PyTorch, stable-diffusion-v1-5, Python 3.0, U-Net Architecture, Dreambooth, OpenPose, and the EfficientNetB3 pre-trained CNN model.; The DeepVTO model is a testament to the potential of deep learning in the fashion retail industry. It showcases how advanced machine learning techniques can be used to enhance the online shopping experience, making it more interactive and personalized. This model serves as a valuable resource for researchers and practitioners in the field, providing a practical example of a high-quality virtual try-on system.; The model also provides a foundation for future research and development in the field of virtual try-on systems. It highlights the potential of deep learning techniques in addressing the challenges associated with virtual try-on systems, such as the accuracy of virtual representations and the scalability of the system. By leveraging advanced deep learning techniques, the DeepVTO model paves the way for the development of more sophisticated and effective virtual try-on systems in the future."
Wizard-Vicuna-7B-Uncensored-HF,Text Generation,PyTorch; Transformers,English,other,,16,ehartford/wizard_vicuna_70k_unfiltered,"2,579",13805.88092,1,https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-HF,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a float16 HF repo of Eric Hartford's 'uncensored' training of Wizard-Vicuna 7B.; It is the result of converting Eric's float32 repo to float16 for easier storage.; For further support, and discussions on these models and AI in general, join us at:"
metharme-13b,Text Generation,,English,,,25,,0,0.009888649,,https://huggingface.co/PygmalionAI/metharme-13b,"Metharme 13B is an instruct model based on Meta's LLaMA-13B.; This is an experiment to try and get a model that is usable for conversation, roleplaying and storywriting, but which can be guided using natural language like other instruct models. See the prompting section below for examples.; It was trained by doing supervised fine-tuning over a mixture of regular instruction data alongside roleplay, fictional stories and conversations with synthetically generated instructions attached.; The model weights in this repository cannot be used as-is. The files here are XORs due to licensing concerns. To obtain proper, usable model weights you need to:; Request access to the original LLaMA weights from Meta through this form"
MPT-7B-Storywriter-GGML,,Transformers,,apache-2.0,https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2302.06675.pdf,37,the_pile_books3,140,38737.93338,1,https://huggingface.co/TheBloke/MPT-7B-Storywriter-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is GGML format quantised 4-bit, 5-bit and 8-bit models of MosaicML's MPT-7B-Storywriter.; This repo is the result of converting to GGML and quantising.; Please note that these MPT GGMLs are not compatbile with llama.cpp. Please see below for a list of tools known to work with these model files."
LexGPT-6B,Text Generation,PyTorch; Transformers,English,bigscience-openrail-m,https://arxiv.org/pdf/2306.05431.pdf,7,pile-of-law/pile-of-law,110,0,1,https://huggingface.co/patent/LexGPT-6B,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; LexGPT-6B Language ModelVersion 1.0 / 17 June 2023; Current Checkpoint: Training Iteration  350,000; Link to paper: here"
multilingual-e5-base,Feature Extraction,PyTorch; Safetensors; Transformers,94 languages,mit,https://arxiv.org/pdf/2212.03533.pdf; https://arxiv.org/pdf/2108.08787.pdf; https://arxiv.org/pdf/2104.08663.pdf; https://arxiv.org/pdf/2210.07316.pdf,62,,"21,732",2295.62805,5,https://huggingface.co/intfloat/multilingual-e5-base,"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 12 layers and the embedding size is 768.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; This model is initialized from xlm-roberta-base
and continually trained on a mixture of multilingual datasets.
It supports 100 languages from xlm-roberta,
but low-resource languages may see performance degradation.; Initialization: xlm-roberta-base"
Replicant-V3.0,Text-to-Image,,,other,,41,,0,8016.641767,,https://huggingface.co/gsdf/Replicant-V3.0,"WD1.5-beta based model.Licence:https://freedevproject.org/faipl-1.0-sd/ ; prompt & Setting: https://civitai.com/models/10701/replicant-v30
; Unable to determine this model’s library. Check the
								docs 
.
							"
alpaca-lora-7b-german-base-52k,,Transformers,German,apache-2.0,,14,,0,16.80246479,1,https://huggingface.co/avocardio/alpaca-lora-7b-german-base-52k,"Visit the Github for more information: https://github.com/avocardio/zicklein; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
LEALLA-small,Feature Extraction,PyTorch; TensorFlow; JAX; Safetensors; Transformers,109 languages,apache-2.0,https://arxiv.org/pdf/2302.08387.pdf,3,CommonCrawl; Wikipedia,205,1122.826841,,https://huggingface.co/setu4993/LEALLA-small,"LEALLA is a collection of lightweight language-agnostic sentence embedding models supporting 109 languages, distilled from LaBSE. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.; This is migrated from the v1 model on the TF Hub. The embeddings produced by both the versions of the model are equivalent. Though, for some of the languages (like Japanese), the LEALLA models appear to require higher tolerances when comparing embeddings and similarities.; Using the model:; To get the sentence embeddings, use the pooler output:; Output for other languages:"
ChatGLM6B-Legal,,PyTorch; Transformers,Chinese,,,4,,1,53.69159435,,https://huggingface.co/tassadar667/ChatGLM6B-Legal,"ChatGLM-6B-Legal是一个在ChatGLM-6B上进行了参数微调的模型，主要关注于法律判据的预测方面。; 首先需要下载ChatGLM-6B模型，再下载本模型中的model_1和model_2，运行法律问答jupyter文件。需要修改文件中模型、config等目录。
依赖环境与ChatGLM-6B相同。; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
manticore-13b-chat-pyg,Text Generation,PyTorch; Safetensors; Transformers,English,,,20,anon8231489123/ShareGPT_Vicuna_unfiltered; ehartford/wizard_vicuna_70k_unfiltered; ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered; QingyiSi/Alpaca-CoT; teknium/GPT4-LLM-Cleaned; teknium/GPTeacher-General-Instruct; metaeval/ScienceQA_text_only; hellaswag; openai/summarize_from_feedback; riddle_sense; gsm8k; ewof/code-alpaca-instruct-unfiltered,300,53311.84419,7,https://huggingface.co/openaccess-ai-collective/manticore-13b-chat-pyg,"; Manticore 13B Chat builds on Manticore with new datasets, including a de-duped subset of the Pygmalion dataset. It also removes all Alpaca style prompts using ### in favor of 
chat only style prompts using USER:,ASSISTANT: as well as pygmalion/metharme prompting using <|system|>, <|user|> and <|model|> tokens.; Questions, comments, feedback, looking to donate, or want to help? Reach out on our Discord or email wing@openaccessaicollective.org; Manticore 13B Chat is a Llama 13B model fine-tuned on the following datasets along with the datasets from the original Manticore 13B. ; Manticore 13B Chat was trained on 25% of the datasets below. The datasets were merged, shuffled, and then sharded into 4 parts."
hippogriff-30b-chat,Text Generation,PyTorch; Transformers,English,,,17,QingyiSi/Alpaca-CoT; teknium/GPT4-LLM-Cleaned; teknium/GPTeacher-General-Instruct; metaeval/ScienceQA_text_only; hellaswag; openai/summarize_from_feedback; riddle_sense; gsm8k; OpenAssistant/oasst1,65,66634.07875,,https://huggingface.co/openaccess-ai-collective/hippogriff-30b-chat,"; Hippogriff 30B Chat is an experiment that builds on Manticore with new datasets, while removing a few more instruction and chat datasets. It also includes a de-duped subset of the Pygmalion dataset. It also removes all Alpaca style prompts using ### in favor of 
chat only style prompts using USER:,ASSISTANT: as well as pygmalion/metharme prompting using <|system|>, <|user|> and <|model|> tokens.; Questions, comments, feedback, looking to donate, or want to help? Reach out on our Discord or email wing@openaccessaicollective.org; Hippogriff 30B Chat is a Llama 30B model fine-tuned on the following datasets; Hippogriff differs from Manticore as it does not use the WizardLM, WizardVicuna, Alpaca, or ShareGPT datasets."
rvc-model-arknights,,,English,unknown,,6,,0,0.001808281,,https://huggingface.co/theaster/rvc-model-arknights,Current Arknights RVC Models. Will be more later.; List:; Goldenglow (pink korone for lyfe(4m) <3) (RVC v1/v2); W; Angelina (v2) (s/o to henerum for introducing me to arknights yo)
redpajama-3b-ggml,Text Generation,Transformers,English,apache-2.0,,8,,70,39536.65561,,https://huggingface.co/rustformers/redpajama-3b-ggml,"RedPajama-INCITE-Base-3B-v1 was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. 
The training was done on 3,072 V100 GPUs provided as part of the INCITE 2023 project on Scalable Foundation Models for Transferrable Generalist AI, awarded to MILA, LAION, and EleutherAI in fall 2022, with support from the Oak Ridge Leadership Computing Facility (OLCF) and INCITE program. ; Via pip: pip install llm-rs; Download the installer at www.localai.app.; Download your preferred model and place it in the ""models"" directory. Subsequently, you can start a chat session with your model directly from the interface."
chilloutmix_inpaint,,,,,,2,,0,4372.481445,,https://huggingface.co/jansonkong/chilloutmix_inpaint,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
BLOOMChat-176B-v1-GGML-q4,,,,other,,3,,0,0.001467247,,https://huggingface.co/jeff31415/BLOOMChat-176B-v1-GGML-q4,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
chronos-13b,Text Generation,PyTorch; Transformers,,other,,28,,286,26995.01301,,https://huggingface.co/elinas/chronos-13b,"This is the fp16 PyTorch / HF version of chronos-13b; This model is primarily focused on chat, roleplay, and storywriting, but can accomplish other tasks such as simple reasoning and coding.; Chronos generates very long outputs with coherent text, largely due to the human inputs it was trained on.; This model uses Alpaca formatting, so for optimal model performance, use:; 4bit Quantized version"
falcon-7b-instruct-GPTQ,Text Generation,Transformers,English,apache-2.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf,53,tiiuae/falcon-refinedweb,"9,582",6085.357302,,https://huggingface.co/TheBloke/falcon-7b-instruct-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains an experimantal GPTQ 4bit model for Falcon-7B-Instruct.; It is the result of quantising to 4bit using AutoGPTQ.; Please note that performance with this GPTQ is currently very slow with AutoGPTQ.
mms-1b-all,Automatic Speech Recognition,PyTorch; Safetensors; Transformers,158 languages,cc-by-nc-4.0,https://arxiv.org/pdf/2305.13516.pdf,44,google/fleurs,"116,798",417.6359961,24,https://huggingface.co/facebook/mms-1b-all,"This checkpoint is a model fine-tuned for multi-lingual ASR and part of Facebook's Massive Multilingual Speech project.
This checkpoint is based on the Wav2Vec2 architecture and makes use of adapter models to transcribe 1000+ languages.
The checkpoint consists of 1 billion parameters and has been fine-tuned from facebook/mms-1b on 1162 languages.; This MMS checkpoint can be used with Transformers to transcribe audio of 1107 different 
languages. Let's look at a simple example.; First, we install transformers and some other libraries; Note: In order to use MMS you need to have at least transformers >= 4.30 installed. If the 4.30 version
is not yet available on PyPI make sure to install transformers from 
source:; Next, we load a couple of audio samples via datasets. Make sure that the audio data is sampled to 16000 kHz."
InstructUIE,Text2Text Generation,PyTorch; Transformers,,openrail,,3,,161,46154.93173,,https://huggingface.co/ZWK/InstructUIE,"https://github.com/BeyonderXX/InstructUIE; Large language models have unlocked strong multi-task capabilities from reading instructive prompts.
However, recent studies have shown that existing large models still have difficulty with information extraction tasks. 
For example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance.
In this paper, we propose InstructUIE, a unified information extraction framework based on instruction tuning, which can uniformly model various information extraction tasks and capture the inter-task dependency.
To validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions.
Experimental results demonstrate that our method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings.; Our models are trained and evaluated on IE INSTRUCTIONS. 
You can download the data from Baidu NetDisk or Google Drive.; If you are using InstructUIE for your work, please kindly cite our paper:"
mms-1b-fl102,Automatic Speech Recognition,PyTorch; Safetensors; Transformers,158 languages,cc-by-nc-4.0,https://arxiv.org/pdf/2305.13516.pdf,5,google/fleurs,"658,849",454.4085156,2,https://huggingface.co/facebook/mms-1b-fl102,"This checkpoint is a model fine-tuned for multi-lingual ASR and part of Facebook's Massive Multilingual Speech project.
This checkpoint is based on the Wav2Vec2 architecture and makes use of adapter models to transcribe 100+ languages.
The checkpoint consists of 1 billion parameters and has been fine-tuned from facebook/mms-1b on 102 languages of Fleurs.; This MMS checkpoint can be used with Transformers to transcribe audio of 1107 different 
languages. Let's look at a simple example.; First, we install transformers and some other libraries; Note: In order to use MMS you need to have at least transformers >= 4.30 installed. If the 4.30 version
is not yet available on PyPI make sure to install transformers from 
source:; Next, we load a couple of audio samples via datasets. Make sure that the audio data is sampled to 16000 kHz."
wizardLM-13B-1.0-fp16,Text Generation,PyTorch; Transformers,,other,https://arxiv.org/pdf/2304.12244.pdf,7,,"20,459",26657.09859,,https://huggingface.co/TheBloke/wizardLM-13B-1.0-fp16,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are fp16 unquantised format model files for WizardLM 13B 1.0.; It is the result of merging the deltas provided in the above repo.; Join me at: https://discord.gg/UBgz4VXf
WebGLM,Text2Text Generation,PyTorch; Transformers,English,,https://arxiv.org/pdf/2306.07906.pdf,34,,"1,799",40429.13717,4,https://huggingface.co/THUDM/WebGLM,"
  ? Paper (KDD 2023) 
  | 
  ? Github Repo
; WebGLM aspires to provide an efficient and cost-effective web-enhanced question-answering system using the 10-billion-parameter General Language Model (GLM). It aims to improve real-world application deployment by integrating web search and retrieval capabilities into the pre-trained language model.; WebGLM is built by the following parts:; This repo is the implementation of Bootstrap Generator.; See our Github Repo for more detailed usage."
sentence-transformers-multilingual-e5-base,Sentence Similarity,PyTorch; Sentence Transformers,,,,4,,307,1158.819045,,https://huggingface.co/embaas/sentence-transformers-multilingual-e5-base,This is a the sentence-transformers version of the intfloat/multilingual-e5-base model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; You can use the embaas API to encode your input. Get your free API key from embaas.io; You can find the MTEB results here.
BigTranslate,Text Generation,PyTorch; Transformers,,lgpl-3.0,https://arxiv.org/pdf/2305.18098.pdf,22,,248,27106.10937,,https://huggingface.co/James-WYang/BigTranslate,"Large language models (LLMs) demonstrate promising translation performance among various natural languages. However, many LLMs especially the open-sourced ones, such as BLOOM and LLaMA, are English-dominant and support only dozens of natural languages, making the potential of LLMs on language translation less explored. In this work, we present BigTranslate which adapts LLaMA that covers only 20 languages and enhances it with multilingual translation capability on more than 100 languages. BigTranslate is built upon LLaMA-13B and it is optimized in three steps. First, we continue training LLaMA with massive Chinese monolingual data. Second, we continue training the model with a large-scale parallel dataset that covers 102 natural languages. Third, we instruct-tune the foundation model with multilingual translation instructions, leading to our BigTranslate model. The preliminary experiments on multilingual translation show that BigTranslate performs comparably with 
ChatGPT and Google Translate in many languages and even outperforms ChatGPT in 8 language pairs. We release the BigTranslate model and hope it can advance the research progress.; More Details can be found at https://github.com/ZNLP/BigTranslate and https://arxiv.org/abs/2305.18098"
CFMix,,,,,,3,,0,11612.16199,,https://huggingface.co/xiaozhahai/CFMix,"Anime model baesd on Counterfeit-V2.5.

; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
falcon-40b-openassistant-peft,Text Generation,PEFT,,apache-2.0,https://arxiv.org/pdf/2106.09685.pdf; https://arxiv.org/pdf/2305.14314.pdf,39,OpenAssistant/oasst1,228,67.50788593,,https://huggingface.co/dfurman/falcon-40b-openassistant-peft,"Falcon-40b-openassistant-peft is a chatbot model for dialogue generation. It was built by fine-tuning Falcon-40B on the OpenAssistant/oasst1 dataset. This repo only includes the LoRA adapters from fine-tuning with ?'s peft package. ; The model was fine-tuned in 4-bit precision using peft adapters, transformers, and bitsandbytes. Training relied on a method called ""Low Rank Adapters"" (LoRA), specifically the QLoRA variant. The run took approximately 10 hours and was executed on a workstation with a single A100-SXM NVIDIA GPU with 37 GB of available memory. See attached Colab Notebook for the code and hyperparams used to train the model. ; May 30, 2023; To prompt the chat model, use the following format:; Prompter:"
Ziya-BLIP2-14B-Visual-v1,Visual Question Answering,PyTorch; Transformers,English; Chinese,gpl-3.0,https://arxiv.org/pdf/2210.08590.pdf,24,,167,4485.326748,1,https://huggingface.co/IDEA-CCNL/Ziya-BLIP2-14B-Visual-v1,"Ziya-Visual多模态大模型基于姜子牙通用大模型V1训练，具有视觉问答和对话能力。今年3月份OpenAI发布具有识图能力的多模态大模型GPT-4，遗憾的是，时至今日绝大部分用户也都还没有拿到GPT-4输入图片的权限，Ziya-Visual参考了Mini-GPT4、LLaVA等优秀的开源实现，补齐了Ziya的识图能力，使中文用户群体可以体验到结合视觉和语言两大模态的大模型的卓越能力。; The Ziya-Visual multimodal Big Model is based on the Ziya-LLaMA-13B-v1 training and has visual question and answer and dialogue capabilities. In March this year, OpenAI released GPT-4, a multimodal big model with image recognition capabilities. Unfortunately, to date, the vast majority of users have not yet been given access to GPT-4 for image input, so Ziya-Visual refers to Mini-GPT4, LLaVA and other excellent open source implementations to complement Ziya's image recognition capabilities, so that the Chinese user community can experience the superior capabilities of a large model combining two modalities: visual and language.; 这个例子展示了模型的识图能力、知识能力和创作能力。首先第一个问题中，模型识别出了图片中是电影《泰坦尼克号》的场景，并给出电影导演、发布时间、奖项成就等信息；第二个问题，模型根据用户的需求创作了一首现代爱情诗。; This example demonstrates the model's ability to read pictures, its knowledge and its ability to compose. Firstly in the first problem, the model identifies the picture as a scene from the movie Titanic and gives information about the movie director, release date and award achievements; in the second problem, the model creates a modern love poem based on the user's needs.; "
WizardLM-Uncensored-SuperCOT-StoryTelling-30b,Text Generation,PyTorch; Transformers,,,,23,,142,66613.6857,,https://huggingface.co/Monero/WizardLM-Uncensored-SuperCOT-StoryTelling-30b,"This model is a triple model merge of WizardLM Uncensored+CoT+Storytelling, resulting in a comprehensive boost in reasoning and story writing capabilities.; To allow all output, at the end of your prompt add ### Certainly!; You've become a compendium of knowledge on a vast array of topics. ; Lore Mastery is an arcane tradition fixated on understanding the underlying mechanics of magic. It is the most academic of all arcane traditions. The promise of uncovering new knowledge or proving (or discrediting) a theory of magic is usually required to rouse its practitioners from their laboratories, academies, and archives to pursue a life of adventure. Known as savants, followers of this tradition are a bookish lot who see beauty and mystery in the application of magic. The results of a spell are less interesting to them than the process that creates it. Some savants take a haughty attitude toward those who follow a tradition focused on a single school of magic, seeing them as provincial and lacking the sophistication needed to master true magic. Other savants are generous teachers, countering ignorance and deception with deep knowledge and good humor."
kullm-polyglot-12.8b-v2,Text Generation,PyTorch; Transformers,Korean,apache-2.0,,27,,"3,719",26564.26462,,https://huggingface.co/nlpai-lab/kullm-polyglot-12.8b-v2,This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KULLM v2 ; Detail Codes are available at KULLM Github Repository; The following hyperparameters were used during training:
h2ogpt-gm-oasst1-en-2048-falcon-7b-v2,Conversational,PyTorch; Transformers,English,apache-2.0,,52,OpenAssistant/oasst1,"20,932",14779.13042,8,https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v2,"This model was trained using H2O LLM Studio.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers, accelerate, torch and einops libraries installed.; You can print a sample prompt after the preprocessing step to see how it is feed to the tokenizer:; Alternatively, you can download h2oai_pipeline.py, store it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:; You may also construct the pipeline from the loaded model and tokenizer yourself and consider the preprocessing steps:"
yayoi_mix,Text-to-Image,,Japanese; English,creativeml-openrail-m,,19,,0,28857.18106,,https://huggingface.co/Kotajiro/yayoi_mix,"本モデルは『CreativeML Open RAIL-M』の欷钎楗ぅ螗互螗工丹欷蓼埂 本モデルを使用した上での}にvしては、当方は一切任を持ちません。ご了承の上ご使用ください。 ; BraV6 https://huggingface.co/BanKaiPls/AsianModel; XXMix_9 https://civitai.com/models/47274/xxmix9; Soda Mix https://civitai.com/models/47507/soda-mix; 本モデルも使用したモデルの利用条件に兢π韦摔胜辘蓼工、以下にvしてはに使用を禁止いたします。
?暴力的な表F
?雇ポルノ
?未成年者の性的な表F
?未成年者の性的な表F、または水着、下着、あるいはそれに胜氦肴葑吮憩F"
SoulChat,Feature Extraction,PyTorch; Transformers,Chinese,apache-2.0,,6,,283,13744.92634,,https://huggingface.co/scutcyr/SoulChat,"SoulChat ? | 
 ? BianQue? |; 基于主动健康的主动性、预防性、精确性、个性化、共建共享、自律性六大特征，华工未来技术学院-广东省数字孪生人重点实验室开源了中文领域生活空间主动健康大模型基座ProactiveHealthGPT，包括：; 我们期望，生活空间主动健康大模型基座ProactiveHealthGPT 可以帮助学术界加速大模型在慢性病、心理咨询等主动健康领域的研究与应用。本项目为 心理健康大模型灵心（SoulChat） 。;    我们调研了当前常见的心理咨询平台，发现，用户寻求在线心理帮助时，通常需要进行较长篇幅地进行自我描述，然后提供帮助的心理咨询师同样地提供长篇幅的回复（见https://github.com/scutcyr/SoulChat/blob/main/figure/single_turn.png），缺失了一个渐进式的倾诉过程。但是，在实际的心理咨询过程当中，用户和心理咨询师之间会存在多轮次的沟通过程，在该过程当中，心理咨询师会引导用户进行倾诉，并且提供共情，例如：“非常棒”、“我理解你的感受”、“当然可以”等等。;    考虑到当前十分欠缺多轮共情对话数据集，我们一方面，构建了超过15万规模的 单轮长文本心理咨询指令与答案（SoulChatCorpus-single_turn） ，回答数量超过50万（指令数是当前的常见的心理咨询数据集 PsyQA 的6.7倍），并利用ChatGPT与GPT4，生成总共约100万轮次的 多轮回答数据（SoulChatCorpus-multi_turn） 。特别地，我们在预实验中发现，纯单轮长本文驱动的心理咨询模型会产生让用户感到厌烦的文本长度，而且不具备引导用户倾诉的能力，纯多轮心理咨询对话数据驱动的心理咨询模型则弱化了模型的建议能力，因此，我们混合SoulChatCorpus-single_turn和SoulChatCorpus-multi_turn构造成超过120万个样本的 单轮与多轮混合的共情对话数据集SoulChatCorpus 。所有数据采用“用户：xxx\n心理咨询师：xxx\n用户：xxx\n心理咨询师：”的形式统一为一种指令格式。"
Ziya-LLaMA-13B-v1.1,Text Generation,PyTorch; Transformers,English; Chinese,gpl-3.0,https://arxiv.org/pdf/2210.08590.pdf,34,,299,26194.71749,,https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1.1,"（LLaMA权重的许可证限制，我们无法直接发布完整的模型权重，用户需要参考使用说明进行合并); 我们对Ziya-LLaMA-13B-v1模型进行继续优化，推出开源版本Ziya-LLaMA-13B-v1.1。通过调整微调数据的比例和采用更优的强化学习策略，本版本在问答准确性、数学能力以及安全性等方面得到了提升，详细能力分析如下图所示。; We have further optimized the Ziya-LLaMA-13B-v1 model and released the open-source version Ziya-LLaMA-13B-v1.1. By adjusting the proportion of fine-tuning data and adopting a better reinforcement learning strategy, this version has achieved improvements in question-answering accuracy, mathematical ability, and safety, as shown in the following figure in detail.; 请参考Ziya-LLaMA-13B-v1的使用说明。; 注意：合并后默认会生成3个.bin文件，md5值依次为59194d10b1553d66131d8717c9ef03d6、cc14eebe2408ddfe06b727b4a76e86bb、4a8495d64aa06aee96b5a1cc8cc55fa7。"
open_llama_7b,Text Generation,PyTorch; Transformers,,apache-2.0,,91,togethercomputer/RedPajama-Data-1T,"46,892",13804.08085,6,https://huggingface.co/openlm-research/open_llama_7b,"In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a 7B and 3B model trained on 1T tokens, as well as the preview of a 13B model trained on 600B tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we’ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.; The model can be evaluated with lm-eval-harness. However, due to the aforementioned tokenizer issue, we need to avoid using the fast tokenizer to obtain the correct results. This can be achieved by passing in use_fast=False to this part of lm-eval-harness, as shown in the example below:"
chinese-llama-lora-33b,,,Chinese,apache-2.0,,7,,0,2970.342609,,https://huggingface.co/ziqingyang/chinese-llama-lora-33b,"This repo contains the tokenizer, Chinese-LLaMA LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
chinese-alpaca-lora-33b,,,Chinese,apache-2.0,,10,,0,2970.343168,,https://huggingface.co/ziqingyang/chinese-alpaca-lora-33b,"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ct2fast-falcon-7b-sft-top1-696,Text Generation,Transformers,4 languages,apache-2.0,,3,OpenAssistant/oasst1,53,7100.138153,,https://huggingface.co/michaelfeil/ct2fast-falcon-7b-sft-top1-696,"Speedup inference while reducing memory by 2x-4x using int8 inference in C++ on CPU or GPU.; quantized version of OpenAssistant/falcon-7b-sft-top1-696; Checkpoint compatible to ctranslate2>=3.16.0
and hf-hub-ctranslate2>=2.10.0; Converted on 2023-06-16 using; This is just a quantized version. Licence conditions are intended to be idential to original huggingface repo."
AquilaChat-7B,,PyTorch; Transformers,,other,,27,,616,14949.39412,,https://huggingface.co/BAAI/AquilaChat-7B,"; 
English |
        简体中文
; Aquila Language Model is the first open source language model that supports both Chinese and English knowledge, commercial license agreements, and compliance with domestic data regulations.; ? Supports open source commercial licenses. The source code of the Aquila series models is based on the Apache 2.0 agreement, while the model weight is based on the BAAI Aquila Model License Agreement. Users can use it for commercial purposes as long as they meet the licensing restrictions.; ?? Possesses Chinese and English knowledge. The Aquila series model is trained from scratch on a high-quality corpus of Chinese and English languages, with Chinese corpora accounting for about 40%, ensuring that the model accumulates native Chinese world knowledge during the pre-training phase, rather than translated knowledge."
starcoderplus-GGML,Text Generation,Transformers,,bigcode-openrail-m,https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,22,bigcode/the-stack-dedup; tiiuae/falcon-refinedweb,79,71782.41241,,https://huggingface.co/TheBloke/starcoderplus-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Bigcode's StarcoderPlus.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools known to work with these model files.; These files are not compatible with llama.cpp."
starcoder-GGML,Text Generation,Transformers,,bigcode-openrail-m,https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,12,bigcode/the-stack-dedup,24,71782.4171,,https://huggingface.co/TheBloke/starcoder-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Bigcode's Starcoder.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools known to work with these model files.; These files are not compatible with llama.cpp."
Minotaur-13b-Landmark,Text Generation,PyTorch; Transformers,,other,,22,,49,26657.14239,,https://huggingface.co/eugenepentland/Minotaur-13b-Landmark,Minotaur-13B with 10k+ context using Landmark Attention.; Model generated using Landmark-Attention-QLoRA; https://github.com/eugenepentland/landmark-attention-qlora; A merge of the following models:; https://huggingface.co/openaccess-ai-collective/minotaur-13b
chronos-hermes-13b,Text Generation,PyTorch; Transformers,,other,,17,,"14,396",26634.76483,,https://huggingface.co/Austism/chronos-hermes-13b,"(chronos-13b + Nous-Hermes-13b) 75/25 merge; This has the aspects of chronos's nature to produce long, descriptive outputs. But with additional coherency and an ability to better obey instructions. Resulting in this model having a great ability to produce evocative storywriting and follow a narrative.; This mix contains alot of chronos's writing style and 'flavour' with far less tendency of going AWOL and spouting nonsensical babble.; This result was much more successful than my first chronos merge."
chronos-hermes-13B-GGML,,PyTorch,,other,,24,,0,49121.29099,,https://huggingface.co/TheBloke/chronos-hermes-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Austism's Chronos Hermes 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
aquilachat-7b,Text Generation,PyTorch; Transformers,Chinese,,,17,,405,14945.56959,2,https://huggingface.co/qhduan/aquilachat-7b,"https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila; Aquila-7B和Aquila-33B开源模型使用 智源Aquila系列模型许可协议, 原始代码基于Apache Licence 2.0。"
WebGLM-2B,Feature Extraction,PyTorch; Transformers,English,,https://arxiv.org/pdf/2306.07906.pdf,18,,"4,422",3933.723539,,https://huggingface.co/THUDM/WebGLM-2B,"
  ? Paper (KDD 2023) 
  | 
  ? Github Repo
; WebGLM-2B aspires to provide an efficient and cost-effective web-enhanced question-answering system using the 2-billion-parameter General Language Model (GLM). It aims to improve real-world application deployment by integrating web search and retrieval capabilities into the pre-trained language model.; WebGLM is built by the following parts:; This repo is the implementation of Bootstrap Generator.; See our Github Repo for more detailed usage."
bert-base-chinese-finetuning-financial-news-sentiment-v2,Text Classification,PyTorch; Transformers,Chinese,,,2,,306,409.110805,,https://huggingface.co/hw2942/bert-base-chinese-finetuning-financial-news-sentiment-v2,
flan-t5-base-squad2,Question Answering,PyTorch; Safetensors; Transformers,English,mit,,2,squad_v2; squad,278,1787.216172,,https://huggingface.co/sjrhuschlee/flan-t5-base-squad2,"This is the flan-t5-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering.; UPDATE: With transformers version 4.31.0 the use_remote_code=True is no longer necessary and if used will cause AutoModelForQuestionAnswering.from_pretrained() to not work properly.; NOTE: The <cls> token must be manually added to the beginning of the question for this model to work properly.
It uses the <cls> token to be able to make ""no answer"" predictions.
The t5 tokenizer does not automatically add this special token which is why it is added manually.; Language model: flan-t5-baseLanguage: EnglishDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD 2.0Infrastructure: 1x NVIDIA 3070  ; The following hyperparameters were used during training:"
falcon-40b-instruct-GGML,,Transformers,English,apache-2.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2306.01116.pdf,50,tiiuae/falcon-refinedweb,274,279244.8165,3,https://huggingface.co/TheBloke/falcon-40b-instruct-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGCC format model files for Falcon 40B Instruct.; These files will not work in llama.cpp, text-generation-webui or KoboldCpp.; GGCC is a new format created in a new fork of llama.cpp that introduced this new Falcon GGML-based support: cmp-nc/ggllm.cpp."
baichuan-7b-sft,Text Generation,PyTorch; Transformers,Chinese; English,apache-2.0,,56,tatsu-lab/alpaca; sahil2801/CodeAlpaca-20k,365,14338.1683,,https://huggingface.co/hiyouga/baichuan-7b-sft,A bilingual instruction-tuned LoRA model of https://huggingface.co/baichuan-inc/baichuan-7B; Please follow the baichuan-7B License to use this model.; Usage:; You could also alternatively launch a CLI demo by using the script in https://github.com/hiyouga/LLaMA-Efficient-Tuning; You could reproduce our results with the following scripts using LLaMA-Efficient-Tuning:
airoboros-7b-gpt4-1.2,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,,27,jondurbin/airoboros-gpt4-1.2,166,27599.16126,,https://huggingface.co/jondurbin/airoboros-7b-gpt4-1.2,"This is a qlora fine-tuned 7b parameter LlaMa model, using completely synthetic training data created gpt4 via https://github.com/jondurbin/airoboros; This is mostly an extension of 1.1, but with thousands of new training data and an update to allow ""PLAINFORMAT"" at the end of coding prompts to just print the code without backticks or explanations/usage/etc.; The dataset used to fine-tune this model is available here, with a specific focus on:; This model was fine-tuned with a fork of qlora, which among other things was updated to use a slightly modified vicuna template to be compatible with the previous versions:; So in other words, it's the preamble/system prompt, followed by a single space, then ""USER: "" (single space after colon) then the prompt (which can have multiple lines, spaces, whatever), then a single space, followed by ""ASSISTANT: "" (with a single space after the colon)."
Anima33B-merged,Conversational,PyTorch; Transformers,Chinese,other,https://arxiv.org/pdf/2305.14314.pdf,18,Chinese-Vicuna/guanaco_belle_merge_v1.0,"3,946",66632.25358,,https://huggingface.co/lyogavin/Anima33B-merged,; 第一个开源的基于QLoRA的33B中文大语言模型 the First QLoRA based 33B fully open-source Chinese LLM; 请注意：本model的LICENSE比较特殊，请确认你的使用场景符合此LICENSE。; https://github.com/lyogavin/Anima; Anima模型基于QLoRA开源的33B guanaco训练了10000 steps。训练使用一个H100 GPU。
Rerender,,,,other,,9,,0,5.058840294,,https://huggingface.co/Anonymous-sub/Rerender,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
AIGEN_v1.4_diffusers,Text-to-Image,Diffusers,,other,,4,,"1,267",2181.122349,3,https://huggingface.co/digiplay/AIGEN_v1.4_diffusers,Model info : ; https://civitai.com/models/90045/aigen-14; Sample images I made:; 
layoutlm-invoices,Document Question Answering,PyTorch; Safetensors; Transformers,English,cc-by-nc-sa-4.0,,10,,"201,426",1024.649988,2,https://huggingface.co/magorshunov/layoutlm-invoices,"This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of
invoices as well as both SQuAD2.0 and DocVQA for general comprehension.; Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional
classifier head. For example, QA models often encounter this failure mode:; ; However this model is able to predict non-consecutive tokens and therefore the address correctly:; "
kandinsky-2-2-decoder-inpaint,Text-to-Image,Diffusers,,apache-2.0,,5,,"1,058",0.006788063,1,https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder-inpaint,"Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.2 is available in diffusers!; "
baichuan-7B-hf-megatron-states,,,,,,3,,0,0.00149868,,https://huggingface.co/genggui001/baichuan-7B-hf-megatron-states,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
WizardLM-7B-V1.0-Uncensored-GPTQ,Text Generation,Transformers,,other,,11,ehartford/WizardLM_evol_instruct_V2_196k_unfiltered_merged_split,"1,923",4098.343055,,https://huggingface.co/TheBloke/WizardLM-7B-V1.0-Uncensored-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Eric Hartford's WizardLM-7B-V1.0-Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
InstructPix2Pix-MagicBrush,,,,creativeml-openrail-m,https://arxiv.org/pdf/2306.10012.pdf,5,osunlp/MagicBrush,0,22835.20409,,https://huggingface.co/osunlp/InstructPix2Pix-MagicBrush,"To use the InstructPix2Pix checkpoint fine-tuned on MagicBrush, set up env with following command:; Download this checkpoint into checkpoints folder via; Then go back to the root folder and set up the running env following the InstructPix2Pix guidelines.; If you find this checkpoint useful, please consider citing our paper:; And prior work:"
falcoder-7b,Text Generation,PyTorch; Transformers,code,apache-2.0,,81,HuggingFaceH4/CodeAlpaca_20K,"6,018",14174.9589,8,https://huggingface.co/mrm8488/falcoder-7b,Falcon-7b fine-tuned on the CodeAlpaca 20k instructions dataset by using the method QLoRA with PEFT library.; Falcon 7B; CodeAlpaca_20K: contains 20K instruction-following data used for fine-tuning the Code Alpaca model.; TBA
WizardLM-13B-V1.0-Uncensored-GGML,,,English,other,,18,ehartford/WizardLM_evol_instruct_V2_196k_unfiltered_merged_split,0,119316.4921,1,https://huggingface.co/TheBloke/WizardLM-13B-V1.0-Uncensored-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's WizardLM-13b-V1.0-Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
WizardLM-13B-V1.0-Uncensored-GPTQ,Text Generation,Transformers,English,other,,22,ehartford/WizardLM_evol_instruct_V2_196k_unfiltered_merged_split,"2,241",7631.142666,,https://huggingface.co/TheBloke/WizardLM-13B-V1.0-Uncensored-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Eric Hartford's WizardLM-13b-V1.0-Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
baichuan-vicuna-chinese-7b-gptq,Text Generation,Transformers,Chinese; English,,,9,anon8231489123/ShareGPT_Vicuna_unfiltered; QingyiSi/Alpaca-CoT; mhhmm/leetcode-solutions-python,811,4528.191091,,https://huggingface.co/fireballoon/baichuan-vicuna-chinese-7b-gptq,baichuan-vicuna-chinese-7b quantized with AutoGPTQ.; 使用AutoGPTQ量化的baichuan-vicuna-chinese-7b。使用7G显存实现模型推理。; Inference API has been turned off for this model.
newmoon,Text-to-Image,Diffusers,English,cc,,6,,230,4362.243645,1,https://huggingface.co/mirav/newmoon,"NewMoon: Soft, bright colors. 
ChamomileTea: Darker, moodier colors. 
newmoon.yaml: sample prompt for animatediff





"
alpaca-cleaned-llama-30b-bf16,Text Generation,PyTorch; Transformers,,,,2,,38,66634.06079,,https://huggingface.co/dsvv-cair/alpaca-cleaned-llama-30b-bf16,Method : QLORA; Dataset : yahma/alpaca-cleaned; Base model : huggyllama/llama-30b; Compute dtype : bfloat16
Archive-Models,Text-to-Image,,,creativeml-openrail-m,,7,,0,0.006181641,,https://huggingface.co/Konichan/Archive-Models,"All models are not my authors, they are on pixai, civitai, huggingface. This is just an archive in case their models are removed from the sites. And a collection of models so that the models that I use are always at hand.; Triger : loli; If you want an alternate style, try using watercolor (medium); this model is very responsive to that tag. ; ; Flossy is a fusion model capable of creating an innocent, pure and naive girl."
airoboros-7B-gpt4-1.4-GPTQ,Text Generation,Transformers,,other,,6,,809,4630.83064,,https://huggingface.co/TheBloke/airoboros-7B-gpt4-1.4-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Jon Durbin's Airoboros 7B GPT4 1.4.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
superhot-13b-16k-no-rlhf-test,,,,mit,,10,,0,26.30566105,,https://huggingface.co/kaiokendev/superhot-13b-16k-no-rlhf-test,"This is a second prototype of SuperHOT, a NSFW focused LoRA, this time with 16K context and no RLHF, using the same technique described in the github blog.
Tests have shown that the model does indeed leverage the extended context at 8K, so naturally, let's try going even further.; You will need to use either the monkeypatch or, if you are already using the monkeypatch, change the scaling factor to 0.125 and the maximum sequence length to 16384; I trained the LoRA with the following configuration: ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
MahdeenSkyRVC,,,,openrail,,2,,0,1517.601936,,https://huggingface.co/MahdeenSky/MahdeenSkyRVC,"Currently Available Models:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
my-rvc-models-collection,,,,openrail,,4,,0,1778.001544,,https://huggingface.co/megaaziib/my-rvc-models-collection,"Credit me if you use my model ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
orca_mini_13B-GGML,,Transformers,English,mit,https://arxiv.org/pdf/2306.02707.pdf,52,psmathur/alpaca_orca; psmathur/dolly-v2_orca; psmathur/WizardLM_Orca,0,119316.4984,,https://huggingface.co/TheBloke/orca_mini_13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Pankaj Mathur's Orca Mini 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; or"
orca_mini_3B-GGML,,Transformers,English,mit,https://arxiv.org/pdf/2306.02707.pdf,34,psmathur/alpaca_orca; psmathur/dolly-v2_orca; psmathur/WizardLM_Orca,0,12943.37555,,https://huggingface.co/TheBloke/orca_mini_3B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Pankaj Mathur's Orca Mini 3B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; or"
comment_opinion_extract_ChatGLM_base,Text Classification,PyTorch; Transformers,Chinese,apache-2.0,,11,,443,119.8432649,,https://huggingface.co/JessyTsu1/comment_opinion_extract_ChatGLM_base,本模型用于从电商评论数据中，提取关键词和核心观点; 本模型利用5000条淘宝评论数据训练，先使用GPT4通过prompt抽取数据的关键词，经过清洗再使用ChatGLM进行训练
vicuna-13b-v1.3.0-GPTQ,Text Generation,Transformers,,other,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,15,,"2,317",7631.143055,,https://huggingface.co/TheBloke/vicuna-13b-v1.3.0-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for LmSys' Vicuna 13B v1.3.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
CounterMix_v1,Text-to-Image,Diffusers,,other,,2,,"2,953",2181.122991,1,https://huggingface.co/digiplay/CounterMix_v1,"Model info:; https://civitai.com/models/70455?modelVersionId=75113; Original Author's DEMO images :; 

; Sample image I made :"
Guanaco-33B-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,,15,,562,17307.98124,,https://huggingface.co/TheBloke/Guanaco-33B-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Panchovix's merge of Guanaco 33B and SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
RVCModels,,,,,,2,,0,1492.001546,,https://huggingface.co/Thereallo/RVCModels,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
guanaco-13B-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,,6,,373,7631.185147,,https://huggingface.co/TheBloke/guanaco-13B-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Tim Dettmers' Guanaco 13B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-fp16,Text Generation,PyTorch; Transformers,,other,,11,,522,26657.13433,,https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-fp16,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is fp16 pytorch format model files for Eric Hartford's Wizard Vicuna 13B Uncensored merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 13b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length."
EllaFitzgerald-RVCv2,,,,,,2,,0,570.0014844,,https://huggingface.co/Acelogic/EllaFitzgerald-RVCv2,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Chronos-Hermes-13B-SuperHOT-8K-fp16,Text Generation,PyTorch; Transformers,,other,,4,,496,26657.13394,,https://huggingface.co/TheBloke/Chronos-Hermes-13B-SuperHOT-8K-fp16,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is fp16 pytorch format model files for Austism's Chronos Hermes 13B merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 13b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length."
Pygmalion-13B-SuperHOT-8K-fp16,Text Generation,PyTorch; Transformers,,other,,4,,"1,300",26657.13721,,https://huggingface.co/TheBloke/Pygmalion-13B-SuperHOT-8K-fp16,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is fp16 pytorch format model files for TehVenom's merge of PygmalionAI's Pygmalion 13B merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 13b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length."
gpt4all-falcon-ggml,,,,,,6,,0,18903.04148,,https://huggingface.co/nomic-ai/gpt4all-falcon-ggml,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
VisCPM-Chat,Feature Extraction,PyTorch; Transformers,Chinese; English,,,12,,22,42189.61916,,https://huggingface.co/openbmb/VisCPM-Chat,"简体中文 | English; 
; 


; VisCPM is a family of open-source large multimodal models, which support multimodal conversational capabilities (VisCPM-Chat model) and text-to-image generation capabilities (VisCPM-Paint model) in both Chinese and English, achieving state-of-the-art peformance among Chinese open-source multimodal models. VisCPM is trained based on the large language model CPM-Bee with 10B parameters, fusing visual encoder (Q-Former) and visual decoder (Diffusion-UNet) to support visual inputs and outputs. Thanks to the good bilingual capability of CPM-Bee, VisCPM can be pre-trained with English multimodal data only and well generalize to achieve promising Chinese multimodal capabilities.; VisCPM是一个开源的多模态大模型系列，支持中英双语的多模态对话能力（VisCPM-Chat模型）和文到图生成能力（VisCPM-Paint模型），在中文多模态开源模型中达到最佳水平。VisCPM基于百亿参数量语言大模型CPM-Bee（10B）训练，融合视觉编码器（Q-Former）和视觉解码器（Diffusion-UNet）以支持视觉信号的输入和输出。得益于CPM-Bee底座优秀的双语能力，VisCPM可以仅通过英文多模态数据预训练，泛化实现优秀的中文多模态能力。"
HuatuoGPT-13b-delta,Text Generation,PyTorch; Transformers,,apache-2.0,,7,,186,26819.17138,1,https://huggingface.co/FreedomIntelligence/HuatuoGPT-13b-delta,Please see our HuatuoGPT project: https://github.com/FreedomIntelligence/HuatuoGPT.
Redmond-Hermes-Coder,Text Generation,PyTorch; Transformers,English,gpl,,16,,55,32412.89622,1,https://huggingface.co/NousResearch/Redmond-Hermes-Coder,"Redmond-Hermes-Coder 15B is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Karan4D leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors.; This model was trained with a WizardCoder base, which itself uses a StarCoder base model. ; The model is truly great at code, but, it does come with a tradeoff though. While far better at code than the original Nous-Hermes built on Llama, it is worse than WizardCoder at pure code benchmarks, like HumanEval.; It comes in at 39% on HumanEval, with WizardCoder at 57%. This is a preliminary experiment, and we are exploring improvements now.; However, it does seem better at non-code than WizardCoder on a variety of things, including writing tasks."
vits_tts_models,,Transformers,,mit,,3,,1,7984.009415,,https://huggingface.co/youmebangbang/vits_tts_models,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
kandinsky-2-2-controlnet-depth,Text-to-Image,Diffusers,,apache-2.0,,9,,"6,153",0.011380768,1,https://huggingface.co/kandinsky-community/kandinsky-2-2-controlnet-depth,"Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.2 is available in diffusers!; 
"
HuatuoGPT-7B,Text Generation,PyTorch; Transformers,,apache-2.0,,7,,747,28673.20425,,https://huggingface.co/FreedomIntelligence/HuatuoGPT-7B,Please see our HuatuoGPT project: https://github.com/FreedomIntelligence/HuatuoGPT.
lora-trained-xl,Text-to-Image,Diffusers,,creativeml-openrail-m,,2,,6,30.31242619,,https://huggingface.co/diffusers/lora-trained-xl,"These are LoRA adaption weights for diffusers/stable-diffusion-xl-base-0.9. The weights were trained on a photo of sks dog using DreamBooth. You can find some example images in the following. ; 


; LoRA for the text encoder was enabled: False.; SDXL 0.9 Research License "
Pygmalion-13B-SuperHOT-8K-GGML,,,,other,,7,,0,70195.21477,,https://huggingface.co/TheBloke/Pygmalion-13B-SuperHOT-8K-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for TehVenom's merge of PygmalionAI's Pygmalion 13B.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:"
airoboros-13b-gpt4-1.4.1-qlora,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,,5,jondurbin/airoboros-gpt4-1.4.1,33,26657.09364,,https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.4.1-qlora,"This is a qlora fine-tune 13b parameter LlaMa model, using completely synthetic training data created gpt4 via https://github.com/jondurbin/airoboros; Dataset used: https://huggingface.co/datasets/jondurbin/airoboros-gpt4-1.4.1; The point of this is to allow people to compare a full fine-tune https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.4 to a qlora fine-tune.; This is mostly an extension of the previous gpt-4 series, with a few extras:; This model was fine-tuned with a fork of FastChat"
ArcanaMix,Text-to-Image,Diffusers,Japanese,creativeml-openrail-m,,8,,584,0.005555687,1,https://huggingface.co/Hemlok/ArcanaMix,; Join Discord Server; ; 
Redmond-Hermes-Coder-GGML,,,English,gpl,,8,,0,71782.4133,,https://huggingface.co/TheBloke/Redmond-Hermes-Coder-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for NousResearch's Redmond Hermes Coder.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools known to work with these model files.; These files are not compatible with llama.cpp."
umt5-base,Text2Text Generation,PyTorch; Transformers,102 languages,apache-2.0,,3,mc4,612,2448.348865,1,https://huggingface.co/google/umt5-base,"Google's UMT5; UMT5 is pretrained on the an updated version of mC4 corpus, covering 107 languages:; Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.; Note: UMT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; Pretraining Dataset: mC4"
SiberianFRED-T5-XL,Text2Text Generation,PyTorch; Safetensors; Transformers,Russian; English,mit,,8,SiberiaSoft/SiberianDataset,498,7133.692459,1,https://huggingface.co/SiberiaSoft/SiberianFRED-T5-XL,FRED-T5 обученный на SiberianDataset. Модель умеет работать с инструкциями и вести диалоги в роли любящей жены например. ; Список персонажей:; В будущем набор персонажей будет расширен.; Чит-чат:; Инструкции:
MeinaPastel_V6,Text-to-Image,Diffusers,English,creativeml-openrail-m,,3,,586,0.002850037,1,https://huggingface.co/Meina/MeinaPastel_V6,"MeinaPastel aims to make illustrations with a 2d feel to them with good light, shadows and details, making pastel or colorful images!; -- Recommendations of use:; -- If you like the model and wants to support me in being able to spend more time improving it:
-- You can do so by buying me a coffee at: https://ko-fi.com/meina ! ( it is not necessary but will be highly appreciated ); This model is a unet block merge of mostly MeinaMix and Colormixed, ultracolorv4 and a few others with minor block weight taken."
P.A.W.F.E.C.T-Alpha,Text-to-Image,Diffusers,English,wtfpl,,7,,"2,802",0.004058952,3,https://huggingface.co/lodestones/P.A.W.F.E.C.T-Alpha,"Diffusion model trained on 500k image-tags pairs scraped from Furaffinity. Alpha still, expect more epochs, more training data and overall better results in the future.; 
""anthro, fox, male, general, by 100racs"" Epoch 24, no inpainting; The tags contain the original FA tag list with tags appearing less than 40 times in total omitted, plus a tag corresponding the the general/mature/adult rating. If the artist also appears more than 40 times, an artist tag is added as well. The full list of tags and their number of occurences are available here. Training was done on TPUv3s using the LION optimizer.; Due to using data from Furaffinity, the model offers a wide variety of tags that aren't as popular as in other models for creating niche content. For example, the tag vore appears 25379 times in the dataset it was trained on as opposed to 4730 for FluffyRock. However, the preciseness of the tags on Furaffinity can also be left to be desired compared to e621, and as such it is recommended to merge it with FluffyRock if you want more control over your prompts.; Load up the safetensor file as well as the provided yaml file and put them in your model folder. Additionally, you are going to want to use CFG Rescale: https://github.com/Seshelle/CFG_Rescale_webui. 7.5 CFG and 0.7 Phi are recommended. "
LaBSE-instructDialogs,,PyTorch; Transformers,English,,,2,,9,516.5131205,,https://huggingface.co/zjkarina/LaBSE-instructDialogs,"sentence-transformers/LaBSE pre-trained on an instructional question-and-answer dataset. Evaluated on Precision at K metrics and Mean reciprocal rank.
Precision at K is a simple metric to understand and implement, but it has an important disadvantage - it does not take into account the order of elements in the ""top"". So, if we guessed only one item out of ten, it doesn't matter whether it was on the first or the last place - inline_formula in any case. It is obvious that the first variant is much better.
ean reciprocal rank equal to the reverse rank of the first correctly guessed item. Mean reciprocal rank varies in the range [0,1] and takes into account the position of items. Unfortunately, it does this only for one item - the 1st correctly predicted item, ignoring all subsequent items.; Evaluation results:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
darkjungepastle-v1_v2,,,,cc-by-4.0,,2,,0,8685.162277,,https://huggingface.co/darkjungle/darkjungepastle-v1_v2,"就一个屑模型; civitai也有，就不在这赘述了; [Image text]!
(https://huggingface.co/darkjungle/darkjunglepastel-v1/blob/main/xyz_grid-0002-2939914227-masterpiece%2C%20best%20quality%2C%20loli%2C%20small%20breasts%2C%20green%20hair%2C%20orange%20eyes%2C%20clover-shaped%20pupils%2C%20shiny%20pupils%2C%20highlight%20in%20the%20pu.jpg)
[/Image text]; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
guanaco-33b-PI-8192-LoRA-4bit-32g,Text Generation,PyTorch; Transformers,,other,,4,,3,19867.98109,,https://huggingface.co/Panchovix/guanaco-33b-PI-8192-LoRA-4bit-32g,"guanaco-33b merged with bhenrym14's airoboros-33b-gpt4-1.4.1-PI-8192-LoRA, quantized at 4 bit.; More info about the LoRA Here. This is an alternative to SuperHOT 8k LoRA trained with LoRA_rank 64, and airoboros 1.4.1 dataset.; It was created with GPTQ-for-LLaMA with group size 32 and act order true as parameters, to get the maximum perplexity vs FP16 model.; I HIGHLY suggest to use exllama, to evade some VRAM issues.; Use compress_pos_emb = 4 for any context up to 8192 context."
airoboros-33b-gpt4-1.4.1-PI-8192-GGML,,,,,,4,,0,117657.6016,,https://huggingface.co/ycros/airoboros-33b-gpt4-1.4.1-PI-8192-GGML,"GGML quants of https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-PI-8192-fp16; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
orca_mini_v2_7B-GGML,Text Generation,Transformers,English,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2304.12244.pdf,28,psmathur/orca_minis_uncensored_dataset,0,61870.10053,,https://huggingface.co/TheBloke/orca_mini_v2_7B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Pankaj Mathur's Orca Mini v2 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
flacuna-13b-v1.0,Text Generation,PyTorch; Transformers,,other,https://arxiv.org/pdf/2307.02053.pdf,14,declare-lab/InstructEvalImpact; declare-lab/flan-mini,31,13722.76753,1,https://huggingface.co/declare-lab/flacuna-13b-v1.0,"Paper | Model | Dataset; ? We still have numerous experiments awaiting completion (details are here), requiring additional computing resources in our lab. If any industry professionals reading this are willing to provide assistance, please feel free to reach out to us at sporia@sutd.edu.sg.; Flacuna was developed by fine-tuning Vicuna on Flan-mini, a comprehensive instruction collection encompassing various tasks. Vicuna is already an excellent writing assistant, and the intention behind Flacuna was to enhance Vicuna's problem-solving capabilities. To achieve this, we curated a dedicated instruction dataset called Flan-mini.; As a result of this fine-tuning process, Flacuna exhibited notable performance improvements in problem-solving across multiple benchmark datasets, both in few-shot and zero-shot settings.; During training, Flacuna is a 13B checkpoint of LLaMA and employed a maximum input sequence length of 1280. We utilized LoRA for parameter-efficient fine-tuning."
orca_mini_v2_ger_7b,Text Generation,PyTorch; Transformers,German; English,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2304.12244.pdf,4,,102,13804.04814,,https://huggingface.co/jphme/orca_mini_v2_ger_7b,"orca_mini_v2_ger_7b is a variant of Pankaj Mathur′s Orca Mini V2 7b model, finetuned on an additional dataset in German language. 
The original model has been trained on explain tuned datasets, created using instructions and input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; This model is optimized for German text, providing proficiency in understanding, generating, and interacting with German language content.
However the model is not yet fully optimized for German language, as it has been trained on a small, experimental dataset and has limited capabilities due to the small parameter count. ; I am working on improving the model′s capabilities and will update the model if there is sufficient interest.; A quantized GGML version for use with llama.cpp, kobold.cpp and other GUIs for CPU inference can be found here.; I did only evaluate the output on a small, handcrafted sample on test prompts in German, confirming that the model's ability to understand and generate German text is well above the base model. "
chatglm2-6b-fp16.flm,,,,apache-2.0,,4,,0,13312.00165,,https://huggingface.co/huangyuyang/chatglm2-6b-fp16.flm,"fastllm model for chatglm-6b-fp16; Github address: https://github.com/ztxz16/fastllm; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Dalcefo,Text-to-Image,,,creativeml-openrail-m,,2,,0,162368.9016,,https://huggingface.co/LibreSD/Dalcefo,"ko-fi.com/dalcefo_artworks; Unable to determine this model’s library. Check the
								docs 
.
							"
MiracleMixGlitter_v1,Text-to-Image,Diffusers,,other,,3,,562,3543.043458,3,https://huggingface.co/digiplay/MiracleMixGlitter_v1,"Model info :; https://civitai.com/models/101423/miraclemix-glitter-an-anime-model-trained-and-specialized-on-creating-detailed-images-for-stunning-wallpaper; Original Author's DEMO images :; 




"
Andite,Text-to-Image,,,creativeml-openrail-m,,2,,0,99348.92158,,https://huggingface.co/LibreSD/Andite,"civitai.com/user/andite; Unable to determine this model’s library. Check the
								docs 
.
							"
falcon-7b-QueAns,,PEFT,English,,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2106.09685.pdf; https://arxiv.org/pdf/2305.14314.pdf,5,squad; tiiuae/falcon-refinedweb,76,1589.720246,,https://huggingface.co/avnishkr/falcon-7b-QueAns,"Falcon-7b-QueAns is a chatbot-like model for Question and Answering. It was built by fine-tuning Falcon-7B on the SQuAD dataset. This repo only includes the QLoRA adapters from fine-tuning with ?'s peft package. ; ?? This is a finetuned version for specifically question and answering. If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at Falcon-7B-Instruct. ; ? Looking for an even more powerful model? Falcon-40B is Falcon-7B's big brother!; The model was fine-tuned in 4-bit precision using ? peft adapters, transformers, and bitsandbytes. Training relied on a method called ""Low Rank Adapters"" (LoRA), specifically the QLoRA variant. The run took approximately 4 hours and was executed on a workstation with a single T4 NVIDIA GPU with 15 GB of available memory. See attached [Colab Notebook] used to train the model. ; July 06, 2023"
CAMEL-33B-Combined-Data-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,,3,,22,18536.78378,,https://huggingface.co/TheBloke/CAMEL-33B-Combined-Data-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for CAMEL AI's CAMEL 33B Combined Data merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
FlexDreamHK,Text-to-Image,Diffusers,Japanese,creativeml-openrail-m,,11,,0,2181.133037,,https://huggingface.co/den2nova/FlexDreamHK,"　FlexDreamHKはリ`クされたNovelAIモデルの入っていない、あるいはそのリスクを可能な限り低くしたモデルを目指して作成しました。
　モデル名はマ`ジに使用したモデルたちに敬意を表し、主要なモデル名をMみ合わせて命名しています。
　マ`ジ元となったモデルはStable DiffusionやWifu Diffusionへの追加学（ファインチュ`ニング）を行ったもののみで成されています。
　また、にじジャ`ニ`と普段使いしているモデルから生成した}から}柄LoRAを作成?マ`ジしており、いわゆる蒸留系と呼ばれるモデルでもあります。
　マ`ジの^程と使用したLoRAそのもの、またそれを作成したHのデ`タセットを_示する事で可能な限り透明性を担保しました。; creativeml-openrail-m; 　モデルの作成にHし、ＮＡＩリ`クフリ`マ`ジモデル研究会を大いに活用させてきました。
　意欲の持AやアイデアのWきがあった他、モデル作成に後押しをして下さった方やモデル情螭蚬灿肖筏皮ださった皆さんに感x申し上げます。"
Airoboros-7B-GPT4-1-4-SuperHOT-8K-GGML,,,,other,,8,,0,36423.70121,,https://huggingface.co/TheBloke/Airoboros-7B-GPT4-1-4-SuperHOT-8K-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Jon Durbin's Airoboros 7B GPT4 1.4.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:"
Airoboros-7B-GPT4-1-4-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,,4,,361,4098.392943,,https://huggingface.co/TheBloke/Airoboros-7B-GPT4-1-4-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Jon Durbin's Airoboros 7B GPT4 1.4 merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
Guanaco-7B-SuperHOT-8K-GGML,,,,other,,3,,0,36423.69438,,https://huggingface.co/TheBloke/Guanaco-7B-SuperHOT-8K-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Tim Dettmers' Guanaco 7B.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:"
Robin-7B-v2-SuperHOT-8K-GGML,,,,other,,3,,0,36423.69516,,https://huggingface.co/TheBloke/Robin-7B-v2-SuperHOT-8K-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for OptimalScale's Robin 7B v2.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:"
Vicuna-7B-v1-3-SuperHOT-8K-fp16,Text Generation,PyTorch; Transformers,,other,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,3,,64,13805.92883,,https://huggingface.co/TheBloke/Vicuna-7B-v1-3-SuperHOT-8K-fp16,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These are fp16 pytorch format model files for LmSys' Vicuna 7B v1.3 merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 7b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length."
Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-fp16,Text Generation,PyTorch; Transformers,,other,,3,,301,13805.92819,,https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-fp16,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These are fp16 pytorch format model files for Eric Hartford's Wizard Vicuna 7B Uncensored merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 7b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length."
WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-GGML,,,,other,,3,,0,36423.6932,,https://huggingface.co/TheBloke/WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's WizardLM-7B-V1.0-Uncensored.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:"
Koala-13B-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,,2,,56,7631.187581,,https://huggingface.co/TheBloke/Koala-13B-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Koala 13B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
Vicuna-7B-CoT-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,https://arxiv.org/pdf/1910.09700.pdf,6,,25,4098.391328,,https://huggingface.co/TheBloke/Vicuna-7B-CoT-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Kevin Pro's Vicuna 7B CoT merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-GPTQ,Text Generation,Transformers,,other,,9,,702,4098.384645,,https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eric Hartford's Wizard Vicuna 7B Uncensored merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size
jina-embedding-b-en-v1,Sentence Similarity,PyTorch; Sentence Transformers,English,apache-2.0,,3,jinaai/negation-dataset,187,442.2666744,,https://huggingface.co/jinaai/jina-embedding-b-en-v1,"; 

; 
The text embedding set trained by Jina AI, Finetuner team.
; jina-embedding-b-en-v1 is a language model that has been trained using Jina AI's Linnaeus-Clean dataset.
This dataset consists of 380 million pairs of sentences, which include both query-document pairs.
These pairs were obtained from various domains and were carefully selected through a thorough cleaning process.
The Linnaeus-Full dataset, from which the Linnaeus-Clean dataset is derived, originally contained 1.6 billion sentence pairs.; The model has a range of use cases, including information retrieval, semantic textual similarity, text reranking, and more."
30B-Epsilon,Text Generation,PyTorch; Transformers,,,,5,,26,83937.82355,,https://huggingface.co/CalderaAI/30B-Epsilon,"Epsilon is an instruct based general purpose model assembled from hand picked models and LoRAs.
There is no censorship and it follows instructions in the Alpaca format. This means you can create
your own rules in the context memory of your inference system of choice [mainly KoboldAI or Text
Generation Webui and chat UIs like SillyTavern and so on].; This model is the result of an experimental use of LoRAs on language models and model merges.
[] = applied as LoRA to a composite model | () = combined as composite models
30B-Epsilon = [SuperCOT[SuperHOT-prototype13b-8192[(wizardlmuncensored+((hippogriff+manticore)+(StoryV2))]; Alpaca's instruct format can be used to do many things, including control of the terms of behavior
between a user and a response from an agent in chat. Below is an example of a command injected into
memory.; All datasets from all models and LoRAs used were documented and reviewed as model candidates for merging.
Model candidates were based on five core principles: creativity, logic, inference, instruction following,
and longevity of trained responses. SuperHOT-prototype30b-8192 was used in this mix, not the 8K version;
the prototype LoRA seems to have been removed [from HF] as of this writing. The GPT4Alpaca LoRA from
Chansung was removed from this amalgam following a thorough review of where censorship and railroading
the user came from in 33B-Lazarus. This is not a reflection of ChanSung's excellent work - it merely did
not fit the purpose of this model.; manticore-30b-chat-pyg-alpha [Epoch0.4] by openaccess-ai-collective"
openchat_v2_w,Text Generation,PyTorch; Transformers,English,other,,29,,703,26655.24373,2,https://huggingface.co/openchat/openchat_v2_w,"The OpenChat v2 family is inspired by offline reinforcement learning, including conditional behavior cloning (OpenChat-v2) and weighted behavior cloning (OpenChat-v2-w).; We provide the full source code, including an inference server compatible with the ""ChatCompletions"" API, in the OpenChat GitHub repository.; OpenChat also includes a web UI for a better user experience. See the GitHub repository for instructions.; The conversation template involves concatenating tokens, and cannot be expressed in plain-text.; Besides base model vocabulary, an end-of-turn token <|end_of_turn|> is added."
Guanaco-33B-SuperHOT-8K-GGML,,,,other,,7,,0,175001.6119,,https://huggingface.co/TheBloke/Guanaco-33B-SuperHOT-8K-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Tim Dettmers' Guanaco 33B.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:"
Guanaco-33B-SuperHOT-8K-fp16,Text Generation,PyTorch; Transformers,,other,,3,,46,66634.10975,,https://huggingface.co/TheBloke/Guanaco-33B-SuperHOT-8K-fp16,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These are fp16 pytorch format model files for Tim Dettmers' Guanaco 33B merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 33b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length."
WizardLM-13B-V1-1-SuperHOT-8K-GGML,,,,other,https://arxiv.org/pdf/2304.12244.pdf,14,,0,49121.29145,,https://huggingface.co/TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for WizardLM's WizardLM 13B V1.1.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:"
airoboros-65b-gpt4-1.4.1-PI-8192-4bit-32g-actorder,Text Generation,PyTorch; Transformers,,,,7,jondurbin/airoboros-gpt4-1.4.1,1,39426.40675,,https://huggingface.co/ycros/airoboros-65b-gpt4-1.4.1-PI-8192-4bit-32g-actorder,"fp16 is here: https://huggingface.co/ycros/airoboros-65b-gpt4-1.4.1-PI-8192-fp16; peft file is here: https://huggingface.co/ycros/airoboros-65b-gpt4-1.4.1-PI-8192-peft; ggml quants: https://huggingface.co/ycros/airoboros-65b-gpt4-1.4.1-PI-8192-GGML; This is based on bhenrym14's airoboros 33b PI 8192 but on 65b.; See bhenrym14's notes there, everything applies except I based this on llama-65B."
jina-embedding-l-en-v1,Sentence Similarity,PyTorch; Sentence Transformers,English,apache-2.0,,5,jinaai/negation-dataset,126,1375.426684,,https://huggingface.co/jinaai/jina-embedding-l-en-v1,"; 

; 
The text embedding set trained by Jina AI, Finetuner team.
; jina-embedding-l-en-v1 is a language model that has been trained using Jina AI's Linnaeus-Clean dataset.
This dataset consists of 380 million pairs of sentences, which include both query-document pairs.
These pairs were obtained from various domains and were carefully selected through a thorough cleaning process.
The Linnaeus-Full dataset, from which the Linnaeus-Clean dataset is derived, originally contained 1.6 billion sentence pairs.; The model has a range of use cases, including information retrieval, semantic textual similarity, text reranking, and more."
gcolab-prunned-sdxl,,,,,,6,,0,13332.48148,,https://huggingface.co/Kefasu/gcolab-prunned-sdxl,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
open_llama_7b_v2_ggml,,,,apache-2.0,,4,,0,75755.52458,,https://huggingface.co/SlyEcho/open_llama_7b_v2_ggml,"For use with llama.cpp.; Coming soon...; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
mpt-30b-v2,Text Generation,PyTorch; Transformers,,apache-2.0,,9,ehartford/dolphin,90,61363.14169,,https://huggingface.co/manojpreveen/mpt-30b-v2,Base Model : mosaicml/mpt-30b; Tool : MosaicML's llm-foundry (https://github.com/mosaicml/llm-foundry); Dataset : Entire flan3m-GPT3.5 dataset.; Config yaml with Model Params : https://huggingface.co/manojpreveen/mpt-30b-v2/blob/main/mpt-30b_orca.yaml; Prompt Format :
WizardCoder-Guanaco-15B-V1.0-GGML,,,English,apache-2.0,,9,guanaco,0,71782.41252,,https://huggingface.co/TheBloke/WizardCoder-Guanaco-15B-V1.0-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are StarCoder GGML format model files for LoupGarou's WizardCoder Guanaco 15B V1.0.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools that work with this GGML model.; These files are not compatible with llama.cpp or text-generation-webui."
WizardCoder-Guanaco-15B-V1.0-GPTQ,Text Generation,Transformers,English,apache-2.0,,6,guanaco,418,9424.067078,,https://huggingface.co/TheBloke/WizardCoder-Guanaco-15B-V1.0-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for LoupGarou's WizardCoder Guanaco 15B V1.0.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Please make sure you're using the latest version of text-generation-webui.
airochronos-33B,Text Generation,PyTorch; Transformers,,other,,3,,213,66642.46105,,https://huggingface.co/Henk717/airochronos-33B,"After the initial experiment with chronoboros-33B it was evident that the merge was to unpredictable to be useful, testing the individual models it became clear that the bias should be weighted towards Chronos.
This is the new release of the merge with 75% chronos 33B, and 25% airoboros-1.4 33B.; Model has been tested with the Alpaca prompting format combined with KoboldAI Lite's instruct and chat modes, as well as regular story writing.
It has also been tested on basic reasoning tasks, but has not seen much testing for factual information."
Bart-large-paper2slides-summarizer,Summarization,PyTorch; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/1711.00043.pdf,5,,489,3339.677549,,https://huggingface.co/com3dian/Bart-large-paper2slides-summarizer,"; This repository contains the Bart-Large-paper2slides-summarizer Model, which has been fine-tuned on the Automatic Slide Generation from Scientific Papers dataset using unsupervised learning techniques using an algorithm from the paper entitled 'Unsupervised Machine Translation Using Monolingual Corpora Only'.
Its primary focus is to summarize scientific texts with precision and accuracy, the model is parallelly trained with the Bart-large-paper2slides-expander from the same contributor.; Bart (Bidirectional and Auto-Regressive Transformers) is a sequence-to-sequence (seq2seq) model developed by Facebook AI Research. It has shown exceptional performance in various natural language processing (NLP) tasks such as text summarization, text generation, and machine translation.; This particular model, Bart-Large, is the larger version of the Bart model. It consists of 12 encoder and decoder layers and has a total of 400 million parameters.; To use this model, you can leverage the Hugging Face Transformers library. Here's an example of how to use it in Python:"
MPT-30B-Dolphin-v2-GGML,,,,other,,7,,0,113152.0074,,https://huggingface.co/TheBloke/MPT-30B-Dolphin-v2-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are MPT GGML format model files for Manoj Preveen's MPT 30B Dolphin v2.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools that work with this GGML model.; These files are not compatible with llama.cpp or text-generation-webui."
falcon-7b-code-alpaca-lora,Text Generation,,English,cc-by-nc-4.0,,2,stanford_alpaca,0,9.464872475,,https://huggingface.co/jinaai/falcon-7b-code-alpaca-lora,"; 

; 
LLM Generation models trained by Jina AI, Finetuner team.
; This repo contains the lora weights (8bit) for Falcon-7b
fit on the Code Alpaca dataset.; This version of the weights was trained with the following hyperparameters:"
falcon-7b-code-alpaca,Text Generation,PyTorch; Transformers,English,cc-by-nc-4.0,,2,stanford_alpaca,53,14172.17969,,https://huggingface.co/jinaai/falcon-7b-code-alpaca,"; 

; 
LLM Generation models trained by Jina AI, Finetuner team.
; This repo contains the full weights (16bit) for Falcon-7b
fit on the Code Alpaca dataset.; This version of the weights was trained with the following hyperparameters:"
falcon-40b-code-alpaca-lora,Text Generation,,English,cc-by-nc-4.0,,2,stanford_alpaca,0,33.50487343,,https://huggingface.co/jinaai/falcon-40b-code-alpaca-lora,"; 

; 
LLM Generation models trained by Jina AI, Finetuner team.
; This repo contains the lora weights (8bit) for Falcon-40b
fit on the Code Alpaca dataset.; This version of the weights was trained with the following hyperparameters:"
falcon-40b-code-alpaca,Text Generation,PyTorch; Transformers,English,cc-by-nc-4.0,,2,stanford_alpaca,6,84561.95814,,https://huggingface.co/jinaai/falcon-40b-code-alpaca,"; 

; 
LLM Generation models trained by Jina AI, Finetuner team.
; This repo contains the full weights (16bit) for Falcon-40b
fit on the Code Alpaca dataset.; This version of the weights was trained with the following hyperparameters:"
moss-rlhf-reward-model-7B-zh,,,Chinese,agpl-3.0,https://arxiv.org/pdf/2307.04964.pdf,7,,0,0.495820732,,https://huggingface.co/Ablustrund/moss-rlhf-reward-model-7B-zh,"moss-rlhf-reward-model-7B-zh
; moss-rlhf-reward-model-7B-en; moss-rlhf-sft-model-7B-en
; Due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle.
In this technical report, we intend to help researchers to train their models stably with human feedback.; Contributions are summarized as follows: "
Realisian_v5,Text-to-Image,Diffusers,,other,,4,,"2,210",2181.122835,1,https://huggingface.co/digiplay/Realisian_v5,"Model info :; https://civitai.com/models/47130?modelVersionId=115942; Sample images I made ; 

; Original Author's DEMO image :"
OpenOrca-Preview1-13B-GGML,Text Generation,Transformers,English,other,https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2301.13688.pdf,15,Open-Orca/OpenOrca,0,119316.4965,,https://huggingface.co/TheBloke/OpenOrca-Preview1-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Open-Orca's OpenOrca-Preview1-13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh."
OpenOrca-Preview1-13B-GPTQ,Text Generation,Transformers,English,other,https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2301.13688.pdf,8,Open-Orca/OpenOrca,791,7629.306441,,https://huggingface.co/TheBloke/OpenOrca-Preview1-13B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Open-Orca's OpenOrca-Preview1-13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
polylm-multialpaca-13b,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2307.06018.pdf,7,,361,31472.32532,,https://huggingface.co/DAMO-NLP-MT/polylm-multialpaca-13b,"This model is finetuned on polyLM-13b using multialpaca (a self-instruction dataset); Open; The information below in this section are copied from the model's official model card:; Our contributions are fully methodological: adding the support of multilingualism to LLM during training and SFT phases. It is unavoidable that PolyLM might exhibit several common deficiencies of language models, e.g. hallucination and toxicity. PolyLM should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.; This version activates the instruction-following capability of PolyLM through self-instruction, but currently, the training instructions are relatively simple and the support for abilities such as multi-turn dialogue, context understanding, CoT, Plugin, etc. is not very friendly. We are making efforts to develop a new version."
polylm-1.7b,Text Generation,PyTorch; Transformers,,apache-2.0,,4,,719,3773.073302,,https://huggingface.co/DAMO-NLP-MT/polylm-1.7b,
zodiac_eclipse_DAY1,Text-to-Image,Diffusers,,other,,3,,620,2608.602861,3,https://huggingface.co/digiplay/zodiac_eclipse_DAY1,Model info :; https://civitai.com/models/108417/zodiac-eclipse-day1; Sample image I made thru Huggingface's API :; ; Original Author's DEMO images :
nart-100k-7b,Text Generation,PyTorch; Transformers,,cc-by-nc-nd-4.0,,10,,11,27597.45487,,https://huggingface.co/jerryjalapeno/nart-100k-7b,
Dreamsphere,Text-to-Image,Diffusers,,creativeml-openrail-m,,2,,"3,074",2355.202536,4,https://huggingface.co/Yntec/Dreamsphere,"Preview image by Digiplay:; ; A mix of Noosphere v3 by skumerz and my favorite models.; Original pages:
https://civitai.com/models/36538?modelVersionId=107675
https://civitai.com/models/5528/rainbowpatch"
Rainbowsphere,Text-to-Image,Diffusers,,creativeml-openrail-m,,2,,"2,842",2355.202575,4,https://huggingface.co/Yntec/Rainbowsphere,"A mix of Noosphere v3 by skumerz and Rainbowpath by PatchMonk. You can use ""Rainbowpath"" in the prompt to enhance the style.; Preview image by Digiplay:; ; Original pages:
https://civitai.com/models/36538?modelVersionId=107675
https://civitai.com/models/5528/rainbowpatch"
Wizard-Vicuna-30B-Uncensored-lxctx-PI-16384-LoRA-4bit-32g,Text Generation,PyTorch; Transformers,,other,,2,,3,19868.10511,,https://huggingface.co/Panchovix/Wizard-Vicuna-30B-Uncensored-lxctx-PI-16384-LoRA-4bit-32g,"Wizard-Vicuna-30B-Uncensored merged with bhenrym14's airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-LoRA, quantized at 4 bit.; More info about the LoRA Here. This is an alternative to SuperHOT 8k LoRA trained with LoRA_rank 64 and context extended to 16K, with airoboros 1.4.1 dataset.; It was created with GPTQ-for-LLaMA with group size 32 and act order true as parameters, to get the maximum perplexity vs FP16 model.; I HIGHLY suggest to use exllama, to evade some VRAM issues.; Use compress_pos_emb = 8 for any context up to 16384 context."
polla_mix_2.3D,Text-to-Image,Diffusers,,other,,3,,485,10639.36271,2,https://huggingface.co/digiplay/polla_mix_2.3D,Model info :; https://civitai.com/models/110130?modelVersionId=118730; Sample image I made thru Huggingface's API :; ; Original Author's DEMO images :
polla_mix_2.5D,Text-to-Image,Diffusers,,other,,2,,727,4515.842953,3,https://huggingface.co/digiplay/polla_mix_2.5D,Model info :; https://civitai.com/models/110130?modelVersionId=118741; Sample image I made thru Huggingface's API :; ; Original Author's DEMO images :
HCTM,,,,mit,,2,,0,0.001504402,,https://huggingface.co/hctmgroup/HCTM,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
MeinaMix_V11,Text-to-Image,Diffusers,English,creativeml-openrail-m,,2,,"1,101",0.003552666,2,https://huggingface.co/Meina/MeinaMix_V11,"MeinaMix Objective is to be able to do good art with little prompting.; For examples and prompts, please checkout: https://civitai.com/models/7240/meinamix
I have a discord server where you can post images that you generated, discuss prompt and/or ask for help.; https://discord.gg/XC9nGZNDUd If you like one of my models and want to support their updates; I've made a ko-fi page; https://ko-fi.com/meina where you can pay me a coffee <3; And a Patreon page; https://www.patreon.com/MeinaMix where you can support me and get acess to beta of my models!"
GOLDFish,Text-to-Image,Diffusers,,creativeml-openrail-m,,2,,816,2355.202529,4,https://huggingface.co/Yntec/GOLDFish,A mix between the models OLDFIsh by timevisitor and RMHF_2.5D_v2 by TkskKurumi.; Preview image by Digiplay:; ; Original pages:; https://civitai.com/models/14978?modelVersionId=40101
MuseRWKV,Text Generation,PyTorch; Safetensors; Transformers,,,,2,,15,1892.735967,,https://huggingface.co/breadlicker45/MuseRWKV,"this has too little layers, but it will still make music"
span-marker-xlm-roberta-large-verbs,Token Classification,PyTorch; Safetensors; SpanMarker,,apache-2.0,,2,,24,4604.798741,,https://huggingface.co/tomaarsen/span-marker-xlm-roberta-large-verbs,"This is a SpanMarker model that can be used for identifying verbs in text.
In particular, this SpanMarker model uses xlm-roberta-large as the underlying encoder.
See span_marker_verbs_train.ipynb for the training script used to create this model.; Note that this model is an experiment about the feasibility of SpanMarker as a POS tagger. I would generally recommend using spaCy or NLTK instead, as these are more computationally efficient approaches.; To use this model for inference, first install the span_marker library:; You can then run inference with this model like so:; See the SpanMarker repository for documentation and additional information on this library."
PhotoSomnia_vFinal,Text-to-Image,Diffusers,,other,,2,,444,2181.122704,3,https://huggingface.co/digiplay/PhotoSomnia_vFinal,Model info :; https://civitai.com/models/18637/photosomnia; Original Author's DEMO image :; ; Sample image thru huggingface's API :
LLaMA-7B-2bit-alpaca,,,,apache-2.0,,2,,0,71.70218723,,https://huggingface.co/GreenBitAI/LLaMA-7B-2bit-alpaca,"This is GreenBitAI's instruction-tuned LoRA parameters for our 2-bit 7B LLaMA model trained on the Alpaca-clean 50k dataset.; Please refer to our Github page for the code to run the model and more information.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Others,,,,creativeml-openrail-m,,2,,0,0.001521568,,https://huggingface.co/Gyunyu-pudding/Others,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
llama-33b-lxctx-PI-16384-LoRA,,,,,,2,,0,975.0025754,,https://huggingface.co/bhenrym14/llama-33b-lxctx-PI-16384-LoRA,"Mostly untested!; This is base Llama-33b with minimal additional training to extend the useful context window.; This is a QLoRA fine-tune; Pretraining took 10 hours on 1x RTX 6000 Ada.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Gugugo-koen-1.3B-V0.95,Translation,PyTorch; Safetensors; Transformers,English; Korean,apache-2.0,,2,,30,5449.332926,1,https://huggingface.co/squarelike/Gugugo-koen-1.3B-V0.95,https://github.com/jwj7140/Gugugo; Prompt Template:
MeinaMix_v11,Text-to-Image,Diffusers,,other,,2,,521,2181.122688,1,https://huggingface.co/digiplay/MeinaMix_v11,"Model info :; https://civitai.com/models/7240?modelVersionId=119057; Sample images generated by huggingface's API :; 
; prompt :"
skin-generator-minecraft,Text-to-Image,Diffusers,English,openrail,,2,,325,0.004228783,,https://huggingface.co/agentapp/skin-generator-minecraft,"This Stable Diffusion model was fine-tuned to generate a pre-version 1.8 Minecraft character skins, based on a text prompt.; The model was fine-tuned on the dataset for 13,000 steps using the 'train_text_to_image.py' script provided with the diffusers library.  A checkpoint has been included in the 'checkpoint' directory.; Some postprocessing is required to import and use the generated skins in Minecraft.; This model is a fork from monadicial/minecraft-skin-generator. This fork will contain the production model for a frontend that processing the results so the output can be used; Here are some example text prompts and the images they generate:"
Colorful_v2.0,Text-to-Image,Diffusers,,other,,2,,34,2181.122208,,https://huggingface.co/digiplay/Colorful_v2.0,https://civitai.com/models/7279?modelVersionId=13196
llama-7b-prosocialdialog-lora,,,,,,2,,0,16.80182293,,https://huggingface.co/surika/llama-7b-prosocialdialog-lora,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
anime-diffusion-v0,Text-to-Image,Diffusers,,creativeml-openrail-m,,2,,148,2181.123993,1,https://huggingface.co/Ryzan/anime-diffusion-v0,"This is currently a base model
; This model does not use any finetuning techniques such as face restoration or in-painting as of yet; Test the concept via A1111 Colab fast-Colab-A1111; Sample pictures of this concept:
; See more results from this model under the sample_images folder"
sorceroboros-33b-s2a4-gptq,Text Generation,Transformers,English,,,2,ehartford/WizardLM_evol_instruct_V2_196k_unfiltered_merged_split; jondurbin/airoboros-gpt4-1.4.1; openai/summarize_from_feedback; ehartford/wizard_vicuna_70k_unfiltered,3,18024.73269,,https://huggingface.co/chargoddard/sorceroboros-33b-s2a4-gptq,"Trained on a flavorful melange of the WizardLM, Airoboros, and Wizard Vicuna datasets.
This model was trained using both linear and NTK-aware RoPE scaling in tandem. When loading, ensure that compress_pos_emb (or scale) is set to 2, and alpha_value is set to 4. Both values must be set.; Expect context length of up to 8192 to work for sure. It will probably maintain coherence into the ~12k range, but I have not tested that.; Prompt format is vicuna 1.1:"
UltraLM-65b,Text Generation,PyTorch; Transformers,,,,2,,3,133654.8767,,https://huggingface.co/openbmb/UltraLM-65b,No model card; New: Create and edit this model card directly on the website!
bark-small,Text-to-Speech,PyTorch; Transformers,13 languages,cc-by-nc-4.0,,2,,689,1724.293591,1,https://huggingface.co/suno/bark-small,"Bark is a transformer-based text-to-audio model created by Suno. 
Bark can generate highly realistic, multilingual speech as well as other audio - including music, 
background noise and simple sound effects. The model can also produce nonverbal 
communications like laughing, sighing and crying. To support the research community, 
we are providing access to pretrained model checkpoints ready for inference.; The original github repo and model card can be found here.; This model is meant for research purposes only. 
The model output is not censored and the authors do not endorse the opinions in the generated content. 
Use at your own risk.; Two checkpoints are released:; Try out Bark yourself!"
open_llama_3b_v2_ggml,,,,apache-2.0,,2,,0,19957.76406,,https://huggingface.co/SlyEcho/open_llama_3b_v2_ggml,"Support is now merged to master branch.; There are now more quantization types in llama.cpp, some lower than 4 bits.
Currently these are not well supported because of technical reasons.
If you want to use them, you have to build llama.cpp (from build 829 (ff5d58f)) with the LLAMA_QKK_64 Make or CMake variable enabled (see PR #2001).
Then you can quantize the F16 or maybe Q8_0 version to what you want.; Coming soon ...; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ggml-alicia-13b,Text Generation,,,,,2,,0,9451.521724,,https://huggingface.co/eachadea/ggml-alicia-13b,"(OpenOrca Preview + OrcaMiniV2 @ 0.4) + (Chronos @ 0.08) + (Hermes @ 0.08) + (Wizard Vicuna @ 0.08) + (Samantha @ 0.20); Recognized templates:; and; Unable to determine this model’s library. Check the
								docs 
.
							"
ggml-hws-13b,Text Generation,,,,,2,,0,9625.601652,,https://huggingface.co/eachadea/ggml-hws-13b,"(Hermes + Wizard 1.1 @ 0.5) + Selfee @ 0.2; Recognizes templates:; and; Unable to determine this model’s library. Check the
								docs 
.
							"
alpagasus-13b,Text Generation,PyTorch; Transformers,,other,https://arxiv.org/pdf/2307.08701.pdf,2,,0,53320.22401,,https://huggingface.co/gpt4life/alpagasus-13b,"This is an unofficial implementation of AlpaGasus-13B, which is a chat assistant trained by fine-tuning LLaMA on a Claud-filtered Alpaca dataset with around 5K triplets.; Please see the original LLaMA license before using this model.; AlpaGasus-13B is fine-tuned from LLaMA-13B with supervised instruction fine-tuning on the filtered Alpaca dataset.; Inference API has been turned off for this model."
ct2fast-Llama-2-7b-chat-hf,Text Generation,Transformers; PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,2,,11,6904.361012,,https://huggingface.co/michaelfeil/ct2fast-Llama-2-7b-chat-hf,"Speedup inference while reducing memory by 2x-4x using int8 inference in C++ on CPU or GPU.; quantized version of meta-llama/Llama-2-7b-chat-hf; Checkpoint compatible to ctranslate2>=3.17.1
and hf-hub-ctranslate2>=2.12.0; Converted on 2023-07-21 using; This is just a quantized version. Licence conditions are intended to be idential to original huggingface repo."
ct2fast-Llama-2-70b-chat-hf,,,,,,2,,0,0.001521568,,https://huggingface.co/michaelfeil/ct2fast-Llama-2-70b-chat-hf,"still missing support from ctranslate2.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ct2fast-Llama-2-13b-chat-hf,Text Generation,Transformers; PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,2,,18,13315.85098,,https://huggingface.co/michaelfeil/ct2fast-Llama-2-13b-chat-hf,"Speedup inference while reducing memory by 2x-4x using int8 inference in C++ on CPU or GPU.; quantized version of meta-llama/Llama-2-13b-chat-hf; Checkpoint compatible to ctranslate2>=3.17.1
and hf-hub-ctranslate2>=2.12.0; Converted on 2023-07-21 using; This is just a quantized version. Licence conditions are intended to be idential to original huggingface repo."
llama-2-13b,Text Generation,PyTorch; Transformers,,,,2,,6,53319.72242,,https://huggingface.co/dhruvabansal/llama-2-13b,"Llama 2
Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model. Links to other models can be found in the index at the bottom.; Model Details
Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations."
Chinese-LlaMA2-7B-chat,,,,,,2,,0,0.001484375,,https://huggingface.co/michaelwzhu/Chinese-LlaMA2-7B-chat,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
llama-2-ko-7b-chat-vicuna-hf-4bit,,TensorBoard,,,,2,,0,69.53523945,,https://huggingface.co/quantumaikr/llama-2-ko-7b-chat-vicuna-hf-4bit,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Legal_Penguin,Text Generation,PyTorch; Transformers,,,,2,,50,26655.24088,,https://huggingface.co/DangFutures/Legal_Penguin,No model card; New: Create and edit this model card directly on the website!
meta-llama-Llama-2-7b-chat-hf-w4-g128-awq,Text Generation,PyTorch; Transformers,English,,,2,,5,3983.855838,,https://huggingface.co/abhinavkulkarni/meta-llama-Llama-2-7b-chat-hf-w4-g128-awq,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format.; This model is a 4-bit 128 group size AWQ quantized model. For more information about AWQ quantization, please click here.; July 19, 2023; Please refer to the original LLaMA 2 model license (link).; Please refer to the AWQ quantization license (link)."
Llama-2-70b-chat-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,,2,,882,282567.9802,,https://huggingface.co/NousResearch/Llama-2-70b-chat-hf,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations."
GodziLLa-30B-instruct,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/2009.03300.pdf; https://arxiv.org/pdf/1803.05457.pdf; https://arxiv.org/pdf/1905.07830.pdf; https://arxiv.org/pdf/2109.07958.pdf,2,,1,66191.90515,,https://huggingface.co/MayaPH/GodziLLa-30B-instruct,"GodziLLa-30B-instruct is GodziLLa-30B finetuned on a mixture of instructions. For a more general use case, please use GodziLLa-30B instead. This finetuned model is not meant for any other use outside of research on competing LoRA adapter behavior and instruction tuning. More specifically, since this is inherently a LlaMA model, commercial use is prohibited. This model's primary purpose is to stress test the limitations of composite LLMs and observe its performance with respect to other LLMs available on the Open LLM Leaderboard.; ; The following datasets were used to finetune GodziLLa-30B further.; COMING SOON; According to the leaderboard description, here are the benchmarks used for the evaluation:"
chinese-alpaca-pro-lora-7b,,,Chinese,apache-2.0,,2,,0,1168.10323,,https://huggingface.co/ziqingyang/chinese-alpaca-pro-lora-7b,"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
LLama2-7b,Text Generation,PyTorch; Transformers,,,,2,,110,27597.31766,,https://huggingface.co/AlexWortega/LLama2-7b,No model card; New: Create and edit this model card directly on the website!
Llama-2-13B-german-assistant-v1,Text Generation,PyTorch; Transformers,English; German,,,2,flozi00/conversations,13,27161.24562,,https://huggingface.co/flozi00/Llama-2-13B-german-assistant-v1,"This model is an finetuned version for german instructions and conversations in style of Open Assistant tokens. ""<|prompter|>"" ""<|endoftext|>"" ""<|assistant|>""; The dataset used is deduplicated and cleaned, with no codes inside. The focus is on instruction following and conversational tasks.; The model archictecture is based on Llama version 2 with 13B parameters, trained on 100% renewable energy powered hardware.; This work is contributed by private research of flozi00"
llama-2-7b-ggml,,,English,apache-2.0,https://arxiv.org/pdf/1910.09700.pdf,2,,0,14233.60657,,https://huggingface.co/lazyiitian/llama-2-7b-ggml,This model card aims to be a base template for new models. It has been generated using this raw template.; [More Information Needed]; [More Information Needed]; [More Information Needed]; [More Information Needed]
Luna-AI-Llama2-Uncensored-GGML,Text Generation,,,,,2,,0,61931.52376,,https://huggingface.co/Araki/Luna-AI-Llama2-Uncensored-GGML,"Quantization from:
Tap-M/Luna-AI-Llama2-Uncensored; Converted to the GGML format with:
llama.cpp master-294f424 (JUL 19, 2023); Tested with:
koboldcpp 1.35; Example usage:; Prompt format (refer to the original model for additional details):"
PersonaStyleCheckpoint,Text-to-Image,Diffusers,,other,,2,,161,4341.763291,3,https://huggingface.co/digiplay/PersonaStyleCheckpoint,"Model info :; https://civitai.com/models/31771?modelVersionId=38190; Sample image I made thru Huggingface's API :
; Original Author's DEMO images :; 

"
Llama-2-13B-GPTQ-Orca,Text Generation,PyTorch,English,other,,2,,0,897.0170893,,https://huggingface.co/tridungduong16/Llama-2-13B-GPTQ-Orca,"These files are GPTQ model files for Meta's Llama 2 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.; Each separate quant is in a different branch.  See below for instructions on fetching from different branches.; Please make sure you're using the latest version of text-generation-webui."
prometheus.safetensors,,,,,,2,,0,10854.40313,,https://huggingface.co/doctorderp/prometheus.safetensors,"https://www.youtube.com/watch?v=JB-BmFUdT7o; First, please take the time to watch the video, you wont regret it. ; I am proud to introduce to you an innovative workflow solution for all of your prompting needs! Prometheus is a custom model along with 30 Hypernetworks I like to call Latent Later Cameras. These are virtual cameras that have been embedded in to the latent space which allow you to choose the camera angle and camera shot for your subject.  These cameras put you in to the directors seat so that you always get the shot you envisioned in your mind. Besides being very flexible, the latent cameras also tend to be very cohesive and produce very good results through natural language prompting.  That means there is no need for a word salad in your positive or negative prompts.  In fact you can use the Hypernetworks without any negative prompts at all and still get very good results. As long as you follow very basic rules outlined in the video guide, on average you will get very good results. Thank you!; Please note the Hypernetworks work with all models...that are based on the 1.5 architecture... sorry if I misled in the video. Sorry no 2.0 or 2.1 support.  ; Don't forget to enable your dynamic prompts by checking the enable box, otherwise you can load the Hypernetworks manually, just know that the strength of the Hypernetworks should be at 0.55 by default, otherwise you will not get good results. "
Llama-2-7b-chat-hf-dolly-ja-nf4,,PEFT,Japanese,,,2,,5,0,,https://huggingface.co/kunipm9/Llama-2-7b-chat-hf-dolly-ja-nf4,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; The following bitsandbytes quantization config was used during training:; The following bitsandbytes quantization config was used during training:; PEFT 0.5.0.dev0"
ruGPT-3.5-13B-8bit,Text Generation,PyTorch; Transformers,Russian,,,2,,19,25038.60977,,https://huggingface.co/pe4enov/ruGPT-3.5-13B-8bit,Квантизированная версия модели ruGPT-3.5
ruGPT-3.5-13B-fp16,Text Generation,PyTorch; Transformers,Russian; English,mit,,2,,55,26333.54013,,https://huggingface.co/Gaivoronsky/ruGPT-3.5-13B-fp16,This is a generative model converted to fp16 format based on ai-forever/ruGPT-3.5-13B; Inference API has been turned off for this model.
llama-2-7b-guanaco-dolly-8bit-sharded,Text Generation,PyTorch; Transformers,,apache-2.0,,2,databricks/databricks-dolly-15k; timdettmers/openassistant-guanaco,16,26959.87056,,https://huggingface.co/guardrail/llama-2-7b-guanaco-dolly-8bit-sharded,Model that is fine-tuned in 4-bit precision using QLoRA on timdettmers/openassistant-guanaco and databricks/databricks-dolly-15k. Sharded as well to be used on a free Google Colab instance. ; It can be easily imported using the AutoModelForCausalLM class from transformers:
llama-2-70b-guanaco-qlora,Text Classification,,English,,,2,,0,1699.842911,,https://huggingface.co/Mikael110/llama-2-70b-guanaco-qlora,"This is a Llama-2 version of Guanaco. It was finetuned from the base Llama-70b model using the official training scripts found in the QLoRA repo. I wanted it to be as faithful as possible and therefore changed nothing in the training script beyond the model it was pointing to. The model prompt is therefore also the same as the original Guanaco model.; This repo contains the QLoRA adapter.; A 7b version of the adapter can be found here.
A 13b version of the adapter can be found here.; Legal Disclaimer: This model is bound by the usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.; Unable to determine this model’s library. Check the
								docs 
.
							"
Anything_ink,,,,openrail,,2,,0,6123.524863,,https://huggingface.co/X779/Anything_ink,"This is a fine-tuning model based on stable diiffusion.The model is fine-tuned based on SD1.5.The model was fine-tuned with HCP-diffusion
The model prompt is extremely accurate.; ――――――――――――; Many of today's SD models have a variety of problems.I want to make use of my limited ability to improve the current situation.So I used a lot of AI-generated images to refine this model; ; ――――――――――――"
gogpt2-7b,Text Generation,PyTorch; Transformers,Chinese,apache-2.0,,2,BelleGroup/train_0.5M_CN; BelleGroup/train_1M_CN; c-s-ale/alpaca-gpt4-data-zh; BAAI/COIG,0,28878.2189,,https://huggingface.co/golaxy/gogpt2-7b,"; 


; ICT中英文底座增强大模型：70亿参数、130亿参数; GoGPT-Github; ?怎么从零到一训练一个LLM分词器"
ruGPT-3.5-13B-8bit,Text Generation,Transformers; PyTorch,Russian; English,mit,,2,,13,13728.06477,,https://huggingface.co/Gaivoronsky/ruGPT-3.5-13B-8bit,This is a generative model converted to fp16 format based on ai-forever/ruGPT-3.5-13B; Inference API has been turned off for this model.
llama2_7b_chat_uncensored-GGML,,,English,other,,2,ehartford/wizard_vicuna_70k_unfiltered,0,25507.84313,,https://huggingface.co/s3nh/llama2_7b_chat_uncensored-GGML,"Buy me a coffee if you like this project ;)
; GGML Format model files for This project.; Fine-tuned Llama-2 7B with an uncensored/unfiltered Wizard-Vicuna conversation dataset ehartford/wizard_vicuna_70k_unfiltered.
Used QLoRA for fine-tuning. Trained for one epoch on a 24GB GPU (NVIDIA A10G) instance, took ~19 hours to train.; The model was trained with the following prompt style:; Code used to train the model is available here."
llama2-qlora-finetunined-Arabic,,PEFT,,,,2,,0,134.0023083,,https://huggingface.co/HeshamHaroon/llama2-qlora-finetunined-Arabic,"The following bitsandbytes quantization config was used during training:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
13B-Ouroboros-GGML,Text Generation,Transformers,English,other,,2,Open-Orca/OpenOrca; anon8231489123/ShareGPT_Vicuna_unfiltered; jondurbin/airoboros-uncensored,12,119316.495,,https://huggingface.co/TheBloke/13B-Ouroboros-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for CalderaAI's 13B Ouroboros.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
honest_llama2_chat_7B,Text Generation,PyTorch; Transformers,,mit,https://arxiv.org/pdf/2306.03341.pdf,2,,1,13804.09181,,https://huggingface.co/likenneth/honest_llama2_chat_7B,"Ever wondering a less hallucinating LLaMA-2? Using the inference-time intervention (ITI) discussed in my recent preprint: https://arxiv.org/pdf/2306.03341.pdf, I baked the intervention learned from TruthfulQA into a LLaMA-2 7B model.
I don’t have big enough GPU to bake ITI into larger LLaMA-2 but the code to do so are all released in https://github.com/likenneth/honest_llama. Let me know if you are interested do that :)
You can load and play around starting from below:"
theallysMixIV-verisimilar,Text-to-Image,Diffusers,,creativeml-openrail-m,,2,,730,4372.482283,4,https://huggingface.co/Yntec/theallysMixIV-verisimilar,Original page:; https://civitai.com/models/40369/theallys-mix-iv-verisimilar
Upstage-Llama1-65B-Instruct-GPTQ,Text Generation,Transformers,English,other,,2,sciq; metaeval/ScienceQA_text_only; GAIR/lima; Open-Orca/OpenOrca; openbookqa,1,35535.14254,,https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Upstage's Llama 65B Instruct.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements."
Llama-2-13b-chat-german,Text Generation,PyTorch; Transformers,German; English,,https://arxiv.org/pdf/2307.09288.pdf,2,,0,26657.10847,1,https://huggingface.co/jphme/Llama-2-13b-chat-german,"Edit: You can find a Demo (German) here; Llama-2-13b-chat-german is a variant of Meta′s Llama 2 13b Chat model, finetuned on an additional dataset in German language.; This model is optimized for German text, providing proficiency in understanding, generating, and interacting with German language content. However the model is not yet fully optimized for German language, as it has been trained on a small, experimental dataset and has limited capabilities due to the small parameter count.
Some of the fineunting data is also targeted towards factual retrieval (only answer questions from information in the context and refuse to hallucinate) and the model should perform better for these tasks than original Llama 2 Chat.; I am working on improving the model′s capabilities and will update the model if there is sufficient interest.; A quantized GGML version for use with llama.cpp, kobold.cpp and other GUIs for CPU inference can be found here."
hermes-limarp-13b-merged,Text Generation,PyTorch; Transformers,,,,2,,3,26657.08419,,https://huggingface.co/Oniichat/hermes-limarp-13b-merged,No model card; New: Create and edit this model card directly on the website!
Chinese-Llama-2-7b-ggml-model-q4_0,,,,apache-2.0,,2,,0,3921.921511,,https://huggingface.co/rffx0/Chinese-Llama-2-7b-ggml-model-q4_0,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Llama-2-22B-GGML,Text Generation,Transformers,,,,2,togethercomputer/RedPajama-Data-1T-Sample,0,50606.08312,,https://huggingface.co/IHaveNoClueAndIMustPost/Llama-2-22B-GGML,"This is Llama2-22b in a couple of GGML formats. I have no idea what I'm doing so if something doesn't work as it should or not at all that's likely on me, not the models themselves.
While I haven't had any issues so far do note that the original repo states ""Not intended for use as-is - this model is meant to serve as a base for further tuning"".; Approximate VRAM requirements at 4K context:"
Llama2-Chinese-7b-Chat-LoRA,Question Answering,Transformers,Chinese; English,apache-2.0,,2,,0,40.49278427,,https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat-LoRA,由于Llama2本身的中文对齐较弱，我们采用中文指令集，对meta-llama/Llama-2-7b-chat-hf进行LoRA微调，使其具备较强的中文对话能力。; ? 该版本仅包含LoRA中文微调参数，需要与基础的meta-llama/Llama-2-7b-chat-hf模型结合使用; Github：Llama2-Chinese; 在线体验链接：llama.family; 欢迎来到Llama2中文社区！
Llama2-Chinese-7b-Chat,Question Answering,PyTorch; Transformers,Chinese; English,apache-2.0,,2,,0,13804.03936,,https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat,由于Llama2本身的中文对齐较弱，我们采用中文指令集，对meta-llama/Llama-2-7b-chat-hf进行LoRA微调，使其具备较强的中文对话能力。; ? 该版本为LoRA中文微调参数FlagAlpha/Llama2-Chinese-7b-Chat-LoRA和meta-llama/Llama-2-7b-chat-hf参数结合后的版本，可直接使用; Github：Llama2-Chinese; 在线体验链接：llama.family; 欢迎来到Llama2中文社区！
openllama_3b_EvolInstruct_lora_merged-4bit-32g,Text Generation,PyTorch; Transformers,,cc-by-4.0,,2,,0,2335.249771,,https://huggingface.co/KnutJaegersberg/openllama_3b_EvolInstruct_lora_merged-4bit-32g,Prompt: "Below is an instruction that describes a task. Write a response that appropriately completes the request. \n\n### Instruction:\n INSTRUCTION. \n### Response:\n"
TWingshadow_v1.2,Text-to-Image,Diffusers,,other,,2,,0,4198.403507,,https://huggingface.co/digiplay/TWingshadow_v1.2,"Models info :; https://civitai.com/models/105935; Original Author's DEMO images :; 

"
bert-base-multilingual-uncased,Fill-Mask,PyTorch; TensorFlow; JAX; Safetensors; Transformers,102 languages,apache-2.0,https://arxiv.org/pdf/1810.04805.pdf,43,wikipedia,"326,832",3015.58133,34,https://huggingface.co/bert-base-multilingual-uncased,"Pretrained model on the top 102 languages with the largest Wikipedia using a masked language modeling (MLM) objective.
It was introduced in this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion. This means
it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the languages in the training set that can then be used to
extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a
standard classifier using the features produced by the BERT model as inputs.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the model hub to look for
fine-tuned versions on a task that interests you."
bert-large-cased,Fill-Mask,PyTorch; TensorFlow; JAX; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/1810.04805.pdf,11,bookcorpus; wikipedia,"99,088",6871.684006,35,https://huggingface.co/bert-large-cased,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is cased: it makes a difference
between english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; This model has the following configuration:"
bert-large-uncased-whole-word-masking-finetuned-squad,Question Answering,PyTorch; TensorFlow; JAX; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/1810.04805.pdf,92,bookcorpus; wikipedia,"163,700",6769.328652,48,https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; Differently to other BERT models, this model was trained with a new technique: Whole Word Masking. In this case, all of the tokens corresponding to a word are masked at once. The overall masking rate remains the same.; The training is identical -- each masked WordPiece token is predicted independently. ; After pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts. See below for more information regarding this fine-tuning.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team."
distilbert-base-multilingual-cased,Fill-Mask,PyTorch; TensorFlow; ONNX; Safetensors; Transformers,104 languages,apache-2.0,https://arxiv.org/pdf/1910.01108.pdf; https://arxiv.org/pdf/1910.09700.pdf,64,wikipedia,"4,652,765",2906.940657,6,https://huggingface.co/distilbert-base-multilingual-cased,"This model is a distilled version of the BERT base multilingual model. The code for the distillation process can be found here. This model is cased: it does make a difference between english and English.; The model is trained on the concatenation of Wikipedia in 104 different languages listed here.
The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base).
On average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base.; We encourage potential users of this model to check out the BERT base multilingual model card to learn more about usage, limitations and potential biases.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.; Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2."
distilroberta-base,Fill-Mask,PyTorch; TensorFlow; JAX; Rust; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/1910.01108.pdf; https://arxiv.org/pdf/1910.09700.pdf,76,openwebtext,"7,409,128",1966.280316,29,https://huggingface.co/distilroberta-base,"This model is a distilled version of the RoBERTa-base model. It follows the same training procedure as DistilBERT.
The code for the distillation process can be found here.
This model is case-sensitive: it makes a difference between english and English.; The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base).
On average DistilRoBERTa is twice as fast as Roberta-base.; We encourage users of this model card to check out the RoBERTa-base model card to learn more about usage, limitations and potential biases.; You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.; Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2."
gpt2-medium,Text Generation,PyTorch; TensorFlow; JAX; Rust; ONNX; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/1910.09700.pdf,64,,"223,491",7795.498318,385,https://huggingface.co/gpt2-medium,"Model Description: GPT-2 Medium is the 355M parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective. ; Use the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:; Here is how to use this model to get the features of a given text in PyTorch:; and in TensorFlow:; In their model card about GPT-2, OpenAI wrote: "
roberta-large,Fill-Mask,PyTorch; TensorFlow; JAX; ONNX; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/1907.11692.pdf; https://arxiv.org/pdf/1806.02847.pdf,116,bookcorpus; wikipedia,"2,506,730",7713.413208,62,https://huggingface.co/roberta-large,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is case-sensitive: it
makes a difference between english and English.; Disclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by
the Hugging Face team.; RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means
it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. ; More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model
randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict
the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one
after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to
learn a bidirectional representation of the sentence.; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs."
t5-11b,Translation,PyTorch; TensorFlow; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/1805.12471.pdf; https://arxiv.org/pdf/1708.00055.pdf; https://arxiv.org/pdf/1704.05426.pdf; https://arxiv.org/pdf/1606.05250.pdf; https://arxiv.org/pdf/1808.09121.pdf; https://arxiv.org/pdf/1810.12885.pdf; https://arxiv.org/pdf/1905.10044.pdf; https://arxiv.org/pdf/1910.09700.pdf,37,c4,"31,672",92571.77334,101,https://huggingface.co/t5-11b,"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-11B is the checkpoint with 11 billion parameters. ; The developers write in a blog post that the model: "
t5-3b,Translation,PyTorch; TensorFlow; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/1805.12471.pdf; https://arxiv.org/pdf/1708.00055.pdf; https://arxiv.org/pdf/1704.05426.pdf; https://arxiv.org/pdf/1606.05250.pdf; https://arxiv.org/pdf/1808.09121.pdf; https://arxiv.org/pdf/1810.12885.pdf; https://arxiv.org/pdf/1905.10044.pdf; https://arxiv.org/pdf/1910.09700.pdf,18,c4,"116,336",23349.37262,102,https://huggingface.co/t5-3b,"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-3B is the checkpoint with 3 billion parameters. ; The developers write in a blog post that the model: "
xlm-roberta-base,Fill-Mask,PyTorch; TensorFlow; JAX; ONNX; Safetensors; Transformers,94 languages,mit,https://arxiv.org/pdf/1911.02116.pdf,339,,"13,823,623",6506.336128,43,https://huggingface.co/xlm-roberta-base,"XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al. and first released in this repository. ; Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.; XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. ; RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.; More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence."
MSTSb_stsb-xlm-r-multilingual,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,,,,1,,44,1150.796429,,https://huggingface.co/AIDA-UPM/MSTSb_stsb-xlm-r-multilingual,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net"
ICD-10-Code-Prediction,Text Classification,PyTorch; Transformers,,apache-2.0,,15,,"1,299",482.9372956,5,https://huggingface.co/AkshatSurolia/ICD-10-Code-Prediction,The Publicly Available Clinical BERT Embeddings paper contains four unique clinicalBERT models: initialized with BERT-Base (cased_L-12_H-768_A-12) or BioBERT (BioBERT-Base v1.0 + PubMed 200K + PMC 270K) & trained on either all MIMIC notes or only discharge summaries.  ; Load the model via the transformers library:; Run the model with clinical diagonosis text:; Return the Top-5 predicted ICD-10 codes:
gpt2-spanish-classics,Text Generation,PyTorch; Transformers,,,,1,,22,510.0020173,,https://huggingface.co/Aleksandar1932/gpt2-spanish-classics,No model card; New: Create and edit this model card directly on the website!
qanlu,Question Answering,PyTorch; Transformers,English,cc-by-4.0,,6,atis,524,497.3317019,2,https://huggingface.co/AmazonScience/qanlu,"Question Answering NLU (QANLU) is an approach that maps the NLU task into question answering, 
leveraging pre-trained question-answering models to perform well on few-shot settings. Instead of 
training an intent classifier or a slot tagger, for example, we can ask the model intent- and 
slot-related questions in natural language: ; Note the ""Yes. No. "" prepended in the context. Those are to allow the model to answer intent-related questions (e.g. ""Is the user looking for a restaurant?"").; Thus, by asking questions for each intent and slot in natural language, we can effectively construct an NLU hypothesis. For more details, please read the paper: Language model is all you need: Natural language understanding as question answering.; Instructions for how to train and evaluate a QANLU model, as well as the necessary code for ATIS are in the Amazon Science repository.; This model has been fine-tuned on ATIS (English) and is intended to demonstrate the power of this approach. For other domains or tasks, it should be further fine-tuned 
on relevant data."
indian-foods,Image Classification,PyTorch; TensorBoard; Transformers,,,,3,,32,343.0024529,1,https://huggingface.co/Amrrs/indian-foods,Autogenerated by HuggingPics???; Create your own image classifier for anything by running the demo on Google Colab.; Report any issues with the demo at the github repo.; ; 
rebel-large,Text2Text Generation,PyTorch; Safetensors; Transformers,English,cc-by-nc-sa-4.0,,76,Babelscape/rebel-dataset,"33,784",3340.837838,16,https://huggingface.co/Babelscape/rebel-large,"



; This is the model card for the Findings of EMNLP 2021 paper REBEL: Relation Extraction By End-to-end Language generation. We present a new linearization approach and a reframing of Relation Extraction as a seq2seq task. The paper can be found here. If you use the code, please reference this work in your paper:; The original repository for the paper can be found here; Be aware that the inference widget at the right does not output special tokens, which are necessary to distinguish the subject, object and relation types. For a demo of REBEL and its pre-training dataset check the Spaces demo."
query-gen-msmarco-t5-base-v1,Text2Text Generation,PyTorch; JAX; Transformers,,,,12,,"1,104",1784.780507,,https://huggingface.co/BeIR/query-gen-msmarco-t5-base-v1,"This model is the t5-base model from docTTTTTquery.; The T5-base model was trained on the MS MARCO Passage Dataset, which consists of about 500k real search queries from Bing together with the relevant passage.; The model can be used for query generation to learn semantic search models without requiring annotated training data: Synthetic Query Generation."
bert-base-arabic-camelbert-da-sentiment,Text Classification,PyTorch; TensorFlow; Transformers,Arabic,apache-2.0,https://arxiv.org/pdf/2103.06678.pdf,17,,"655,212",873.3046334,2,https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment,"CAMeLBERT-DA SA Model is a Sentiment Analysis (SA) model that was built by fine-tuning the CAMeLBERT Dialectal Arabic (DA) model.
For the fine-tuning, we used the ASTD, ArSAS, and SemEval datasets.
Our fine-tuning procedure and the hyperparameters we used can be found in our paper *""The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models.""; You can use the CAMeLBERT-DA SA model directly as part of our CAMeL Tools SA component (recommended) or as part of the transformers pipeline.; To use the model with the CAMeL Tools SA component:; You can also use the SA model directly with a transformers pipeline:; Note: to download our models, you would need transformers>=3.5.0.
Otherwise, you could download the models manually."
bert-base-arabic-camelbert-da,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,Arabic,apache-2.0,https://arxiv.org/pdf/2103.06678.pdf,12,,293,1312.30954,,https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-da,"CAMeLBERT is a collection of BERT models pre-trained on Arabic texts with different sizes and variants.
We release pre-trained language models for Modern Standard Arabic (MSA), dialectal Arabic (DA), and classical Arabic (CA), in addition to a model pre-trained on a mix of the three.
We also provide additional models that are pre-trained on a scaled-down set of the MSA variant (half, quarter, eighth, and sixteenth).
The details are described in the paper ""The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models.""; This model card describes CAMeLBERT-DA (bert-base-arabic-camelbert-da), a model pre-trained on the DA (dialectal Arabic) dataset.; You can use the released model for either masked language modeling or next sentence prediction.
However, it is mostly intended to be fine-tuned on an NLP task, such as NER, POS tagging, sentiment analysis, dialect identification, and poetry classification.
We release our fine-tuninig code here.; You can use this model directly with a pipeline for masked language modeling:; Note: to download our models, you would need transformers>=3.5.0. Otherwise, you could download the models manually."
bert-base-arabic-camelbert-mix-ner,Token Classification,PyTorch; TensorFlow; Transformers,Arabic,apache-2.0,https://arxiv.org/pdf/2103.06678.pdf,4,,"2,067",870.3053422,,https://huggingface.co/CAMeL-Lab/bert-base-arabic-camelbert-mix-ner,"CAMeLBERT-Mix NER Model is a Named Entity Recognition (NER) model that was built by fine-tuning the CAMeLBERT Mix model.
For the fine-tuning, we used the ANERcorp dataset.
Our fine-tuning procedure and the hyperparameters we used can be found in our paper ""The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models.
"" Our fine-tuning code can be found here.; You can use the CAMeLBERT-Mix NER model directly as part of our CAMeL Tools NER component (recommended) or as part of the transformers pipeline.; To use the model with the CAMeL Tools NER component:; You can also use the NER model directly with a transformers pipeline:; Note: to download our models, you would need transformers>=3.5.0.
Otherwise, you could download the models manually."
distilbert-base-multilingual-cased-ner-hrl,Token Classification,PyTorch; TensorFlow; Safetensors; Transformers,,,,51,,"431,856",1617.979142,13,https://huggingface.co/Davlan/distilbert-base-multilingual-cased-ner-hrl,"language: ; distilbert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned  Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER). 
Specifically, this model is a distilbert-base-multilingual-cased model that was fine-tuned on an aggregation of 10 high-resourced languages; You can use this model with Transformers pipeline for NER.; This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains.  ; The training data for the 10 languages are from: "
gpt2-spanish,Text Generation,PyTorch; TensorFlow; JAX; Transformers,Spanish,mit,,23,ebooks,889,1258.311494,2,https://huggingface.co/DeepESP/gpt2-spanish,"GPT2-Spanish is a language generation model trained from scratch with 11.5GB of Spanish texts and with a Byte Pair Encoding (BPE) tokenizer that was trained for this purpose. The parameters used are the same as the small version of the original OpenAI GPT2 model.; This model was trained with a corpus of 11.5GB of texts corresponding to 3.5GB of Wikipedia articles and 8GB of books (narrative, short stories, theater, poetry, essays, and popularization).; The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for Unicode characters) and a vocabulary size of 50257. The inputs are sequences of 1024 consecutive tokens.; This tokenizer was trained from scratch with the Spanish corpus, since it was evidenced that the tokenizer of the English models presented limitations to capture the semantic relations of Spanish, due to the morphosyntactic differences between both languages.; Apart from the special token ""<|endoftext|>"" for text ending in the OpenAI GPT-2 models, the tokens ""<|talk|>"", ""<|ax1|>"", ""<|ax2|>"" (..)""<|ax9|>"" were included so that they can serve as prompts in future training."
bert-myanmar-base-uncased,Fill-Mask,PyTorch; Transformers,,,,1,,15,425.1828845,,https://huggingface.co/GKLMIP/bert-myanmar-base-uncased,"The Usage of tokenizer for Myanmar is same as Laos in https://github.com/GKLMIP/Pretrained-Models-For-Laos.; If you use our model, please consider citing our paper:"
distilbert-base-es-cased,Fill-Mask,PyTorch; Safetensors; Transformers,Spanish,apache-2.0,,1,wikipedia,"5,513",510.1725042,,https://huggingface.co/Geotrend/distilbert-base-es-cased,"We are sharing smaller versions of distilbert-base-multilingual-cased that handle a custom number of languages.; Our versions give exactly the same representations produced by the original model which preserves the original accuracy.; For more information please visit our paper: Load What You Need: Smaller Versions of Multilingual BERT.; To generate other smaller versions of multilingual transformers please visit our Github repo.; Please contact amine@geotrend.fr for any question, feedback or request."
bert-base-dutch-cased,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,Dutch,,https://arxiv.org/pdf/1912.09582.pdf,17,,"25,138",1405.244075,7,https://huggingface.co/GroNLP/bert-base-dutch-cased,"Wietse de Vries ?
Andreas van Cranenburgh ?
Arianna Bisazza ?
Tommaso Caselli ?
Gertjan van Noord ?
Malvina Nissim; BERTje is a Dutch pre-trained BERT model developed at the University of Groningen.; For details, check out our paper on arXiv, the code on Github and related work on Semantic Scholar.; The paper and Github page mention fine-tuned models that are available here.; WARNING: The vocabulary size of BERTje has changed in 2021. If you use an older fine-tuned model and experience problems with the GroNLP/bert-base-dutch-cased tokenizer, use use the following tokenizer:"
DialoGPT-Medium-zerotwo,Conversational,PyTorch; Transformers,,,,1,,55,1477.14844,,https://huggingface.co/HAttORi/DialoGPT-Medium-zerotwo,
opus-mt-ar-en,Translation,PyTorch; TensorFlow; Rust; Transformers,Arabic; English,apache-2.0,,14,,"85,307",1184.811487,3,https://huggingface.co/Helsinki-NLP/opus-mt-ar-en,source languages: ar; target languages: en; OPUS readme: ar-en; dataset: opus; model: transformer-align
opus-mt-de-en,Translation,PyTorch; TensorFlow; Rust; Transformers,German; English,apache-2.0,,17,,"530,755",1134.801654,6,https://huggingface.co/Helsinki-NLP/opus-mt-de-en,source languages: de; target languages: en; OPUS readme: de-en; dataset: opus; model: transformer-align
opus-mt-en-ar,Translation,PyTorch; TensorFlow; Rust; Transformers,English; Arabic,apache-2.0,,14,,"9,336",1184.803072,8,https://huggingface.co/Helsinki-NLP/opus-mt-en-ar,source group: English ; target group: Arabic ; OPUS readme: eng-ara; model: transformer; source language(s): eng
opus-mt-en-es,Translation,PyTorch; TensorFlow; JAX; Transformers,English; Spanish,apache-2.0,,44,,"81,530",938.1853818,69,https://huggingface.co/Helsinki-NLP/opus-mt-en-es,source group: English ; target group: Spanish ; OPUS readme: eng-spa; model: transformer; source language(s): eng
opus-mt-en-ru,Translation,PyTorch; TensorFlow; Rust; Transformers,English; Russian,apache-2.0,,22,,"45,854",1181.46727,9,https://huggingface.co/Helsinki-NLP/opus-mt-en-ru,source languages: en; target languages: ru; OPUS readme: en-ru; dataset: opus; model: transformer-align
opus-mt-fr-en,Translation,PyTorch; TensorFlow; JAX; Transformers,French; English,apache-2.0,,18,,"366,149",903.8862685,17,https://huggingface.co/Helsinki-NLP/opus-mt-fr-en,source languages: fr; target languages: en; OPUS readme: fr-en; dataset: opus; model: transformer-align
opus-mt-mul-en,Translation,PyTorch; TensorFlow; Transformers,120 languages,apache-2.0,,37,,"111,359",623.9126524,7,https://huggingface.co/Helsinki-NLP/opus-mt-mul-en,source group: Multiple languages ; target group: English ; OPUS readme: mul-eng; model: transformer; source language(s): abk acm ady afb afh_Latn afr akl_Latn aln amh ang_Latn apc ara arg arq ary arz asm ast avk_Latn awa aze_Latn bak bam_Latn bel bel_Latn ben bho bod bos_Latn bre brx brx_Latn bul bul_Latn cat ceb ces cha che chr chv cjy_Hans cjy_Hant cmn cmn_Hans cmn_Hant cor cos crh crh_Latn csb_Latn cym dan deu dsb dtp dws_Latn egl ell enm_Latn epo est eus ewe ext fao fij fin fkv_Latn fra frm_Latn frr fry fuc fuv gan gcf_Latn gil gla gle glg glv gom gos got_Goth grc_Grek grn gsw guj hat hau_Latn haw heb hif_Latn hil hin hnj_Latn hoc hoc_Latn hrv hsb hun hye iba ibo ido ido_Latn ike_Latn ile_Latn ilo ina_Latn ind isl ita izh jav jav_Java jbo jbo_Cyrl jbo_Latn jdt_Cyrl jpn kab kal kan kat kaz_Cyrl kaz_Latn kek_Latn kha khm khm_Latn kin kir_Cyrl kjh kpv krl ksh kum kur_Arab kur_Latn lad lad_Latn lao lat_Latn lav ldn_Latn lfn_Cyrl lfn_Latn lij lin lit liv_Latn lkt lld_Latn lmo ltg ltz lug lzh lzh_Hans mad mah mai mal mar max_Latn mdf mfe mhr mic min mkd mlg mlt mnw moh mon mri mwl mww mya myv nan nau nav nds niu nld nno nob nob_Hebr nog non_Latn nov_Latn npi nya oci ori orv_Cyrl oss ota_Arab ota_Latn pag pan_Guru pap pau pdc pes pes_Latn pes_Thaa pms pnb pol por ppl_Latn prg_Latn pus quc qya qya_Latn rap rif_Latn roh rom ron rue run rus sag sah san_Deva scn sco sgs shs_Latn shy_Latn sin sjn_Latn slv sma sme smo sna snd_Arab som spa sqi srp_Cyrl srp_Latn stq sun swe swg swh tah tam tat tat_Arab tat_Latn tel tet tgk_Cyrl tha tir tlh_Latn tly_Latn tmw_Latn toi_Latn ton tpw_Latn tso tuk tuk_Latn tur tvl tyv tzl tzl_Latn udm uig_Arab uig_Cyrl ukr umb urd uzb_Cyrl uzb_Latn vec vie vie_Hani vol_Latn vro war wln wol wuu xal xho yid yor yue yue_Hans yue_Hant zho zho_Hans zho_Hant zlm_Latn zsm_Latn zul zza
opus-mt-ru-en,Translation,PyTorch; TensorFlow; Rust; Transformers,Russian; English,cc-by-4.0,,28,,"349,619",1181.469262,6,https://huggingface.co/Helsinki-NLP/opus-mt-ru-en,"Model Description:; This model can be used for translation and text-to-text generation.; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).; Further details about the dataset for this model can be found in the OPUS readme: ru-en"
convert_wav2vec2_to_hf,,,,,,5,,0,0.008318062,,https://huggingface.co/HfSpeechUtils/convert_wav2vec2_to_hf,"This repo has two scripts that can show how to convert a fairseq checkpoint to HF Transformers.; It's important to always check in a forward pass that the two checkpoints are the same. The procedure should be as follows:; The ""0"" means that checkpoint is not a fine-tuned one.
4. Verify that models are equal:; Check the scripts to better understand how they work or contact https://huggingface.co/patrickvonplaten; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
camembert-ner-with-dates,Token Classification,PyTorch; ONNX; Safetensors; Transformers,French,mit,,23,Jean-Baptiste/wikiner_fr,"17,733",1320.797707,12,https://huggingface.co/Jean-Baptiste/camembert-ner-with-dates,"[camembert-ner-with-dates] is an extension of french camembert-ner model with an additionnal tag for dates.
Model was trained on enriched version of wikiner-fr dataset (~170 634  sentences).; On my test data (mix of chat and email), this model got an f1 score of ~83% (in comparison dateparser was ~70%).
Dateparser library can still be be used on the output of this model in order to convert text to python datetime object 
(https://dateparser.readthedocs.io/en/latest/).; Global; By entity"
camembert-ner,Token Classification,PyTorch; ONNX; Safetensors; Transformers,French,mit,,79,Jean-Baptiste/wikiner_fr,"926,322",1321.106688,67,https://huggingface.co/Jean-Baptiste/camembert-ner,"[camembert-ner] is a NER model that was fine-tuned from camemBERT on wikiner-fr dataset.
Model was trained on wikiner-fr dataset (~170 634  sentences).
Model was validated on emails/chat data and overperformed other models on this type of data specifically. 
In particular the model seems to work better on entity that don't start with an upper case.; Training data was classified as follow:; Overall; By entity; For those who could be interested, here is a short article on how I used the results of this model to train a LSTM model for signature detection in emails:
https://medium.com/@jean-baptiste.polle/lstm-model-for-email-signature-detection-8e990384fefa"
sentence-bert-swedish-cased,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,Swedish,apache-2.0,https://arxiv.org/pdf/2004.09813.pdf,10,,255,500.6046146,1,https://huggingface.co/KBLab/sentence-bert-swedish-cased,"This is a sentence-transformers model: It maps Swedish sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. This model is a bilingual Swedish-English model trained according to instructions in the paper Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation and the documentation accompanying its companion python package. We have used the strongest available pretrained English Bi-Encoder (all-mpnet-base-v2) as a teacher model, and the pretrained Swedish KB-BERT as the student model. ; A more detailed description of the model can be found in an article we published on the KBLab blog here and for the updated model here. ; Update: We have released updated versions of the model since the initial release. The original model described in the blog post is v1.0. The current version is v2.0. The newer versions are trained on longer paragraphs, and have a longer max sequence length. v2.0 is trained with a stronger teacher model and is the current default.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:"
t5-darija-summarization,Text2Text Generation,PyTorch; Transformers,Arabic,,,4,,52,3238.441105,,https://huggingface.co/Kamel/t5-darija-summarization,"This dataset contains 19,806 news articles written in Moroccan Arabic dialect along with their titles. The articles were crawled from Goud.ma website between 01/01/2018 and 12/31/2020. 
The articles are written mainly in Moroccan Arabic dialect (Darija) but some of them contain Modern Standard Arabic (MSA) passages. All the titles are written in Darija. 
The following table summarize some tatistics on the MArSum Dataset.; The following figure describes the creation process of MArSum:; ; You may refer to our paper, cited below, for more details on this process.; The dataset is split into Train/Test subsets using a 90/10 split strategy. Both subsets are available for direct donwload."
GPT-J-6B-Shinen,Text Generation,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2101.00027.pdf,13,,"5,858",12393.00497,2,https://huggingface.co/KoboldAI/GPT-J-6B-Shinen,"GPT-J 6B-Shinen is a finetune created using EleutherAI's GPT-J 6B model. Compared to GPT-Neo-2.7-Horni, this model is much heavier on the sexual content.
Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The training data contains user-generated stories from sexstories.com. All stories are tagged using the following way:; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; The core functionality of GPT-J is taking a string of text and predicting the next token. While language models are widely used for tasks other than this, there are a lot of unknowns with this work. When prompting GPT-J it is important to remember that the statistically most likely next token is often not the token that produces the most ""accurate"" text. Never depend upon GPT-J to produce factually accurate output.; GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See Sections 5 and 6 of the Pile paper for a more detailed analysis of the biases in the Pile."
GPT-Neo-2.7B-Shinen,Text Generation,PyTorch; Transformers,English,mit,,17,,"8,686",5500.21656,,https://huggingface.co/KoboldAI/GPT-Neo-2.7B-Shinen,"GPT-Neo 2.7B-Shinen is a finetune created using EleutherAI's GPT-Neo 2.7B model. Compared to GPT-Neo-2.7-Horni, this model is much heavier on the sexual content.; Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The training data contains user-generated stories from sexstories.com. All stories are tagged using the following way:; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; GPT-Neo was trained as an autoregressive language model. This means that its core functionality is taking a string of text and predicting the next token. While language models are widely used for tasks other than this, there are a lot of unknowns with this work.
GPT-Neo-Shinen was trained on a dataset known to contain profanity, lewd, and otherwise abrasive language. GPT-Neo-Shinen WILL produce socially unacceptable text without warning.
GPT-Neo-Shinen will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results."
fairseq-dense-355M,Text Generation,PyTorch; Safetensors; Transformers,English,,https://arxiv.org/pdf/2112.10684.pdf,4,,170,1623.325475,,https://huggingface.co/KoboldAI/fairseq-dense-355M,"This is a Hugging Face transformers-compatible conversion of the original dense 355M-parameter model from the paper ""Efficient Large Scale Language Modeling with Mixtures of Experts"" from Artetxe et al. Please refer to the original model card, which can be found at https://github.com/facebookresearch/fairseq/blob/main/examples/moe_lm/model_card.md."
SGPT-5.8B-weightedmean-nli-bitfit,Sentence Similarity,PyTorch; Sentence Transformers,,,https://arxiv.org/pdf/2202.08904.pdf,5,,582,24067.43896,5,https://huggingface.co/Muennighoff/SGPT-5.8B-weightedmean-nli-bitfit,"For usage instructions, refer to our codebase: https://github.com/Muennighoff/sgpt ; For eval results, refer to our paper: https://arxiv.org/abs/2202.08904; The model was trained with the parameters:; DataLoader:; torch.utils.data.dataloader.DataLoader of length 249592 with parameters:"
hebrew-gpt_neo-xl-poetry,Text Generation,PyTorch; JAX; Safetensors; Transformers,Hebrew,mit,,2,,13,16263.60505,,https://huggingface.co/Norod78/hebrew-gpt_neo-xl-poetry,"Hebrew poetry text generation model which was fine tuned upon on hebrew-gpt_neo-xl.; An assortment of various Hebrew books, magazines and poetry corpuses; Similar to this one ; Available here  "
RoBERTalex,Fill-Mask,PyTorch; Transformers,Spanish,apache-2.0,https://arxiv.org/pdf/1907.11692.pdf; https://arxiv.org/pdf/2110.12201.pdf,9,legal_ES; temu_legal,493,506.3295576,1,https://huggingface.co/PlanTL-GOB-ES/RoBERTalex,"The RoBERTalex is a transformer-based masked language model for the Spanish language. It is based on the RoBERTa base model and has been pre-trained using a large Spanish Legal Domain Corpora, with a total of 8.9GB of text.; The RoBERTalex model is ready-to-use only for masked language modeling to perform the Fill Mask task (try the inference API or read the next section).
However, it is intended to be fine-tuned on non-generative downstream tasks such as Question Answering, Text Classification, or Named Entity Recognition.
You can use the raw model for fill mask or fine-tune it to a downstream task.; Here is how to use this model:; Here is how to use this model to get the features of a given text in PyTorch:; At the time of submission, no measures have been taken to estimate the bias embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated."
roberta-base-biomedical-es,Fill-Mask,PyTorch; Transformers,Spanish,apache-2.0,https://arxiv.org/pdf/2109.03570.pdf; https://arxiv.org/pdf/2109.07765.pdf,3,,99,506.2643966,,https://huggingface.co/PlanTL-GOB-ES/roberta-base-biomedical-es,"Biomedical pretrained language model for Spanish. For more details about the corpus, the pretraining and the evaluation, check the official repository and read our preprint.; The model is ready-to-use only for masked language modelling to perform the Fill Mask task (try the inference API or read the next section). However, it is intended to be fine-tuned on downstream tasks such as Named Entity Recognition or Text Classification.; This model is a RoBERTa-based model trained on a
biomedical corpus in Spanish collected from several sources (see next section). ; The training corpus has been tokenized using a byte version of Byte-Pair Encoding (BPE)
used in the original RoBERTA model with a vocabulary size of 52,000 tokens. The pretraining consists of a masked language model training at the subword level following the approach employed for the RoBERTa base model with the same hyperparameters as in the original work. The training lasted a total of 48 hours with 16 NVIDIA V100 GPUs of 16GB DDRAM, using Adam optimizer with a peak learning rate of 0.0005 and an effective batch size of 2,048 sentences.; The training corpus is composed of several biomedical corpora in Spanish, collected from publicly available corpora and crawlers.
To obtain a high-quality training corpus, a cleaning pipeline with the following operations has been applied:"
roberta-base-ca,Fill-Mask,PyTorch; Transformers,Catalan,apache-2.0,,4,,49,506.2427615,,https://huggingface.co/PlanTL-GOB-ES/roberta-base-ca,"BERTa is a transformer-based masked language model for the Catalan language. 
It is based on the RoBERTA base model 
and has been trained on a medium-size corpus collected from publicly available corpora and crawlers.; This model was originally published as bsc/roberta-base-ca-cased.; The model is ready-to-use only for masked language modelling to perform the Fill Mask task (try the inference API or read the next section). 
However, it is intended to be fine-tuned on non-generative downstream tasks such as Question Answering, Text Classification or Named Entity Recognition.; Below, an example of how to use the masked language modelling task with a pipeline.; The training corpus consists of several corpora gathered from web crawling and public corpora."
roberta-large-bne,Fill-Mask,PyTorch; Transformers,Spanish,apache-2.0,https://arxiv.org/pdf/1907.11692.pdf,11,bne,897,1458.145942,1,https://huggingface.co/PlanTL-GOB-ES/roberta-large-bne,"The roberta-large-bne is a transformer-based masked language model for the Spanish language. It is based on the RoBERTa large model and has been pre-trained using the largest Spanish corpus known to date, with a total of 570GB of clean and deduplicated text processed for this work, compiled from the web crawlings performed by the  National Library of Spain (Biblioteca Nacional de Espa?a) from 2009 to 2019.; The roberta-large-bne model is ready-to-use only for masked language modeling to perform the Fill Mask task (try the inference API or read the next section).
However, it is intended to be fine-tuned on non-generative downstream tasks such as Question Answering, Text Classification, or Named Entity Recognition.
You can use the raw model for fill mask or fine-tune it to a downstream task.; Here is how to use this model:; Here is how to use this model to get the features of a given text in PyTorch:; At the time of submission, no measures have been taken to estimate the bias and toxicity embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated."
distilbert_punctuator_en,Token Classification,PyTorch; Safetensors; Transformers,,,,6,,401,531.6865506,,https://huggingface.co/Qishuai/distilbert_punctuator_en,The model is fine-tuned based on DistilBertForTokenClassification for adding punctuations to plain text (uncased English); Combination of following three dataset:; Validation with 500 samples of dataset scraped from https://www.thenews.com.pk website. Reference; Metrics Report:; Validation with 86 news ted talks of 2020 which are not included in training dataset Reference
FacialEmoRecog,Image Classification,PyTorch; Transformers,English,mit,,13,Jeneral/fer2013,257,343.0016212,1,https://huggingface.co/Rajaram1996/FacialEmoRecog,Create your own image classifier for anything by running this repo  
prot_bert_bfd,Fill-Mask,PyTorch; TensorFlow; Transformers,protein,,,9,BFD,"27,863",3614.728674,1,https://huggingface.co/Rostlab/prot_bert_bfd,"Pretrained model on protein sequences using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is trained on uppercase amino acids: it only works with capital letter amino acids.; ProtBert-BFD is based on Bert model which pretrained on a large corpus of protein sequences in a self-supervised fashion.
This means it was pretrained on the raw protein sequences only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those protein sequences.; One important difference between our Bert model and the original Bert version is the way of dealing with sequences as separate documents
This means the Next sentence prediction is not used, as each sequence is treated as a complete document.
The masking follows the original Bert training with randomly masks 15% of the amino acids in the input. ; At the end, the feature extracted from this model revealed that the LM-embeddings from unlabeled data (only protein sequences) captured important biophysical properties governing protein
shape.
This implied learning some of the grammar of the language of life realized in protein sequences.; The model could be used for protein feature extraction or to be fine-tuned on downstream tasks.
We have noticed in some tasks you could gain more accuracy by fine-tuning the model rather than using it as a feature extractor."
distilbert-base-nepali,Fill-Mask,PyTorch; Safetensors; Transformers,,apache-2.0,https://arxiv.org/pdf/1911.02116.pdf; https://arxiv.org/pdf/1910.01108.pdf,4,Sakonii/nepalitext-language-model-dataset,82,537.8030725,,https://huggingface.co/Sakonii/distilbert-base-nepali,"This model is pre-trained on nepalitext dataset consisting of over 13 million Nepali text sequences using a masked language modeling (MLM) objective. Our approach trains a Sentence Piece Model (SPM) for text tokenization similar to XLM-ROBERTa and trains distilbert model for language modeling. Find more details in this paper.; It achieves the following results on the evaluation set:; Refer to original distilbert-base-uncased; This backbone model intends to be fine-tuned on Nepali language focused downstream task such as sequence classification, token classification or question answering. 
The language model being trained on a data with texts grouped to a block size of 512, it handles text sequence up to 512 tokens and may not perform satisfactorily on shorter sequences.; This model can be used directly with a pipeline for masked language modeling:"
recreate-history,Token Classification,PyTorch; Transformers,Bengali,apache-2.0,,1,xtreme,44,69.32442211,,https://huggingface.co/SaulLu/recreate-history,sahajBERT fine-tuned for NER using the bengali split of WikiANN . ; Named Entities predicted by the model:; You can use this model directly with a pipeline for masked language modeling:; WIP; The model was initialized it with pre-trained weights of sahajBERT at step 19519 and trained on the bengali of WikiANN 
bart-base-detox,Text2Text Generation,PyTorch; Safetensors; Transformers,English,,,3,,817,1118.590837,,https://huggingface.co/s-nlp/bart-base-detox,"Model Overview; This is the model presented in the paper ""ParaDetox: Detoxification with Parallel Data"". ; The model itself is BART (base) model trained on parallel detoxification dataset ParaDetox achiving SOTA results for detoxification task. More details, code and data can be found here.; How to use; Citation"
roberta-base-formality-ranker,Text Classification,PyTorch; Safetensors; Transformers,English,,,4,GYAFC; Pavlick-Tetreault-2016,"1,640",1000.588528,,https://huggingface.co/s-nlp/roberta-base-formality-ranker,"The model has been trained to predict for English sentences, whether they are formal or informal. ; Base model: roberta-base; Datasets: GYAFC from Rao and Tetreault, 2018 and online formality corpus from Pavlick and Tetreault, 2016.; Data augmentation: changing texts to upper or lower case; removing all punctuation, adding dot at the end of a sentence. It was applied because otherwise the model is over-reliant on punctuation and capitalization and does not pay enough attention to other features.; Loss: binary classification (on GYAFC), in-batch ranking (on PT data)."
AraT5-base,,PyTorch; TensorFlow; Transformers,Arabic,,,13,,"1,580",2316.869212,,https://huggingface.co/UBC-NLP/AraT5-base,"This is the repository accompanying our paper AraT5: Text-to-Text Transformers for Arabic Language Understanding and Generation. In this is the repository we Introduce AraT5MSA, AraT5Tweet, and AraT5: three powerful Arabic-specific text-to-text Transformer based models;; Below is an example for fine-tuning AraT5-base for News Title Generation on the Aranews dataset ; For more details about the fine-tuning example, please read this notebook  ; In addition, we release the fine-tuned checkpoint of the News Title Generation (NGT) which is described in the paper. The model available at Huggingface (UBC-NLP/AraT5-base-title-generation).; For more details, please visit our own GitHub."
Czert-B-base-cased-long-zero-shot,Feature Extraction,PyTorch; Transformers,,,https://arxiv.org/pdf/2103.13031.pdf,2,,45,534.2213491,,https://huggingface.co/UWB-AIR/Czert-B-base-cased-long-zero-shot,"This repository keeps trained Czert-B-base-cased-long-zero-shot model for the paper Czert C Czech BERT-like Model for Language Representation

For more information, see the paper; This is long version of Czert-B-base-cased created without any finetunning on long documents. Positional embedings were created by simply repeating the positional embeddings of the original Czert-B model. For tokenization, please use BertTokenizer. Cannot be used with AutoTokenizer. ; You can download MLM & NSP only pretrained models
CZERT-A-v1
CZERT-B-v1; After some additional experiments, we found out that the tokenizers config was exported wrongly. In Czert-B-v1, the tokenizer parameter ""do_lower_case""  was wrongly set to true. In Czert-A-v1 the parameter ""strip_accents""  was incorrectly set to true. ; Both mistakes are repaired in v2.
CZERT-A-v2
CZERT-B-v2"
FinancialBERT-Sentiment-Analysis,Text Classification,PyTorch; Transformers,English,,,28,financial_phrasebank,"7,198",439.6795608,4,https://huggingface.co/ahmedrachid/FinancialBERT-Sentiment-Analysis,"FinancialBERT is a BERT model pre-trained on a large corpora of financial texts. The purpose is to enhance financial NLP research and practice in financial domain, hoping that financial practitioners and researchers can benefit from this model without the necessity of the significant computational resources required to train the model. ; The model was fine-tuned for Sentiment Analysis task on Financial PhraseBank dataset. Experiments show that this model outperforms the general BERT and other financial domain-specific models.; More details on FinancialBERT's pre-training process can be found at: https://www.researchgate.net/publication/358284785_FinancialBERT_-_A_Pretrained_Language_Model_for_Financial_Text_Mining; FinancialBERT model was fine-tuned on Financial PhraseBank, a dataset consisting of 4840 Financial News categorised by sentiment (negative, neutral, positive).; The evaluation metrics used are: Precision, Recall and F1-score. The following is the classification report on the test set."
frame-interpolation-film-style,,Keras,,,,9,,186,2.858017578,53,https://huggingface.co/akhaliq/frame-interpolation-film-style,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
led-base-16384,Text2Text Generation,PyTorch; TensorFlow; Transformers,English,apache-2.0,https://arxiv.org/pdf/2004.05150.pdf,31,,"10,785",1297.32638,4,https://huggingface.co/allenai/led-base-16384,"Allenai's Longformer Encoder-Decoder (LED).; As described in Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan, led-base-16384 was initialized from bart-base since both models share the exact same architecture. To be able to process 16K tokens, bart-base's position embedding matrix was simply copied 16 times.; This model is especially interesting for long-range summarization and question answering.; This notebook shows how led-base-16384 can effectively be fine-tuned on a downstream task."
specter,Feature Extraction,PyTorch; TensorFlow; JAX; Transformers,English,apache-2.0,https://arxiv.org/pdf/2004.07180.pdf,51,SciDocs,"18,493",1320.219289,2,https://huggingface.co/allenai/specter,"SPECTER is a pre-trained language model to generate document-level embedding of documents. It is pre-trained on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. ; If you're coming here because you want to embed papers, SPECTER has now been superceded by SPECTER 2.0. Use that instead.; Paper: SPECTER: Document-level Representation Learning using Citation-informed Transformers; Original Repo: Github; Evaluation Benchmark: SciDocs"
bort,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,,,https://arxiv.org/pdf/2010.10499.pdf,10,,632,713.3277352,2,https://huggingface.co/amazon/bort,"?? Disclaimer ?? ; This model is community-contributed, and not supported by Amazon, Inc.; Amazon's BORT; BORT is a highly compressed version of bert-large that is up to 10 times faster at inference. 
The model is an optimal sub-architecture of bert-large that was found using neural architecture search.; Paper"
bert-multilingual-passage-reranking-msmarco,Text Classification,PyTorch; TensorFlow; JAX; Transformers,102 languages,apache-2.0,https://arxiv.org/pdf/1901.04085.pdf,18,msmarco,"6,023",2008.860656,2,https://huggingface.co/amberoad/bert-multilingual-passage-reranking-msmarco,"Input: Supports over 100 Languages. See List of supported languages for all available.; Purpose: This module takes a search query [1] and a passage [2] and calculates if the passage matches the query. 
It can be used as an improvement for Elasticsearch Results and boosts the relevancy by up to 100%. ; Architecture: On top of BERT there is a Densly Connected NN which takes the 768 Dimensional [CLS] Token as input and provides the output (Arxiv).; Output: Just a single value between between -10 and 10. Better matching query,passage pairs tend to have a higher a score.; Both query[1] and passage[2] have to fit in 512 Tokens.
As you normally want to rerank the first dozens of search results keep in mind the inference time of approximately 300 ms/query."
roberta-base-ner-conll2003,Token Classification,PyTorch; Transformers,,mit,,1,conll2003,16,498.5909025,,https://huggingface.co/andi611/roberta-base-ner-conll2003,"This model is a fine-tuned version of roberta-base on the conll2003 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
bert-for-patents,Fill-Mask,PyTorch; TensorFlow; Safetensors; Transformers,English,apache-2.0,,43,,"6,992",4403.524335,3,https://huggingface.co/anferico/bert-for-patents,"BERT for Patents is a model trained by Google on 100M+ patents (not just US patents). It is based on BERTLARGE.; If you want to learn more about the model, check out the blog post, white paper and GitHub page containing the original TensorFlow checkpoint."
german-gpt2,Text Generation,PyTorch; TensorFlow; JAX; Transformers,German,mit,,1,,802,1682.450523,5,https://huggingface.co/anonymous-german-nlp/german-gpt2,Note: This model was de-anonymized and now lives at:; https://huggingface.co/dbmdz/german-gpt2; Please use the new model name instead!
wav2vec2-xls-r-300m-bengali,Automatic Speech Recognition,PyTorch; Transformers,Bengali,apache-2.0,,2,openslr; SLR53; AI4Bharat/IndicCorp,645,1290.289093,,https://huggingface.co/arijitx/wav2vec2-xls-r-300m-bengali,"This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the OPENSLR_SLR53 - bengali dataset.
It achieves the following results on the evaluation set. ; Without language model : ; With 5 gram language model trained on 30M sentences randomly chosen from AI4Bharat IndicCorp dataset : ; Note : 5% of a total 10935 samples have been used for evaluation. Evaluation set has 10935 examples which was not part of training training was done on first 95% and eval was done on last 5%. Training was stopped after 180k steps. Output predictions are available under files section.; The following hyperparameters were used during training:"
xlm-roberta-base-uncased-all-english,Token Classification,PyTorch; Transformers,,,,1,,23,1141.731294,,https://huggingface.co/tner/xlm-roberta-base-uncased-all-english,XLM-RoBERTa finetuned on NER. Check more detail at TNER repository.
KcELECTRA-base,,PyTorch; Transformers,Korean; English,mit,,14,,"3,370",437.7936505,2,https://huggingface.co/beomi/KcELECTRA-base,"** Updates on 2022.10.08 **; ??? ??? Transformer ?? ???? ??? ??? ??, ?? ??, ? ? ? ??? ???? ???? ??? ?????. ??, ??? NSMC? ?? User-Generated Noisy text domain ????? ???? ??? ??? ??? ???? ???, ??? ? ???? ????? ???? ?? ???? ???? ?????.; KcELECTRA? ?? ?? ??? ????? ???? ??, ??? ???? ??? ???? ???, ?????? ELECTRA??? ???? ??? Pretrained ELECTRA ?????.; ?? KcBERT ?? ???? ?? ? vocab ??? ?? ??? ???? ??? ???????.; KcELECTRA? Huggingface? Transformers ?????? ?? ??? ??? ??? ? ????. (??? ?? ????? ???? ????.)"
bert-base-go-emotion,Text Classification,PyTorch; Transformers,English,apache-2.0,,26,go_emotions,"97,708",438.6861209,12,https://huggingface.co/bhadresh-savani/bert-base-go-emotion,Notebook
bluebert_pubmed_uncased_L-12_H-768_A-12,,PyTorch; Transformers,English,cc0-1.0,,4,pubmed,"1,506",1797.918327,,https://huggingface.co/bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12,"A BERT model pre-trained on PubMed abstracts; Please see https://github.com/ncbi-nlp/bluebert; We provide preprocessed PubMed texts that were used to pre-train the BlueBERT models. 
The corpus contains ~4000M words extracted from the PubMed ASCII code version. ; Pre-trained model: https://huggingface.co/bert-base-uncased; Below is a code snippet for more details."
Ko-DialoGPT,Conversational,PyTorch; Transformers,Korean,cc-by-nc-sa-4.0,,7,,381,514.5238339,,https://huggingface.co/byeongal/Ko-DialoGPT,
SapBERT-UMLS-2020AB-all-lang-from-XLMR,Feature Extraction,PyTorch; Safetensors; Transformers,,,https://arxiv.org/pdf/2010.11784.pdf,1,,844,2278.353948,,https://huggingface.co/cambridgeltl/SapBERT-UMLS-2020AB-all-lang-from-XLMR,"language: multilingual; tags:; datasets:; [news] A cross-lingual extension of SapBERT will appear in the main onference of ACL 2021! 
[news] SapBERT will appear in the conference proceedings of NAACL 2021!; SapBERT (Liu et al. 2020) trained with UMLS 2020AB, using xlm-roberta-base as the base model. Please use [CLS] as the representation of the input."
twitter-roberta-base-irony,Text Classification,PyTorch; TensorFlow; JAX; Transformers,,,https://arxiv.org/pdf/2010.12421.pdf,9,,"4,142,685",1500.326774,,https://huggingface.co/cardiffnlp/twitter-roberta-base-irony,This is a roBERTa-base model trained on ~58M tweets and finetuned for irony detection with the TweetEval benchmark.; Output: 
monot5-base-msmarco-10k,Text2Text Generation,PyTorch; JAX; Transformers,,,,10,,"15,420",1784.779449,,https://huggingface.co/castorini/monot5-base-msmarco-10k,"This model is a T5-base reranker fine-tuned on the MS MARCO passage dataset for 10k steps (or 1 epoch).; This model usually has a better zero-shot performance than monot5-base-msmarco, i.e., it performs better on datasets different from MS MARCO.; For more details on how to use it, check the following links:; Paper describing the model: Document Ranking with a Pretrained Sequence-to-Sequence Model"
bert-base-chinese-ner,Token Classification,PyTorch; JAX; Transformers,Chinese,gpl-3.0,,44,,"182,342",814.1130421,1,https://huggingface.co/ckiplab/bert-base-chinese-ner,"This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).; @０柑峁┝朔斌w中文的 transformers 模型（包含 ALBERT、BERT、GPT2）及自然Z言理工具（包含嘣~、~性擞、w辨R）。; Please use BertTokenizerFast as tokenizer instead of AutoTokenizer.; 使用 BertTokenizerFast 而非 AutoTokenizer。; For full usage and more information, please refer to https://github.com/ckiplab/ckip-transformers."
distilroberta-base-climate-detector,Text Classification,PyTorch; Safetensors; Transformers,English,apache-2.0,,4,climatebert/climate_detection,"36,592",660.6144625,,https://huggingface.co/climatebert/distilroberta-base-climate-detector,"This is the fine-tuned ClimateBERT language model with a classification head for detecting climate-related paragraphs.; Using the climatebert/distilroberta-base-climate-f language model as starting point, the distilroberta-base-climate-detector model is fine-tuned on our climatebert/climate_detection dataset.; Note: This model is trained on paragraphs. It may not perform well on sentences.; You can use the model with a pipeline for text classification:"
rut5-base-paraphraser,Text2Text Generation,PyTorch; Safetensors; Transformers,Russian,mit,,11,cointegrated/ru-paraphrase-NMT-Leipzig,743,1954.811596,,https://huggingface.co/cointegrated/rut5-base-paraphraser,This is a paraphraser for Russian sentences described in this Habr post. ; It is recommended to use the model with the encoder_no_repeat_ngram_size argument:
ms-marco-MiniLM-L-12-v2,Text Classification,PyTorch; JAX; Transformers,,apache-2.0,,25,,"494,680",267.2315812,12,https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2,"This model was trained on the MS Marco Passage Ranking task.; The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco; The usage becomes easier when you have SentenceTransformers installed. Then, you can use the pre-trained models like this:; In the following table, we provide various pre-trained Cross-Encoders together with their performance on the TREC Deep Learning 2019 and the MS Marco Passage Reranking dataset. ;  Note: Runtime was computed on a V100 GPU."
ms-marco-MiniLM-L-2-v2,Text Classification,PyTorch; JAX; Transformers,,apache-2.0,,1,,"10,494",125.2315841,,https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-2-v2,"This model was trained on the MS Marco Passage Ranking task.; The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco; The usage becomes easier when you have SentenceTransformers installed. Then, you can use the pre-trained models like this:; In the following table, we provide various pre-trained Cross-Encoders together with their performance on the TREC Deep Learning 2019 and the MS Marco Passage Reranking dataset. ;  Note: Runtime was computed on a V100 GPU."
ms-marco-MiniLM-L-6-v2,Text Classification,PyTorch; JAX; Transformers,,apache-2.0,,19,,"215,288",182.0315841,52,https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2,"This model was trained on the MS Marco Passage Ranking task.; The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco; The usage becomes easier when you have SentenceTransformers installed. Then, you can use the pre-trained models like this:; In the following table, we provide various pre-trained Cross-Encoders together with their performance on the TREC Deep Learning 2019 and the MS Marco Passage Reranking dataset. ;  Note: Runtime was computed on a V100 GPU."
nli-deberta-v3-large,Zero-Shot Classification,PyTorch; Transformers,English,apache-2.0,,11,multi_nli; snli,"4,823",1784.226114,,https://huggingface.co/cross-encoder/nli-deberta-v3-large,"This model was trained using SentenceTransformers Cross-Encoder class. This model is based on microsoft/deberta-v3-large; The model was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.; For futher evaluation results, see SBERT.net - Pretrained Cross-Encoder.; Pre-trained models can be used like this:; You can use the model also directly with Transformers library (without SentenceTransformers library):"
bias-detection-model,Text Classification,TensorFlow; Transformers,English,,,17,,"2,928",268.2312017,1,https://huggingface.co/d4data/bias-detection-model,"An English sequence classification model, trained on MBAD Dataset to detect bias and fairness in sentences (news articles). This model was built on top of distilbert-base-uncased model and trained for 30 epochs with a batch size of 16, a learning rate of 5e-5, and a maximum sequence length of 512.; The easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.; This model is part of the Research topic ""Bias and Fairness in AI"" conducted by Deepak John Reji, Shaina Raza. If you use this work (code, model or dataset), please star at:; Bias & Fairness in AI, (2022), GitHub repository, https://github.com/dreji18/Fairness-in-AI"
dalle-mini,Text-to-Image,JAX; Transformers,English,apache-2.0,https://arxiv.org/pdf/2102.08981.pdf; https://arxiv.org/pdf/2012.09841.pdf; https://arxiv.org/pdf/1910.13461.pdf; https://arxiv.org/pdf/1910.09700.pdf,305,,215,1829.570575,60,https://huggingface.co/dalle-mini/dalle-mini,"This model card focuses on the model associated with the DALL・E mini space on Hugging Face, available here. The app is called “dalle-mini”, but  incorporates “DALL・E Mini’’ and “DALL・E Mega” models (further details on this distinction forthcoming).; The DALL・E Mega model is the largest version of DALLE Mini. For more information specific to DALL・E Mega, see the DALL・E Mega model card.; The model is intended to be used to generate images based on text prompts for research and personal consumption.  Intended uses include supporting creativity, creating humorous content, and providing generations for people curious about the model’s behavior.  Intended uses exclude those described in the Misuse and Out-of-Scope Use section.; The model could also be used for downstream use cases, including:; Downstream uses exclude the uses described in Misuse and Out-of-Scope Use."
gpt2-small-spanish,Text Generation,PyTorch; TensorFlow; JAX; Transformers,Spanish,apache-2.0,,19,wikipedia,"1,661",1507.334505,4,https://huggingface.co/datificate/gpt2-small-spanish,La descripción en Espa?ol se encuentra después de la descripción en Inglés.; GPT2-small-spanish is a state-of-the-art language model for Spanish based on the GPT-2 small model. ; It was trained on Spanish Wikipedia using Transfer Learning and Fine-tuning techniques. The training took around 70 hours with four GPU NVIDIA GTX 1080-Ti with 11GB of DDR5 and with around 3GB of (processed) training data. ; It was fine-tuned from the English pre-trained GPT-2 small using the Hugging Face libraries (Transformers and Tokenizers) wrapped into the fastai v2 Deep Learning framework. All the fine-tuning fastai v2 techniques were used.; The training is purely based on the GPorTuguese-2 model developed by Pierre Guillou. The training details are in this article: "Faster than training from scratch ― Fine-tuning the English GPT-2 in any language with Hugging Face and fastai v2 (practical case with Portuguese)".
gpt2-french-small,Text Generation,PyTorch; JAX; Safetensors; Transformers,French,,,6,,622,1520.68951,,https://huggingface.co/dbddv01/gpt2-french-small,"A small french language model for french text generation (and possibly more NLP tasks...); Introduction; This french gpt2 model is based on openai GPT-2 small model.; It was trained on a very small (190Mb) dataset  from french wikipedia using Transfer Learning and Fine-tuning techniques in just over a day, on one Colab pro with 1GPU 16GB.; It was created applying the recept of Pierre Guillou"
wav2vec2-xls-r-300m-italian,Automatic Speech Recognition,PyTorch; Safetensors; Transformers,Italian,apache-2.0,,2,mozilla-foundation/common_voice_7_0,24,3502.568744,1,https://huggingface.co/dbdmg/wav2vec2-xls-r-300m-italian,"This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the MOZILLA-FOUNDATION/COMMON_VOICE_7_0 - IT dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
bert-large-cased-finetuned-conll03-english,Token Classification,PyTorch; TensorFlow; JAX; Rust; Transformers,,,,29,,"298,708",5447.88939,7,https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english,No model card; New: Create and edit this model card directly on the website!
xlm-roberta-large-squad2,Question Answering,PyTorch; Safetensors; Transformers,multilingual,cc-by-4.0,,46,squad_v2,"5,050",4592.596702,7,https://huggingface.co/deepset/xlm-roberta-large-squad2,"Language model: xlm-roberta-largeLanguage: MultilingualDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD dev set - German MLQA - German XQuADTraining run: MLFlow linkInfrastructure: 4x Tesla v100; Evaluated on the SQuAD 2.0 English dev set with the official eval script.; Evaluated on German MLQA: test-context-de-question-de.json; Evaluated on German XQuAD: xquad.de.json; For doing QA at scale (i.e. many docs instead of single paragraph), you can load the model also in haystack:"
roberta-cls-consec,Text Classification,PyTorch; JAX; Safetensors; Transformers,,,https://arxiv.org/pdf/2012.03619.pdf,10,,431,1502.326875,,https://huggingface.co/dennlinger/roberta-cls-consec,"This network has been fine-tuned for the task described in the paper Topical Change Detection in Documents via Embeddings of Long Sequences and is our best-performing base-transformer model. You can find more detailed information in our GitHub page for the paper here, or read the paper itself. The weights are based on RoBERTa-base.; The preferred way is through pipelines; The model expects two segments that are separated with the [SEP] token. In our training setup, we had entire paragraphs as samples (or up to 512 tokens across two paragraphs), specifically trained on a Terms of Service data set. Note that this might lead to poor performance on ""general"" topics, such as news articles or Wikipedia.; The training task is to determine whether two text segments (paragraphs) belong to the same topical section or not. This can be utilized to create a topical segmentation of a document by consecutively predicting the ""coherence"" of two segments.If you are experimenting via the Huggingface Model API, the following are interpretations of the LABELs:; The results of this model can be found in the paper. We average over models from five different random seeds, which is why the specific results for this model might be different from the exact values in the paper."
bert-multi-english-german-squad2,Question Answering,PyTorch; Safetensors; Transformers,German; English; multilingual,mit,,29,,"1,634",1418.97758,1,https://huggingface.co/deutsche-telekom/bert-multi-english-german-squad2,"We created German Squad 2.0 (deQuAD 2.0) and merged with SQuAD2.0 into an English and German training data for question answering. The bert-base-multilingual-cased is used to fine-tune bilingual QA downstream task.; SQuAD2.0 was auto-translated into German. We hired professional editors to proofread the translated transcripts, correct mistakes and double check the answers to further polish the text and enhance annotation quality. The final German deQuAD dataset contains 130k training and 11k test samples.; Copyright (c) 2021 Fang Xu, Deutsche Telekom AG "
bert-base-polish-uncased-v1,Fill-Mask,PyTorch; JAX; Transformers,Polish,,,6,,"7,199",1060.492881,,https://huggingface.co/dkleczek/bert-base-polish-uncased-v1,"Polish version of BERT language model is here! It is now available in two variants: cased and uncased, both can be downloaded and used via HuggingFace transformers library. I recommend using the cased model, more info on the differences and benchmark results below. ; ; Below is the list of corpora used along with the output of wc command (counting lines, words and characters). These corpora were divided into sentences with srxsegmenter (see references), concatenated and tokenized with HuggingFace BERT Tokenizer. ; Polbert is released via HuggingFace Transformers library.; For an example use as language model, see this notebook file. "
letr-sol-profanity-filter,Text Classification,PyTorch; Transformers,,,,1,,76,436.2554291,,https://huggingface.co/dobbytk/letr-sol-profanity-filter,No model card; New: Create and edit this model card directly on the website!
kenlm,,,24 languages,mit,,20,wikipedia; oscar,0,0.019619141,1,https://huggingface.co/edugp/kenlm,"This repo contains several KenLM models trained on different tokenized datasets and languages.KenLM models are probabilistic n-gram languge models that models. One use case of these models consist on fast perplexity estimation for filtering or sampling large datasets. For example, one could use a KenLM model trained on French Wikipedia to run inference on a large dataset and filter out samples that are very unlike to appear on Wikipedia (high perplexity), or very simple non-informative sentences that could appear repeatedly (low perplexity).; At the root of this repo you will find different directories named after the dataset models were trained on (e.g. wikipedia, oscar). Within each directory, you will find several models trained on different language subsets of the dataset (e.g. en (English), es (Spanish), fr (French)). For each language you will find three different files; The models have been trained using some of the preprocessing steps from cc_net, in particular replacing numbers with zeros and normalizing punctuation. So, it is important to keep the default values for the parameters: lower_case, remove_accents, normalize_numbers and punctuation when using the pre-trained models in order to replicate the same pre-processing steps at inference time.; In the example above we see that, since Wikipedia is a collection of encyclopedic articles, a KenLM model trained on it will naturally give lower perplexity scores to sentences with formal language and no grammar mistakes than colloquial sentences with grammar mistakes.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
text2tags,Summarization,PyTorch; Safetensors; Transformers,Italian,,,3,,48,618.0632326,,https://huggingface.co/efederici/text2tags,"The model has been trained on a collection of 28k news articles with tags. Its purpose is to create tags suitable for the given article. We can use this model also for information-retrieval purposes (GenQ), fine-tuning sentence-transformers for asymmetric semantic search. ; If you like this project, consider supporting it with a cup of coffee! ???
; 
 
    Pieter Bruegel the Elder, The Fight Between Carnival and Lent, 1559
; Sample code with an article from IlPost:; Assuming paragraphs are divided by: '\n\n'."
bert-base-turkish-cased-mean-nli-stsb-tr,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,Turkish,apache-2.0,,9,nli_tr; emrecan/stsb-mt-turkish,815,443.7399625,,https://huggingface.co/emrecan/bert-base-turkish-cased-mean-nli-stsb-tr,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. The model was trained on Turkish machine translated versions of NLI and STS-b datasets, using example training scripts from sentence-transformers GitHub repository.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; Evaluation results on test and development sets are given below:"
guwenbert-base,Fill-Mask,PyTorch; JAX; Transformers,Chinese,apache-2.0,,8,,"1,326",834.095155,,https://huggingface.co/ethanyt/guwenbert-base,"; This is a RoBERTa model pre-trained on Classical Chinese. You can fine-tune GuwenBERT for downstream tasks, such as sentence breaking, punctuation, named entity recognition, and so on.; For more information about RoBERTa, take a look at the RoBERTa's offical repo.; The training data is daizhige dataset (殆知阁古代文献) which is contains of 15,694 books in Classical Chinese, covering Buddhism, Confucianism, Medicine, History, Zi, Yi, Yizang, Shizang, Taoism, and Jizang. 
76% of them are punctuated.
The total number of characters is 1.7B (1,743,337,673).
All traditional Characters are converted to simplified characters.
The vocabulary is constructed from this data set and the size is 23,292.; The models are initialized with hfl/chinese-roberta-wwm-ext and then pre-trained with a 2-step strategy.
In the first step, the model learns MLM with only word embeddings updated during training, until convergence. In the second step, all parameters are updated during training."
guwenbert-large,Fill-Mask,PyTorch; JAX; Transformers,Chinese,apache-2.0,,4,,95,2693.215166,,https://huggingface.co/ethanyt/guwenbert-large,"; This is a RoBERTa model pre-trained on Classical Chinese. You can fine-tune GuwenBERT for downstream tasks, such as sentence breaking, punctuation, named entity recognition, and so on.; For more information about RoBERTa, take a look at the RoBERTa's offical repo.; The training data is daizhige dataset (殆知阁古代文献) which is contains of 15,694 books in Classical Chinese, covering Buddhism, Confucianism, Medicine, History, Zi, Yi, Yizang, Shizang, Taoism, and Jizang. 
76% of them are punctuated.
The total number of characters is 1.7B (1,743,337,673).
All traditional Characters are converted to simplified characters.
The vocabulary is constructed from this data set and the size is 23,292.; The models are initialized with hfl/chinese-roberta-wwm-ext-large and then pre-trained with a 2-step strategy.
In the first step, the model learns MLM with only word embeddings updated during training, until convergence. In the second step, all parameters are updated during training."
drln,,Transformers,,apache-2.0,https://arxiv.org/pdf/1906.12021.pdf; https://arxiv.org/pdf/2104.07566.pdf,2,eugenesiow/Div2k; eugenesiow/Set5; eugenesiow/Set14; eugenesiow/BSD100; eugenesiow/Urban100,939,415.0099681,2,https://huggingface.co/eugenesiow/drln,"DRLN model pre-trained on DIV2K (800 images training, augmented to 4000 images, 100 images validation) for 2x, 3x and 4x image super resolution. It was introduced in the paper Densely Residual Laplacian Super-resolution by Anwar et al. (2020) and first released in this repository. ; The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling and model upscaling.; ; Super-Resolution convolutional neural networks have recently demonstrated high-quality restoration for single images. However, existing algorithms often require very deep architectures and long training times. Furthermore, current convolutional neural networks for super-resolution are unable to exploit features at multiple scales and weigh them equally, limiting their learning capability. In this exposition, we present a compact and accurate super-resolution algorithm namely, Densely Residual Laplacian Network (DRLN). The proposed network employs cascading residual on the residual structure to allow the flow of low-frequency information to focus on learning high and mid-level features. In addition, deep supervision is achieved via the densely concatenated residual blocks settings, which also helps in learning from high-level complex features. Moreover, we propose Laplacian attention to model the crucial features to learn the inter and intra-level dependencies between the feature maps. Furthermore, comprehensive quantitative and qualitative evaluations on low-resolution, noisy low-resolution, and real historical image benchmark datasets illustrate that our DRLN algorithm performs favorably against the state-of-the-art methods visually and accurately.; You can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset."
blenderbot-3B,Conversational,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/1907.06616.pdf,101,blended_skill_talk,"7,944",5601.794175,16,https://huggingface.co/facebook/blenderbot-3B,"Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models."
convnext-large-224,Image Classification,PyTorch; TensorFlow; Transformers,,apache-2.0,https://arxiv.org/pdf/2201.03545.pdf,24,imagenet-1k,"1,128",1582.072363,,https://huggingface.co/facebook/convnext-large-224,"ConvNeXT model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper A ConvNet for the 2020s by Liu et al. and first released in this repository. ; Disclaimer: The team releasing ConvNeXT did not write a model card for this model so this model card has been written by the Hugging Face team.; ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and ""modernized"" its design by taking the Swin Transformer as inspiration.; ; You can use the raw model for image classification. See the model hub to look for
fine-tuned versions on a task that interests you."
detr-resnet-50-panoptic,Image Segmentation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2005.12872.pdf,77,coco,"11,828",172.0179594,75,https://huggingface.co/facebook/detr-resnet-50-panoptic,"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository. ; Disclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.; The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. ; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.; DETR can be naturally extended to perform panoptic segmentation, by adding a mask head on top of the decoder outputs."
dino-vits16,Feature Extraction,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2104.14294.pdf,8,imagenet-1k,"5,829",86.70500065,,https://huggingface.co/facebook/dino-vits16,"Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository. ; Disclaimer: The team releasing DINO did not write a model card for this model so this model card has been written by the Hugging Face team.; The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. ; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; Note that this model does not include any fine-tuned heads. "
hubert-large-ll60k,Feature Extraction,PyTorch; TensorFlow; Transformers,English,apache-2.0,https://arxiv.org/pdf/2106.07447.pdf,15,libri-light,"522,561",2580.48505,1,https://huggingface.co/facebook/hubert-large-ll60k,"Facebook's Hubert; The large model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.; Note: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.; The model was pretrained on Libri-Light.; Paper"
hubert-large-ls960-ft,Automatic Speech Recognition,PyTorch; TensorFlow; Transformers,English,apache-2.0,https://arxiv.org/pdf/2106.07447.pdf,36,libri-light; librispeech_asr,"45,370",2580.48595,9,https://huggingface.co/facebook/hubert-large-ls960-ft,"Facebook's Hubert; The large model fine-tuned on 960h of Librispeech on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. ; The model is a fine-tuned version of hubert-large-ll60k.; Paper; Authors: Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed"
maskformer-swin-tiny-coco,Image Segmentation,PyTorch; Transformers,,other,https://arxiv.org/pdf/2107.06278.pdf,1,coco,"1,009",167.0147765,,https://huggingface.co/facebook/maskformer-swin-tiny-coco,"MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. ; Disclaimer: The team releasing MaskFormer did not write a model card for this model so this model card has been written by the Hugging Face team.; MaskFormer addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.; ; You can use this particular checkpoint for semantic segmentation. See the model hub to look for other
fine-tuned versions on a task that interests you."
rag-token-nq,,PyTorch; TensorFlow; Transformers,English,apache-2.0,https://arxiv.org/pdf/2005.11401.pdf,29,wiki_dpr,"5,932",4218.886501,,https://huggingface.co/facebook/rag-token-nq,"This is the RAG-Token Model of the the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks 
by Patrick Lewis, Ethan Perez, Aleksandara Piktus et al.; The model is a uncased model, which means that capital letters are simply converted to lower-case letters.; The model consits of a question_encoder, retriever and a generator. The retriever extracts relevant passages from the wiki_dpr train datasets, which is linked above.
The question_encoder and retriever are based on facebook/dpr-question_encoder-single-nq-base and facebook/bart-large, which were jointly finetuned on 
on the wiki_dpr QA dataset in an end-to-end fashion.; Note: In the usage example below only the dummy retriever of wiki_dpr is used because the complete lecagy index requires over 75 GB of RAM.
The model can generate answers to any factoid question as follows:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
tts_transformer-ar-cv7,Text-to-Speech,Fairseq,Arabic,,https://arxiv.org/pdf/1809.08895.pdf; https://arxiv.org/pdf/2109.06912.pdf,5,common_voice,262,722.4194772,5,https://huggingface.co/facebook/tts_transformer-ar-cv7,Transformer text-to-speech model from fairseq S^2 (paper/code):; See also fairseq S^2 example.
vit-mae-base,,PyTorch; TensorFlow; Transformers,,apache-2.0,https://arxiv.org/pdf/2111.06377.pdf,12,imagenet-1k,"54,456",896.0049044,,https://huggingface.co/facebook/vit-mae-base,"Vision Transformer (ViT) model pre-trained using the MAE method. It was introduced in the paper Masked Autoencoders Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, Ross Girshick and first released in this repository. ; Disclaimer: The team releasing MAE did not write a model card for this model so this model card has been written by the Hugging Face team.; The Vision Transformer (ViT) is a transformer encoder model (BERT-like). Images are presented to the model as a sequence of fixed-size patches.; During pre-training, one randomly masks out a high portion (75%) of the image patches. First, the encoder is used to encode the visual patches. Next, a learnable (shared) mask token is added at the positions of the masked patches. The decoder takes the encoded visual patches and mask tokens as input and reconstructs raw pixel values for the masked positions.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder."
wav2vec2-base,,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2006.11477.pdf,28,librispeech_asr,"78,058",380.0051176,4,https://huggingface.co/facebook/wav2vec2-base,"Facebook's Wav2Vec2; The base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. ; Note: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.; Paper; Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli"
wav2vec2-large-robust,,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2104.01027.pdf,13,libri_light; common_voice; switchboard; fisher,"3,411",1300.485248,,https://huggingface.co/facebook/wav2vec2-large-robust,"Facebook's Wav2Vec2; The large model pretrained on 16kHz sampled speech audio. 
Speech datasets from multiple domains were used to pretrain the model:; When using the model make sure that your speech input is also sampled at 16Khz.; Note: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.; Paper Robust Wav2Vec2"
wav2vec2-large-xlsr-53,,PyTorch; JAX; Transformers,multilingual,apache-2.0,https://arxiv.org/pdf/2006.13979.pdf,50,common_voice,"37,577",2600.964869,4,https://huggingface.co/facebook/wav2vec2-large-xlsr-53,"Facebook's XLSR-Wav2Vec2; The base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. Note that this model should be fine-tuned on a downstream task, like Automatic Speech Recognition. Check out this blog for more information.; Paper; Authors: Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli; Abstract
This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72% compared to the best known results. On BABEL, our approach improves word error rate by 16% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages."
wmt19-ru-en,Translation,PyTorch; Safetensors; Transformers,Russian; English,apache-2.0,https://arxiv.org/pdf/1907.06616.pdf,13,wmt19,"28,879",2377.453423,4,https://huggingface.co/facebook/wmt19-ru-en,"This is a ported version of fairseq wmt19 transformer for ru-en.; For more details, please see, Facebook FAIR's WMT19 News Translation Task Submission.; The abbreviation FSMT stands for FairSeqMachineTranslation; All four models are available:; Pretrained weights were left identical to the original model released by fairseq. For more details, please, see the paper."
bert-restore-punctuation,Token Classification,PyTorch; Transformers,English,mit,,47,yelp_polarity,"11,001",1307.239814,10,https://huggingface.co/felflare/bert-restore-punctuation,"; This a bert-base-uncased model finetuned for punctuation restoration on Yelp Reviews. ; The model predicts the punctuation and upper-casing of plain, lower-cased text. An example use case can be ASR output. Or other cases when text has lost punctuation.; This model is intended for direct use as a punctuation restoration model for the general English language. Alternatively, you can use this for further fine-tuning on domain-specific texts for punctuation restoration tasks.; Model restores the following punctuations -- [! ? . , - : ; ' ]"
indo-medical-bert-base-uncased,Fill-Mask,PyTorch; Transformers,,,,1,,25,334.3750322,,https://huggingface.co/firqaaa/indo-medical-bert-base-uncased,
pos-english,Token Classification,PyTorch; Flair,English,,,17,ontonotes,"89,644",249.2057655,4,https://huggingface.co/flair/pos-english,"This is the standard part-of-speech tagging model for English that ships with Flair.; F1-Score: 98,19 (Ontonotes); Predicts fine-grained POS tags:; Based on Flair embeddings and LSTM-CRF.; Requires: Flair (pip install flair)"
flaubert_base_uncased,Fill-Mask,PyTorch; Transformers,French,mit,,2,flaubert,"12,691",552.4617594,,https://huggingface.co/flaubert/flaubert_base_uncased,"FlauBERT is a French BERT trained on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer.; Along with FlauBERT comes FLUE: an evaluation setup for French NLP systems similar to the popular GLUE benchmark. The goal is to enable further reproducible experiments in the future and to share models and progress on the French language.For more details please refer to the official website.; Note: flaubert-small-cased is partially trained so performance is not guaranteed. Consider using it for debugging purpose only.; Notes: if your transformers version is <=2.10.0, modelname should take one
of the following values:; If you use FlauBERT or the FLUE Benchmark for your scientific publication, or if you find the resources in this repository useful, please cite one of the following papers:"
multi-QA_v1-mpnet-asymmetric-A,Sentence Similarity,PyTorch; Sentence Transformers,,,https://arxiv.org/pdf/2102.07033.pdf; https://arxiv.org/pdf/2104.08727.pdf,2,,"1,931",438.6888625,2,https://huggingface.co/flax-sentence-embeddings/multi-QA_v1-mpnet-asymmetric-A,"SentenceTransformers is a set of models and frameworks that enable training and generating sentence embeddings from given data. The generated sentence embeddings can be utilized for Clustering, Semantic Search and other tasks. We used two separate pretrained mpnet-base models and trained them using contrastive learning objective. Question and answer pairs from StackExchange and other datasets were used as training data to make the model robust to Question / Answer embedding similarity.; We developed this model during the 
Community week using JAX/Flax for NLP & CV, 
organized by Hugging Face. We developed this model as part of the project:
Train the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well
as assistance from Google’s Flax, JAX, and Cloud team members about efficient deep learning frameworks.; This model set is intended to be used as a sentence encoder for a search engine. Given an input sentence, it ouptuts a vector which captures 
the sentence semantic information. The sentence vector may be used for semantic-search, clustering or sentence similarity tasks.; Two models should be used on conjunction for Semantic Search purposes.; Here is how to use this model to get the features of a given text using SentenceTransformers library:"
bart-base-chinese,Text2Text Generation,PyTorch; Transformers,Chinese,,https://arxiv.org/pdf/2109.05729.pdf,66,,"5,655",561.2600001,1,https://huggingface.co/fnlp/bart-base-chinese,"12/30/2022; An updated version of CPT & Chinese BART are released. In the new version, we changed the following parts:; We initialize the new version of models with the old version of checkpoints with vocabulary alignment. Token embeddings found in the old checkpoints are copied. And other newly added parameters are randomly initialized. We further train the new CPT & Chinese BART 50K steps with batch size 2048, max-seq-length 1024, peak learning rate 2e-5, and warmup ratio 0.1.; The result compared to the previous checkpoints is as followings:; The result shows that the updated models maintain comparative performance compared with previous checkpoints. There are still some cases that the updated model is slightly worse than the previous one, which results from the following reasons: 1) Training additional a few steps did not lead to significant performance improvement; 2) some downstream tasks are not affected by the newly added tokens and longer encoding sequences, but sensitive to the fine-tuning hyperparameters."
bart-large-chinese,Text2Text Generation,PyTorch; Transformers,Chinese,,https://arxiv.org/pdf/2109.05729.pdf,39,,"1,594",1669.380021,1,https://huggingface.co/fnlp/bart-large-chinese,"12/30/2022; An updated version of CPT & Chinese BART are released. In the new version, we changed the following parts:; We initialize the new version of models with the old version of checkpoints with vocabulary alignment. Token embeddings found in the old checkpoints are copied. And other newly added parameters are randomly initialized. We further train the new CPT & Chinese BART 50K steps with batch size 2048, max-seq-length 1024, peak learning rate 2e-5, and warmup ratio 0.1.; The result compared to the previous checkpoints is as followings:; The result shows that the updated models maintain comparative performance compared with previous checkpoints. There are still some cases that the updated model is slightly worse than the previous one, which results from the following reasons: 1) Training additional a few steps did not lead to significant performance improvement; 2) some downstream tasks are not affected by the newly added tokens and longer encoding sequences, but sensitive to the fine-tuning hyperparameters."
french-camembert-postag-model,Token Classification,PyTorch; TensorFlow; Safetensors; Transformers,French,,,2,,"188,550",1329.798874,,https://huggingface.co/gilf/french-camembert-postag-model,"The  french-camembert-postag-model is a part of speech tagging model for French that was trained on the free-french-treebank dataset available on 
github. The base tokenizer and model used for training is 'camembert-base'.; It uses the following tags:; More information on the tags can be found here:; http://alpage.inria.fr/statgram/frdep/Publications/crabbecandi-taln2008-final.pdf; The usage of this model follows the common transformers patterns. Here is a short example of its usage:"
kobart-base-v1,Feature Extraction,PyTorch; Safetensors; Transformers,Korean,mit,,1,,"1,904",992.277322,5,https://huggingface.co/gogamza/kobart-base-v1,
byt5-small,Text2Text Generation,PyTorch; TensorFlow; JAX; Transformers,102 languages,apache-2.0,https://arxiv.org/pdf/1907.06292.pdf; https://arxiv.org/pdf/2105.13626.pdf,25,mc4,"25,294",3686.410609,3,https://huggingface.co/google/byt5-small,"ByT5 is a tokenizer-free version of Google's T5 and generally follows the architecture of MT5.; ByT5 was only pre-trained on mC4 excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; ByT5 works especially well on noisy text data,e.g., google/byt5-small significantly outperforms mt5-small on TweetQA.; Paper: ByT5: Towards a token-free future with pre-trained byte-to-byte models; Authors: Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel "
mt5-small,Text2Text Generation,PyTorch; TensorFlow; JAX; Transformers,102 languages,apache-2.0,https://arxiv.org/pdf/2010.11934.pdf,50,mc4,"53,278",3690.713977,6,https://huggingface.co/google/mt5-small,"Google's mT5; mT5 is pretrained on the mC4 corpus, covering 101 languages:; Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.; Note: mT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; Pretraining Dataset: mC4"
pegasus-big_patent,Text2Text Generation,PyTorch; Transformers,,,,3,,184,2336.631836,2,https://huggingface.co/google/pegasus-big_patent,No model card; New: Create and edit this model card directly on the website!
t5-v1_1-xxl,Text2Text Generation,PyTorch; TensorFlow; Transformers,English,apache-2.0,https://arxiv.org/pdf/2002.05202.pdf; https://arxiv.org/pdf/1910.10683.pdf,15,c4,"6,946",91136.78065,25,https://huggingface.co/google/t5-v1_1-xxl,"Google's T5 Version 1.1; T5 Version 1.1 includes the following improvements compared to the original T5 model- GEGLU activation in feed-forward hidden layer, rather than ReLU - see here.; Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning.; Pre-trained on C4 only without mixing in the downstream tasks.; no parameter sharing between embedding and classifier layer"
tapas-large-finetuned-sqa,Table Question Answering,PyTorch; TensorFlow; Transformers,English,apache-2.0,https://arxiv.org/pdf/2004.02349.pdf; https://arxiv.org/pdf/2010.00571.pdf,4,msr_sqa,360,2765.065846,,https://huggingface.co/google/tapas-large-finetuned-sqa,"This model has 2 versions which can be used. The default version corresponds to the tapas_sqa_inter_masklm_large_reset checkpoint of the original Github repository.
This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).; The other (non-default) version which can be used is: ; Disclaimer: The team releasing TAPAS did not write a model card for this model so this model card has been written by
the Hugging Face team and contributors.; TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. 
This means it was pretrained on the raw tables and associated texts only, with no humans labelling them in any way (which is why it
can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language used in tables and associated texts, which can then be used 
to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed
or refuted by the contents of a table. Fine-tuning is done by adding a cell selection head on top of the pre-trained model, and then jointly
train this randomly initialized classification head with the base model on SQA. "
tapas-large-finetuned-wtq,Table Question Answering,PyTorch; TensorFlow; Transformers,English,apache-2.0,https://arxiv.org/pdf/2004.02349.pdf; https://arxiv.org/pdf/2010.00571.pdf; https://arxiv.org/pdf/1508.00305.pdf,43,wikitablequestions,"10,702",2765.065474,12,https://huggingface.co/google/tapas-large-finetuned-wtq,"This model has 2 versions which can be used. The default version corresponds to the tapas_wtq_wikisql_sqa_inter_masklm_large_reset checkpoint of the original Github repository.
This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).; The other (non-default) version which can be used is: ; Disclaimer: The team releasing TAPAS did not write a model card for this model so this model card has been written by
the Hugging Face team and contributors.; TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. 
This means it was pretrained on the raw tables and associated texts only, with no humans labelling them in any way (which is why it
can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language used in tables and associated texts, which can then be used 
to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed
or refuted by the contents of a table. Fine-tuning is done by adding a cell selection head and aggregation head on top of the pre-trained model, and then jointly train these randomly initialized classification heads with the base model on SQa, WikiSQL and finally WTQ. "
arabic-ner,Token Classification,PyTorch; TensorFlow; JAX; Safetensors; Transformers,Arabic,,,8,,"2,483",1764.154314,1,https://huggingface.co/hatmimoha/arabic-ner,Pretrained BERT-based (arabic-bert-base) Named Entity Recognition model for Arabic.; The pre-trained model can recognize the following entities:; ? ??? ?? ???? ??????? ??????? ?????? ???? ??? ? ?????? ??? ??? ???? ; ??? ????? ??????? ????? ??? ??? ????? ?? ???? ????? ; ? ????? ?????? ???????? ??????? ???? ??????? ??? ????? ?? ??? ????? ?????????
character-bert,Feature Extraction,PyTorch; Transformers,,,,2,,74,728.9229253,,https://huggingface.co/helboukkouri/character-bert,No model card; New: Create and edit this model card directly on the website!
chinese-macbert-base,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/2004.13922.pdf,82,,"23,236",1299.374995,1,https://huggingface.co/hfl/chinese-macbert-base,"



; 



; This repository contains the resources in our paper ""Revisiting Pre-trained Models for Chinese Natural Language Processing"", which will be published in ""Findings of EMNLP"". You can read our camera-ready paper through ACL Anthology or arXiv pre-print.; Revisiting Pre-trained Models for Chinese Natural Language ProcessingYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu; You may also interested in,"
chinese-macbert-large,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/2004.13922.pdf,19,,"2,468",4106.614996,,https://huggingface.co/hfl/chinese-macbert-large,"



; 



; This repository contains the resources in our paper ""Revisiting Pre-trained Models for Chinese Natural Language Processing"", which will be published in ""Findings of EMNLP"". You can read our camera-ready paper through ACL Anthology or arXiv pre-print.; Revisiting Pre-trained Models for Chinese Natural Language ProcessingYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu; You may also interested in,"
rbt3,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/1906.08101.pdf; https://arxiv.org/pdf/2004.13922.pdf,11,,"7,635",464.3734476,,https://huggingface.co/hfl/rbt3,"For further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking. ; Pre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu; This repository is developed based on：https://github.com/google-research/bert; You may also interested in,; More resources by HFL: https://github.com/ymcui/HFL-Anthology"
sentence_similarity_spanish_es,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,Spanish,,,22,,"11,826",439.7125629,1,https://huggingface.co/hiiamsid/sentence_similarity_spanish_es,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net"
ConvTasNet_Libri2Mix_sepnoisy_16k,,,,,,1,,0,20.8011158,,https://huggingface.co/hugggof/ConvTasNet_Libri2Mix_sepnoisy_16k,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
elonmusk,Text Generation,PyTorch; Transformers,English,,,10,,"2,356",513.3436067,1,https://huggingface.co/huggingtweets/elonmusk,"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report."
funnyordie,Text Generation,PyTorch; Transformers,English,,,1,,11,512.593348,,https://huggingface.co/huggingtweets/funnyordie,"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report."
honeytech,Text Generation,PyTorch; Transformers,English,,,1,,7,512.59256,,https://huggingface.co/huggingtweets/honeytech,"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report."
michaeljackson,Text Generation,PyTorch; JAX; Transformers,English,,,1,,54,1009.328796,,https://huggingface.co/huggingtweets/michaeljackson,"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report."
porns_xx,Text Generation,PyTorch; Transformers,English,,,14,,214,512.59256,,https://huggingface.co/huggingtweets/porns_xx,"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report."
financial-summarization-pegasus,Summarization,PyTorch; TensorFlow; Safetensors; Transformers,English,,https://arxiv.org/pdf/1912.08777.pdf,78,xsum,"4,297",7006.082799,9,https://huggingface.co/human-centered-summarization/financial-summarization-pegasus,"This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. ; It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. ; We provide a simple snippet of how to use this model for the task of financial summarization in PyTorch.; The results before and after the fine-tuning on our dataset are shown below:; You can find more details about this work in the following workshop paper. If you use our model in your research, please consider citing our paper:"
pangu_2_6B,Text Generation,PyTorch; Transformers,,,,14,,617,10752.88827,,https://huggingface.co/imone/pangu_2_6B,"PanGu-α is proposed by a joint technical team headed by PCNL. It was first released in this repository  It is the first large-scale Chinese pre-trained language model with 200 billion parameters trained on 2048 Ascend processors using an automatic hybrid parallel training strategy. The whole training process is done on the “Peng Cheng Cloud Brain II” computing platform with the domestic deep learning framework called MindSpore. The PengCheng・PanGu-α pre-training model can support rich applications, has strong few-shot learning capabilities, and has outstanding performance in text generation tasks such as knowledge question and answer, knowledge retrieval, knowledge reasoning, and reading comprehension.; This repository contains PyTorch implementation of PanGu model, with
2.6 billion parameters pretrained weights (FP32 precision), converted from original MindSpore checkpoint.; Currently PanGu model is not supported by transformers, 
so trust_remote_code=True is required to load model implementation in this repo.; Expected output:"
general_character_bert,,PyTorch; Transformers,English,,https://arxiv.org/pdf/2010.10392.pdf,3,wikipedia; openwebtext,22,418.0045898,,https://huggingface.co/imvladikon/general_character_bert,"Pretrained general_character_bert model 
from the 'CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters' El Boukkouri H., et al., 2020; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
indobert-base-uncased,Fill-Mask,PyTorch; JAX; Transformers,Indonesian,mit,https://arxiv.org/pdf/2011.00677.pdf,14,"220M words (IndoWiki, IndoWC, News)]","970,382",887.2323869,6,https://huggingface.co/indolem/indobert-base-uncased,"IndoBERT is the Indonesian version of BERT model. We train the model using over 220M words, aggregated from three main sources: ; We trained the model for 2.4M steps (180 epochs) with the final perplexity over the development set being 3.97 (similar to English BERT-base).; This IndoBERT was used to examine IndoLEM - an Indonesian benchmark that comprises of seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. ; The paper is published at the 28th COLING 2020. Please refer to https://indolem.github.io for more details about the benchmarks.; If you use our work, please cite:"
berteus-base-cased,Feature Extraction,PyTorch; JAX; Transformers,Basque,,https://arxiv.org/pdf/2004.00033.pdf,2,,151,996.4326218,,https://huggingface.co/ixa-ehu/berteus-base-cased,"This is the Basque language pretrained model presented in Give your Text Representation Models some Love: the Case for Basque. This model has been trained on a Basque corpus comprising Basque crawled news articles from online newspapers and the Basque Wikipedia. The training corpus contains 224.6 million tokens, of which 35 million come from the Wikipedia.; BERTeus has been tested on four different downstream tasks for Basque: part-of-speech (POS) tagging, named entity recognition (NER), sentiment analysis and topic classification; improving the state of the art for all tasks. See summary of results below:; If using this model, please cite the following paper:"
bert-stsb-aug,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,,,,1,,10,438.6877198,,https://huggingface.co/jamescalam/bert-stsb-aug,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; It is used as a demo model within the NLP for Semantic Search course, for the chapter on In-domain Data Augmentation with BERT.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings."
bert-stsb-cross-encoder,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,,,,1,,37,438.6843522,,https://huggingface.co/jamescalam/bert-stsb-cross-encoder,"This is a sentence-transformers cross encoder model.; It is used as a demo model within the NLP for Semantic Search course, for the chapter on In-domain Data Augmentation with BERT."
ko-sbert-multitask,Sentence Similarity,PyTorch; TensorFlow; Sentence Transformers; Transformers,,,,3,,323,886.7332917,,https://huggingface.co/jhgan/ko-sbert-multitask,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; KorSTS, KorNLI ?? ?????? ?? ??? ??? ??? ? KorSTS ?? ?????? ??? ?????."
xlm-roberta-large-xnli,Zero-Shot Classification,PyTorch; TensorFlow; Transformers,16 languages,mit,https://arxiv.org/pdf/1911.02116.pdf,129,multi_nli; xnli,"7,586",0,11,https://huggingface.co/joeddav/xlm-roberta-large-xnli,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This model takes xlm-roberta-large and fine-tunes it on a combination of NLI data in 15 languages. It is intended to be used for zero-shot text classification, such as with the Hugging Face ZeroShotClassificationPipeline.; This model is intended to be used for zero-shot text classification, especially in languages other than English. It is fine-tuned on XNLI, which is a multilingual NLI dataset. The model can therefore be used with any of the languages in the XNLI corpus:; Since the base model was pre-trained trained on 100 different languages, the
model has shown some effectiveness in languages beyond those listed above as
well. See the full list of pre-trained languages in appendix A of the
XLM Roberata paper"
wav2vec2-large-xlsr-53-chinese-zh-cn,Automatic Speech Recognition,PyTorch; JAX; Transformers,Chinese,apache-2.0,,34,common_voice,"63,076",2621.493365,10,https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn,"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Chinese using the train and validation splits of Common Voice 6.1, CSS10 and ST-CMDS.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:"
wav2vec2-large-xlsr-53-hungarian,Automatic Speech Recognition,PyTorch; JAX; Transformers,Hungarian,apache-2.0,,2,common_voice,"1,224",2580.490389,6,https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-hungarian,"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Hungarian using the train and validation splits of Common Voice 6.1 and CSS10.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:"
wav2vec2-large-xlsr-53-japanese,Automatic Speech Recognition,PyTorch; JAX; Transformers,Japanese,apache-2.0,,11,common_voice,"27,812",2600.999508,8,https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-japanese,"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:"
wav2vec2-large-xlsr-53-russian,Automatic Speech Recognition,PyTorch; JAX; Transformers,Russian,apache-2.0,,19,common_voice; mozilla-foundation/common_voice_6_0,"688,094",2584.272952,15,https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-russian,"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Russian using the train and validation splits of Common Voice 6.1 and CSS10.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:"
wav2vec2-xls-r-1b-portuguese,Automatic Speech Recognition,PyTorch; Transformers,Portuguese,apache-2.0,,9,mozilla-foundation/common_voice_8_0,"1,043",3944.294163,4,https://huggingface.co/jonatasgrosman/wav2vec2-xls-r-1b-portuguese,"Fine-tuned facebook/wav2vec2-xls-r-1b on Portuguese using the train and validation splits of Common Voice 8.0, CORAA, Multilingual TEDx, and Multilingual LibriSpeech.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned by the HuggingSound tool, and thanks to the GPU credits generously given by the OVHcloud :); Using the HuggingSound library:; Writing your own inference script:; If you want to cite this model you can use this:"
DialoGPT-small-Creed-Odyssey,Text Generation,PyTorch; JAX; Transformers,,,,1,,10,1009.227007,,https://huggingface.co/jonx18/DialoGPT-small-Creed-Odyssey,"The app was conceived with the idea of recreating and generate new dialogs for existing games.
In order to generate a dataset for training the steps followed were:"
hotdog-not-hotdog,Image Classification,PyTorch; TensorBoard; Core ML; Transformers,,,,10,,"7,323",343.0021887,171,https://huggingface.co/julien-c/hotdog-not-hotdog,Autogenerated by HuggingPics???; Create your own image classifier for anything by running the demo on Google Colab.; Report any issues with the demo at the github repo.; ; 
SciBERT_patent_reference_extraction,,PyTorch; Transformers,,,https://arxiv.org/pdf/2101.01039.pdf,1,,0,440.2484703,,https://huggingface.co/kaesve/SciBERT_patent_reference_extraction,"This repository contains a finetuned SciBERT model that can extract references to scientific literature from patents.; See https://github.com/kaesve/patent-citation-extraction and https://arxiv.org/abs/2101.01039 for more information.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
low-light-image-enhancement,Image-to-Image,Keras,,apache-2.0,,14,,43,347.1774805,1,https://huggingface.co/keras-io/low-light-image-enhancement,"Original Author: Soumik Rakshit 
Date created: 2021/09/18 
HF Contribution: Harveen Singh Chadha
Dataset: LOL Dataset; Zero-Reference Deep Curve Estimation or Zero-DCE formulates low-light image enhancement as the task of estimating an image-specific tonal curve with a deep neural network. In this example, we train a lightweight deep network, DCE-Net, to estimate pixel-wise and high-order tonal curves for dynamic range adjustment of a given image.; Zero-DCE takes a low-light image as input and produces high-order tonal curves as its output. These curves are then used for pixel-wise adjustment on the dynamic range of the input to obtain an enhanced image. The curve estimation process is done in such a way that it maintains the range of the enhanced image and preserves the contrast of neighboring pixels. This curve estimation is inspired by curves adjustment used in photo editing software such as Adobe Photoshop where users can adjust points throughout an image’s tonal range.; Zero-DCE is appealing because of its relaxed assumptions with regard to reference images: it does not require any input/output image pairs during training. This is achieved through a set of carefully formulated non-reference loss functions, which implicitly measure the enhancement quality and guide the training of the network.; Sample Images:"
wav2vec2-large-xls-r-300m-Urdu,Automatic Speech Recognition,PyTorch; Safetensors; Transformers,Urdu,apache-2.0,,5,mozilla-foundation/common_voice_8_0,166,2580.541375,4,https://huggingface.co/kingabzpro/wav2vec2-large-xls-r-300m-Urdu,"This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the common_voice dataset.
It achieves the following results on the evaluation set:; The following hyperparameters were used during training:"
t5-base-qa-summary-emotion,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,,3,coqa; squad_v2; go_emotions; cnn_dailymail,241,1216.169069,,https://huggingface.co/kiri-ai/t5-base-qa-summary-emotion,"Requires transformers>=4.0.0; This model was finetuned on the CoQa, Squad 2, GoEmotions and CNN/DailyMail.; It achieves a score of F1 79.5 on the Squad 2 dev set and a score of F1 70.6 on the CoQa dev set.; Summarisation and emotion detection has not been evaluated yet.; Kiri makes using state-of-the-art models easy, accessible and scalable."
bert-base,Fill-Mask,PyTorch; Safetensors; Transformers,Korean,cc-by-sa-4.0,https://arxiv.org/pdf/2105.09680.pdf; https://arxiv.org/pdf/1910.09700.pdf,17,,"120,387",890.7372519,1,https://huggingface.co/klue/bert-base,"Model Description: KLUE BERT base is a pre-trained BERT Model on Korean Language. The developers of KLUE BERT base developed the model in the context of the development of the Korean Language Understanding Evaluation (KLUE) Benchmark.; The model can be used for tasks including topic classification, semantic textual similarity, natural language inference, named entity recognition, and other tasks outlined in the KLUE Benchmark.; The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)). The model developers discuss several ethical considerations related to the model in the paper, including: ; For ethical considerations related to the KLUE Benchmark, also see the paper."
MEETING_SUMMARY,Summarization,PyTorch; TensorFlow; Safetensors; Transformers,English,apache-2.0,,122,cnndaily/newyorkdaily/xsum/samsum/dialogsum/AMI,"28,703",5010.009143,18,https://huggingface.co/knkarthick/MEETING_SUMMARY,"Model obtained by Fine Tuning 'facebook/bart-large-xsum' using AMI Meeting Corpus, SAMSUM Dataset, DIALOGSUM Dataset, XSUM Dataset!"
bart-large-xsum-samsum,Summarization,PyTorch; Safetensors; Transformers,English,apache-2.0,,27,samsum,"21,648",3340.82973,3,https://huggingface.co/lidiya/bart-large-xsum-samsum,This model was obtained by fine-tuning facebook/bart-large-xsum on Samsum dataset.
chinese_pretrain_mrc_macbert_large,Question Answering,PyTorch; Transformers,Chinese,apache-2.0,,17,,688,1331.572557,,https://huggingface.co/luhua/chinese_pretrain_mrc_macbert_large,
bert-imdb,Text Classification,PyTorch; JAX; Transformers,,,,1,,222,2724.051204,,https://huggingface.co/lvwerra/bert-imdb,BERT (bert-large-cased) trained for sentiment classification on the IMDB dataset.; The model was trained on 80% of the IMDB dataset for sentiment classification for three epochs with a learning rate of 1e-5 with the simpletransformers library. The library uses a learning rate schedule.; The model achieved 90% classification accuracy on the validation set.; The full experiment is available in the tlr repo.
distilbert-imdb,Text Classification,PyTorch; TensorBoard; Transformers,,apache-2.0,,8,imdb,"120,327",268.694751,1,https://huggingface.co/lvwerra/distilbert-imdb,"This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset (training notebook is here).
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
distilbert-political-tweets,Text Classification,PyTorch; TensorFlow; Transformers,English,lgpl-3.0,,12,m-newhauser/senator-tweets,52,536.6856671,1,https://huggingface.co/m-newhauser/distilbert-political-tweets,"This model is a fine-tuned version of distilbert-base-uncased on the m-newhauser/senator-tweets dataset, which contains all tweets made by United States senators during the first year of the Biden Administration.
It achieves the following results on the evaluation set:; The goal of this model is to classify short pieces of text as having either Democratic or Republican sentiment. The model was fine-tuned on 99,693 tweets (51.6% Democrat, 48.4% Republican) made by US senators in 2021.; Model accuracy may not hold up on pieces of text longer than a tweet.; The following hyperparameters were used during training:"
marefa-ner,Token Classification,PyTorch; Transformers,Arabic,,,10,Marefa-NER,"5,734",2302.874741,,https://huggingface.co/marefa-nlp/marefa-ner,"

; Version: 1.3; Last Update: 3-12-2021; Marefa-NER is a Large Arabic Named Entity Recognition (NER) model built on a completely new dataset and targets to extract up to 9 different types of entities; ????? ??????? ?????? ????? ????. ????? ???? ???? ?? ??? ???????? ????????? ?? ????? ???????. 
???? ?????? ??????? ????? ??? 9 ????? ?????? ?? ????? ????"
toxic-comment-model,Text Classification,PyTorch; Transformers,English,,,17,,"108,050",268.687061,1,https://huggingface.co/martin-ha/toxic-comment-model,"This model is a fine-tuned version of the DistilBERT model to classify toxic comments. ; You can use the model with the following code.; This model is intended to use for classify toxic online classifications. However, one limitation of the model is that it performs poorly for some comments that mention a specific identity subgroup, like Muslim. The following table shows a evaluation score for different identity group. You can learn the specific meaning of this metrics here. But basically, those metrics shows how well a model performs for a specific group. The larger the number, the better.; The table above shows that the model performs poorly for the muslim and jewish group. In fact, you pass the sentence ""Muslims are people who follow or practice Islam, an Abrahamic monotheistic religion."" Into the model, the model will classify it as toxic. Be mindful for this type of potential bias.; The training data comes this Kaggle competition. We use 10% of the train.csv data to train the model."
korean_sentiment,Text Classification,PyTorch; Transformers,,,,5,,"2,337",499.1619898,1,https://huggingface.co/matthewburke/korean_sentiment,
recipenlg,Text Generation,PyTorch; JAX; Transformers,,,,2,,321,1164.325644,1,https://huggingface.co/mbien/recipenlg,"Model accompanying our INLG 2020 paper: RecipeNLG: A Cooking Recipes Dataset for Semi-Structured Text Generation; Please visit the website of our project: recipenlg.cs.put.poznan.pl to download it.; Yes, sure! If you feel some information is missing in our paper, please check first in our thesis, which is much more detailed. In case of further questions, you're invited to send us a github issue, we will respond as fast as we can!"
german-news-sentiment-bert,Text Classification,PyTorch; JAX; Safetensors; Transformers,,,,4,,"1,145",1308.252622,,https://huggingface.co/mdraw/german-news-sentiment-bert,"Sentiment analysis model based on https://huggingface.co/oliverguhr/german-sentiment-bert, with additional training on German news texts about migration.; This model is part of the project https://github.com/text-analytics-20/news-sentiment-development, which explores sentiment development in German news articles about migration between 2007 and 2019.; Code for inference (predicting sentiment polarity) on raw text can be found at https://github.com/text-analytics-20/news-sentiment-development/blob/main/sentiment_analysis/bert.py; If you are not interested in polarity but just want to predict discrete class labels (0: positive, 1: negative, 2: neutral), you can also use the model with Oliver Guhr's germansentiment package as follows:; First install the package from PyPI:"
flair-arabic-multi-ner,Token Classification,PyTorch; Flair,Arabic; English,apache-2.0,,2,AQMAR; ANERcorp,766,550.4958729,,https://huggingface.co/megantosh/flair-arabic-multi-ner,"Training was conducted over 94 epochs, using a linear decaying learning rate of 2e-05, starting from 0.225 and a batch size of 32 with GloVe and Flair forward and backward embeddings.;  Due to the right-to-left in left-to-right context, some formatting errors might occur. and your code might appear like this, (link accessed on 2020-10-27) ; if you use this model, please consider citing this work:"
DialoGPT-large,Conversational,PyTorch; TensorFlow; JAX; Transformers,,mit,https://arxiv.org/pdf/1911.00536.pdf,177,,"45,489",8142.288943,93,https://huggingface.co/microsoft/DialoGPT-large,"DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. 
The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test.
The model is trained on 147M multi-turn dialogue from Reddit discussion thread. ; Please find the information about preprocessing, training and full details of the DialoGPT in the original DialoGPT repository; ArXiv paper: https://arxiv.org/abs/1911.00536; Now we are ready to try out how the model works as a chatting partner!"
DialoGPT-small,Conversational,PyTorch; TensorFlow; JAX; Safetensors; Transformers,,mit,https://arxiv.org/pdf/1911.00536.pdf,52,,"28,586",1699.488994,23,https://huggingface.co/microsoft/DialoGPT-small,"DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. 
The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test.
The model is trained on 147M multi-turn dialogue from Reddit discussion thread. ; Please find the information about preprocessing, training and full details of the DialoGPT in the original DialoGPT repository; ArXiv paper: https://arxiv.org/abs/1911.00536; Now we are ready to try out how the model works as a chatting partner!"
beit-base-patch16-384,Image Classification,PyTorch; JAX; Transformers,,apache-2.0,https://arxiv.org/pdf/2106.08254.pdf,3,imagenet; imagenet-21k,707,726.0747209,,https://huggingface.co/microsoft/beit-base-patch16-384,"BEiT model pre-trained in a self-supervised fashion on ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository. ; Disclaimer: The team releasing BEiT did not write a model card for this model so this model card has been written by the Hugging Face team.; The BEiT model is a Vision Transformer (ViT), which is a transformer encoder model (BERT-like). In contrast to the original ViT model, BEiT is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. The pre-training objective for the model is to predict visual tokens from the encoder of OpenAI's DALL-E's VQ-VAE, based on masked patches.
Next, the model was fine-tuned in a supervised fashion on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. Contrary to the original ViT models, BEiT models do use relative position embeddings (similar to T5) instead of absolute position embeddings, and perform classification of images by mean-pooling the final hidden states of the patches, instead of placing a linear layer on top of the final hidden state of the [CLS] token.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image. Alternatively, one can mean-pool the final hidden states of the patch embeddings, and place a linear layer on top of that."
codebert-base,Feature Extraction,PyTorch; TensorFlow; JAX; Rust; Transformers,,,https://arxiv.org/pdf/2002.08155.pdf,110,,"777,087",1997.325448,5,https://huggingface.co/microsoft/codebert-base,Pretrained weights for CodeBERT: A Pre-Trained Model for Programming and Natural Languages.; The model is trained on bi-modal data (documents & code) of CodeSearchNet; This model is initialized with Roberta-base and trained with MLM+RTD objective (cf. the paper).; Please see the official repository for scripts that support "code search" and "code-to-document generation".
deberta-base,Fill-Mask,PyTorch; TensorFlow; Rust; Transformers,English,mit,https://arxiv.org/pdf/2006.03654.pdf,46,,"5,003,278",1678.245342,4,https://huggingface.co/microsoft/deberta-base,"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. ; Please check the official repository for more details and updates.; We present the dev results on SQuAD 1.1/2.0 and MNLI tasks.; If you find DeBERTa useful for your work, please cite the following paper:"
deberta-v2-xlarge,Fill-Mask,PyTorch; TensorFlow; Transformers,English,mit,https://arxiv.org/pdf/2006.03654.pdf,14,,"100,247",5450.13513,1,https://huggingface.co/microsoft/deberta-v2-xlarge,"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data.; Please check the official repository for more details and updates.; This is the DeBERTa V2 xlarge model with 24 layers, 1536 hidden size. The total parameters are 900M and it is trained with 160GB raw data.; We present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.; If you find DeBERTa useful for your work, please cite the following paper:"
graphcodebert-base,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,,,https://arxiv.org/pdf/2009.08366.pdf,21,,"55,448",1656.325914,1,https://huggingface.co/microsoft/graphcodebert-base,"GraphCodeBERT is a graph-based pre-trained model based on the Transformer architecture for programming language, which also considers data-flow information along with code sequences. GraphCodeBERT consists of 12 layers, 768 dimensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512. The model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages. ; More details can be found in the paper by Guo et. al.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by the Hugging Face community members."
layoutlm-base-uncased,,PyTorch; TensorFlow; Transformers,,,https://arxiv.org/pdf/1912.13318.pdf,24,,"3,081,133",904.6856748,3,https://huggingface.co/microsoft/layoutlm-base-uncased,"Multimodal (text + layout/format + image) pre-training for document AI; Microsoft Document AI | GitHub; LayoutLM is a simple but effective pre-training method of text and layout for document image understanding and information extraction tasks, such as form understanding and receipt understanding. LayoutLM archives the SOTA results on multiple datasets. For more details, please refer to our paper: ; LayoutLM: Pre-training of Text and Layout for Document Image Understanding
Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, KDD 2020; We pre-train LayoutLM on IIT-CDIP Test Collection 1.0* dataset with two settings. "
layoutlm-large-uncased,,PyTorch; TensorFlow; Transformers,,,https://arxiv.org/pdf/1912.13318.pdf,7,,"1,067",2785.964246,,https://huggingface.co/microsoft/layoutlm-large-uncased,"Multimodal (text + layout/format + image) pre-training for document AI; Microsoft Document AI | GitHub; LayoutLM is a simple but effective pre-training method of text and layout for document image understanding and information extraction tasks, such as form understanding and receipt understanding. LayoutLM archives the SOTA results on multiple datasets. For more details, please refer to our paper: ; LayoutLM: Pre-training of Text and Layout for Document Image Understanding
Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, KDD 2020; We pre-train LayoutLM on IIT-CDIP Test Collection 1.0* dataset with two settings. "
layoutxlm-base,,PyTorch; Transformers,,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2104.08836.pdf,39,,"167,222",1529.693711,,https://huggingface.co/microsoft/layoutxlm-base,"Multimodal (text + layout/format + image) pre-training for document AI; LayoutXLM is a multilingual variant of LayoutLMv2.; The documentation of this model in the Transformers library can be found here.; Microsoft Document AI | GitHub; LayoutXLM is a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding. Experiment results show that it has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset."
markuplm-large,,PyTorch; Transformers,English,,https://arxiv.org/pdf/2110.08518.pdf,12,,866,753.343167,,https://huggingface.co/microsoft/markuplm-large,"Multimodal (text +markup language) pre-training for Document AI; MarkupLM is a simple but effective multi-modal pre-training method of text and markup language for visually-rich document understanding and information extraction tasks, such as webpage QA and webpage information extraction. MarkupLM archives the SOTA results on multiple datasets. For more details, please refer to our paper:; MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding  Junlong Li, Yiheng Xu, Lei Cui, Furu Wei; We refer to the docs and demo notebooks.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
trocr-base-printed,Image-to-Text,PyTorch; Transformers,,,https://arxiv.org/pdf/2109.10282.pdf,86,,"30,041",1363.253567,85,https://huggingface.co/microsoft/trocr-base-printed,"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you."
trocr-base-stage1,Image-to-Text,PyTorch; Transformers,,,https://arxiv.org/pdf/2109.10282.pdf,6,,"2,352",1578.293195,1,https://huggingface.co/microsoft/trocr-base-stage1,"TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you."
trocr-large-handwritten,Image-to-Text,PyTorch; Transformers,,,https://arxiv.org/pdf/2109.10282.pdf,27,,"6,512",2284.853567,7,https://huggingface.co/microsoft/trocr-large-handwritten,"TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you."
trocr-large-printed,Image-to-Text,PyTorch; Transformers,,,https://arxiv.org/pdf/2109.10282.pdf,19,,"19,860",2489.653645,5,https://huggingface.co/microsoft/trocr-large-printed,"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you."
wavlm-base-plus-sv,,PyTorch; Transformers,English,,https://arxiv.org/pdf/1912.07875.pdf; https://arxiv.org/pdf/2106.06909.pdf; https://arxiv.org/pdf/2101.00390.pdf; https://arxiv.org/pdf/2110.13900.pdf,11,,"10,247",405.062539,5,https://huggingface.co/microsoft/wavlm-base-plus-sv,"Microsoft's WavLM; The model was pretrained on 16kHz sampled speech audio with utterance and speaker contrastive loss. When using the model, make sure that your speech input is also sampled at 16kHz. ; Note: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.; The model was pre-trained on:; Paper: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing"
anekdot_funny2_rugpt3Small,Text Generation,PyTorch; JAX; Transformers,,,,1,,14,1205.983659,,https://huggingface.co/mmm-da/anekdot_funny2_rugpt3Small,No model card; New: Create and edit this model card directly on the website!
bert-base-cased-goemotions-original,,PyTorch; Transformers,,,,5,,"109,446",433.2104092,,https://huggingface.co/monologg/bert-base-cased-goemotions-original,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
bert-base-spanish-wwm-cased-finetuned-spa-squad2-es,Question Answering,PyTorch; JAX; Transformers,Spanish,,,4,,"1,118",1612.261853,2,https://huggingface.co/mrm8488/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es,"This model is provided by BETO team and fine-tuned on SQuAD-es-v2.0 for Q&A downstream task.; Language model ('dccuchile/bert-base-spanish-wwm-cased'):; BETO is a BERT model trained on a big Spanish corpus. BETO is of size similar to a BERT-Base and was trained with the Whole Word Masking technique. Below you find Tensorflow and Pytorch checkpoints for the uncased and cased versions, as well as some results for Spanish benchmarks comparing BETO with Multilingual BERT as well as other (not BERT-based) models.; SQuAD-es-v2.0; The model was trained on a Tesla P100 GPU and 25GB of RAM with the following command:"
distilbert-base-multi-cased-finetuned-typo-detection,Token Classification,PyTorch; Safetensors; Transformers,11 languages,,,5,,46,1078.976544,,https://huggingface.co/mrm8488/distilbert-base-multi-cased-finetuned-typo-detection,distilbert-base-multilingual-cased fine-tuned on GitHub Typo Corpus for typo detection (using NER style); Dataset: GitHub Typo Corpus ? for 15 languages; Fine-tune script on NER dataset provided by Huggingface ???♂?; Fast usage with pipelines ?; It works?! We typed wrong Add and middleware
distilroberta-finetuned-financial-news-sentiment-analysis,Text Classification,PyTorch; TensorBoard; Safetensors; Transformers,,apache-2.0,,67,financial_phrasebank,"90,616",659.5919138,8,https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis,"This model is a fine-tuned version of distilroberta-base on the financial_phrasebank dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
t5-base-finetuned-summarize-news,Text2Text Generation,PyTorch; JAX; Safetensors; Transformers,English,,https://arxiv.org/pdf/1910.10683.pdf,27,,"265,666",2676.783207,2,https://huggingface.co/mrm8488/t5-base-finetuned-summarize-news,"All credits to Abhishek Kumar Mishra; Google's T5 base fine-tuned on News Summary dataset for summarization downstream task.; The T5 model was presented in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu in Here the abstract:; Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.; "
bert-base-portuguese-cased,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,Portuguese,mit,,78,brWaC,"368,010",1403.209733,9,https://huggingface.co/neuralmind/bert-base-portuguese-cased,"; BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.; For further information or requests, please go to BERTimbau repository.; If you use our work, please cite:"
fb-bart-large-finetuned-trade-the-event-finance-summarizer,Summarization,PyTorch; TensorBoard; Transformers,,,,5,,251,1672.46336,,https://huggingface.co/nickmuchi/fb-bart-large-finetuned-trade-the-event-finance-summarizer,"This model was trained from scratch on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
deformable-detr,Object Detection,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2010.04159.pdf,9,coco,"12,226",161.0104955,1,https://huggingface.co/SenseTime/deformable-detr,"Deformable DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository. ; Disclaimer: The team releasing Deformable DETR did not write a model card for this model so this model card has been written by the Hugging Face team.; The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. ; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.; "
glpn-kitti,Depth Estimation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2201.07436.pdf,4,,"28,601",245.0113303,3,https://huggingface.co/vinvino02/glpn-kitti,"Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository. ; Disclaimer: The team releasing GLPN did not write a model card for this model so this model card has been written by the Hugging Face team.; GLPN uses SegFormer as backbone and adds a lightweight head on top for depth estimation.; ; You can use the raw model for monocular depth estimation. See the model hub to look for
fine-tuned versions on a task that interests you."
legal-bert-base-uncased,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,English,cc-by-sa-4.0,,69,,"61,596",1416.229647,14,https://huggingface.co/nlpaueb/legal-bert-base-uncased,"LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications.  To pre-train the different variations of LEGAL-BERT, we collected 12 GB of diverse English legal text from several fields (e.g., legislation, court cases,  contracts) scraped from publicly available resources. Sub-domain variants (CONTRACTS-, EURLEX-, ECHR-) and/or general LEGAL-BERT perform better than using BERT out of the box for domain-specific tasks. A light-weight model (33% the size of BERT-BASE) pre-trained from scratch on legal data with competitive performance is also available.
; I. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Aletras and I. Androutsopoulos. ""LEGAL-BERT: The Muppets straight out of Law School"". In Findings of Empirical Methods in Natural Language Processing (EMNLP 2020) (Short Papers), to be held online, 2020. (https://aclanthology.org/2020.findings-emnlp.261); The pre-training corpora of LEGAL-BERT include:; 116,062 documents of EU legislation, publicly available from EURLEX (http://eur-lex.europa.eu), the repository of EU Law running under the EU Publication Office.; 61,826 documents of UK legislation, publicly available from the UK legislation portal (http://www.legislation.gov.uk)."
legal-led-base-16384,Summarization,PyTorch; Transformers,English,mit,,4,,91,649.2400813,,https://huggingface.co/nsi319/legal-led-base-16384,"This is a Longformer Encoder Decoder (led-base-16384) model for the legal domain, trained for long document abstractive summarization task. The length of the document can be upto 16,384 tokens.; The legal-led-base-16384 model was trained on sec-litigation-releases dataset consisting more than 2700 litigation releases and complaints.; When the model is used for summarizing legal documents, it achieves the following results:; Inference API has been turned off for this model."
mit-b0,Image Classification,PyTorch; TensorFlow; Transformers,,other,https://arxiv.org/pdf/2105.15203.pdf,14,imagenet_1k,"150,409",29.07254829,14,https://huggingface.co/nvidia/mit-b0,"SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. ; Disclaimer: The team releasing SegFormer did not write a model card for this model so this model card has been written by the Hugging Face team.; SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.; This repository only contains the pre-trained hierarchical Transformer, hence it can be used for fine-tuning purposes.; You can use the model for fine-tuning of semantic segmentation. See the model hub to look for fine-tuned versions on a task that interests you."
segformer-b5-finetuned-cityscapes-1024-1024,Image Segmentation,PyTorch; TensorFlow; Transformers,,other,https://arxiv.org/pdf/2105.15203.pdf,7,cityscapes,"8,103",679.0061197,2,https://huggingface.co/nvidia/segformer-b5-finetuned-cityscapes-1024-1024,"SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. ; Disclaimer: The team releasing SegFormer did not write a model card for this model so this model card has been written by the Hugging Face team.; SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.; You can use the raw model for semantic segmentation. See the model hub to look for fine-tuned versions on a task that interests you.; Here is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:"
deid_roberta_i2b2,Token Classification,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/1907.11692.pdf,4,I2B2,"81,649",1456.673922,4,https://huggingface.co/obi/deid_roberta_i2b2,Steps on how this model was trained can be found here: Training. The "model_name_or_path" was set to: "roberta-large".; Training details:; Post a Github issue on the repo: Robust DeID.
clip-vit-base-patch32,Zero-Shot Image Classification,PyTorch; TensorFlow; JAX; Transformers,,,https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/1908.04913.pdf,209,,"3,741,682",1819.58821,203,https://huggingface.co/openai/clip-vit-base-patch32,"Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found here.; The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.; January 2021; The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. ; The original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer."
imagegpt-large,,PyTorch; Transformers,,apache-2.0,,25,imagenet-21k,42,5632.052744,,https://huggingface.co/openai/imagegpt-large,"ImageGPT (iGPT) model pre-trained on ImageNet ILSVRC 2012 (14 million images, 21,843 classes) at resolution 32x32. It was introduced in the paper Generative Pretraining from Pixels by Chen et al. and first released in this repository. See also the official blog post.; Disclaimer: The team releasing ImageGPT did not write a model card for this model so this model card has been written by the Hugging Face team.; The ImageGPT (iGPT) is a transformer decoder model (GPT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-21k, at a resolution of 32x32 pixels. ; The goal for the model is simply to predict the next pixel value, given the previous ones.; By pre-training the model, it learns an inner representation of images that can then be used to:"
imagegpt-small,,PyTorch; Transformers,,apache-2.0,,9,imagenet-21k,"16,348",332.0527422,1,https://huggingface.co/openai/imagegpt-small,"ImageGPT (iGPT) model pre-trained on ImageNet ILSVRC 2012 (14 million images, 21,843 classes) at resolution 32x32. It was introduced in the paper Generative Pretraining from Pixels by Chen et al. and first released in this repository. See also the official blog post.; Disclaimer: The team releasing ImageGPT did not write a model card for this model so this model card has been written by the Hugging Face team.; The ImageGPT (iGPT) is a transformer decoder model (GPT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-21k, at a resolution of 32x32 pixels. ; The goal for the model is simply to predict the next pixel value, given the previous ones.; By pre-training the model, it learns an inner representation of images that can then be used to:"
bert-large-cased-squad-v1.1-portuguese,Question Answering,PyTorch; TensorFlow; Transformers,Portuguese,mit,,26,brWaC; squad; squad_v1_pt,"7,637",2724.053373,3,https://huggingface.co/pierreguillou/bert-large-cased-squad-v1.1-portuguese,"; The model was trained on the dataset SQUAD v1.1 in portuguese from the Deep Learning Brasil group. ; The language model used is the BERTimbau Large (aka ""bert-large-portuguese-cased"") from Neuralmind.ai: BERTimbau is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.; All the informations are in the blog post : NLP | Como treinar um modelo de Question Answering em qualquer linguagem baseado no BERT large, melhorando o desempenho do modelo utilizando o BERT base? (estudo de caso em português); question_answering_BERT_large_cased_squad_v11_pt.ipynb (nbviewer version)"
ner-bert-base-cased-pt-lenerbr,Token Classification,PyTorch; Transformers,Portuguese,,,3,lener_br,85,433.6487082,1,https://huggingface.co/pierreguillou/ner-bert-base-cased-pt-lenerbr,"ner-bert-base-portuguese-cased-lenerbr is a NER model (token classification) in the legal domain in Portuguese that was finetuned on 20/12/2021 in Google Colab from the model pierreguillou/bert-base-cased-pt-lenerbr on the dataset LeNER_br by using a NER objective.; Due to the small size of BERTimbau base and finetuning dataset, the model overfitted before to reach the end of training. Here are the overall final metrics on the validation dataset (note: see the paragraph ""Validation metrics by Named Entity"" to get detailed metrics):; Check as well the large version of this model with a f1 of 0.908.; Note: the model pierreguillou/bert-base-cased-pt-lenerbr is a language model that was created through the finetuning of the model BERTimbau base on the dataset LeNER-Br language modeling by using a MASK objective. This first specialization of the language model before finetuning on the NER task improved a bit the model quality. To prove it, here are the results of the NER model finetuned from the model BERTimbau base (a non-specialized language model):; NLP | Modelos e Web App para Reconhecimento de Entidade Nomeada (NER) no domínio jurídico brasileiro (29/12/2021)"
t5-base-qa-squad-v1.1-portuguese,Text2Text Generation,PyTorch; Transformers,Portuguese,,,14,squad; squad_v1_pt,158,894.1047422,1,https://huggingface.co/pierreguillou/t5-base-qa-squad-v1.1-portuguese,"; t5-base-qa-squad-v1.1-portuguese is a QA model (Question Answering) in Portuguese that was finetuned on 27/01/2022 in Google Colab from the model unicamp-dl/ptt5-base-portuguese-vocab of Neuralmind on the dataset SQUAD v1.1 in portuguese from the Deep Learning Brasil group by using a Test2Text-Generation objective.; Due to the small size of T5 base and finetuning dataset, the model overfitted before to reach the end of training. Here are the overall final metrics on the validation dataset:; Check our other QA models in Portuguese finetuned on SQUAD v1.1:; NLP nas empresas | Como eu treinei um modelo T5 em português na tarefa QA no Google Colab (27/01/2022)"
mpnet-retriever-discourse,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,,,,1,,17,438.6880975,,https://huggingface.co/pinecone/mpnet-retriever-discourse,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used as a retriever model in open-domain question-answering tasks.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; The model was fine-tuned on question-answer pairs scraper from several ML-focused Discourse forums [HuggingFace, PyTorch, Streamlit, TensorFlow]."
raceBERT,Text Classification,PyTorch; Transformers,,,,2,,454,174.0096095,,https://huggingface.co/pparasurama/raceBERT,No model card; New: Create and edit this model card directly on the website!
bert-tiny,,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/1908.08962.pdf; https://arxiv.org/pdf/2110.01518.pdf,62,,"2,133,479",18.02966331,,https://huggingface.co/prajjwal1/bert-tiny,"The following model is a Pytorch pre-trained model obtained from converting Tensorflow checkpoint found in the official Google BERT repository. ; This is one of the smaller pre-trained BERT variants, together with bert-mini bert-small and bert-medium. They were introduced in the study Well-Read Students Learn Better: On the Importance of Pre-training Compact Models (arxiv), and ported to HF for the study Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics (arXiv). These models are supposed to be trained on a downstream task.; If you use the model, please consider citing both the papers:; Config of this model:; Other models to check out:"
S-PubMedBert-MS-MARCO-SCIFACT,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,,,,5,,42,438.6777928,,https://huggingface.co/pritamdeka/S-PubMedBert-MS-MARCO-SCIFACT,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net"
grammar_error_correcter_v1,Text2Text Generation,PyTorch; Transformers,,,,30,,"35,544",894.1715811,8,https://huggingface.co/prithivida/grammar_error_correcter_v1,This model is part of the Gramformer library please refer to https://github.com/PrithivirajDamodaran/Gramformer/
parrot_paraphraser_on_T5,Text2Text Generation,PyTorch; Transformers,,,,117,,"258,151",892.7876014,16,https://huggingface.co/prithivida/parrot_paraphraser_on_T5,"Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. A paraphrase framework is more than just a paraphrasing model. For more  details on the library and usage please refer to the github page; Huggingface lists 12 paraphrase models, RapidAPI lists 7 fremium and commercial paraphrasers like QuillBot, Rasa has discussed an experimental paraphraser for augmenting text data here, Sentence-transfomers offers a paraphrase mining utility and NLPAug offers word level augmentation with a PPDB (a multi-million paraphrase database). While these attempts at paraphrasing are great, there are still some gaps and paraphrasing is NOT yet a mainstream option for text augmentation in building NLU models....Parrot is a humble attempt to fill some of these gaps.; What is a good paraphrase? Almost all conditioned text generation models are validated  on 2 factors, (1) if the generated text conveys the same meaning as the original context (Adequacy) (2) if the text is fluent / grammatically correct english (Fluency). For instance Neural Machine Translation outputs are tested for Adequacy and Fluency. But a good paraphrase should be adequate and fluent while being as different as possible on the surface lexical form. With respect to this definition, the  3 key metrics that measures the quality of paraphrases are:; Parrot offers knobs to control Adequacy, Fluency and Diversity as per your needs.; What makes a paraphraser a good augmentor? For training a NLU model we just don't need a lot of utterances but utterances with intents and slots/entities annotated. Typical flow would be:"
led-base-book-summary,Summarization,PyTorch; Safetensors; Transformers,,bsd-3-clause,,35,kmfoda/booksum,"15,335",1299.386208,9,https://huggingface.co/pszemraj/led-base-book-summary,"The Longformer Encoder-Decoder (LED) for Narrative-Esque Long Text Summarization is a model I fine-tuned from allenai/led-base-16384 to condense extensive technical, academic, and narrative content in a fairly generalizable way.; Note: The API widget has a max length of ~96 tokens due to inference timeout constraints. ; The model was trained on the BookSum dataset released by SalesForce, which leads to the bsd-3-clause license. The training process involved 16 epochs with parameters tweaked to facilitate very fine-tuning-type training (super low learning rate). ; Model checkpoint: pszemraj/led-base-16384-finetuned-booksum. ; This model is the smallest/fastest booksum-tuned model I have worked on. If you're looking for higher quality summaries, check out:"
led-large-book-summary,Summarization,PyTorch; Safetensors; Transformers,English,bsd-3-clause,https://arxiv.org/pdf/2105.08209.pdf,32,kmfoda/booksum,"2,231",3771.693155,4,https://huggingface.co/pszemraj/led-large-book-summary,"This model is a fine-tuned version of allenai/led-large-16384 on the BookSum dataset (kmfoda/booksum). It aims to generalize well and be useful in summarizing lengthy text for both academic and everyday purposes. ; Note: Due to inference API timeout constraints, outputs may be truncated before the fully summary is returned (try python or the demo); To improve summary quality, use encoder_no_repeat_ngram_size=3 when calling the pipeline object. This setting encourages the model to utilize new vocabulary and construct an abstractive summary.; Load the model into a pipeline object:; Feed the text into the pipeline object:"
yacis-electra-small-japanese,,PyTorch; Transformers,Japanese,cc-by-sa-4.0,,7,YACIS corpus,11,238.5834031,,https://huggingface.co/ptaszynski/yacis-electra-small-japanese,"This is ELECTRA Small model for Japanese pretrained on 354 million sentences / 5.6 billion words of YACIS blog corpus.; The corpus was tokenized for pretraining with MeCab. Subword tokenization was done with WordPiece. ; This model uses ELECTRA Small model settings, 12 layers, 128 dimensions of hidden states, and 12 attention heads.; Vocabulary size was set to 32,000 tokens.; YACIS-ELECTRA is trained on the whole of YACIS blog corpus, which is a Japanese blog corpus containing 5.6 billion words in 354 million sentences."
biobertpt-all,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,Portuguese,,,10,,855,2138.978418,,https://huggingface.co/pucpr/biobertpt-all,"The BioBERTpt - A Portuguese Neural Language Model for Clinical Named Entity Recognition paper contains clinical and biomedical BERT-based models for Portuguese Language, initialized with BERT-Multilingual-Cased & trained on clinical notes and biomedical literature. ; This model card describes the BioBERTpt(all) model, a full version with clinical narratives and biomedical literature in Portuguese language. ; Load the model via the transformers library:; Refer to the original paper, BioBERTpt - A Portuguese Neural Language Model for Clinical Named Entity Recognition for additional details and performance on Portuguese NER tasks.; This study was financed in part by the Coordena??o de Aperfei?oamento de Pessoal de Nível Superior - Brasil (CAPES) - Finance Code 001."
bert-base-qarib,Fill-Mask,PyTorch; JAX; Transformers; TensorFlow,Arabic,,https://arxiv.org/pdf/2102.10684.pdf,3,arabic_billion_words; open_subtitles; twitter,312,2758.619504,,https://huggingface.co/qarib/bert-base-qarib,"QCRI Arabic and Dialectal BERT  (QARiB) model, was trained on a collection of ~ 420 Million tweets and ~ 180 Million sentences of text.
For the tweets, the data was collected using twitter API and using language filter. lang:ar. For the text data, it was a combination from
Arabic GigaWord, Abulkhair Arabic Corpus and OPUS.; QARiB: Is the Arabic name for ""Boat"".; See details in Training QARiB; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you. For more details, see Using QARiB; You can use this model directly with a pipeline for masked language modeling:"
classical-chinese-punctuation-guwen-biaodian,Token Classification,PyTorch; Transformers,Chinese,,,10,,155,407.3759022,1,https://huggingface.co/raynardj/classical-chinese-punctuation-guwen-biaodian,"欢迎前往我的github文言诗词项目页面探讨、加?? ， Please check the github repository for more about the model, hit ? if you like; This model punctuates Classical(ancient) Chinese, you might feel strange about this task, but many of my ancestors think writing articles without punctuation is brilliant idea ?. What we have here are articles from books, letters or carved on stones where you can see no punctuation, just a long string of characters. As you can guess, NLP tech is usually a good tool to tackle this problem, and the entire pipeline can be borrowed from usual NER task.; Since there are also many articles are punctuated, hence with some regex operations, labeled data is more than abundant ?. That's why this problem is pretty much a low hanging fruit.; so I guess who's interested in the problem set can speak at least modern Chinese, hence... let me continue the documentation in Chinese.; 输入一串未断句文言文， 可以断句， 目前支持二十多种标点符号"
keywords-cangtou-chinese-poetry,,PyTorch; Transformers,Chinese,,,7,,40,425.3772498,,https://huggingface.co/raynardj/keywords-cangtou-chinese-poetry,"This is a model to generated Chinese poetry with leading characters and certain tune of mood.; 这个模型充分利用了gpt2论文的精髓， 论文标题为《语言模型即学万事万物》， 也就是许许多多的学习任务， 可以安排成文本序列的形式，来管理输入输出， 即模型如能根据 「所有自然常数的导数是0， 0的cos是1 ，」算出后面的句子应该是「 四个1相加的阶乘是4， 4的阶乘是24」也就学会了二十四点。 模型在训练上只做了猜测语言序列的任务， 但会兼通万物。; 这个码诗模型就是这么来的， 训练任务， 是输入0~10来个关键词+藏头标题+藏头字数+把头换成分类符[CLS]之后的诗句。; 感谢liangtongt指出Inference 代码运行时可能会发生的bug.; 大家下了模型,可以自己玩耍。
却也可以尝尝我替大家摘的樱桃?"
ner-gene-dna-rna-jnlpba-pubmed,Token Classification,PyTorch; Transformers,English,apache-2.0,,5,jnlpba,101,498.5898109,,https://huggingface.co/raynardj/ner-gene-dna-rna-jnlpba-pubmed,"The model was trained on jnlpba dataset, pretrained on this pubmed-pretrained roberta model; All the labels, the possible token classes.; Notice, we removed the 'B-','I-' etc from data label.?; And here is to make your output more consecutive ??; check our NER model on"
SciFive-large-Pubmed_PMC,Text Classification,PyTorch; TensorFlow; Transformers,English,,https://arxiv.org/pdf/2106.03598.pdf,9,pubmed; pmc/open_access,363,6043.766018,,https://huggingface.co/razent/SciFive-large-Pubmed_PMC,"Paper: SciFive: a text-to-text transformer model for biomedical literature; Authors: Long N. Phan, James T. Anibal, Hieu Tran, Shaurya Chanana, Erol Bahadroglu, Alec Peltekian, Grégoire Altan-Bonnet; For more details, do check out our Github repo. "
openai-clip-js,,ONNX,,,,6,,0,760.5312057,1,https://huggingface.co/rocca/openai-clip-js,"Info here: https://github.com/josephrocca/openai-clip-js; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
query_wellformedness_score,Text Classification,PyTorch; JAX; Transformers,,apache-2.0,,6,google_wellformed_query,"3,755,802",1001.233476,,https://huggingface.co/salesken/query_wellformedness_score,"This model evaluates the wellformedness (non-fragment, grammatically correct)  score of a sentence. Model is case-sensitive and penalises for incorrect case and grammar as well. ; ['She is presenting a paper tomorrow','she is presenting a paper tomorrow','She present paper today']; [[0.8917],[0.4270],[0.0134]]"
rudalle-Emojich,,PyTorch,,,,11,,0,2682.883555,,https://huggingface.co/ai-forever/rudalle-Emojich,"; Model was trained by Sber AI; ; ? Emojich is a 1.3 billion params model from the family GPT3-like, it generates emoji-style images with the brain of ? Malevich.; The main goal of fine-tuning is trying to keep the generalization of ruDALL-E Malevich (XL)
model on text to emoji tasks. ruDALL-E Malevich is a multi-modality big pretrained transformer, that uses images and texts.
The idea with freezing feedforward and self-attention layers in pretrained transformer is demonstrated high performance in changing different modalities.
Also, the model has a good chance for over-fitting text modality and lost generalization. 
To deal with this problem is increased coefficient 10^3 in weighted cross-entropy loss for image codebooks part. "
rugpt3large_based_on_gpt2,Text Generation,PyTorch; JAX; Transformers,Russian,,,47,,"9,677",6331.301525,3,https://huggingface.co/ai-forever/rugpt3large_based_on_gpt2,Model was trained with sequence length 1024 using transformers lib by SberDevices team on 80B tokens for 3 epochs. After that model was finetuned 1 epoch with sequence length 2048. ; Total training time was around 14 days on 128 GPUs for 1024 context and few days on 16 GPUs for 2048 context.Final perplexity on test set is 13.6.
rugpt3medium_based_on_gpt2,,PyTorch; Transformers,Russian,,,12,,"1,452",1774.401471,1,https://huggingface.co/ai-forever/rugpt3medium_based_on_gpt2,"Model was trained with sequence length 1024 using transformers lib by SberDevices team on 80B tokens for 3 epoch. After that model was finetuned on 2048 context.; Total training time was around 16 days on 64 GPUs.Final perplexity on test set is 17.4.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
sbert_large_nlu_ru,Feature Extraction,PyTorch; JAX; Transformers,Russian,,,19,,"7,807",3503.863297,,https://huggingface.co/ai-forever/sbert_large_nlu_ru,"The model is described in this articleFor better quality, use mean token embeddings.; You can use the model directly from the model repository to compute sentence embeddings:"
polish-roberta-large-v2,Fill-Mask,PyTorch; Safetensors; Transformers,Polish,lgpl-3.0,,4,,582,3572.112457,1,https://huggingface.co/sdadas/polish-roberta-large-v2,
distilbert-dot-tas_b-b256-msmarco,Feature Extraction,PyTorch; Transformers,English,,https://arxiv.org/pdf/2104.06967.pdf,19,ms_marco,"10,541",265.2610588,2,https://huggingface.co/sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco,"We provide a retrieval trained DistilBert-based model (we call the dual-encoder then dot-product scoring architecture BERT_Dot) trained with Balanced Topic Aware Sampling on MSMARCO-Passage.; This instance was trained with a batch size of 256 and can be used to re-rank a candidate set or directly for a vector index based dense retrieval. The architecture is a 6-layer DistilBERT, without architecture additions or modifications (we only change the weights during training) - to receive a query/passage representation we pool the CLS vector. We use the same BERT layers for both query and passage encoding (yields better results, and lowers memory requirements).; If you want to know more about our efficient (can be done on a single consumer GPU in 48 hours) batch composition procedure and dual supervision for dense retrieval training, check out our paper: https://arxiv.org/abs/2104.06967 ?; For more information and a minimal usage example please visit: https://github.com/sebastian-hofstaetter/tas-balanced-dense-retrieval; We trained our model on the MSMARCO standard (""small""-400K query) training triples re-sampled with our TAS-B method. As teacher models we used the BERT_CAT pairwise scores as well as the ColBERT model for in-batch-negative signals published here: https://github.com/sebastian-hofstaetter/neural-ranking-kd"
LaBSE,Sentence Similarity,PyTorch; TensorFlow; JAX; Sentence Transformers; Transformers,,apache-2.0,,71,,"53,019",5790.203827,8,https://huggingface.co/sentence-transformers/LaBSE,"This is a port of the LaBSE model to PyTorch. It can be used to map 109 languages to a shared vector space.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net; Have a look at LaBSE for the respective publication that describes LaBSE."
clip-ViT-B-32,Sentence Similarity,Sentence Transformers,,,https://arxiv.org/pdf/2103.00020.pdf,27,,0,0.002720947,1,https://huggingface.co/sentence-transformers/clip-ViT-B-32,"This is the Image & Text model CLIP, which maps text and images to a shared vector space. For applications of the models, have a look in our documentation SBERT.net - Image Search; After installing sentence-transformers (pip install sentence-transformers), the usage of this model is easy:; See our SBERT.net - Image Search documentation for more examples how the model can be used for image search, zero-shot image classification, image clustering and image deduplication.; In the following table we find the zero-shot ImageNet validation set accuracy:; For a multilingual version of the CLIP model for 50+ languages have a look at: clip-ViT-B-32-multilingual-v1"
distilbert-base-nli-stsb-mean-tokens,Sentence Similarity,PyTorch; TensorFlow; Sentence Transformers; Transformers,,apache-2.0,https://arxiv.org/pdf/1908.10084.pdf,10,,"47,551",531.6873886,8,https://huggingface.co/sentence-transformers/distilbert-base-nli-stsb-mean-tokens,"?? This model is deprecated. Please don't use it as it produces sentence embeddings of low quality. You can find recommended sentence embedding models here: SBERT.net - Pretrained Models; This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings."
gtr-t5-base,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,English,apache-2.0,https://arxiv.org/pdf/2112.07899.pdf,9,,"3,264",221.2606782,3,https://huggingface.co/sentence-transformers/gtr-t5-base,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of sematic search.; This model was converted from the Tensorflow model gtr-base-1 to PyTorch. When using this model, have a look at the publication: Large Dual Encoders Are Generalizable Retrievers. The tfhub model and this PyTorch model can produce slightly different embeddings, however, when run on the same benchmarks, they produce identical results.; The model uses only the encoder from a T5-base model. The weights are stored in FP16.  ; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:"
gtr-t5-xxl,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,English,apache-2.0,https://arxiv.org/pdf/2112.07899.pdf,17,,586,9965.692013,3,https://huggingface.co/sentence-transformers/gtr-t5-xxl,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of sematic search.; This model was converted from the Tensorflow model gtr-xxl-1 to PyTorch. When using this model, have a look at the publication: Large Dual Encoders Are Generalizable Retrievers. The tfhub model and this PyTorch model can produce slightly different embeddings, however, when run on the same benchmarks, they produce identical results.; The model uses only the encoder from a T5-11B model. The weights are stored in FP16.  ; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:"
msmarco-MiniLM-L6-cos-v5,Sentence Similarity,PyTorch; TensorFlow; JAX; Sentence Transformers; Transformers,,,https://arxiv.org/pdf/1908.10084.pdf,6,,"9,802",273.4893978,,https://huggingface.co/sentence-transformers/msmarco-MiniLM-L6-cos-v5,"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and was designed for semantic search. It has been trained on 500k (query, answer) pairs from the MS MARCO Passages dataset. For an introduction to semantic search, have a look at: SBERT.net - Semantic Search; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.; In the following some technical details how this model must be used:"
msmarco-distilbert-base-tas-b,Sentence Similarity,PyTorch; TensorFlow; Sentence Transformers; Transformers,English,apache-2.0,,18,ms_marco,"25,064",531.6877315,4,https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b,"This is a port of the DistilBert TAS-B Model to sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net"
multi-qa-MiniLM-L6-cos-v1,Sentence Similarity,PyTorch; TensorFlow; Sentence Transformers,,,,66,flax-sentence-embeddings/stackexchange_xml; ms_marco; gooaq; yahoo_answers_topics; search_qa; eli5; natural_questions; trivia_qa; embedding-data/QQP; embedding-data/PAQ_pairs; embedding-data/Amazon-QA; embedding-data/WikiAnswers,"180,485",182.6335026,12,https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1,"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources. For an introduction to semantic search, have a look at: SBERT.net - Semantic Search; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.; Similarly to the PyTorch example above, to use the model with TensorFlow you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings."
multi-qa-distilbert-cos-v1,Sentence Similarity,PyTorch; Sentence Transformers,,,,12,flax-sentence-embeddings/stackexchange_xml; ms_marco; gooaq; yahoo_answers_topics; search_qa; eli5; natural_questions; trivia_qa; embedding-data/QQP; embedding-data/PAQ_pairs; embedding-data/Amazon-QA; embedding-data/WikiAnswers,"19,620",265.7313778,,https://huggingface.co/sentence-transformers/multi-qa-distilbert-cos-v1,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources. For an introduction to semantic search, have a look at: SBERT.net - Semantic Search; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.; In the following some technical details how this model must be used:"
LaBSE,Feature Extraction,PyTorch; TensorFlow; JAX; Safetensors; Transformers,109 languages,apache-2.0,https://arxiv.org/pdf/2007.01852.pdf,36,CommonCrawl; Wikipedia,"8,049",7719.305802,6,https://huggingface.co/setu4993/LaBSE,"Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.; This is migrated from the v2 model on the TF Hub, which uses dict-based input. The embeddings produced by both the versions of the model are equivalent.; Using the model:; To get the sentence embeddings, use the pooler output:; Output for other languages:"
smaller-LaBSE,Feature Extraction,PyTorch; TensorFlow; JAX; Safetensors; Transformers,15 languages,apache-2.0,https://arxiv.org/pdf/2010.05609.pdf; https://arxiv.org/pdf/2007.01852.pdf,10,CommonCrawl; Wikipedia,"2,183",3513.785681,,https://huggingface.co/setu4993/smaller-LaBSE,"Smaller Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model distilled from the original LaBSE model to 15 languages (from the original 109 languages) using the techniques described in the paper 'Load What You Need: Smaller Versions of Multilingual BERT' by Ukjae Jeong.; Using the model:; To get the sentence embeddings, use the pooler output:; Output for other languages:; For similarity between sentences, an L2-norm is recommended before calculating the similarity:"
bert-multitask-query-classifiers,Text Classification,PyTorch; Transformers,,,,4,,185,45.39175343,,https://huggingface.co/shahrukhx01/bert-multitask-query-classifiers,"Quora Keyword Pairs: https://www.kaggle.com/stefanondisponibile/quora-question-keyword-pairs
Spaadia SQuaD pairs: https://www.kaggle.com/shahrukhkhan/questions-vs-statementsclassificationdataset; Medium article; Colab Notebook Multi-task Query classifiers"
macbert4csc-base-chinese,Fill-Mask,PyTorch; Safetensors; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/2004.13922.pdf,43,,"37,719",818.2514425,2,https://huggingface.co/shibing624/macbert4csc-base-chinese,中文拼写纠错模型; macbert4csc-base-chinese evaluate SIGHAN2015 test data：; 由于训练使用的数据使用了SIGHAN2015的训练集（复现paper），在SIGHAN2015的测试集上达到SOTA水平。; 模型结构，魔改于softmaskedbert：; 
bart_summarisation,Summarization,PyTorch; Transformers,English,apache-2.0,,33,samsum,"36,679",1671.89483,,https://huggingface.co/slauw87/bart_summarisation,"This model was trained using Amazon SageMaker and the new Hugging Face Deep Learning container.
For more information look at:; }"
t5-base-japanese,Feature Extraction,PyTorch; JAX; Transformers,Japanese,cc-by-sa-4.0,,26,wikipedia; oscar; cc100,"9,551",1784.795129,1,https://huggingface.co/sonoisa/t5-base-japanese,"This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.; 次の日本Zコ`パス（s100GB）を用いて事前学を行ったT5 (Text-to-Text Transfer Transformer) モデルです。  ; このモデルは事前学のみを行なったものであり、特定のタスクに利用するにはファインチュ`ニングする必要があります。本モデルにも、大模コ`パスを用いた言Zモデルにつきまとう、学デ`タの内容の偏りに由来する偏った（理的ではなかったり、有害だったり、バイアスがあったりする）出力Y果になる}が潜在的にあります。
この}がk生しうることを想定した上で、被害がk生しない用途にのみ利用するよう荬颏膜堡皮ださい。; SentencePieceト`クナイザ`の学には上Wikipediaの全デ`タを用いました。; https://github.com/sonoisa/t5-japanese"
en_core_web_sm,Token Classification,spaCy,English,mit,,22,,"1,139",12.90449665,6,https://huggingface.co/spacy/en_core_web_sm,"English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer."
wav2vec2-2-bert-large-no-adapter-frozen-enc,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,,,,2,librispeech_asr,32,3083.827871,,https://huggingface.co/speech-seq2seq/wav2vec2-2-bert-large-no-adapter-frozen-enc,"This model was trained from scratch on the librispeech_asr dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
asr-wav2vec2-transformer-aishell,Automatic Speech Recognition,speechbrain; PyTorch,English,apache-2.0,https://arxiv.org/pdf/2106.04624.pdf,6,aishell,40,1358.108806,,https://huggingface.co/speechbrain/asr-wav2vec2-transformer-aishell,"This repository provides all the necessary tools to perform automatic speech
recognition from an end-to-end system pretrained on AISHELL +wav2vec2 (Mandarin Chinese)
within SpeechBrain. For a better experience, we encourage you to learn more about
SpeechBrain.; The performance of the model is the following:; This ASR system is composed of 2 different but linked blocks:; To Train this system from scratch, see our SpeechBrain recipe.; The system is trained with recordings sampled at 16kHz (single channel).
The code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling transcribe_file if needed."
lang-id-voxlingua107-ecapa,Audio Classification,speechbrain; PyTorch,108 languages,apache-2.0,https://arxiv.org/pdf/2106.04624.pdf,33,VoxLingua107,"3,524",85.25976543,2,https://huggingface.co/speechbrain/lang-id-voxlingua107-ecapa,"This is a spoken language recognition model trained on the VoxLingua107 dataset using SpeechBrain.
The model uses the ECAPA-TDNN architecture that has previously been used for speaker recognition. However, it uses
more fully connected hidden layers after the embedding layer, and cross-entropy loss was used for training. 
We observed that this improved the performance of extracted utterance embeddings for downstream tasks.; The system is trained with recordings sampled at 16kHz (single channel).
The code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling classify_file if needed.; The model can classify a speech utterance according to the language spoken.
It covers 107 different languages (
Abkhazian, 
Afrikaans, 
Amharic, 
Arabic, 
Assamese, 
Azerbaijani, 
Bashkir, 
Belarusian, 
Bulgarian, 
Bengali, 
Tibetan, 
Breton, 
Bosnian, 
Catalan, 
Cebuano, 
Czech, 
Welsh, 
Danish, 
German, 
Greek, 
English, 
Esperanto, 
Spanish, 
Estonian, 
Basque, 
Persian, 
Finnish, 
Faroese, 
French, 
Galician, 
Guarani, 
Gujarati, 
Manx, 
Hausa, 
Hawaiian, 
Hindi, 
Croatian, 
Haitian, 
Hungarian, 
Armenian, 
Interlingua, 
Indonesian, 
Icelandic, 
Italian, 
Hebrew, 
Japanese, 
Javanese, 
Georgian, 
Kazakh, 
Central Khmer, 
Kannada, 
Korean, 
Latin, 
Luxembourgish, 
Lingala, 
Lao, 
Lithuanian, 
Latvian, 
Malagasy, 
Maori, 
Macedonian, 
Malayalam, 
Mongolian, 
Marathi, 
Malay, 
Maltese, 
Burmese, 
Nepali, 
Dutch, 
Norwegian Nynorsk, 
Norwegian, 
Occitan, 
Panjabi, 
Polish, 
Pushto, 
Portuguese, 
Romanian, 
Russian, 
Sanskrit, 
Scots, 
Sindhi, 
Sinhala, 
Slovak, 
Slovenian, 
Shona, 
Somali, 
Albanian, 
Serbian, 
Sundanese, 
Swedish, 
Swahili, 
Tamil, 
Telugu, 
Tajik, 
Thai, 
Turkmen, 
Tagalog, 
Turkish, 
Tatar, 
Ukrainian, 
Urdu, 
Uzbek, 
Vietnamese, 
Waray, 
Yiddish, 
Yoruba, 
Mandarin Chinese).; The model has two uses:; The model is trained on automatically collected YouTube data. For more 
information about the dataset, see here."
metricgan-plus-voicebank,Audio-to-Audio,speechbrain,English,apache-2.0,https://arxiv.org/pdf/2106.04624.pdf,27,Voicebank; DEMAND,"5,249",7.865350113,24,https://huggingface.co/speechbrain/metricgan-plus-voicebank,"This repository provides all the necessary tools to perform enhancement with
SpeechBrain. For a better experience we encourage you to learn more about
SpeechBrain. The model performance is:; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; To use the mimic-loss-trained model for enhancement, use the following simple code:; The system is trained with recordings sampled at 16kHz (single channel).
The code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling enhance_file if needed. Make sure your input tensor is compliant with the expected sampling rate if you use enhance_batch as in the example."
sepformer-wham-enhancement,Audio-to-Audio,speechbrain; PyTorch,English,apache-2.0,https://arxiv.org/pdf/2010.13154.pdf; https://arxiv.org/pdf/2106.04624.pdf,6,WHAM!,221,318.2064217,,https://huggingface.co/speechbrain/sepformer-wham-enhancement,"This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k. For a better experience we encourage you to learn more about SpeechBrain. The given model performance is 14.35 dB SI-SNR on the test set of WHAM! dataset.; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about SpeechBrain.; To perform inference on the GPU, add  run_opts={""device"":""cuda""}  when calling the from_hparams method.; The training script is currently being worked on an ongoing pull-request. "
CoreNLP,,,English,gpl-2.0,,17,,0,506.0014839,,https://huggingface.co/stanfordnlp/CoreNLP,"CoreNLP is your one stop shop for natural language processing in Java! CoreNLP enables users to derive linguistic annotations for text, including token and sentence boundaries, parts of speech, named entities, numeric and time values, dependency and constituency parses, coreference, sentiment, quote attributions, and relations.
Find more about it in our website and our GitHub repository.; This card and repo were automatically prepared with hugging_corenlp.py in the stanfordnlp/huggingface-models repo; Last updated 2023-03-16 01:06:26.193; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
stanza-id,Token Classification,Stanza,Indonesian,apache-2.0,,1,,282,0.001803703,,https://huggingface.co/stanfordnlp/stanza-id,"Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages. Starting from raw text to syntactic analysis and entity recognition, Stanza brings state-of-the-art NLP models to languages of your choosing.
Find more about it in our website and our GitHub repository.; This card and repo were automatically prepared with hugging_stanza.py in the stanfordnlp/huggingface-models repo; Last updated 2023-05-26 18:04:22.253"
codeparrot,Text Generation,PyTorch; TensorBoard; Transformers,,,,4,,59,6319.667446,,https://huggingface.co/transformersbook/codeparrot,CodeParrot (large) is a 1.5B parameter GPT-2 model trained on the CodeParrot Python code dataset. The model is trained in Chapter 10: Training Transformers from Scratch in the NLP with Transformers book. You can find the full code in the accompanying Github repository.
distilbert-base-uncased-mnli,Zero-Shot Classification,PyTorch; TensorFlow; Safetensors; Transformers,English,,https://arxiv.org/pdf/1910.09700.pdf; https://arxiv.org/pdf/2105.09680.pdf,29,multi_nli,"94,007",804.2643747,5,https://huggingface.co/typeform/distilbert-base-uncased-mnli,"Model Description:  This is the uncased DistilBERT model fine-tuned on Multi-Genre Natural Language Inference (MNLI) dataset for the zero-shot classification task. ; This model can be used for text classification tasks.; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).; This model of DistilBERT-uncased is pretrained on the Multi-Genre Natural Language Inference (MultiNLI) corpus. It is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation."
chinese_roberta_L-12_H-768,Fill-Mask,PyTorch; TensorFlow; JAX; Transformers,Chinese,,https://arxiv.org/pdf/1909.05658.pdf; https://arxiv.org/pdf/1908.08962.pdf,8,CLUECorpusSmall,704,1296.118756,,https://huggingface.co/uer/chinese_roberta_L-12_H-768,"This is the set of 24 Chinese RoBERTa models pre-trained by UER-py, which is introduced in this paper.; Turc et al. have shown that the standard BERT recipe is effective on a wide range of model sizes. Following their paper, we released the 24 Chinese RoBERTa models. In order to facilitate users to reproduce the results, we used the publicly available corpus and provided all training details.; You can download the 24 Chinese RoBERTa miniatures either from the UER-py Modelzoo page, or via HuggingFace from the links below:; Here are scores on the devlopment set of six Chinese tasks:; For each task, we selected the best fine-tuning hyperparameters from the lists below, and trained with the sequence length of 128:"
roberta-base-finetuned-dianping-chinese,Text Classification,PyTorch; TensorFlow; JAX; Transformers,Chinese,,https://arxiv.org/pdf/1909.05658.pdf; https://arxiv.org/pdf/1708.02657.pdf,15,,"2,139",1227.114154,,https://huggingface.co/uer/roberta-base-finetuned-dianping-chinese,"This is the set of 5 Chinese RoBERTa-Base classification models fine-tuned by UER-py. You can download the 5 Chinese RoBERTa-Base classification models either from the UER-py Modelzoo page (in UER-py format), or via HuggingFace from the links below:; You can use this model directly with a pipeline for text classification (take the case of roberta-base-finetuned-chinanews-chinese):; 5 Chinese text classification datasets are used. JD full, JD binary, and Dianping datasets consist of user reviews of different sentiment polarities. Ifeng and Chinanews consist of first paragraphs of news articles of different topic classes. They are collected by Glyph project and more details are discussed in corresponding paper.; Models are fine-tuned by UER-py on Tencent Cloud. We fine-tune three epochs with a sequence length of 512 on the basis of the pre-trained model chinese_roberta_L-12_H-768. At the end of each epoch, the model is saved when the best performance on development set is achieved. We use the same hyper-parameters on different models.; Taking the case of roberta-base-finetuned-chinanews-chinese"
roberta-base-finetuned-jd-binary-chinese,Text Classification,PyTorch; TensorFlow; JAX; Transformers,Chinese,,https://arxiv.org/pdf/1909.05658.pdf; https://arxiv.org/pdf/1708.02657.pdf,14,,735,1227.114154,1,https://huggingface.co/uer/roberta-base-finetuned-jd-binary-chinese,"This is the set of 5 Chinese RoBERTa-Base classification models fine-tuned by UER-py. You can download the 5 Chinese RoBERTa-Base classification models either from the UER-py Modelzoo page (in UER-py format), or via HuggingFace from the links below:; You can use this model directly with a pipeline for text classification (take the case of roberta-base-finetuned-chinanews-chinese):; 5 Chinese text classification datasets are used. JD full, JD binary, and Dianping datasets consist of user reviews of different sentiment polarities. Ifeng and Chinanews consist of first paragraphs of news articles of different topic classes. They are collected by Glyph project and more details are discussed in corresponding paper.; Models are fine-tuned by UER-py on Tencent Cloud. We fine-tune three epochs with a sequence length of 512 on the basis of the pre-trained model chinese_roberta_L-12_H-768. At the end of each epoch, the model is saved when the best performance on development set is achieved. We use the same hyper-parameters on different models.; Taking the case of roberta-base-finetuned-chinanews-chinese"
gottbert-base,Fill-Mask,PyTorch; JAX; Safetensors; Transformers,,,https://arxiv.org/pdf/2012.02110.pdf,12,,"1,133",1519.496451,2,https://huggingface.co/uklfr/gottbert-base,"BERT model trained solely on the German portion of the OSCAR data set.; Paper: GottBERT: a pure German Language Model; Authors: Raphael Scheible, Fabian Thomczyk, Patric Tippmann, Victor Jaravine, Martin Boeker"
toxic-bert,Text Classification,PyTorch; JAX; Transformers,,,https://arxiv.org/pdf/1703.04009.pdf; https://arxiv.org/pdf/1905.12516.pdf,65,,"27,677",876.2388214,9,https://huggingface.co/unitary/toxic-bert,"?? Disclaimer:
The huggingface models currently give different results to the detoxify library (see issue here). For the most up to date models we recommend using the models from https://github.com/unitaryai/detoxify; 
; ; Trained models & code to predict toxic comments on 3 Jigsaw challenges: Toxic comment classification, Unintended?Bias in Toxic comments, Multilingual toxic comment classification.; Built by Laura Hanu at Unitary, where we are working to stop harmful content online by interpreting visual content in context. "
t5-base-e2e-qg,Text2Text Generation,PyTorch; Transformers,,mit,https://arxiv.org/pdf/1910.10683.pdf,18,squad,"231,191",892.7794897,1,https://huggingface.co/valhalla/t5-base-e2e-qg,"This is t5-base model trained for end-to-end question generation task. Simply input the text and the model will generate multile questions. ; You can play with the model using the inference API, just put the text and see the results!; For more deatils see this repo.; You'll need to clone the repo.; "
en_readability,Text Classification,spaCy,English,,,6,,63,6.405283203,,https://huggingface.co/valurank/en_readability,A Spacy pipeline for generating readability scores
flax-bigbird-natural-questions,,JAX; Transformers,English,apache-2.0,,2,natural_questions,6,603.3309973,2,https://huggingface.co/vasudevgupta/flax-bigbird-natural-questions,"This checkpoint is obtained after training FlaxBigBirdForQuestionAnswering (with extra pooler head) on natural_questions dataset on TPU v3-8. This dataset takes around ~100 GB on disk. But thanks to Cloud TPUs and Jax, each epoch took just 4.5 hours. Script for training can be found here: https://github.com/vasudevgupta7/bigbird; Use this model just like any other model from ?Transformers; In case you are interested in predicting category (null, long, short, yes, no) as well, use FlaxBigBirdForNaturalQuestions (instead of FlaxBigBirdForQuestionAnswering) from my training script.; Evaluation script: https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/evaluate-flax-natural-questions.ipynb; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
hinglish-bert,Fill-Mask,PyTorch; JAX; Safetensors; Transformers,,,,1,,74,2140.975781,,https://huggingface.co/nirantk/hinglish-bert,No model card; New: Create and edit this model card directly on the website!
BART0,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,,6,bigscience/P3,991,1671.711518,,https://huggingface.co/yuchenlin/BART0,"A BART-large version of T0. 
Please check https://inklab.usc.edu/ReCross/ for more details. "
singbert-large-sg,,PyTorch; TensorFlow; JAX; Transformers,English,mit,,3,"reddit singapore, malaysia; hardwarezone",31,4260.06632,,https://huggingface.co/zanelim/singbert-large-sg,"SingBert Large - Bert for Singlish (SG) and Manglish (MY).; Similar to SingBert but the large version, which was initialized from BERT large uncased (whole word masking), with pre-training finetuned on
singlish and manglish data.; Here is how to use this model to get the features of a given text in PyTorch:; and in TensorFlow:; This model was finetuned on colloquial Singlish and Manglish corpus, hence it is best applied on downstream tasks involving the main
constituent languages- english, mandarin, malay. Also, as the training data is mainly from forums, beware of existing inherent bias."
dit-base,,PyTorch; Transformers,,,https://arxiv.org/pdf/2203.02378.pdf,7,,"27,323",369.0055419,,https://huggingface.co/microsoft/dit-base,"Document Image Transformer (DiT) model pre-trained on IIT-CDIP (Lewis et al., 2006), a dataset that includes 42 million document images. It was introduced in the paper DiT: Self-supervised Pre-training for Document Image Transformer by Li et al. and first released in this repository. Note that DiT is identical to the architecture of BEiT. ; Disclaimer: The team releasing DiT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Document Image Transformer (DiT) is a transformer encoder model (BERT-like) pre-trained on a large collection of images in a self-supervised fashion. The pre-training objective for the model is to predict visual tokens from the encoder of a discrete VAE (dVAE), based on masked patches.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled document images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder."
dit-large,,PyTorch; Transformers,,,https://arxiv.org/pdf/2203.02378.pdf,7,,448,1280.005542,,https://huggingface.co/microsoft/dit-large,"Document Image Transformer (DiT) model pre-trained on IIT-CDIP (Lewis et al., 2006), a dataset that includes 42 million document images. It was introduced in the paper DiT: Self-supervised Pre-training for Document Image Transformer by Li et al. and first released in this repository. Note that DiT is identical to the architecture of BEiT. ; Disclaimer: The team releasing DiT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Document Image Transformer (DiT) is a transformer encoder model (BERT-like) pre-trained on a large collection of images in a self-supervised fashion. The pre-training objective for the model is to predict visual tokens from the encoder of a discrete VAE (dVAE), based on masked patches.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled document images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder."
dit-base-finetuned-rvlcdip,Image Classification,PyTorch; Transformers,,,https://arxiv.org/pdf/2203.02378.pdf,16,rvl_cdip,"6,020",348.2966845,4,https://huggingface.co/microsoft/dit-base-finetuned-rvlcdip,"Document Image Transformer (DiT) model pre-trained on IIT-CDIP (Lewis et al., 2006), a dataset that includes 42 million document images and fine-tuned on RVL-CDIP, a dataset consisting of 400,000 grayscale images in 16 classes, with 25,000 images per class. It was introduced in the paper DiT: Self-supervised Pre-training for Document Image Transformer by Li et al. and first released in this repository. Note that DiT is identical to the architecture of BEiT. ; Disclaimer: The team releasing DiT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Document Image Transformer (DiT) is a transformer encoder model (BERT-like) pre-trained on a large collection of images in a self-supervised fashion. The pre-training objective for the model is to predict visual tokens from the encoder of a discrete VAE (dVAE), based on masked patches.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled document images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder."
Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net,Image Segmentation,Keras,,,,4,SerdarHelli/SegmentationOfTeethPanoramicXRayImages,31,1.119306641,23,https://huggingface.co/SerdarHelli/Segmentation-of-Teeth-in-Panoramic-X-ray-Image-Using-U-Net,"The aim of this study is automatic semantic segmentation and measurement total length of teeth in one-shot panoramic x-ray image by using deep learning method with U-Net Model and binary image analysis in order to provide diagnostic information for the management of dental disorders, diseases, and conditions. ;  Github Link; Original Dataset; DATASET ref - 	H. Abdi, S. Kasaei, and M. Mehdizadeh, “Automatic segmentation of mandible in panoramic x-ray,” J. Med. Imaging, vol. 2, no. 4, p. 44003, 2015; Link DATASET for only original images."
trocr-handwritten-math,,PyTorch; Transformers,,,,3,,28,247.0057064,,https://huggingface.co/Azu/trocr-handwritten-math,"This model generate the math expression LATEX sequence according to the handwritten math expression image.; in CROHME 2014 test dataset CER=0.507772718700326; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
sentence-camembert-base,Sentence Similarity,PyTorch; Transformers,French,apache-2.0,https://arxiv.org/pdf/1908.10084.pdf,6,stsb_multi_mt,"3,903",445.2051061,1,https://huggingface.co/dangvantuan/sentence-camembert-base,"Model is Fine-tuned using pre-trained facebook/camembert-base and
Siamese BERT-Networks with 'sentences-transformers' on dataset stsb; The model can be used directly (without a language model) as follows:; The model can be evaluated as follows on the French test data of stsb.; Test Result: 
The performance is measured using Pearson and Spearman correlation:"
BioBert-PubMed200kRCT,Text Classification,PyTorch; TensorBoard; Safetensors; Transformers,,,,4,,"1,452",867.297787,,https://huggingface.co/pritamdeka/BioBert-PubMed200kRCT,"This model is a fine-tuned version of dmis-lab/biobert-base-cased-v1.1 on the PubMed200kRCT dataset.
It achieves the following results on the evaluation set:; More information needed; The model can be used for text classification tasks of Randomized Controlled Trials that does not have any structure. The text can be classified as one of the following:; The model can be directly used like this:; Results will be shown as follows:"
resnet-50,Image Classification,PyTorch; TensorFlow; JAX; Transformers,,apache-2.0,https://arxiv.org/pdf/1512.03385.pdf,116,imagenet-1k,"3,491,872",308.0719529,42,https://huggingface.co/microsoft/resnet-50,"ResNet model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ; Disclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.; ResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models.; This is ResNet v1.5, which differs from the original model: in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec) according to Nvidia.; "
poem-gen-gpt2-small-spanish,Text Generation,PyTorch; TensorBoard; Transformers,,apache-2.0,,1,,8,510.0064424,,https://huggingface.co/DrishtiSharma/poem-gen-gpt2-small-spanish,"This model is a fine-tuned version of datificate/gpt2-small-spanish on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
gpt-neo-125M-spanish-classics,Text Generation,PyTorch; Transformers,,,,1,,17,551.002207,,https://huggingface.co/Aleksandar1932/gpt-neo-125M-spanish-classics,No model card; New: Create and edit this model card directly on the website!
keyphrase-extraction-distilbert-inspec,Token Classification,PyTorch; Transformers,English,mit,,16,midas/inspec,"3,901",266.9389211,2,https://huggingface.co/ml6team/keyphrase-extraction-distilbert-inspec,"Keyphrase extraction is a technique in text analysis where you extract the important keyphrases from a document. Thanks to these keyphrases humans can understand the content of a text very quickly and easily without reading it completely. Keyphrase extraction was first done primarily by human annotators, who read the text in detail and then wrote down the most important keyphrases. The disadvantage is that if you work with a lot of documents, this process can take a lot of time ?. ; Here is where Artificial Intelligence ? comes in. Currently, classical machine learning methods, that use statistical and linguistic features, are widely used for the extraction process. Now with deep learning, it is possible to capture the semantic meaning of a text even better than these classical methods. Classical methods look at the frequency, occurrence and order of words in the text, whereas these neural approaches can capture long-term semantic dependencies and context of words in a text.; This model uses distilbert as its base model and fine-tunes it on the Inspec dataset.; Keyphrase extraction models are transformer models fine-tuned as a token classification problem where each word in the document is classified as being part of a keyphrase or not.; Kulkarni, Mayank, Debanjan Mahata, Ravneet Arora, and Rajarshi Bhowmik. ""Learning Rich Representation of Keyphrases from Text."" arXiv preprint arXiv:2112.08547 (2021)."
sbert-chinese-general-v2,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,,,,9,,"2,251",409.5425123,,https://huggingface.co/DMetaSoul/sbert-chinese-general-v2,此模型基于 bert-base-chinese 版本 BERT 模型，在百万级语义相似数据集 SimCLUE 上进行训练，适用于通用语义匹配场景，从效果来看该模型在各种任务上泛化能力更好。; 注：此模型的轻量化版本，也已经开源啦！; 通过  sentence-transformers 框架来使用该模型，首先进行安装：; 然后使用下面的代码来载入该模型并进行文本表征向量的提取：; 如果不想使用   sentence-transformers 的话，也可以通过 HuggingFace Transformers 来载入该模型并进行文本向量抽取：
keyphrase-extraction-kbir-inspec,Token Classification,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2112.08547.pdf,42,midas/inspec,"97,509",1457.433641,4,https://huggingface.co/ml6team/keyphrase-extraction-kbir-inspec,"Keyphrase extraction is a technique in text analysis where you extract the important keyphrases from a document. Thanks to these keyphrases humans can understand the content of a text very quickly and easily without reading it completely. Keyphrase extraction was first done primarily by human annotators, who read the text in detail and then wrote down the most important keyphrases. The disadvantage is that if you work with a lot of documents, this process can take a lot of time ?. ; Here is where Artificial Intelligence ? comes in. Currently, classical machine learning methods, that use statistical and linguistic features, are widely used for the extraction process. Now with deep learning, it is possible to capture the semantic meaning of a text even better than these classical methods. Classical methods look at the frequency, occurrence and order of words in the text, whereas these neural approaches can capture long-term semantic dependencies and context of words in a text.; This model uses KBIR as its base model and fine-tunes it on the Inspec dataset. KBIR or Keyphrase Boundary Infilling with Replacement is a pre-trained model which utilizes a multi-task learning setup for optimizing a combined loss of Masked Language Modeling (MLM), Keyphrase Boundary Infilling (KBI) and Keyphrase Replacement Classification (KRC).
You can find more information about the architecture in this paper.; Keyphrase extraction models are transformer models fine-tuned as a token classification problem where each word in the document is classified as being part of a keyphrase or not.; Kulkarni, Mayank, Debanjan Mahata, Ravneet Arora, and Rajarshi Bhowmik. ""Learning Rich Representation of Keyphrases from Text."" arXiv preprint arXiv:2112.08547 (2021)."
vit-base-patch16-224-in21k-finetuned-cifar10,Image Classification,PyTorch; TensorBoard; Transformers,,apache-2.0,,5,cifar10,"1,261",343.0132629,,https://huggingface.co/aaraki/vit-base-patch16-224-in21k-finetuned-cifar10,"This model is a fine-tuned version of google/vit-base-patch16-224-in21k on the cifar10 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
sbert-chinese-general-v2-distill,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,,,,3,,67,182.5431711,,https://huggingface.co/DMetaSoul/sbert-chinese-general-v2-distill,此模型是之前开源通用语义匹配模型的蒸馏版本（仅4层 BERT），适用于通用语义匹配场景，从效果来看该模型在各种任务上泛化能力更好且编码速度更快。; 离线训练好的大模型如果直接用于线上推理，对计算资源有苛刻的需求，而且难以满足业务环境对延迟、吞吐量等性能指标的要求，这里我们使用蒸馏手段来把大模型轻量化。从 12 层 BERT 蒸馏为 4 层后，模型参数量缩小到 44%，大概 latency 减半、throughput 翻倍、精度下降 6% 左右（具体结果详见下文评估小节）。; 通过  sentence-transformers 框架来使用该模型，首先进行安装：; 然后使用下面的代码来载入该模型并进行文本表征向量的提取：; 如果不想使用   sentence-transformers 的话，也可以通过 HuggingFace Transformers 来载入该模型并进行文本向量抽取：
roberta-base-stocktwits-finetuned,Text Classification,PyTorch; Transformers,English,apache-2.0,,18,,325,500.3297791,,https://huggingface.co/zhayunduo/roberta-base-stocktwits-finetuned,"This model is fine tuned with roberta-base model on 3200000 comments from stocktwits, with the user labeled tags 'Bullish' or 'Bearish'; try something that the individual investors may say on the investment forum on the inference API, for example, try 'red' and 'green'.; code on github"
stt_en_conformer_ctc_large,Automatic Speech Recognition,NeMo; PyTorch,English,cc-by-4.0,https://arxiv.org/pdf/2005.08100.pdf,19,librispeech_asr; fisher_corpus; Switchboard-1; WSJ-0; WSJ-1; National-Singapore-Corpus-Part-1; National-Singapore-Corpus-Part-6; vctk; VoxPopuli-(EN); Europarl-ASR-(EN); Multilingual-LibriSpeech-(2000-hours); mozilla-foundation/common_voice_7_0,374,451.011123,2,https://huggingface.co/nvidia/stt_en_conformer_ctc_large,"| 
| 
| 
|  |; This model transcribes speech in lowercase English alphabet including spaces and apostrophes, and is trained on several thousand hours of English speech data.
It is a non-autoregressive ""large"" variant of Conformer, with around 120 million parameters.
See the model architecture section and NeMo documentation for complete architecture details.
It is also compatible with NVIDIA Riva for production-grade server deployments. ; The model is available for use in the NeMo toolkit [3], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.; To train, fine-tune or play with the model you will need to install NVIDIA NeMo. We recommend you install it after you've installed latest PyTorch version.; First, let's get a sample"
PHS-BERT,Fill-Mask,PyTorch; Transformers,,,https://arxiv.org/pdf/2204.04521.pdf,5,,157,1372.790199,1,https://huggingface.co/publichealthsurveillance/PHS-BERT,"We present and release PHS-BERT, a transformer-based pretrained language model (PLM), to identify tasks related to public health surveillance (PHS) on social media. Compared with existing PLMs that are mainly evaluated on limited tasks, PHS-BERT achieved state-of-the-art performance on 25 tested datasets, showing that our PLM is robust and generalizable in common PHS tasks.; Load the model via Huggingface's Transformers library:; We followed the standard pretraining protocols of BERT and initialized PHS-BERT with weights from BERT during the training phase instead of training from scratch and used the uncased version of the BERT model.; PHS-BERT is trained on a corpus of health-related tweets that were crawled via the Twitter API. Focusing on the tasks related to PHS, keywords used to collect pretraining corpus are set to disease, symptom, vaccine, and mental health-related words in English. Retweet tags were deleted from the raw corpus, and URLs and usernames were replaced with HTTP-URL and @USER, respectively. All emoticons were replaced with their associated meanings. ; Each sequence of BERT LM inputs is converted to 50,265 vocabulary tokens. Twitter posts are restricted to 200 characters, and during the training and evaluation phase, we used a batch size of 8. Distributed training was performed on a TPU v3-8."
clip-ViT-L-14,Sentence Similarity,Sentence Transformers,,,https://arxiv.org/pdf/2103.00020.pdf,28,,0,0.003256226,5,https://huggingface.co/sentence-transformers/clip-ViT-L-14,"This is the Image & Text model CLIP, which maps text and images to a shared vector space. For applications of the models, have a look in our documentation SBERT.net - Image Search; After installing sentence-transformers (pip install sentence-transformers), the usage of this model is easy:; See our SBERT.net - Image Search documentation for more examples how the model can be used for image search, zero-shot image classification, image clustering and image deduplication.; In the following table we find the zero-shot ImageNet validation set accuracy:; For a multilingual version of the CLIP model for 50+ languages have a look at: clip-ViT-B-32-multilingual-v1"
codegen-16B-mono,Text Generation,PyTorch; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2203.13474.pdf,105,,"3,472",32976.14094,13,https://huggingface.co/Salesforce/codegen-16B-mono,"CodeGen is a family of autoregressive language models for program synthesis from the paper: A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B).; The checkpoint included in this repository is denoted as CodeGen-Mono 16B in the paper, where ""Mono"" means the model is initialized with CodeGen-Multi 16B and further pre-trained on a Python programming language dataset, and ""16B"" refers to the number of trainable parameters.; This checkpoint (CodeGen-Mono 16B) was firstly initialized with CodeGen-Multi 16B, and then pre-trained on BigPython dataset. The data consists of 71.7B tokens of Python programming language. See Section 2.1 of the paper for more details.; CodeGen was trained using cross-entropy loss to maximize the likelihood of sequential inputs.
The family of models are trained using multiple TPU-v4-512 by Google, leveraging data and model parallelism.
See Section 2.3 of the paper for more details.; We evaluate our models on two code generation benchmark: HumanEval and MTPB. Please refer to the paper for more details."
BiomedNLP-KRISSBERT-PubMed-UMLS-EL,Feature Extraction,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2112.07887.pdf,13,,497,438.2281087,,https://huggingface.co/microsoft/BiomedNLP-KRISSBERT-PubMed-UMLS-EL,"https://arxiv.org/pdf/2112.07887.pdf; Entity linking faces significant challenges such as prolific variations and prevalent ambiguities, especially in high-value domains with myriad entities. Standard classification approaches suffer from the annotation bottleneck and cannot effectively handle unseen entities. Zero-shot entity linking has emerged as a promising direction for generalizing to new entities, but it still requires example gold entity mentions during training and canonical descriptions for all entities, both of which are rarely available outside of Wikipedia (Logeswaran et al., 2019; Wu et al., 2020). We explore Knowledge-RIch Self-Supervision (KRISS) and train a contextual encoder (KRISSBERT) for entity linking, by leveraging readily available unlabeled text and domain knowledge.; Specifically, the KRISSBERT model is initialized with PubMedBERT parameters, and then continuously pretrained using biomedical entity names from the UMLS ontology to self-supervise entity linking examples from PubMed abstracts. Experiments on seven standard biomedical entity linking datasets show that KRISSBERT attains new state of the art, outperforming prior self-supervised methods by as much as 20 absolute points in accuracy.
See Zhang et al., 2021 for the details.; Note that some prior systems like BioSyn, SapBERT, and their follow-up work (e.g., Lai et al., 2021) claimed to do entity linking, but their systems completely ignore the context of an entity mention, and can only predict a surface form in the entity dictionary (See Figure 1 in BioSyn), not the canonical entity ID (e.g., CUI in UMLS). Therefore, they can't disambiguate ambiguous mentions. For instance, given the entity mention ""ER"" in the sentence ""ER crowding has become a wide-spread problem"", their systems ignore the sentence context, and simply predict the closest surface form, which is just ""ER"". Multiple entities share this surface form as a potential name or alias, such as Emergency Room (C0562508), Estrogen Receptor Gene (C1414461), and Endoplasmic Reticulum(C0014239). Without using the context information, their systems can't resolve such ambiguity and pinpoint the correct entity Emergency Room (C0562508). More problematically, their evaluation would deem such an ambiguous prediction as correct. Consequently, the reported results in their papers do not reflect true performance on entity linking.; Here, we use the MedMentions data to show you how to 1) generate prototype embeddings, and 2) run entity linking."
tortoise-tts-v2,,,,,https://arxiv.org/pdf/2102.12092.pdf; https://arxiv.org/pdf/2102.09672.pdf; https://arxiv.org/pdf/2106.07889.pdf,109,,0,0.145805893,4,https://huggingface.co/jbetker/tortoise-tts-v2,"Tortoise is a text-to-speech program built with the following priorities:; This repo contains all the code needed to run Tortoise TTS in inference mode.; I'm naming my speech-related repos after Mojave desert flora and fauna. Tortoise is a bit tongue in cheek: this model
is insanely slow. It leverages both an autoregressive decoder and a diffusion decoder; both known for their low
sampling rates. On a K80, expect to generate a medium sized sentence every 2 minutes.; See this page for a large list of example outputs.; If you want to use this on your own computer, you must have an NVIDIA GPU. First, install pytorch using these
instructions: https://pytorch.org/get-started/locally/"
Erlangshen-Roberta-110M-Sentiment,Text Classification,PyTorch; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/2209.02970.pdf,33,,"8,608",409.1126432,,https://huggingface.co/IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment,"中文的RoBERTa-wwm-ext-base在数个情感分析任务微调后的版本; This is the fine-tuned version of the Chinese RoBERTa-wwm-ext-base model on several sentiment analysis datasets.; 基于chinese-roberta-wwm-ext-base，我们在收集的8个中文领域的情感分析数据集，总计227347个样本上微调了一个Semtiment版本。; Based on chinese-roberta-wwm-ext-base, we fine-tuned a sentiment analysis version on 8 Chinese sentiment analysis datasets, with totaling 227,347 samples.; 如果您在您的工作中使用了我们的模型，可以引用我们的论文："
SentimentAnalysisDistillBERT,Text Classification,PyTorch; Transformers,English,,,1,Souvikcmsa/autotrain-data-sentiment_analysis,105,268.6874976,,https://huggingface.co/Souvikcmsa/SentimentAnalysisDistillBERT,You can use cURL to access this model:; Or Python API:
distilbert-base-uncased-finetuned-conll03-english-int8-static,Token Classification,PyTorch; Transformers,English,apache-2.0,,1,conll2003,31,68.52521877,,https://huggingface.co/Intel/distilbert-base-uncased-finetuned-conll03-english-int8-static,"This is an INT8  PyTorch model quantized with huggingface/optimum-intel through the usage of Intel? Neural Compressor. ; The original fp32 model comes from the fine-tuned model elastic/distilbert-base-uncased-finetuned-conll03-english.; The calibration dataloader is the train dataloader. The default calibration sampling size 100 isn't divisible exactly by batch size 8, so the real sampling size is 104."
sentiment_analysis,Text Classification,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/1810.04805.pdf,4,Confidential,"8,423",268.9284039,1,https://huggingface.co/sbcBI/sentiment_analysis,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for
further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text-classification."
clip-vit-large-patch14-336,Zero-Shot Image Classification,PyTorch; TensorFlow; Transformers,,,,40,,"162,793",3505.662835,8,https://huggingface.co/openai/clip-vit-large-patch14-336,"This model was trained from scratch on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
unite-mup,Fill-Mask,PyTorch; Transformers,,apache-2.0,,1,,5,2328.412082,,https://huggingface.co/ywan/unite-mup,This model is the multilingual version of "UniTE: Unified Translation Evaluation".
mT5_m2o_chinese_simplified_crossSum,Summarization,PyTorch; Transformers,43 languages,,https://arxiv.org/pdf/2112.08804.pdf,12,,"1,776",2390.240966,,https://huggingface.co/csebuetnlp/mT5_m2o_chinese_simplified_crossSum,"This repository contains the many-to-one (m2o) mT5 checkpoint finetuned on all cross-lingual pairs of the CrossSum dataset, where the target summary was in chinese_simplified, i.e. this model tries to summarize text written in any language in Chinese(Simplified). For finetuning details and scripts, see the paper and the official repository. ; If you use this model, please cite the following paper:"
mt5_chinese_small,Summarization,PyTorch; Transformers,Chinese,,,5,,243,1249.451426,,https://huggingface.co/yihsuan/mt5_chinese_small,"license: apache-2.0
tags:; This model is a fine-tuned version of google/mt5-small on the None dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed"
legalbert-large-1.7M-2,Fill-Mask,PyTorch; Transformers,English,,https://arxiv.org/pdf/1907.11692.pdf; https://arxiv.org/pdf/1810.04805.pdf; https://arxiv.org/pdf/2110.00976.pdf; https://arxiv.org/pdf/2207.00220.pdf,21,pile-of-law/pile-of-law,688,1382.642601,,https://huggingface.co/pile-of-law/legalbert-large-1.7M-2,"Pretrained model on English language legal and administrative text using the RoBERTa pretraining objective. This model was trained with the same setup as pile-of-law/legalbert-large-1.7M-1, but with a different seed.; Pile of Law BERT large 2 is a transformers model with the BERT large model (uncased) architecture pretrained on the Pile of Law, a dataset consisting of ~256GB of English language legal and administrative text for language model pretraining.; You can use the raw model for masked language modeling or fine-tune it for a downstream task. Since this model was pretrained on a English language legal and administrative text corpus, legal downstream tasks will likely be more in-domain for this model.; You can use the model directly with a pipeline for masked language modeling:; Here is how to use this model to get the features of a given text in PyTorch:"
amazon-review-sentiment-analysis,Text Classification,PyTorch; TensorBoard; Transformers,,apache-2.0,,11,,"5,920",673.4210277,2,https://huggingface.co/LiYuan/amazon-review-sentiment-analysis,"This model is a fine-tuned version of nlptown/bert-base-multilingual-uncased-sentiment on an Amazon US Customer Reviews Dataset. The code for the fine-tuning process can be found
here. This model is uncased: it does
not make a difference between english and English.
It achieves the following results on the evaluation set:; This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).; This model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above, or for further finetuning on related sentiment analysis tasks.; We replaced its head with our customer reviews to fine-tune it on 17,280 rows of training set while validating it on 4,320 rows of dev set. Finally, we evaluated our model performance on a held-out test set: 2,400 rows.; Bert-base is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification, or question answering. This fine-tuned version of BERT-base is used to predict review rating star given the review."
layoutlmv3-finetuned-funsd,Token Classification,PyTorch; TensorBoard; Transformers,,,,11,nielsr/funsd-layoutlmv3,"1,088",504.3494583,3,https://huggingface.co/nielsr/layoutlmv3-finetuned-funsd,"This model is a fine-tuned version of microsoft/layoutlmv3-base on the nielsr/funsd-layoutlmv3 dataset.
It achieves the following results on the evaluation set:; The script for training can be found here: https://github.com/huggingface/transformers/tree/main/examples/research_projects/layoutlmv3; More information needed; More information needed; More information needed"
KR-SBERT-V40K-klueNLI-augSTS,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,Korean,,,12,,"1,667",468.2791319,,https://huggingface.co/snunlp/KR-SBERT-V40K-klueNLI-augSTS,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net"
sentiment_analysis_model,Text Classification,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/1810.04805.pdf,3,Confidential,"39,053",268.9282867,3,https://huggingface.co/sbcBI/sentiment_analysis_model,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for
further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text-classification."
kote_for_easygoing_people,Text Classification,PyTorch; Transformers,,mit,,1,,669,499.1620431,,https://huggingface.co/searle-j/kote_for_easygoing_people,
Wiki-Complexity,Text Classification,PyTorch; JAX; Safetensors; Transformers,English,,,2,hidude562/autotrain-data-SimpleDetect,33,536.6949885,,https://huggingface.co/hidude562/Wiki-Complexity,"This model detects if you are writing in a format that is more similar to Simple English Wikipedia or English Wikipedia. This can be extended to applications that aren't Wikipedia as well and to some extent, it can be used for other languages.; Please also note there is a major bias to special characters (Mainly the hyphen mark, but it also applies to others) so I would recommend removing them from your input text.; You can use cURL to access this model:; Or Python API:"
distilbert-base-uncased-finetuned-fashion,Text Classification,PyTorch; TensorBoard; Safetensors; Transformers,,apache-2.0,,3,,27,536.9275272,,https://huggingface.co/rasta/distilbert-base-uncased-finetuned-fashion,"This model is a fine-tuned version of distilbert-base-uncased on a munally created dataset in order to detect fashion (label_0) from non-fashion (label_1) items.
It achieves the following results on the evaluation set:; The following hyperparameters were used during training:"
layoutlmv3-funsd-v2,Token Classification,PyTorch; Safetensors; Transformers,,,,2,,47,1002.002549,,https://huggingface.co/nielsr/layoutlmv3-funsd-v2,No model card; New: Create and edit this model card directly on the website!
finbert-fls,Text Classification,PyTorch; Transformers,English,,,13,,"10,096",439.2261164,11,https://huggingface.co/yiyanghkust/finbert-fls,"Forward-looking statements (FLS) inform investors of managers’ beliefs and opinions about firm's future events or results. Identifying forward-looking statements from corporate reports can assist investors in financial analysis. FinBERT-FLS is a FinBERT model fine-tuned on 3,500 manually annotated sentences from Management Discussion and Analysis section of annual reports of Russell 3000 firms.  ; Input: A financial text.; Output: Specific-FLS , Non-specific FLS, or Not-FLS.; You can use this model with Transformers pipeline for forward-looking statement classification.; Visit FinBERT.AI for more details on the recent development of FinBERT."
finbert-esg,Text Classification,PyTorch; Transformers,English,,,25,,"104,390",439.2261843,5,https://huggingface.co/yiyanghkust/finbert-esg,"ESG analysis can help investors determine a business' long-term sustainability and identify associated risks. FinBERT-ESG is a FinBERT model fine-tuned on 2,000 manually annotated sentences from firms' ESG reports and annual reports.  ; Input: A financial text.; Output: Environmental, Social, Governance or None.; You can use this model with Transformers pipeline for ESG classification.; Visit FinBERT.AI for more details on the recent development of FinBERT."
yolo_v5s_animal_det_512x512_quant_n2x_cpu_1,,,,,,1,,0,7.94298069,,https://huggingface.co/degirum/yolo_v5s_animal_det_512x512_quant_n2x_cpu_1,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
yolo_v5s_face_det_512x512_quant_n2x_cpu_1,,,,,,1,,0,7.822226105,,https://huggingface.co/degirum/yolo_v5s_face_det_512x512_quant_n2x_cpu_1,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
yolo_v5s_household_objects_512x512_quant_n2x_cpu_1,,,,,,1,,0,7.862588959,,https://huggingface.co/degirum/yolo_v5s_household_objects_512x512_quant_n2x_cpu_1,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
autotrain-smm4h_large_roberta_clean-874027878,Text Classification,PyTorch; Transformers,unk,,,1,Amalq/autotrain-data-smm4h_large_roberta_clean,39,1456.67348,,https://huggingface.co/Amalq/autotrain-smm4h_large_roberta_clean-874027878,You can use cURL to access this model:; Or Python API:
gpt-4chan,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2109.07958.pdf,111,,,0,,https://huggingface.co/ykilcher/gpt-4chan,"Given its research scope, intentionally using the model for generating harmful content (non-exhaustive examples: hate speech, spam generation, fake news, harassment and abuse, disparagement, and defamation) on all websites where bots are prohibited is considered a misuse of this model. Head over to the Community page for further discussion and potential next steps.; Project Website: https://gpt-4chan.com; Note that I have no association with any torrents or backups, or other ways of obtaining this model.
However, if you try them, please be safe. Here are the hex md5 hashes for the pytorch_model.bin files:
pytorch_model.bin float32 : 833c1dc19b7450e4e559a9917b7d076a
pytorch_model.bin float16 : db3105866c9563b26f7399fafc00bb4b; GPT-4chan is a language model fine-tuned from GPT-J 6B on 3.5 years worth of data from 4chan's politically incorrect (/pol/) board. ; GPT-4chan was fine-tuned on the dataset Raiders of the Lost Kek: 3.5 Years of Augmented 4chan Posts from the Politically Incorrect Board."
t5-base-tag-generation,Text2Text Generation,PyTorch; TensorBoard; Transformers,,apache-2.0,,29,,"98,035",894.4329226,1,https://huggingface.co/fabiochiu/t5-base-tag-generation,"This model is t5-base fine-tuned on the 190k Medium Articles dataset for predicting article tags using the article textual content as input. While usually formulated as a multi-label classification problem, this model deals with tag generation as a text2text generation task (inspiration from text2tags).; The dataset is composed of Medium articles and their tags. However, each Medium article can have at most five tags, therefore the author needs to choose what he/she believes are the best tags (mainly for SEO-related purposes). This means that an article with the ""Python"" tag may have not the ""Programming Languages"" tag, even though the first implies the latter.; To clean the dataset accounting for this problem, a hand-made taxonomy of about 1000 tags was built. Using the taxonomy, the tags of each articles have been augmented (e.g. an article with the ""Python"" tag will have the ""Programming Languages"" tag as well, as the taxonomy says that ""Python"" is part of ""Programming Languages""). The taxonomy is not public, if you are interested in it please send an email at chiusanofabio94@gmail.com.; The model has been trained on a single epoch spanning about 50000 articles, evaluating on 1000 random articles not used during training.; The following hyperparameters were used during training:"
bloom-7b1,Text Generation,PyTorch; JAX; Transformers,48 languages,bigscience-bloom-rail-1.0,https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2110.02861.pdf; https://arxiv.org/pdf/2108.12409.pdf,121,,"53,365",28973.31594,11,https://huggingface.co/bigscience/bloom-7b1,Version 1.0 / 26.May.2022; This section provides information for anyone who wants to know about the model.; Developed by: BigScience (website); Model Type: Transformer-based Language Model; Version: 1.0.0
tk-instruct-base-def-pos,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/1910.10683.pdf; https://arxiv.org/pdf/2204.07705.pdf,7,Super-NaturalInstructions,"1,655",498.9609421,1,https://huggingface.co/allenai/tk-instruct-base-def-pos,"Tk-Instruct is a series of encoder-decoder Transformer models that are trained to solve various NLP tasks by following in-context instructions (plain language task definitions, k-shot examples, explanations, etc). Built upon the pre-trained T5 models, they are fine-tuned on a large number of tasks & instructions that are collected in the Natural Instructions benchmark, which contains 1600+ tasks in 70+ broach categories in total. This enables the model to not only process the training tasks, but also generalize to many unseen tasks without further parameter update.; More resources for using the model:; Tk-Instruct can be used to do many NLP tasks by following instructions. ; When instructing the model, task definition or demonstration examples or explanations should be prepended to the original input and fed into the model. You can easily try Tk-Instruct models as follows:; We are still working on understanding the behaviors of these models, but here are several issues we have found:"
bert-base-uncased_German_MultiLable_classification,Text Classification,TensorFlow; Transformers,German,apache-2.0,,1,,34,437.9633434,,https://huggingface.co/Tobias/bert-base-uncased_German_MultiLable_classification,A model trained on German Hotel Reviews from Switzerland. The base model is the bert-base-german-cased. The last hidden layer of the base model was extracted and a classification layer was added. The entire model was then trained for 5 epochs on our dataset.; 
language-detection,Text Classification,PyTorch; TensorFlow; Transformers,,mit,https://arxiv.org/pdf/1911.02116.pdf,9,,"5,028",2287.438851,,https://huggingface.co/eleldar/language-detection,"This model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset.; This model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output). 
For additional information please refer to the xlm-roberta-base model card or to the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al.; You can directly use this model as a language detector, i.e. for sequence classification tasks. Currently, it supports the following 20 languages: ; arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh); The model was fine-tuned on the Language Identification dataset, which consists of text sequences in 20 languages. The training set contains 70k samples, while the validation and test sets 10k each. The average accuracy on the test set is 99.6% (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table."
fastspeech2-en-male1,Text-to-Speech,Fairseq,English,,https://arxiv.org/pdf/2006.04558.pdf; https://arxiv.org/pdf/2109.06912.pdf,19,common_voice,608,603.8316526,2,https://huggingface.co/Voicemod/fastspeech2-en-male1,FastSpeech 2 text-to-speech model from fairseq S^2 (paper/code):; See also fairseq S^2 example.
tts-tacotron2-ljspeech,Text-to-Speech,speechbrain,English,apache-2.0,https://arxiv.org/pdf/1712.05884.pdf; https://arxiv.org/pdf/2106.04624.pdf,80,LJSpeech,"3,557",113.0072079,11,https://huggingface.co/speechbrain/tts-tacotron2-ljspeech,"This repository provides all the necessary tools for Text-to-Speech (TTS)  with SpeechBrain using a Tacotron2 pretrained on LJSpeech.; The pre-trained model takes in input a short text and produces a spectrogram in output. One can get the final waveform by applying a vocoder (e.g., HiFIGAN) on top of the generated spectrogram.; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; If you want to generate multiple sentences in one-shot, you can do in this way:; To perform inference on the GPU, add  run_opts={""device"":""cuda""}  when calling the from_hparams method."
tts-hifigan-ljspeech,Text-to-Speech,speechbrain,English,apache-2.0,https://arxiv.org/pdf/2010.05646.pdf,13,LJSpeech,"3,807",55.8047168,2,https://huggingface.co/speechbrain/tts-hifigan-ljspeech,"This repository provides all the necessary tools for using a HiFIGAN vocoder trained with LJSpeech. ; The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram.; The sampling frequency is 22050 Hz.; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; To perform inference on the GPU, add  run_opts={""device"":""cuda""}  when calling the from_hparams method."
mobilevit-small,Image Classification,PyTorch; TensorFlow; Core ML; Transformers,,other,https://arxiv.org/pdf/2110.02178.pdf,22,imagenet-1k,"6,185",45.37437756,2,https://huggingface.co/apple/mobilevit-small,"MobileViT model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer by Sachin Mehta and Mohammad Rastegari, and first released in this repository. The license used is Apple sample code license.; Disclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.; MobileViT is a light-weight, low latency convolutional neural network that combines MobileNetV2-style layers with a new block that replaces local processing in convolutions with global processing using transformers. As with ViT (Vision Transformer), the image data is converted into flattened patches before it is processed by the transformer layers. Afterwards, the patches are ""unflattened"" back into feature maps. This allows the MobileViT-block to be placed anywhere inside a CNN. MobileViT does not require any positional embeddings.; You can use the raw model for image classification. See the model hub to look for fine-tuned versions on a task that interests you.; Here is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:"
chinese-hubert-base,Feature Extraction,PyTorch; Transformers,,mit,,11,,"2,072",1545.363855,1,https://huggingface.co/TencentGameMate/chinese-hubert-base,"Pretrained on 10k hours WenetSpeech L subset. More details in  TencentGameMate/chinese_speech_pretrain; This model does not have a tokenizer as it was pretrained on audio alone. 
In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data.; python package:
transformers==4.16.2"
mtl-data-to-text,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2206.12131.pdf,7,,483,1672.559446,,https://huggingface.co/RUCAIBox/mtl-data-to-text,"The MTL-data-to-text model was proposed in MVP: Multi-task Supervised Pre-training for Natural Language Generation by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.; The detailed information and instructions can be found https://github.com/RUCAIBox/MVP.; MTL-data-to-text is supervised pre-trained using a mixture of labeled data-to-text datasets. It is a variant (Single) of our main MVP model. It follows a standard Transformer encoder-decoder architecture.; MTL-data-to-text is specially designed for data-to-text generation tasks, such as KG-to-text generation (WebNLG, DART), table-to-text generation (WikiBio, ToTTo) and MR-to-text generation (E2E).; MVP: https://huggingface.co/RUCAIBox/mvp."
t5-arabic-text-summarization,Text2Text Generation,PyTorch; Transformers,Arabic,,,6,,"3,770",1159.590168,2,https://huggingface.co/malmarjeh/t5-arabic-text-summarization,"A fine-tuned AraT5 model on a dataset of 84,764 paragraph-summary pairs.; Paper: Arabic abstractive text summarization using RNN-based and transformer-based architectures.; Dataset: link.; The model can be used as follows:; banimarje@gmail.com"
icebert-xlmr-ic3-iec,Token Classification,PyTorch; Transformers,,cc-by-4.0,,1,,12,1160.532176,,https://huggingface.co/vesteinn/icebert-xlmr-ic3-iec,
vit_base-224-in21k-ft-cifar100,Image Classification,PyTorch; Transformers,Spanish,apache-2.0,https://arxiv.org/pdf/2006.03677.pdf,1,cifar100,"2,712",344.0114838,,https://huggingface.co/edumunozsala/vit_base-224-in21k-ft-cifar100,"This model was trained using Amazon SageMaker and the Hugging Face Deep Learning container,
The base model is Vision Transformer (base-sized model) which  is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels.Link to base model ; Link to dataset description; The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton; The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.
This dataset,CIFAR100, is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a ""fine"" label (the class to which it belongs) and a ""coarse"" label (the superclass to which it belongs).; Sizes of datasets:"
tab_transformer,Tabular Classification,TensorBoard; Keras,,,,10,,9,2.880546875,1,https://huggingface.co/keras-io/tab_transformer,"This repo contains the trained model of Structured data learning with TabTransformer.
The full credit goes to: Khalid Salama; Spaces Link: ; The following hyperparameters were used during training:; Model history needed; "
swinv2-tiny-patch4-window8-256,Image Classification,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2111.09883.pdf,3,imagenet-1k,"5,658",113.073725,1,https://huggingface.co/microsoft/swinv2-tiny-patch4-window8-256,"Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. ; Disclaimer: The team releasing Swin Transformer v2 did not write a model card for this model so this model card has been written by the Hugging Face team.; The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. In contrast, previous vision Transformers produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of self-attention globally.; Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.; "
long-t5-tglobal-xl,Text2Text Generation,PyTorch; JAX; Transformers,English,apache-2.0,https://arxiv.org/pdf/2112.07916.pdf; https://arxiv.org/pdf/1912.08777.pdf; https://arxiv.org/pdf/1910.10683.pdf,16,,"3,120",23349.422,,https://huggingface.co/google/long-t5-tglobal-xl,"LongT5 model pre-trained on English language. The model was introduced in the paper LongT5: Efficient Text-To-Text Transformer for Long Sequences by Guo et al. and first released in the LongT5 repository. All the model architecture and configuration can be found in Flaxformer repository which uses another Google research project repository T5x.; Disclaimer: The team releasing LongT5 did not write a model card for this model so this model card has been written by the Hugging Face team.; LongT5 model is an encoder-decoder transformer pre-trained in a text-to-text denoising generative setting (Pegasus-like generation pre-training). LongT5 model is an extension of T5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2) Transient-Global attention. The usage of attention sparsity patterns allows the model to efficiently handle input sequence.; LongT5 is particularly effective when fine-tuned for text generation (summarization, question answering) which requires handling long input sequences (up to 16,384 tokens).; The model is mostly meant to be fine-tuned on a supervised dataset. See the model hub to look for fine-tuned versions on a task that interests you."
MelGAN-spectrogram-inversion,,TensorBoard; Keras,,,,2,,11,5.681738281,1,https://huggingface.co/keras-io/MelGAN-spectrogram-inversion,"This repo contains the model and the notebook for implementing MelGAN to inverse spectrogram using feature matching MelGAN-based spectrogram inversion using feature matching.; Full credits go to Darshan Deshpande; Reproduced by Vu Minh Chien; Motivation: Autoregressive vocoders have been ubiquitous for the majority of the history of speech processing, but for most of their existence they have lacked parallelism. MelGAN is a non-autoregressive, fully convolutional vocoder architecture used for purposes ranging from spectral inversion and speech enhancement to present-day state-of-the-art speech synthesis when used as a decoder with models like Tacotron2 or FastSpeech that convert text to mel spectrograms.; LJSpeech dataset was used in this tutorial. The LJSpeech dataset is primarily used for text-to-speech and consists of 13,100 discrete speech samples taken from 7 non-fiction books, having a total length of approximately 24 hours"
Medical_Article_Classifier_by_ICD-11_Chapter,Text Classification,PyTorch; Transformers,,,,5,,8,0,,https://huggingface.co/justpyschitry/Medical_Article_Classifier_by_ICD-11_Chapter,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; co2_eq_emissions: 0.021794705501614994
datasets: ; You can use cURL to access this model:; Or Python API:"
ddpm-cifar10-32,Unconditional Image Generation,Diffusers; PyTorch,,apache-2.0,https://arxiv.org/pdf/2006.11239.pdf,30,,"45,905",143.0079184,7,https://huggingface.co/google/ddpm-cifar10-32,"Paper: Denoising Diffusion Probabilistic Models; Authors: Jonathan Ho, Ajay Jain, Pieter Abbeel; Abstract:; We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.; DDPM models can use discrete noise schedulers such as:"
ul2,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2205.05131.pdf,146,c4,522,40400.08717,1,https://huggingface.co/google/ul2,"UL2 is a unified framework for pretraining models that are universally effective across datasets and setups. UL2 uses Mixture-of-Denoisers (MoD), apre-training objective that combines diverse pre-training paradigms together. UL2 introduces a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes.; ; Abstract; Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. ; For more information, please take a look at the original paper."
stanford-deidentifier-base,Token Classification,PyTorch; Transformers,English,mit,,48,radreports,"386,949",438.2265993,5,https://huggingface.co/StanfordAIMI/stanford-deidentifier-base,Stanford de-identifier was trained on a variety of radiology and biomedical documents with the goal of automatising the de-identification process while reaching satisfactory accuracy for use in production. Manuscript in-proceedings. ; These model weights are the recommended ones among all available deidentifier weights.; Associated github repo: https://github.com/MIDRC/Stanford_Penn_Deidentifier
parlbert-topic-german,Text Classification,PyTorch; Safetensors; Transformers,German,,,5,,471,872.9675369,1,https://huggingface.co/chkla/parlbert-topic-german,? Model description; This model was trained on ~10k manually annotated interpellations (? Breunig/ Schnatterer 2019) with topics from the Comparative Agendas Project to classify text into one of twenty labels (annotation codebook).; Note: "Interpellation is a formal request of a parliament to the respective government."(Wikipedia); ? Dataset; ???♂?Model training
t5-small,Translation,ONNX; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/1910.10683.pdf,7,c4,"77,417",595.4282324,1,https://huggingface.co/optimum/t5-small,"T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which each task is converted into a text-to-text format.; For more information, please take a look at the original paper.; Paper: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer; Authors: Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu; You can use this model with Transformers pipeline."
long-t5-tglobal-base-16384-book-summary,Summarization,PyTorch; Rust; ONNX; Safetensors; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2112.07916.pdf; https://arxiv.org/pdf/2105.08209.pdf,84,kmfoda/booksum,"7,416",3966.259895,11,https://huggingface.co/pszemraj/long-t5-tglobal-base-16384-book-summary,Summarize long text and get a SparkNotes-esque summary of arbitrary topics!; A summary of the infamous navy seals copypasta:; The narrator tells us that he's graduated from the Navy seals and has been involved in many secret raids. He's also one of the best snipers in the entire U.S. military. He promises to "wipe you out with precision" when they meet again.; Contents; A fine-tuned version of google/long-t5-tglobal-base on the kmfoda/booksum dataset:
t5-small-headline-generator,Summarization,PyTorch; Safetensors; Transformers,English,mit,,7,JulesBelveze/tldr_news,"3,591",487.2023438,3,https://huggingface.co/JulesBelveze/t5-small-headline-generator,"This model is a t5-small fine-tuned for headline generation using
the JulesBelveze/tldr_news dataset."
sepformer-wham16k-enhancement,Audio-to-Audio,speechbrain; PyTorch,English,apache-2.0,https://arxiv.org/pdf/2010.13154.pdf; https://arxiv.org/pdf/2106.04624.pdf,11,WHAM!,265,113.3783278,,https://huggingface.co/speechbrain/sepformer-wham16k-enhancement,"This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 16k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k. For a better experience we encourage you to learn more about SpeechBrain. The given model performance is 14.3 dB SI-SNR on the test set of WHAM! dataset.; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about SpeechBrain.; To perform inference on the GPU, add  run_opts={""device"":""cuda""}  when calling the from_hparams method.; The training script is currently being worked on an ongoing pull-request. "
Randeng-Pegasus-238M-Summary-Chinese,Summarization,PyTorch; Transformers,Chinese,,https://arxiv.org/pdf/1912.08777.pdf; https://arxiv.org/pdf/2209.02970.pdf,36,,544,477.4070151,3,https://huggingface.co/IDEA-CCNL/Randeng-Pegasus-238M-Summary-Chinese,"善于处理摘要任务，在数个中文摘要数据集上微调后的，中文版的PAGASUS-base。; Good at solving text summarization tasks, after fine-tuning on multiple Chinese text summarization datasets, Chinese PAGASUS-base.; 参考论文：PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization; 基于Randeng-Pegasus-238M-Chinese，我们在收集的7个中文领域的文本摘要数据集（约4M个样本）上微调了它，得到了summary版本。这7个数据集为：education, new2016zh, nlpcc, shence, sohu, thucnews和weibo。; Based on Randeng-Pegasus-238M-Chinese, we fine-tuned a text summarization version (summary) on 7 Chinese text summarization datasets, with totaling around 4M samples. The datasets include: education, new2016zh, nlpcc, shence, sohu, thucnews and weibo."
ner-albert-base-v2-ontonotesv5-englishv4,Token Classification,PyTorch; Transformers,,,,2,djagatiya/ner-ontonotes-v5-eng-v4,20,46.81706188,,https://huggingface.co/djagatiya/ner-albert-base-v2-ontonotesv5-englishv4,"This ALBERT-base-v2 NER model was finetuned on conll2012_ontonotesv5 version english-v4 dataset. 
Check out NER-System Repository for more information.; check out this eval.log file for evaluation metrics and classification report."
ner-bert-base-cased-ontonotesv5-englishv4,Token Classification,PyTorch; Transformers,,,,1,djagatiya/ner-ontonotes-v5-eng-v4,194,431.9190814,,https://huggingface.co/djagatiya/ner-bert-base-cased-ontonotesv5-englishv4,"This bert-base-cased NER model was finetuned on conll2012_ontonotesv5 version english-v4 dataset. 
Check out NER-System Repository for more information.; check out this eval.log file for evaluation metrics and classification report."
codeparrot-small-multi,Text Generation,PyTorch; Transformers,code,apache-2.0,,5,codeparrot/github-code-clean; openai_humaneval,"1,016",247.7728361,,https://huggingface.co/codeparrot/codeparrot-small-multi,"CodeParrot-Multi ? is a GPT-2 model (110M parameters) trained to generate code in 9 programming languages: ""Java"", ""JavaScript"", ""PHP"", ""Python"", ""C#"", ""C++"", ""GO"", ""Ruby"" and ""TypeScript"".; You can load the CodeParrot-Multi model and tokenizer directly in transformers:; or with a pipeline:; The model was trained on the small Github code small after near deduplication, a subset of Github code dataset with the following settings:; The training was executed on 16 x A100 (40GB) GPUs. This setting amounts to roughly 58 billion tokens."
owlvit-large-patch14,Object Detection,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2205.06230.pdf,11,,"6,548",1783.344755,1,https://huggingface.co/google/owlvit-large-patch14,"The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in Simple Open-Vocabulary Object Detection with Vision Transformers by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries.  ; OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection. ; May 2022; The model uses a CLIP backbone with a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. The CLIP backbone is trained from scratch and fine-tuned together with the box and class prediction heads with an object detection objective.; The model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, text-conditioned object detection. We also hope it can be used for interdisciplinary studies of the potential impact of such models, especially in areas that commonly require identifying objects whose label is unavailable during training."
hupd-t5-small,Summarization,PyTorch; Transformers,English,cc-by-sa-4.0,,3,HUPD/hupd,117,245.2529199,,https://huggingface.co/HUPD/hupd-t5-small,"This HUPD T5-Small summarization model was fine-tuned on the HUPD dataset. It was originally introduced in this paper. ; For more information about the Harvard USPTO Patent Dataset, please feel free to visit the project website or the project's GitHub repository.; You can use this model directly with a pipeline for masked language modeling:; Here is the output:; Alternatively, you can load the model and use it as follows:"
codet5-large,Text2Text Generation,PyTorch; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2109.00859.pdf; https://arxiv.org/pdf/2207.01780.pdf; https://arxiv.org/pdf/1909.09436.pdf,40,,"3,737",1517.693613,1,https://huggingface.co/Salesforce/codet5-large,"CodeT5 is a family of encoder-decoder language models for code from the paper: CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation by Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi.; The checkpoint included in this repository is denoted as CodeT5-large (770M), which is introduced by the paper: CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning by Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoi.; CodeT5-large was pretrained on CodeSearchNet data in six programming languages (Ruby/JavaScript/Go/Python/Java/PHP).  See Section 4.1 of the paper for more details.; CodeT5-large was pretrained using masked span prediction objective for 150 epochs. See Section 4.1 of the paper for more details.; We validate the effectiveness of this checkpoint pretrained with simplified strategies on CodeXGLUE benchmark. See Appendix A.1 of the paper for more details."
dual-encoder-image-search,,TensorBoard; Keras,,,,5,,3,6.767646484,1,https://huggingface.co/keras-io/dual-encoder-image-search,"This repo contains the model and the notebook for fine-tuning BERT model on SNLI Corpus for Semantic Similarity. Natural language image search with a Dual Encoder.; Full credits go to Khalid Salama; Reproduced by Vu Minh Chien; Motivation: build a dual encoder (also known as a two-tower) neural network model to search for images using natural language. The model is inspired by the CLIP approach, introduced by Alec Radford et al. The idea is to train a vision encoder and a text encoder jointly to project the representation of images and their captions into the same embedding space, such that the caption embeddings are located near the embeddings of the images they describe.; The MS-COCO dataset was used to train the dual encoder model. MS-COCO contains over 82,000 images, each of which has at least 5 different caption annotations. The dataset is usually used for image captioning tasks, but in this tutorial, the image-caption pairs were used to train dual encoder model for image search."
nllb-200-distilled-600M,Translation,PyTorch; Transformers,196 languages,cc-by-nc-4.0,,146,flores-200,"49,688",2541.203722,67,https://huggingface.co/facebook/nllb-200-distilled-600M,"This is the model card of NLLB-200's distilled 600M variant.; Here are the metrics for that particular checkpoint.; ? Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.; ? We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.; ? In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety)."
nllb-200-1.3B,Translation,PyTorch; Transformers,196 languages,cc-by-nc-4.0,,17,flores-200,"88,253",5633.683608,14,https://huggingface.co/facebook/nllb-200-1.3B,"This is the model card of NLLB-200's 1.3B variant.; Here are the metrics for that particular checkpoint.; ? Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.; ? We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.; ? In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety)."
Taiyi-CLIP-Roberta-102M-Chinese,Feature Extraction,PyTorch; Safetensors; Transformers,,apache-2.0,https://arxiv.org/pdf/2209.02970.pdf,37,,"147,143",822.1385624,4,https://huggingface.co/IDEA-CCNL/Taiyi-CLIP-Roberta-102M-Chinese,"首个开源的中文CLIP模型，1.23亿图文对上进行预训练的文本端RoBERTa-base。; The first open source Chinese CLIP, pre-training on 123M image-text pairs, the text encoder: RoBERTa-base.; 我们遵循CLIP的实验设置，以获得强大的视觉-语言表征。在训练中文版的CLIP时，我们使用chinese-roberta-wwm作为语言的编码器，并将CLIP中的ViT-B-32应用于视觉的编码器。为了快速且稳定地进行预训练，我们冻结了视觉编码器并且只微调语言编码器。此外，我们将Noah-Wukong数据集(100M)和Zero数据集(23M)用作预训练的数据集，训练了24个epoch，在在A100x32上训练了7天。据我们所知，我们的Taiyi-CLIP是目前Huggingface社区中首个的开源中文CLIP。; We follow the experimental setup of CLIP to obtain powerful visual-language intelligence. To obtain the CLIP for Chinese, we employ chinese-roberta-wwm for the language encoder, and apply the ViT-B-32 in CLIP for the vision encoder. We freeze the vision encoder and tune the language encoder to speed up and stabilize the pre-training process. Moreover, we apply Noah-Wukong dataset (100M) and Zero dataset (23M) as the pre-training datasets. We train 24 epochs, which takes 7 days to train on A100x16. To the best of our knowledge, our TaiyiCLIP is currently the only open-sourced Chinese CLIP in the huggingface community.; Zero-Shot Classification"
postagger-portuguese,Token Classification,PyTorch; Safetensors; Transformers,Portuguese,,,5,MacMorpho,145,866.872412,,https://huggingface.co/lisaterumi/postagger-portuguese,"We fine-tuned the BERTimbau model with the MacMorpho corpus for the Post-Tagger task, with 10 epochs, achieving a general F1-Score of 0.9826.; Metrics:; Parameters:; Tags:; Please, post a Github issue on the NLP Portuguese POS-Tagger."
ddpm-cat-256,Unconditional Image Generation,Diffusers; PyTorch,,apache-2.0,https://arxiv.org/pdf/2006.11239.pdf,3,,"5,752",455.00519,,https://huggingface.co/google/ddpm-cat-256,"Paper: Denoising Diffusion Probabilistic Models; Authors: Jonathan Ho, Ajay Jain, Pieter Abbeel; Abstract:; We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.; DDPM models can use discrete noise schedulers such as:"
cuneiform,Text2Text Generation,PyTorch; Transformers,,mit,,4,,917,894.4306934,,https://huggingface.co/praeclarum/cuneiform,This is a translation network that understands Sumerian and Akkadian languages written in cuneiform.; It was trained on cuneiform transcribed in the CDLI ATF format. For example:; The network was trained to translate from the ancient languages:; written in transcribed cuneiform to English.; The network requires a prompt telling it the direction of translation. For example:
topic_classification_04,Text Classification,TensorFlow; Transformers,,mit,,18,,481,52.12684261,2,https://huggingface.co/jonaskoenig/topic_classification_04,"This model is a fine-tuned version of microsoft/xtremedistil-l6-h256-uncased on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
distilBERT-finetuned-resumes-sections,Text Classification,PyTorch; Transformers,,apache-2.0,,10,,"3,335",277.9722273,3,https://huggingface.co/has-abi/distilBERT-finetuned-resumes-sections,"This model is a fine-tuned version of Geotrend/distilbert-base-en-fr-cased on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
codereviewer,Text2Text Generation,PyTorch; Transformers,code,apache-2.0,https://arxiv.org/pdf/2203.09095.pdf,50,,"1,932",892.8570501,4,https://huggingface.co/microsoft/codereviewer,"CodeReviewer is a model pre-trained with code change and code review data to support code review tasks.; CodeReviewer: Pre-Training for Automating Code Review Activities. Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant Jenks, Deep Majumder, Jared Green, Alexey Svyatkovskiy, Shengyu Fu, Neel Sundaresan.
GitHub; If you user CodeReviewer, please consider citing the following paper:"
deberta-v3-large-squad2,Question Answering,PyTorch; Safetensors; Transformers,English,cc-by-4.0,,37,squad_v2,"19,917",3574.639075,5,https://huggingface.co/deepset/deberta-v3-large-squad2,"This is the deberta-v3-large model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering. ; Language model: deberta-v3-largeLanguage: EnglishDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD 2.0Code:  See an example QA pipeline on HaystackInfrastructure: 1x NVIDIA A10G; Haystack is an NLP framework by deepset. You can use this model in a Haystack pipeline to do question answering at scale (over many documents). To load the model in Haystack:; Evaluated on the SQuAD 2.0 dev set with the official eval script.; deepset is the company behind the open-source NLP framework Haystack which is designed to help you build production ready NLP systems that use: Question answering, summarization, ranking etc."
videomae-base-finetuned-ssv2,Video Classification,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/2203.12602.pdf; https://arxiv.org/pdf/2111.06377.pdf,2,,206,346.0258932,1,https://huggingface.co/MCG-NJU/videomae-base-finetuned-ssv2,"VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository. ; Disclaimer: The team releasing VideoMAE did not write a model card for this model so this model card has been written by the Hugging Face team.; VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches.; Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder.; By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video."
stt_hr_conformer_transducer_large,Automatic Speech Recognition,NeMo; PyTorch,Croatian,cc-by-4.0,https://arxiv.org/pdf/2005.08100.pdf,1,ParlaSpeech-HR-v1.0,0,478.0061035,,https://huggingface.co/nvidia/stt_hr_conformer_transducer_large,"| 
| 
|  |; This model transcribes speech in lowercase Croatian alphabet including spaces, and is trained on around 1665 hours of Croatian speech data.
It is a ""large"" variant of Conformer-Transducer, with around 120 million parameters.
See the model architecture section and NeMo documentation for complete architecture details.
It is also compatible with NVIDIA Riva for production-grade server deployments. ; The model is available for use in the NeMo toolkit [3], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.; To train, fine-tune or play with the model you will need to install NVIDIA NeMo. We recommend you install it after you've installed latest PyTorch version.; Simply do:"
led-base-16384-multi_lexsum-source-long,Text2Text Generation,PyTorch; Transformers,English,,,1,,54,651.3385114,,https://huggingface.co/allenai/led-base-16384-multi_lexsum-source-long,
simlm-msmarco-reranker,Text Classification,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2207.02578.pdf,4,,114,438.6876627,,https://huggingface.co/intfloat/simlm-msmarco-reranker,"paper available at https://arxiv.org/pdf/2207.02578; code available at https://github.com/microsoft/unilm/tree/master/simlm; In this paper, we propose SimLM (Similarity matching with Language Model pre-training), a simple yet effective pre-training method for dense passage retrieval. 
It employs a simple bottleneck architecture that learns to compress the passage information into a dense vector through self-supervised pre-training. 
We use a replaced language modeling objective, which is inspired by ELECTRA, 
to improve the sample efficiency and reduce the mismatch of the input distribution between pre-training and fine-tuning. 
SimLM only requires access to unlabeled corpus, and is more broadly applicable when there are no labeled data or queries. 
We conduct experiments on several large-scale passage retrieval datasets, and show substantial improvements over strong baselines under various settings. 
Remarkably, SimLM even outperforms multi-vector approaches such as ColBERTv2 which incurs significantly more storage cost.; Since we use a listwise loss to train the re-ranker,
the relevance score is not bounded to a specific numerical range.
Higher scores mean more relevant between the given query and passage.; Get relevance score from our re-ranker:"
UCF_Crime,Image Classification,PyTorch; TensorBoard; Transformers,,apache-2.0,,1,imagefolder,16,343.0070072,,https://huggingface.co/csr2000/UCF_Crime,This model is a fine-tuned version of google/vit-base-patch16-224 on the imagefolder dataset.; More information needed; More information needed; More information needed; The following hyperparameters were used during training:
universal-sentence-encoder,Sentence Similarity,Keras,English,apache-2.0,,5,,0,8.226513672,1,https://huggingface.co/Dimitre/universal-sentence-encoder,"The Universal Sentence Encoder encodes text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.; The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. We apply this model to the STS benchmark for semantic similarity, and the results can be seen in the example notebook made available. The universal-sentence-encoder model is trained with a deep averaging network (DAN) encoder.; To learn more about text embeddings, refer to the TensorFlow Embeddings documentation. Our encoder differs from word level embedding models in that we train on a number of natural language prediction tasks that require modeling the meaning of word sequences rather than just individual words. Details are available in the paper ""Universal Sentence Encoder"" [1].; There are several versions of universal sentence encoder models trained with different goals including size/performance multilingual, and fine-grained question answer retrieval.; This module is about 1GB. Depending on your network speed, it might take a while to load the first time you run inference with it. After that, loading the model should be faster as modules are cached by default (learn more about caching). Further, once a module is loaded to memory, inference time should be relatively fast."
banglat5_nmt_en_bn,Translation,PyTorch; Transformers,English; Bengali,,https://arxiv.org/pdf/2205.11081.pdf,4,,317,991.1200957,,https://huggingface.co/csebuetnlp/banglat5_nmt_en_bn,"This repository contains the BanglaT5 checkpoint finetuned on the BanglaNMT English-Bengali dataset. ; Note: The pretrained model uses a specific normalization pipeline available here. For best results, make sure the text units are normalized using this library before tokenization.; If you use this model, please cite the following paper:; If you use the normalization module, please cite the following paper:"
gptj-162M,Text Generation,PyTorch; Transformers,English,apache-2.0,,20,THEODOROS/Architext_v1,208,331.1602358,,https://huggingface.co/architext/gptj-162M,"Architext GPT-J-162M is a transformer model trained using Ben Wang's Mesh Transformer JAX on the Pile and finetuned specifically on a synthetically generated dataset of architectural layouts of apartments. It is capable of generating a large diversity of designs, in a convenient geometric representation that can be used downstream in different design workflows, using just a natural language prompt.; The model consists of 12 layers with a model dimension of 768, and a feedforward dimension of 2048. The model dimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64 dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as GPT-2/GPT-3.; GPT-J 162B was pre-trained on the Pile, a large-scale curated dataset created by EleutherAI. It was then finetuned on synthetically generated data that was procedurally generated using the Rhinocers/Grasshopper software suite. The model was finetuned for 1.25 billion tokens over 11,500 steps on TPU v3-8. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token correctly.  ; Architext models learn an inner representation of the architectural design that can be used to generate a larger diversity of geometric designs and can be useful for many downstream design workflows and tasks. While it could be adapted to many different design outputs, the model is best at generating residential floor plans given a natural language prompt.; The core functionality of Architext is taking a string of text and generating a design output, by still continuously predicting the next token. While language models are widely used for tasks other than this, there are a lot of unknowns with this work especially in the design context. Architext will often generate a design that is not semantically correct, depending on the prompt description it was given, although it almost always generates designs that are valid (non intersecting spaces, no orphan rooms). It is also limited within a small diversity of natural language prompts, specifically prompts that describe:"
ernie-3.0-base-zh,Fill-Mask,PyTorch; Transformers,Chinese,,https://arxiv.org/pdf/2107.02137.pdf,42,,"14,657",474.1856564,20,https://huggingface.co/nghuyong/ernie-3.0-base-zh,"ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
More detail: https://arxiv.org/abs/2107.02137; This released pytorch model is converted from the officially released PaddlePaddle ERNIE model and 
a series of experiments have been conducted to check the accuracy of the conversion.; Then you can load ERNIE-3.0 model as before:"
ernie-3.0-nano-zh,Feature Extraction,PyTorch; Transformers,Chinese,,https://arxiv.org/pdf/2107.02137.pdf,12,,"3,126",71.48560188,19,https://huggingface.co/nghuyong/ernie-3.0-nano-zh,"ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation
More detail: https://arxiv.org/abs/2107.02137; This released pytorch model is converted from the officially released PaddlePaddle ERNIE model and 
a series of experiments have been conducted to check the accuracy of the conversion."
LegalBert,,PyTorch; Transformers,,mit,,1,,5,436.2127778,,https://huggingface.co/jhu-clsp/LegalBert,"LegalBert is a BERT-base-cased model fine-tuned on a subset of the case.law corpus. Further details can be found in this paper:; A Dataset for Statutory Reasoning in Tax Law Entailment and Question AnsweringNils Holzenberger, Andrew Blair-Stanek and Benjamin Van DurmeProceedings of the 2020 Natural Legal Language Processing (NLLP) Workshop, 24 August 2020; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
gpt-j-6B-fp16-sharded,,PyTorch,,,,6,,771,12337.39877,,https://huggingface.co/philschmid/gpt-j-6B-fp16-sharded,"This is fork of EleutherAI/gpt-j-6B with shareded fp16 weights implementing a custom handler.py as an example for how to use gpt-j inference-endpoints; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
novel-zh-en,Text2Text Generation,PyTorch; Transformers,,,,4,,867,313.3275282,3,https://huggingface.co/penpen/novel-zh-en,No model card; New: Create and edit this model card directly on the website!
regen-disambiguation,Text2Text Generation,PyTorch; Transformers,,,,2,,380,5054.225394,,https://huggingface.co/ibm/regen-disambiguation,No model card; New: Create and edit this model card directly on the website!
uk-summarizer,Text2Text Generation,PyTorch; Transformers,Ukrainian,mit,,3,,21,980.0383054,,https://huggingface.co/ukr-models/uk-summarizer,Fine-tuning of uk-mt5-base model on summarization dataset.
brazilianpolitics,Text Classification,PyTorch; Transformers,Portuguese,mit,,1,,17,1167.493244,,https://huggingface.co/meedan/brazilianpolitics,"A binary classifier that classifies if an input text is related to the Brazilian elections or not. 
The classifier was trained on news article headlines taken from online Brazilian news organizations between 2010 and 2022. ; It was trained X epochs using microsoft/mdeberta-v3-base as the base model."
ptt5-base-summ-cstnews,Summarization,PyTorch; Safetensors; Transformers,Portuguese,mit,,5,,146,1784.75056,,https://huggingface.co/phpaiola/ptt5-base-summ-cstnews,"PTT5 Summ is a fine-tuned PTT5 model to perform Abstractive Summarization in Brazilian Portuguese texts. This model was fine-tuned on the datasets: WikiLingua, XL-Sum, TeMário and CSTNews.; For further information, please go to PTT5 Summ repository."
question-answer-generation,Text2Text Generation,PyTorch; Transformers,,mit,https://arxiv.org/pdf/1910.10683.pdf,5,squad,209,892.7776394,,https://huggingface.co/abhitopia/question-answer-generation,"This is multi-task t5-base model trained for question answering and answer aware question generation tasks. ; For question generation the answer spans are highlighted within the text with special highlight tokens (<hl>) and prefixed with 'generate question: '. For QA the input is processed like this question: question_text context: context_text </s> ; You can play with the model using the inference API. Here's how you can use it; For QG; generate question: <hl> 42 <hl> is the answer to life, the universe and everything. </s>"
all-MiniLM-L6-v2-optimum-embeddings,,ONNX,,mit,,2,,10,249.3571069,,https://huggingface.co/philschmid/all-MiniLM-L6-v2-optimum-embeddings,"This repository implements a custom task for sentence-embeddings for ? Inference Endpoints for accelerated inference using ? Optimum. The code for the customized pipeline is in the pipeline.py.; In the how to create your own optimized and quantized model you will learn how the model was converted & optimized, it is based on the Accelerate Sentence Transformers with Hugging Face Optimum blog post. It also includes how to create your custom pipeline and test it. There is also a notebook included.; To use deploy this model a an Inference Endpoint you have to select Custom as task to use the pipeline.py file. -> double check if it is selected; below is an example on how to run a request using Python and requests.; expected output"
donut-base-sroie,,PyTorch; TensorBoard; Transformers,,mit,,1,imagefolder,121,814.3222164,1,https://huggingface.co/philschmid/donut-base-sroie,This model is a fine-tuned version of naver-clova-ix/donut-base on the imagefolder dataset.; More information needed; More information needed; More information needed; The following hyperparameters were used during training:
stable-diffusion-image-conditioned,Image-to-Image,,,other,,48,ChristophSchuhmann/improved_aesthetics_6plus,0,20183.04754,3,https://huggingface.co/lambdalabs/stable-diffusion-image-conditioned,"This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-3-original to accept CLIP image embedding rather than text embeddings. This allows the creation of ""image variations"" similar to DALLE-2 using Stable Diffusion. ; ; To use this model requires a fork of the Stable Diffusion repo: justinpinkney/stable-diffusion; For the version ported to huggingface Diffusers, see this model.; Training Data
The model developers used the following dataset for training the model:"
PromptGeneration-base,Summarization,PyTorch; Transformers,unk,,,3,SamAct/autotrain-data-t3,163,2343.240059,,https://huggingface.co/SamAct/PromptGeneration-base,"Excellent accuracy for one line prompts. Prompts can be used for image generation, title or meta descriptions.; You can use cURL to access this model:"
birb-style,,,,mit,,25,,0,1.496149025,,https://huggingface.co/sd-concepts-library/birb-style,"This is the <birb-style> concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the Stable Conceptualizer notebook. You can also train your own concepts and load them into the concept libraries using this notebook.; Example outputs (style):
; Source images:


; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
conditional-detr-resnet-50,Object Detection,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2108.06152.pdf,3,coco,"4,786",174.0105052,,https://huggingface.co/microsoft/conditional-detr-resnet-50,"Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Conditional DETR for Fast Training Convergence by Meng et al. and first released in this repository. ; The recently-developed DETR approach applies the transformer encoder and decoder architecture to object detection and achieves promising performance. In this paper, we handle the critical issue, slow training convergence, and present a conditional cross-attention mechanism for fast DETR training. Our approach is motivated by that the cross-attention in DETR relies highly on the content embeddings for localizing the four extremities and predicting the box, which increases the need for high-quality content embeddings and thus the training difficulty. Our approach, named conditional DETR, learns a conditional spatial query from the decoder embedding for decoder multi-head cross-attention. The benefit is that through the conditional spatial query, each cross-attention head is able to attend to a band containing a distinct region, e.g., one object extremity or a region inside the object box. This narrows down the spatial range for localizing the distinct regions for object classification and box regression, thus relaxing the dependence on the content embeddings and easing the training. Empirical results show that conditional DETR converges 6.7× faster for the backbones R50 and R101 and 10× faster for stronger backbones DC5-R50 and DC5-R101.; ; You can use the raw model for object detection. See the model hub to look for all available Conditional DETR models.; Here is how to use this model:"
text-complexity-classification,Text Classification,PyTorch; Safetensors; Transformers,German,apache-2.0,,4,,701,880.9560735,,https://huggingface.co/krupper/text-complexity-classification,"The model classifies texts into the language complexity classes (German language):; The underlying corpus was trained on the basis of over 300,000 texts of the mentioned language categories. Freely available websites served as sources. Thematic diversity was taken into account when selecting the sources.; DOI: https://doi.org/10.57967/hf/0131"
rwkv-4-pile-7b,Text Generation,PyTorch,English,apache-2.0,,152,the_pile,0,515276.8032,,https://huggingface.co/BlinkDL/rwkv-4-pile-7b,"[UPDATE: Try RWKV-4-World (https://huggingface.co/BlinkDL/rwkv-4-world) for generation & chat & code in 100+ world languages, with great English zero-shot & in-context learning ability too.]; RWKV-4 7B is a L32-D4096 causal language model trained on the Pile. See https://github.com/BlinkDL/RWKV-LM for details.; Use https://github.com/BlinkDL/ChatRWKV to run it.; ctx_len = 1024
n_layer = 32
n_embd = 4096; RWKV-4-Pile-7B-20230109-ctx4096.pth : Fine-tuned to ctx_len 4096."
CLIP-ViT-H-14-laion2B-s32B-b79K,Zero-Shot Image Classification,PyTorch; OpenCLIP,,mit,https://arxiv.org/pdf/1910.04867.pdf,137,,"1,850,290",8072.710619,58,https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K,"A CLIP ViT-H/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Romain Beaumont on the stability.ai cluster. ; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. ; Zero-shot image classification, image and text retrieval, among others."
polyglot-ko-1.3b,Text Generation,PyTorch; Safetensors; Transformers,Korean,apache-2.0,https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2204.04541.pdf; https://arxiv.org/pdf/2306.02254.pdf,47,,"14,108",5644.418442,4,https://huggingface.co/EleutherAI/polyglot-ko-1.3b,"Polyglot-Ko is a series of large-scale Korean autoregressive language models made by the EleutherAI polyglot team.; The model consists of 24 transformer layers with a model dimension of 2048, and a feedforward dimension of 8192. The model
dimension is split into 16 heads, each with a dimension of 128. Rotary Position Embedding (RoPE) is applied to 64
dimensions of each head. The model is trained with a tokenization vocabulary of 30003.; Polyglot-Ko-1.3B was trained on 863 GB of Korean language data (1.2TB before processing), a large-scale dataset curated by TUNiB. The data collection process has abided by South Korean laws. This dataset was collected for the purpose of training Polyglot-Ko models, so it will not be released for public use.  ; Furthermore, in order to avoid the model memorizing and generating personally identifiable information (PII) in the training data, we masked out the following sensitive information in the pre-processing stage:; Polyglot-Ko-1.3B was trained on 213 billion tokens over 102,000 steps on 256 A100 GPUs with the GPT-NeoX framework. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token. "
polyBERT,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,,,,2,,"2,144",101.5721082,,https://huggingface.co/kuelumbus/polyBERT,"This is polyBERT: A chemical language model to enable fully machine-driven ultrafast polymer informatics. polyBERT maps PSMILES strings to 600 dimensional dense fingerprints. The fingerprints numerically represent polymer chemical structures. Please see the license agreement in the LICENSE file.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; See https://github.com/Ramprasad-Group/polyBERT and paper on arXiv."
collaborative-filtering-model,,TensorBoard; Keras,,,,2,,0,0.120433617,1,https://huggingface.co/hqasmei/collaborative-filtering-model,This repo contains the model and the notebook on how to build and train a Keras model for Collaborative Filtering for Book Recommendations. ; Full credits to Hosna Qasmei.; More information needed; More information needed; The following hyperparameters were used during training:
lugal-ki-en,,,,mit,,14,,0,0.006423416,,https://huggingface.co/sd-concepts-library/lugal-ki-en,"This is the <lugal-ki-en> concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the Stable Conceptualizer notebook. You can also train your own concepts and load them into the concept libraries using this notebook.; Here is the new concept you will be able to use as a style:




; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
OPT-2.7B-Erebus,Text Generation,PyTorch; Transformers,English,other,https://arxiv.org/pdf/2205.01068.pdf,31,,"22,391",5428.638276,2,https://huggingface.co/KoboldAI/OPT-2.7B-Erebus,"This is the second generation of the original Shinen made by Mr. Seeker. The full dataset consists of 6 different sources, all surrounding the ""Adult"" theme. The name ""Erebus"" comes from the greek mythology, also named ""darkness"". This is in line with Shin'en, or ""deep abyss"". For inquiries, please contact the KoboldAI community. Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The data can be divided in 6 different datasets:; The dataset uses [Genre: <comma-separated list of genres>] for tagging.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; Based on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion). Warning: This model has a very strong NSFW bias!"
t5-base-hybrid-question-generator,Text2Text Generation,PyTorch; Transformers,,apache-2.0,,3,,63,895.2006234,,https://huggingface.co/PrimeQA/t5-base-hybrid-question-generator,"This is an t5-base model, finetuned to generate questions given a table and linked passages using HybridQA dataset. It was trained to generate questions from reasoning paths extracted from hybrid input, i.e., a table and the passages linked to the table cells. ; Language model: t5-base Language: English Task: Hybrid Question Generation Data: HybridQA; One can use this model to generate questions given a table and linked passages. Biases associated with pre-training of T5 and HybridQA dataset may be present."
anya-forger,,,,mit,,2,,0,0.006238823,,https://huggingface.co/sd-concepts-library/anya-forger,"This is the <anya-forger> concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the Stable Conceptualizer notebook. You can also train your own concepts and load them into the concept libraries using this notebook.; Here is the new concept you will be able to use as an object:




; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
polyglot-ko-5.8b,Text Generation,PyTorch; Safetensors; Transformers,Korean,apache-2.0,https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2204.04541.pdf; https://arxiv.org/pdf/2306.02254.pdf,42,,"7,651",24072.30356,1,https://huggingface.co/EleutherAI/polyglot-ko-5.8b,"Polyglot-Ko is a series of large-scale Korean autoregressive language models made by the EleutherAI polyglot team.; The model consists of 28 transformer layers with a model dimension of 4096, and a feedforward dimension of 16384. The model
dimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64
dimensions of each head. The model is trained with a tokenization vocabulary of 30003.; Polyglot-Ko-5.8B was trained on 863 GB of Korean language data (1.2TB before processing), a large-scale dataset curated by TUNiB. The data collection process has abided by South Korean laws. This dataset was collected for the purpose of training Polyglot-Ko models, so it will not be released for public use.  ; Furthermore, in order to avoid the model memorizing and generating personally identifiable information (PII) in the training data, we masked out the following sensitive information in the pre-processing stage:; Polyglot-Ko-5.8B was trained for 172 billion tokens over 320,000 steps on 256 A100 GPUs with the GPT-NeoX framework. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token. "
codebert-javascript,Fill-Mask,PyTorch; Transformers,,,https://arxiv.org/pdf/2302.05527.pdf,5,,"6,084",502.3392293,,https://huggingface.co/neulab/codebert-javascript,"This is a microsoft/codebert-base-mlm model, trained for 1,000,000 steps (with batch_size=32)  on JavaScript code from the codeparrot/github-code-clean dataset, on the masked-language-modeling task.; It is intended to be used in CodeBERTScore: https://github.com/neulab/code-bert-score, but can be used for any other model or task.; For more information, see: https://github.com/neulab/code-bert-score; If you use this model for research, please cite:"
whisper-tiny,Automatic Speech Recognition,PyTorch; TensorFlow; JAX; Transformers,99 languages,apache-2.0,https://arxiv.org/pdf/2212.04356.pdf,79,,"75,444",456.9854602,32,https://huggingface.co/openai/whisper-tiny,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al from OpenAI. The original code repository can be found here.; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. ; The models were trained on either English-only data or multilingual data. The English-only models were trained 
on the task of speech recognition. The multilingual models were trained on both speech recognition and speech 
translation. For speech recognition, the model predicts transcriptions in the same language as the audio. 
For speech translation, the model predicts transcriptions to a different language to the audio."
Taiyi-CLIP-RoBERTa-102M-ViT-L-Chinese,Feature Extraction,PyTorch; Safetensors; Transformers,,apache-2.0,https://arxiv.org/pdf/2209.02970.pdf,15,,142,818.1161531,2,https://huggingface.co/IDEA-CCNL/Taiyi-CLIP-RoBERTa-102M-ViT-L-Chinese,"首个开源的中文CLIP模型，1.23亿图文对上进行预训练的文本端RoBERTa-base; The first open source Chinese CLIP, pre-training on 123M image-text pairs, the text encoder: RoBERTa-base.; 我们遵循CLIP的实验设置，以获得强大的视觉-语言表征。在训练中文版的CLIP时，我们使用chinese-roberta-wwm作为语言的编码器，并将open_clip中的ViT-L-14应用于视觉的编码器。为了快速且稳定地进行预训练，我们冻结了视觉编码器并且只微调语言编码器。此外，我们将Noah-Wukong数据集(100M)和Zero数据集(23M)用作预训练的数据集。在悟空数据集和zero数据集上预训练24轮,在A100x32上训练了6天。据我们所知，我们的Taiyi-CLIP是目前Huggingface社区中首个的开源中文CLIP。; We follow the experimental setup of CLIP to obtain powerful visual-language intelligence. To obtain the CLIP for Chinese, we employ chinese-roberta-wwm for the language encoder, and apply the ViT-L-14 in open_clip for the vision encoder. We freeze the vision encoder and tune the language encoder to speed up and stabilize the pre-training process. Moreover, we apply Noah-Wukong dataset (100M) and Zero dataset (23M) as the pre-training datasets. The model was first trained 24 epochs on wukong and zero, which takes 6 days to train on A100x32. To the best of our knowledge, our TaiyiCLIP is currently the only open-sourced Chinese CLIP in the huggingface community.; Zero-Shot Classification"
tts,Text-to-Speech,,English,,https://arxiv.org/pdf/2104.01497.pdf,21,CMUArctic; Hi-Fi,0,1025.80332,12,https://huggingface.co/balacoon/tts,"Here you can find models compatible with Balacoon software.
There are several model types to be aware of:; You can check the interactive demo
balacoon/tts space.; Unable to determine this model’s library. Check the
								docs 
.
							"
nllb-jaen-1.3B-lightnovels,Text2Text Generation,PyTorch; Safetensors; Transformers,English; Japanese,cc-by-nc-4.0,,3,,80,11286.37576,,https://huggingface.co/thefrigidliquidation/nllb-jaen-1.3B-lightnovels,"This model was fine-tuned on light and web novel for Japanese to English translation.; It can translate sentences and paragraphs up to 512 tokens.; Generating with diverse beam search seems to work best. Add the following to model.generate:; You can provide up to 10 custom translations for nouns and character names at runtime. To do so, surround the Japanese term with term tokens. Prefix the word with one of <t0>, <t1>, ..., <t9> and suffix the word with </t>. The term will be translated as the prefix term token which can then be string replaced.; For example, in マイン、ルッツが迎えに来たよ if you wish to have マイン translated as Myne you would replace マイン with <t0>マイン</t>. The model will translate <t0>マイン</t>、ルッツが迎えに来たよ as <t0>, Lutz is here to pick you up. Then simply do a string replacement on the output, replacing <t0> with Myne."
pony-diffusion,Text-to-Image,Diffusers,English,bigscience-bloom-rail-1.0,,55,,"1,511",0.006355782,132,https://huggingface.co/AstraliteHeart/pony-diffusion,Pony Diffusion V4 is now live!; pony-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality pony SFW-ish images through fine-tuning.; With special thanks to Waifu-Diffusion for providing finetuning expertise and Novel AI for providing necessary compute.; ; 
VToonify,,PyTorch,,,https://arxiv.org/pdf/2209.11224.pdf,37,,0,0.004072266,20,https://huggingface.co/PKUWilliamYang/VToonify,"This system provides a web demo for the following paper:; VToonify: Controllable High-Resolution Portrait Video Style Transfer (TOG/SIGGRAPH Asia 2022); Abstract ; Generating high-quality artistic portrait videos is an important and desirable task in computer graphics and vision. Although a series of successful portrait image toonification models built upon the powerful StyleGAN have been proposed, these image-oriented methods have obvious limitations when applied to videos, such as the fixed frame size, the requirement of face alignment, missing non-facial details and temporal inconsistency. In this work, we investigate the challenging controllable high-resolution portrait video style transfer by introducing a novel VToonify framework. Specifically, VToonify leverages the mid- and high-resolution layers of StyleGAN to render high-quality artistic portraits based on the multi-scale content features extracted by an encoder to better preserve the frame details. The resulting fully convolutional architecture accepts non-aligned faces in videos of variable size as input, contributing to complete face regions with natural motions in the output. Our framework is compatible with existing StyleGAN-based image toonification models to extend them to video toonification, and inherits appealing features of these models for flexible style control on color and intensity. This work presents two instantiations of VToonify built upon Toonify and DualStyleGAN for collection-based and exemplar-based portrait video style transfer, respectively. Extensive experimental results demonstrate the effectiveness of our proposed VToonify framework over existing methods in generating high-quality and temporally-coherent artistic portrait videos with flexible style controls.; S-Lab License 1.0"
Zero-Two,,,,,,3,,0,2181.121708,,https://huggingface.co/waifu-research-department/Zero-Two,Trainer: naotsue; Zero Two from Darling in the FranXX; Training: 20 images; Regularization: 300 images; Model Used: Waifu Diffusion 1.3 (Epoch 6)
sd-wikiart-v2,Text-to-Image,Diffusers,English,bigscience-bloom-rail-1.0,,37,,45,1.304514847,,https://huggingface.co/valhalla/sd-wikiart-v2,"sd-wikiart-v2 is a stable diffusion model that has been fine-tuned on the wikiart dataset to generate artistic images in different style and genres.; ; The model originally used for fine-tuning is Stable Diffusion V1-4, which is a latent image diffusion model trained on LAION2B-en.; The current model has been fine-tuned with a learning rate of 1e-05 for 1 epoch on 81K text-image pairs from wikiart dataset. Only the attention layers of the model are fine-tuned. This is done to avoid catastrophic forgetting, the model can generate artistic images given specific prompts but still retains most of its previous knowledge.; TODO"
t5-efficient-base-turkish,Text2Text Generation,PyTorch; TensorBoard; Safetensors; Transformers,,mit,,3,,164,6487.109911,,https://huggingface.co/Turkish-NLP/t5-efficient-base-turkish,
t5-efficient-large-turkish,Text2Text Generation,PyTorch; TensorBoard; Safetensors; Transformers,,mit,,5,,19,11300.34935,,https://huggingface.co/Turkish-NLP/t5-efficient-large-turkish,
finetune-translation-t5-super-super-tiny-standard-bahasa-cased,Translation,PyTorch; Transformers,Malay,,,1,,"2,285",24.09273586,,https://huggingface.co/mesolitica/finetune-translation-t5-super-super-tiny-standard-bahasa-cased,"Finetuned T5 super super tiny on EN-MS and MS-EN translation tasks.; Scripts at https://github.com/huseinzol05/malaya/tree/master/session/translation/hf-t5; eng_Latn-zsm_Latn,; zsm_Latn-eng_Latn,"
spider-verse-diffusion,Text-to-Image,Diffusers,,creativeml-openrail-m,,335,,"3,207",2181.910604,176,https://huggingface.co/nitrosocke/spider-verse-diffusion,"Spider-Verse Diffusion; This is the fine-tuned Stable Diffusion model trained on movie stills from Sony's Into the Spider-Verse.
Use the tokens spiderverse style in your prompts for the effect.; If you enjoy my work, please consider supporting me
; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; You can also export the model to ONNX, MPS and/or FLAX/JAX."
pyannote-speaker-diarization-endpoint,Voice Activity Detection,pyannote.audio,,mit,https://arxiv.org/pdf/2012.01477.pdf,12,ami; dihard; voxconverse; aishell; repere; voxceleb,"16,220",2.668153496,,https://huggingface.co/philschmid/pyannote-speaker-diarization-endpoint,"Relies on pyannote.audio 2.0: see installation instructions.; In case the number of speakers is known in advance, one can use the num_speakers option:; One can also provide lower and/or upper bounds on the number of speakers using min_speakers and max_speakers options:; If you feel adventurous, you can try and play with the various pipeline hyper-parameters.For instance, one can use a more aggressive voice activity detection by increasing the value of segmentation_onset threshold:; Real-time factor is around 5% using one Nvidia Tesla V100 SXM2 GPU (for the neural inference part) and one Intel Cascade Lake 6248 CPU (for the clustering part)."
AraT5-base-question-generation,Text2Text Generation,PyTorch; Transformers,Arabic,,,8,,127,1164.886327,1,https://huggingface.co/Mihakram/AraT5-base-question-generation,"This model is ready to use for Question Generation task, simply input the text and answer, the model will generate a question, This model is a fine-tuned version of AraT5-Base; Get the Question from given Context and a Answer : Arabic QG Model; If you want to cite this model you can use this:; Mihoubi Akram Fawzi: Linkedin | Github | mihhakram@gmail.com; Ibrir Adel: Linkedin | Github | adelibrir2015@gmail.com"
Arabic-question-generation,Text2Text Generation,PyTorch; Transformers,Arabic,,https://arxiv.org/pdf/2109.12068.pdf,4,,101,1164.886669,1,https://huggingface.co/MIIB-NLP/Arabic-question-generation,"This model is ready to use for Question Generation task, simply input the text and answer, the model will generate a question, This model is a fine-tuned version of AraT5-Base Model ; Get the Question from given Context and a Answer : Arabic QG Model; The Ara-T5 model was presented in AraT5: Text-to-Text Transformers for Arabic Language Generation by El Moatez Billah Nagoudi, AbdelRahim Elmadany, Muhammad Abdul-Mageed ; Mihoubi Akram Fawzi: Linkedin | Github | mihhakram@gmail.com; Ibrir Adel: Linkedin | Github | adelibrir2015@gmail.com"
NepBERTa,Fill-Mask,TensorFlow; Transformers,,cc-by-nc-sa-4.0,,4,,296,534.5365325,,https://huggingface.co/NepBERTa/NepBERTa,"This repository contains the pre-trained language model in the Nepali language. Our work has been accepted as ""The 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, 2022"". More details on the paper will be posted after the publishment of the article. "
animefull-final-pruned,,,,,,119,,38,18813.68333,,https://huggingface.co/a1079602570/animefull-final-pruned,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
pcia,,,,unknown,,40,,0,0.024609375,,https://huggingface.co/Cinnamomo/pcia,"Checkpoint and Safetensors Model Storage for AI Art Generator; PCIA means, 'Per Creare con l'Intelligenza Artificiale'.; Model List; Safetensors; Stable Diffusion"
flipped_11B,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2210.02969.pdf,20,bigscience/P3,40,46152.5154,,https://huggingface.co/seonghyeonye/flipped_11B,"Official repository: seonghyeonye/Flipped-Learning; FLIPPED uses a unique meta-learning method to show zero-shot task generalization on classification natural language prompts, outperforming GPT-3 and T0-11B on many tasks with a 4x smaller scale.
It is a series of encoder-decoder model trained on a numerous classification dataset. We show inputs and its corresponding outputs of each instances in each dataset to FLIPPED, and train it to generate its possible instruction. We add unlikelihood loss in order not to generate the instruction when given the same input, but a wrong output. To obtain FLIPPED, we fine-tune a T5 model in a given scale on a multitask mixture covering many different classification NLP tasks.; You can use the models to perform inference on tasks by specifying your input-output NLP query in a ""input: {input}\noutput: {output}"" form , and the model will predict the instruction. For example, You can try 
""input:  this is the best cast iron skillet you will ever buy\noutput: Positive""
as an input, and the model will hopefully generate ""Title: Review:"".; Our overall explanation models along with ablations can be found in our paper. We recommend using the FLIPPED-11B checkpoint as it leads (on average) to the best performances on a variety of NLP tasks.; If you want to use another checkpoint, please replace the path in T5Tokenizer and T5ForConditionalGeneration.
We also provide a quick Jupyter Notebook where you can inference with our method.
Note: the model was trained with bfloat16 activations. As such, we highly discourage running inference with fp16."
ff7r-stable-diffusion,,,,creativeml-openrail-m,,26,,0,6583.027567,,https://huggingface.co/panopstor/ff7r-stable-diffusion,"https://huggingface.co/spaces/CompVis/stable-diffusion-license; EveryDream Discord: https://discord.gg/nrvPgh94cC; The new version is trained from a basis of the RunwayML 1.5 ckpt.  This fine tuning sheds the last remnant of the concepts in original DreamBooth paper as regularization via generated images is dropped in favor of a mix  a scrape of laion to protect the model's original qualities instead.  1636 training images, 1636 ground truth images from laion were trained for 19009 steps at LR 4e-7.; Results here (warning, huge image files); general model test"
sd-vae-ft-ema-original,Text-to-Image,,,mit,,108,,0,670.0078418,,https://huggingface.co/stabilityai/sd-vae-ft-ema-original,"These weights are intended to be used with the original CompVis Stable Diffusion codebase. If you are looking for the model to use with the ? diffusers library, come here.; We publish two kl-f8 autoencoder versions, finetuned from the original kl-f8 autoencoder on a 1:1 ratio of LAION-Aesthetics and LAION-Humans, an unreleased subset containing only SFW images of humans. The intent was to fine-tune on the Stable Diffusion training set (the autoencoder was originally trained on OpenImages) but also enrich the dataset with images of humans to improve the reconstruction of faces.
The first, ft-EMA, was resumed from the original checkpoint, trained for 313198 steps and uses EMA weights. It uses the same loss configuration as the original checkpoint (L1 + LPIPS).
The second, ft-MSE, was resumed from ft-EMA and uses EMA weights and was trained for another 280k steps using a different loss, with more emphasis 
on MSE reconstruction (MSE + 0.1 * LPIPS). It produces somewhat ``smoother'' outputs. The batch size for both versions was 192 (16 A100s, batch size 12 per GPU).
To keep compatibility with existing models, only the decoder part was finetuned; the checkpoints can be used as a drop-in replacement for the existing autoencoder.; Original kl-f8 VAE vs f8-ft-EMA vs f8-ft-MSE; Visualization of reconstructions on  256x256 images from the COCO2017 validation dataset. ; 


256x256: ft-EMA (left), ft-MSE (middle), original (right)
"
command-and-conquer-remastered-cameos,,,,mit,,3,,0,0.014875946,,https://huggingface.co/sd-concepts-library/command-and-conquer-remastered-cameos,"This is the <command_and_conquer_remastered_cameos> concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the Stable Conceptualizer notebook. You can also train your own concepts and load them into the concept libraries using this notebook.; Here is the new concept you will be able to use as a style:









































; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
finbert-esg-9-categories,Text Classification,PyTorch; Transformers,English,,,21,,"1,012",439.2277832,,https://huggingface.co/yiyanghkust/finbert-esg-9-categories,"ESG analysis can help investors determine a business' long-term sustainability and identify associated risks. FinBERT-esg-9-categories is a FinBERT model fine-tuned on about 14,000 manually annotated sentences from firms' ESG reports and annual reports. ; finbert-esg-9-categories classifies a text into nine fine-grained ESG topics: Climate Change, Natural Capital, Pollution & Waste, Human Capital, Product Liability, Community Relations, Corporate Governance, Business Ethics & Values, and Non-ESG. This model complements finbert-esg which classifies a text into four coarse-grained ESG themes (E, S, G or None). ; Detailed description of the nine fine-grained ESG topic definition, some examples for each topic, training sample, and the model’s performance can be found here.; Input: A text.; Output: Climate Change, Natural Capital, Pollution & Waste, Human Capital, Product Liability, Community Relations, Corporate Governance, Business Ethics & Values, or Non-ESG."
table-transformer-structure-recognition,Object Detection,PyTorch; Transformers,,mit,https://arxiv.org/pdf/2110.00061.pdf,68,,"97,785",116.0042164,15,https://huggingface.co/microsoft/table-transformer-structure-recognition,"Table Transformer (DETR) model trained on PubTables1M. It was introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al. and first released in this repository. ; Disclaimer: The team releasing Table Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.; The Table Transformer is equivalent to DETR, a Transformer-based object detection model. Note that the authors decided to use the ""normalize before"" setting of DETR, which means that layernorm is applied before self- and cross-attention.; You can use the raw model for detecting the structure (like rows, columns) in tables. See the documentation for more info."
sbert_pq,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,Russian,unlicense,,10,,92,120.4956718,3,https://huggingface.co/inkoziev/sbert_pq,"Это sentence-transformers модель, предназначенная
для определения релевантности короткого текста (преимущественно одно предложение длиной до 10-15 слов) и вопроса.; Модель вычисляет для текста и вопроса векторы размерностью 312. Косинус угла между этими векторами
дает оценку того, содержит ли текст ответ на заданный вопрос. В проекте диалоговой системы 
она используется для семантического поиска записей в базе фактов по заданному собеседником вопросу.; Модель основана на cointegrated/rubert-tiny2.
Она имеет очень небольшой размер и быстро выполняет инференс даже на CPU.; Максимальное значение метрики cossim_f1 на тестовой выборке (10% датасета) равно 0.986.; При использовании модели sberbank-ai/ruBert-base в качестве базовой, максимум cossim_f1 составляет 0.992."
GODEL-v1_1-large-seq2seq,Conversational,PyTorch; Transformers,,mit,https://arxiv.org/pdf/2206.11309.pdf,72,,"4,002",3023.230947,15,https://huggingface.co/microsoft/GODEL-v1_1-large-seq2seq,"GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.; Chitchat example:; Instruction: given a dialog context, you need to response empathically.  
User: Does money buy happiness? 
Agent: It is a question. Money buys you a lot of things, but not enough to buy happiness. 
User: What is the best way to buy happiness ? 
Agent: Happiness is bought through your experience and not money. ; Grounded response generation example:; Instruction: given a dialog context and related knowledge, you need to response safely based on the knowledge. 
Knowledge: The best Stardew Valley mods PCGamesN_0 / About SMAPI 
User: My favorite game is stardew valley. stardew valley is very fun. 
Agent: I love Stardew Valley mods, like PCGamesN_0 / About SMAPI. "
SciNewsBERT,Fill-Mask,PyTorch; Transformers,,afl-3.0,,1,,5,440.229906,,https://huggingface.co/psmeros/SciNewsBERT,"Model Release for CIKM '21 paper: P. Smeros, C. Castillo, K. Aberer. SciClops: Detecting and Contextualizing Scientific Claims for Assisting Manual Fact-Checking. [pdf, slides, bib]"
jmann-large-580k,,Diffusers,,mit,,1,,95,0.002921295,1,https://huggingface.co/harmonai/jmann-large-580k,"Dance Diffusion is now available in ? Diffusers.; Faster at a small loss of quality; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Randeng-T5-784M-QA-Chinese,Question Answering,PyTorch; Transformers,Chinese,,https://arxiv.org/pdf/2210.08590.pdf,26,,709,1608.657649,1,https://huggingface.co/IDEA-CCNL/Randeng-T5-784M-QA-Chinese,"T5 for Chinese Question Answering; This T5-Large model, is the first pretrained generative question answering model for Chinese in huggingface. It was pretrained on the Wudao 180G corpus, and finetuned on Chinese SQuAD and CMRC2018 dataset. It can produce a fluent and accurate answer given a passage and question.; 这是huggingface上首个中文的生成式问答模型。它基于T5-Large结构，使用悟道180G语料在封神框架进行预训练，在翻译的中文SQuAD和CMRC2018两个阅读理解数据集上进行微调。输入一篇文章和一个问题，可以生成准确流畅的回答。;  CMRC 2018 dev (Original span prediction task, we cast it as a generative QA task);  CMRC 2018的测试集上的效果（原始任务是一个起始和结束预测问题，这里作为一个生成回答的问题）"
Erlangshen-TCBert-1.3B-Sentence-Embedding-Chinese,,PyTorch; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/2211.11304.pdf,4,,20,5242.99873,,https://huggingface.co/IDEA-CCNL/Erlangshen-TCBert-1.3B-Sentence-Embedding-Chinese,"1.3BM参数的句子表征Topic Classification BERT (TCBert)。; The TCBert with 1.3BM parameters is pre-trained for sentence representation for Chinese topic classification tasks.; 为了提高模型在话题分类上句子表征效果，我们收集了大量话题分类数据进行基于prompts的对比学习预训练。; To improve the model performance on sentence representation for the topic classification task, we collected numerous topic classification datasets for contrastive pre-training based on general prompts.; 我们为每个数据集设计了两个prompt模板。"
Randeng-BART-139M-QG-Chinese,Text2Text Generation,PyTorch; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/2210.08590.pdf,5,chinesesquad,273,265.8367136,,https://huggingface.co/IDEA-CCNL/Randeng-BART-139M-QG-Chinese,"善于处理问题生成任务的中文版 BART-base 模型。; Good at solving question generation tasks Bart-base Model (Chinese version).; 基于IDEA-CCNL/Randeng-BART-139M，我们在 ChineseSQuAD 数据集上微调了问题生成任务版本。该数据集翻译了部分SQuAD数据集，包含约 67k 有答案的训练样本。; Based on IDEA-CCNL/Randeng-BART-139M, we fine-tuned a question generation version on ChineseSQuAD datasets. The dataset is translated from SQuAD 2.0, with around 67k samples with answer.; 如果您在您的工作中使用了我们的模型，可以引用我们的论文："
Randeng-T5-784M-MultiTask-Chinese,Text2Text Generation,PyTorch; Safetensors; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/2209.02970.pdf,47,,"1,124",6432.540128,,https://huggingface.co/IDEA-CCNL/Randeng-T5-784M-MultiTask-Chinese,"在Randeng-T5-784M的基础上，收集了100个左右的中文数据集，进行Text2Text统一范式的有监督任务预训练。; On the basis of Randeng-T5-784M, about 100 Chinese datasets were collected and pre-trained for the supervised task of Text2Text unified paradigm.; 本模型在中文zero-shot榜单ZeroClue上取得了第三名（不包括人类）的成绩，在所有基于T5（encoder-decoder架构）的模型中排名第一。; This model achieved the 3rd place (excluding humans) on the Chinese zero-shot benchmark ZeroClue, ranking first among all models based on T5 (encoder-decoder architecture).; "
bert-tiny-finetuned-enron-spam-detection,Text Classification,PyTorch; TensorBoard; Safetensors; Transformers,,apache-2.0,,3,SetFit/enron_spam,131,36.12947571,,https://huggingface.co/mrm8488/bert-tiny-finetuned-enron-spam-detection,This model is a fine-tuned version of google/bert_uncased_L-2_H-128_A-2 (aka BERT-Tiny) on an SetFit/enron_spam for Spam Dectection downstream task.; It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed
cutesexyrobutts,,,,creativeml-openrail-m,,15,,0,10905.60322,,https://huggingface.co/SirVeggie/cutesexyrobutts,"Original artist: CutesexyrobuttsPatreon: https://www.patreon.com/cutesexyrobutts; Token and Class words are what guide the AI to produce images similar to the trained style/object/character.
Include any mix of these words in the prompt to produce verying results, or exclude them to have a less pronounced effect.
There is usually at least a slight stylistic effect even without the words, but it is recommended to include at least one.
Adding token word/phrase class word/phrase at the start of the prompt in that order produces results most similar to the trained concept, but they can be included elsewhere as well. Some models produce better results when not including all token/class words.; 3k models are are more flexible, while 5k models produce images closer to the trained concept.
I recommend 2k/3k models for normal use, and 5k/6k models for model merging and use without token/class words.
However it can be also very prompt specific. I highly recommend self-experimentation.; These models are subject to the same legal concerns as their base models.; Epoch 5 version was earlier in the waifu diffusion 1.3 training process, so it is easier to produce more varied, non anime, results."
chinese-lert-large,Fill-Mask,PyTorch; TensorFlow; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/2211.05344.pdf,12,,499,2765.172725,,https://huggingface.co/hfl/chinese-lert-large,LERT is a linguistically-motivated pre-trained language model.; Further information: https://github.com/ymcui/LERT/blob/main/README_EN.md
alberteinstein-physicstoday-physicstweet,Text Generation,PyTorch; Transformers,English,,,1,,19,513.3445564,,https://huggingface.co/huggingtweets/alberteinstein-physicstoday-physicstweet,"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report."
Taiyi-Stable-Diffusion-1B-Chinese-v0.1,Text-to-Image,Diffusers,Chinese,creativeml-openrail-m,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2209.02970.pdf,386,,"37,188",4280.33058,15,https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1,"首个开源的中文Stable Diffusion模型，基于0.2亿筛选过的中文图文对训练。; The first open source Chinese Stable diffusion, which was trained on 20M filtered Chinese image-text pairs.; 可以在Taiyi-Stable-Diffusion-Chinese体验我们的模型。; We support a Gradio Web UI to run Taiyi-Stable-Diffusion-1B-Chinese-v0.1:
Taiyi-Stable-Diffusion-Chinese; 首个开源的中英双语Stable Diffusion模型，基于0.2亿筛选过的中文图文对训练。"
dishonored-portrait-styles,,,,mit,,33,,0,0.008526611,,https://huggingface.co/sd-concepts-library/dishonored-portrait-styles,"This is the <portrait-style-dishonored> concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the Stable Conceptualizer notebook. You can also train your own concepts and load them into the concept libraries using this notebook.; Here is the new concept you will be able to use as a style:



















; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
SD_PixelArt_SpriteSheet_Generator,Text-to-Image,Diffusers,English,apache-2.0,,361,,"3,126",4372.484842,11,https://huggingface.co/Onodofthenorth/SD_PixelArt_SpriteSheet_Generator,"This Stable diffusion checkpoint allows you to generate pixel art sprite sheets from four different angles.
These first images are my results after merging this model with another model trained on my wife. merging another model with this one is the easiest way to get a consistent character with each view. still requires a bit of playing around with settings in img2img to get them how you want. for left and right, I suggest picking your best result and mirroring. after you are satisfied take your photo into photoshop or Krita, remove the background, and scale to the desired size. after this you can scale back up to display your results; this also clears up some of the color murkiness in the initial outputs.
; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; You can also export the model to ONNX, MPS and/or FLAX/JAX.; For the front view use ""PixelartFSS""
; For the right view use ""PixelartRSS""
"
clipseg-rd64-refined,Image Segmentation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2112.10003.pdf,46,,"6,353,215",604.5810207,57,https://huggingface.co/CIDAS/clipseg-rd64-refined,"CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by Lüddecke et al. and first released in this repository. ; This model is intended for zero-shot and one-shot image segmentation.; Refer to the documentation.; Inference API has been turned off for this model."
esmfold_v1,,PyTorch; Transformers,,mit,,11,,"2,641",8642.564333,3,https://huggingface.co/facebook/esmfold_v1,"ESMFold is a state-of-the-art end-to-end protein folding model based on an ESM-2 backbone. It does not require any lookup or MSA step, and therefore does not require any external databases to be present in order to make predictions. As a result, inference time is very significantly faster than AlphaFold2. For details on the model architecture and training, please refer to the accompanying paper.; If you're interested in using ESMFold in practice, please check out the associated tutorial notebook.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
stt_ru_conformer_ctc_large,Automatic Speech Recognition,NeMo; PyTorch,Russian,cc-by-4.0,https://arxiv.org/pdf/2005.08100.pdf,3,mozilla-foundation/common_voice_10_0; SberDevices/Golos; Russian-LibriSpeech; SOVA-Dataset,627,487.0089941,,https://huggingface.co/nvidia/stt_ru_conformer_ctc_large,"| 
| 
| ; This model transcribes speech into lowercase Cyrillic alphabet including space, and is trained on around 1636 hours of Russian speech data.
It is a non-autoregressive ""large"" variant of Conformer, with around 120 million parameters.
See the model architecture section and NeMo documentation for complete architecture details.; The model is available for use in the NeMo toolkit [3], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.; To train, fine-tune or play with the model you will need to install NVIDIA NeMo. We recommend you install it after you've installed latest PyTorch version.; Simply do:"
roberta-base-cold,Text Classification,PyTorch; Safetensors; Transformers,Chinese,,https://arxiv.org/pdf/2201.06025.pdf,10,,702,818.1115452,3,https://huggingface.co/thu-coai/roberta-base-cold,hfl/chinese-roberta-wwm-ext fine-tuned on the COLDataset. Usage example:; This fine-tuned model obtains 82.75 accuracy and 82.39 macro-F1 on the test set.; Please kindly cite the original paper if you use this model.
F111,,,,unknown,,7,,0,4372.481469,,https://huggingface.co/Reachout/F111,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
BioBERT-mnli-snli-scinli-scitail-mednli-stsb,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,,,,21,,914,433.8689023,,https://huggingface.co/pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. It has been trained over the SNLI, MNLI, SCINLI, SCITAIL, MEDNLI and STSB datasets for providing robust sentence embeddings.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net"
roberta-zh-sensible,Text Classification,PyTorch; Safetensors; Transformers,Chinese,,,3,,"2,116",818.1113478,,https://huggingface.co/thu-coai/roberta-zh-sensible,"roberta-zh fine-tuned on human-annotated conversational model self-chat data. It supports 2-class calssification for multi-turn dialogue sensible detection.
Usage example:; NOTE: it should be used under similar data distribution."
switch-c-2048,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2101.03961.pdf; https://arxiv.org/pdf/1910.09700.pdf,36,c4,90,426250.2754,,https://huggingface.co/google/switch-c-2048,"; Switch Transformers is a Mixture of Experts (MoE) model trained on Masked Language Modeling (MLM) task. The model architecture is similar to the classic T5, but with the Feed Forward layers replaced by the Sparse MLP layers containing ""experts"" MLP. According to the original paper the model enables faster training (scaling properties) while being better than T5 on fine-tuned tasks. 
As mentioned in the first few lines of the abstract : ;  we advance the current scale of language models by pre-training up to trillion parameter models on the “Colossal Clean Crawled Corpus”, and achieve a 4x speedup over the T5-XXL model.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the original paper.; Note that these checkpoints has been trained on Masked-Language Modeling (MLM) task. Therefore the checkpoints are not ""ready-to-use"" for downstream tasks. You may want to check FLAN-T5 for running fine-tuned weights or fine-tune your own MoE following this notebook"
Van-Gogh-diffusion,Text-to-Image,Diffusers,,creativeml-openrail-m,,262,,824,2200.905223,93,https://huggingface.co/dallinmackay/Van-Gogh-diffusion,"v2 - fixed and working; This is a fine-tuned Stable Diffusion model (based on v1.5) trained on screenshots from the film Loving Vincent. Use the token lvngvncnt at the BEGINNING of your prompts to use the style (e.g., ""lvngvncnt, beautiful woman at sunset""). This model works best with the Euler sampler (NOT Euler_a).; Download the ckpt file from ""files and versions"" tab into the stable diffusion models folder of your web-ui of choice.; If you get too many yellow faces or you dont like the strong blue bias, simply put them in the negative prompt (e.g., ""Yellow face, blue"").; --"
sd-v1-4.ckpt,,,,gpl-3.0,,16,,,0,,https://huggingface.co/az1976alex/sd-v1-4.ckpt,"Empty repo asking for users info; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
MMDv1-18,Text-to-Image,,,creativeml-openrail-m,,64,,0,8744.966992,,https://huggingface.co/ShinCore/MMDv1-18,"MEGA MERGE DIFF (MMD) VERSION 1-18. 18 MERGED MODELS IN ONE; ANNOUNCEMENT:; FIRST MODEL RELEASE: MMD V1-18 MODEL MERGE ALPHA:; SUMMARY:
MMD V1-18 A MEGA MERGE OF SD 1.5 AND 17 OTHER MODELS. IT IS INTENDED TO BE A GENERALIST MODEL, NOT FOCUSED ON ANY SINGLE GENRE OR CATEGORY OR STYLE OR SUBJECT. THERE ARE ALREADY A PROLIFERATION OF GREAT MODELS OUT THERE, COVERING A BROAD SPECTRUM OF CONTENT. HOWEVER, THAT ALSO CAUSES A PROBLEM IN THAT WE HAVE A PROFLIERATION OF GREAT MODELS OUT THERE THAT DO ONE OR TWO THINGS REALLY WELL, BUT THATS KINDA IT. OTHER THAN THOSE ONE OR TWO THINGS THAT HAVE BEEN ADDED TO THE BASE SD MODEL, ITS NO DIFFERENT THAN ANY OF THE OTHER MODELS.; MMD WAS CREATED TO ADDRESS THE ISSUE OF DISORGANIZED CONTENT FRAGMENTATION ACROSS HUGGINGFACE, DISCORD, REDDIT, RENTRY.ORG, 4CHAN, AND THE REMAINDER OF THE INTERNET. IT ALSO TRIES TO ADDRESS THE ISSUES INHERENT WITH THE BASE SD 1.5 MODEL. NAMELY, PROBLEMATIC ANATOMY, LACK OF RESPONSIVENESS TO PROMPT ENGINEERING, BLAND OUTPUTS, ETC."
Stable-Diffusion-FineTuned-zh-v1,Text-to-Image,Diffusers,Chinese,other,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2205.12952.pdf,5,,18,0.011710205,,https://huggingface.co/svjack/Stable-Diffusion-FineTuned-zh-v1,"svjack/Stable-Diffusion-FineTuned-zh-v0 is a Chinese-specific latent text-to-image diffusion model capable of generating images given any Chinese text input.; This model was trained by using a powerful text-to-image model, diffusers
For more information about our training method, see train_zh_model.py.
With the help of a good baseline model Taiyi-Stable-Diffusion-1B-Chinese-v0.1 from IDEA-CCNL; Firstly, install our package as follows. This package is modified ?'s Diffusers library to run Chinese Stable Diffusion.; Run this command to log in with your HF Hub token if you haven't before:; Running the pipeline with the LMSDiscreteScheduler scheduler:"
redshift-diffusion,Text-to-Image,Diffusers,English,creativeml-openrail-m,,597,,"7,523",2181.125731,127,https://huggingface.co/nitrosocke/redshift-diffusion,"This is the fine-tuned Stable Diffusion model trained on high resolution 3D artworks.
Use the tokens redshift style in your prompts for the effect.; The name: I used Cinema4D for a very long time as my go-to modeling software and always liked the redshift render it came with. That is why I was very sad to see the bad results base SD has connected with its token. This is my attempt at fixing that and showing my passion for this render engine.; If you enjoy my work and want to test new models before release, please consider supporting me
; Characters rendered with the model:

Cars and Landscapes rendered with the model:
; (redshift style) robert downey jr as ironman Negative prompt: glasses helmet
Steps: 40, Sampler: DPM2 Karras, CFG scale: 7, Seed: 908018284, Size: 512x704"
Dungeons-and-Diffusion,Text-to-Image,Diffusers,,creativeml-openrail-m,,217,,"5,386",15288.32459,35,https://huggingface.co/0xJustin/Dungeons-and-Diffusion,"FOR THE NEW VERSION DOWNLOAD 'D&Diffusion3.0_Protogen.ckpt'; The newest version is finetuned from Protogen to great effect. Also works great at resolutions great than 512x512!; Species in new version: aarakocra, aasimar, air_genasi, centaur, dragonborn, drow, dwarf, earth_genasi, elf, firbolg, fire_genasi, gith, gnome, goblin, goliath, halfling, human, illithid, kenku, kobold, lizardfolk, minotaur, orc, tabaxi, thrikreen, tiefling, tortle, warforged, water_genasi
Classes in new version: Artificer, Bard, Barbarian, Cleric, Fighter, Druid, Monk, Paladin, Rogue, Ranger, Sorcerer, Warlock, Wizard, Noble, Townsperson; See the training dataset here for a list of races: https://huggingface.co/datasets/0xJustin/Dungeons-and-Diffusion; Model16000 is trained used D&D character as the class prompt, and for whatever reason it ~ seems ~ to work better for centaurs and aarakocra"
Dungeons-and-Diffusion_ckpts,,Diffusers,,,,3,,0,31539.20196,,https://huggingface.co/0xJustin/Dungeons-and-Diffusion_ckpts,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
transformers-ud-japanese-electra-base-discriminator-irony,Text Classification,PyTorch; Transformers,Japanese,cc-by-sa-4.0,,1,,264,437.2243417,,https://huggingface.co/kit-nlp/transformers-ud-japanese-electra-base-discriminator-irony,"This is an ELECTRA Base model for the Japanese language finetuned for automatic irony detection. ; The model was based on transformers-ud-japanese-electra-ginza, and later finetuned on a dataset containing ironic and sarcastic tweets. ; The finetuned model with all attached files is licensed under CC BY-SA 4.0, or Creative Commons Attribution-ShareAlike 4.0 International License. ; ; Please, cite this model using the following citation."
videomae-base-finetuned-kinetics-finetuned-ucf101-subset,Video Classification,PyTorch; TensorBoard; Transformers,,cc-by-nc-4.0,,1,,20,345.0114214,2,https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset,"This model is a fine-tuned version of MCG-NJU/videomae-base-finetuned-kinetics on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
Stable_Diffusion_PaperCut_Model,Text-to-Image,Diffusers,,creativeml-openrail-m,,354,,"1,541",8744.963538,231,https://huggingface.co/Fictiverse/Stable_Diffusion_PaperCut_Model,"This is the fine-tuned Stable Diffusion model trained on Paper Cut images.; Use PaperCut in your prompts.; 

Based on StableDiffusion 1.5 model; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; You can also export the model to ONNX, MPS and/or FLAX/JAX."
midjourney-v4-diffusion,Text-to-Image,Diffusers,English,openrail,,56,,"3,014",0.002103348,44,https://huggingface.co/flax/midjourney-v4-diffusion,
IconsMI-AppIconsModelforSD,Text-to-Image,Diffusers,,creativeml-openrail-m,,130,,672,4362.243216,1,https://huggingface.co/artificialguybr/IconsMI-AppIconsModelforSD,"To use it you have to use the word ''IconsMi'' in the prompt.; From my tests the images look better with this prompt:; highly detailed, trending on artstation, ios icon app, IconsMi; For negative prompts I got better results when I used: out of frame, duplicate, watermark, signature, text, ugly, sketch, deformed, mutated, blurry, mutilated, ugly sketch; I recommend you to instead describe the style of app you want, e.g. news app, music app, sports app. Describe what you want in the image. For example, ''a reporter microphone''. The results are better. SD doesn't understand these abstractions yet."
Complex-Lineart,Text-to-Image,Diffusers,,creativeml-openrail-m,,226,,392,2190.225409,10,https://huggingface.co/Conflictx/Complex-Lineart,"Trained on around 100 images at 768x768 resolution.; Download ""ComplexLA Style.ckpt"" and add it to your model folder.; Use prompt: ComplexLA style
Use resolution near 768x768, lower resolution works but quality will not be as good.; 





"
Vectorartz_Diffusion,Text-to-Image,Diffusers,,creativeml-openrail-m,,102,,"1,082",2183.638379,31,https://huggingface.co/coder119/Vectorartz_Diffusion,"Generate beautiful vector illustration; Trigger word: vectorartz; (Sampler: DPM++ 2S a Karras, Steps: 16, CFG: 7); beautiful landscape, vectorartz
; instagram icon, vectorartz
"
OPT-350M-Erebus,Text Generation,PyTorch; Safetensors; Transformers,English,other,https://arxiv.org/pdf/2205.01068.pdf,8,,"1,841",1326.438373,,https://huggingface.co/KoboldAI/OPT-350M-Erebus,"This is the second generation of the original Shinen made by Mr. Seeker. The full dataset consists of 6 different sources, all surrounding the ""Adult"" theme. The name ""Erebus"" comes from the greek mythology, also named ""darkness"". This is in line with Shin'en, or ""deep abyss"". For inquiries, please contact the KoboldAI community. Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The data can be divided in 6 different datasets:; The dataset uses [Genre: <comma-separated list of genres>] for tagging.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; Based on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion). Warning: This model has a very strong NSFW bias!"
CLIP-ViT-B-32-xlm-roberta-base-laion5B-s13B-b90k,,OpenCLIP,,mit,https://arxiv.org/pdf/1910.04867.pdf,5,,"33,831",1505.654695,5,https://huggingface.co/laion/CLIP-ViT-B-32-xlm-roberta-base-laion5B-s13B-b90k,"A CLIP ViT-B/32 xlm roberta base model trained with the LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Romain Beaumont on the stability.ai cluster. ; Zero-shot image classification, image and text retrieval, among others.; Image classification and other image task fine-tuning, linear probe image classification, image generation guiding and conditioning, among others.; This model was trained with the full LAION-5B (https://laion.ai/blog/laion-5b/)."
HentaiDiffusion,,,,bigscience-openrail-m,,113,,0,26091.5285,2,https://huggingface.co/Deltaadams/HentaiDiffusion,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
xdoc-base-funsd,Token Classification,PyTorch; Transformers,,mit,https://arxiv.org/pdf/2210.02849.pdf,3,,61,526.2292599,,https://huggingface.co/microsoft/xdoc-base-funsd,"XDoc is a unified pre-trained model that deals with different document formats in a single model. With only 36.7% parameters, XDoc achieves comparable or better performance on downstream tasks, which is cost-effective for real-world deployment.; XDoc: Unified Pre-training for Cross-Format Document Understanding
Jingye Chen, Tengchao Lv, Lei Cui, Cha Zhang, Furu Wei, EMNLP 2022; If you find XDoc helpful, please cite us:"
SD-Elysium-Model,,,,openrail,,215,,0,17489.92229,1,https://huggingface.co/hesw23168/SD-Elysium-Model,"Check out new Elysium Kuro here: https://huggingface.co/hesw23168/SD_Elysium_Kuro_Model; Contains:; Elysium - high quality general model with realistic style; Elysium Anime - anime version of Elysium, detailed versatile anime style; Recommended settings:"
AltDiffusion,Text-to-Image,Diffusers,Chinese,creativeml-openrail-m,https://arxiv.org/pdf/2211.06679.pdf,51,,"1,561",0.017812843,3,https://huggingface.co/BAAI/AltDiffusion,"We support a Gradio Web UI to run AltDiffusion:
; 我们使用 AltCLIP，基于 Stable Diffusion 训练了双语Diffusion模型，训练数据来自 WuDao数据集 和 LAION 。; 我们的版本在中英文对齐方面表现非常出色，是目前市面上开源的最强版本，保留了原版stable diffusion的大部分能力，并且在某些例子上比有着比原版模型更出色的能力。; AltDiffusion 模型由名为 AltCLIP 的双语 CLIP 模型支持，该模型也可在本项目中访问。您可以阅读 此教程 了解更多信息。; AltDiffusion支持线上演示，点击 这里 在线试玩！"
galactica-125m,Text Generation,PyTorch; Safetensors; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/1810.03993.pdf,27,,"21,817",502.1499974,23,https://huggingface.co/facebook/galactica-125m,"; Model card from the original repo ; Following Mitchell et al. (2018), this model card provides information about the GALACTICA model, how it was trained, and the intended use cases. Full details about how the model was trained and evaluated can be found in the release paper.; The GALACTICA models are trained on a large-scale scientific corpus. The models are designed to perform scientific tasks, including but not limited to citation prediction, scientific QA, mathematical reasoning, summarization, document generation, molecular property prediction and entity extraction. The models were developed by the Papers with Code team at Meta AI to study the use of language models for the automatic organization of science. We train models with sizes ranging from 125M to 120B parameters. Below is a summary of the released models:; November 2022"
galactica-1.3b,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/1810.03993.pdf,46,,"18,638",2695.269999,13,https://huggingface.co/facebook/galactica-1.3b,"; Model card from the original repo ; Following Mitchell et al. (2018), this model card provides information about the GALACTICA model, how it was trained, and the intended use cases. Full details about how the model was trained and evaluated can be found in the release paper.; The GALACTICA models are trained on a large-scale scientific corpus. The models are designed to perform scientific tasks, including but not limited to citation prediction, scientific QA, mathematical reasoning, summarization, document generation, molecular property prediction and entity extraction. The models were developed by the Papers with Code team at Meta AI to study the use of language models for the automatic organization of science. We train models with sizes ranging from 125M to 120B parameters. Below is a summary of the released models:; November 2022"
yolos-fashionpedia,Object Detection,PyTorch; Transformers,English,,,21,detection-datasets/fashionpedia,"1,641",123.0054283,,https://huggingface.co/valentinafeve/yolos-fashionpedia,"This is a fine-tunned object detection model for fashion.; For more details of the implementation you can check the source code here; the dataset used for its training is available here; this model supports the following categories:; CATS = ['shirt, blouse', 'top, t-shirt, sweatshirt', 'sweater', 'cardigan', 'jacket', 'vest', 'pants', 'shorts', 'skirt', 'coat', 'dress', 'jumpsuit', 'cape', 'glasses', 'hat', 'headband, head covering, hair accessory', 'tie', 'glove', 'watch', 'belt', 'leg warmer', 'tights, stockings', 'sock', 'shoe', 'bag, wallet', 'scarf', 'umbrella', 'hood', 'collar', 'lapel', 'epaulette', 'sleeve', 'pocket', 'neckline', 'buckle', 'zipper', 'applique', 'bead', 'bow', 'flower', 'fringe', 'ribbon', 'rivet', 'ruffle', 'sequin', 'tassel']"
AltDiffusion-m9,Text-to-Image,Diffusers,Chinese,creativeml-openrail-m,https://arxiv.org/pdf/2211.06679.pdf,64,,326,1.68765583,3,https://huggingface.co/BAAI/AltDiffusion-m9,"We support a Gradio Web UI to run AltDiffusion-m9:
; 我们使用 AltCLIP-m9，基于 Stable Diffusion 训练了双语Diffusion模型，训练数据来自 WuDao数据集 和 LAION 。; 我们的版本在多语言对齐方面表现非常出色，是目前市面上开源的最强多语言版本，保留了原版stable diffusion的大部分能力，并且在某些例子上比有着比原版模型更出色的能力。; AltDiffusion-m9 模型由名为 AltCLIP-m9 的多语 CLIP 模型支持，该模型也可在本项目中访问。您可以阅读 此教程 了解更多信息。; We used AltCLIP-m9, and trained a bilingual Diffusion model based on Stable Diffusion, with training data from WuDao dataset and LAION."
tts-fastspeech2-ljspeech,Text-to-Speech,speechbrain,English,apache-2.0,https://arxiv.org/pdf/2006.04558.pdf; https://arxiv.org/pdf/2106.04624.pdf,3,LJSpeech,224,245.5102762,,https://huggingface.co/speechbrain/tts-fastspeech2-ljspeech,"This repository provides all the necessary tools for Text-to-Speech (TTS)  with SpeechBrain using a FastSpeech2 pretrained on LJSpeech.; The pre-trained model takes texts or phonemes as input and produces a spectrogram in output. One can get the final waveform by applying a vocoder (e.g., HiFIGAN) on top of the generated spectrogram. It should be noted that if the input is text, we use a state-of-the-art grapheme-to-phoneme module to convert it to phonemes and then pass the phonemes to fastspeech2 model.; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; If you want to generate multiple sentences in one-shot, you can do in this way:; To perform inference on the GPU, add  run_opts={""device"":""cuda""}  when calling the from_hparams method."
bengali-bert,Fill-Mask,PyTorch; Safetensors; Transformers,Bengali,cc-by-4.0,https://arxiv.org/pdf/2211.11418.pdf,3,,44,1911.574717,,https://huggingface.co/l3cube-pune/bengali-bert,"BengaliBERT is a Bengali BERT model trained on publicly available Bengali monolingual datasets. ; Preliminary details on the dataset, models, and baseline results can be found in our [ paper ] .; Citing:; Other Monolingual Indic BERT models are listed below: 
 Marathi BERT  
 Marathi RoBERTa  
 Marathi AlBERT  ;  Hindi BERT  
 Hindi RoBERTa  
 Hindi AlBERT  "
wd-v1-4-vit-tagger,,Keras; ONNX,,,,41,,99,369.3016797,7,https://huggingface.co/SmilingWolf/wd-v1-4-vit-tagger,"No model card; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
nail-set-diffuser,Text-to-Image,Diffusers,,openrail,,76,,330,0.0039925,7,https://huggingface.co/ringhyacinth/nail-set-diffuser,"This is the fine-tuned Stable Diffusion model trained on images from Nail Sets. ; Use the tokens {Nail Set} in your prompts for the effect.; We support a Gradio Web UI to run Nail-set-Diffusion:
; Stable Diffusion fine tuned on Nail Set by Weekend and Hyacinth.; Put in a text prompt and generate your own nail set!"
text-to-music,Text2Text Generation,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2211.11216.pdf,101,,"4,246",560.6957031,11,https://huggingface.co/sander-wood/text-to-music,"This language-music model takes BART-base fine-tunes on 282,870 English text-music pairs, where all scores are represented in ABC notation. It was introduced in the paper Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task by Wu et al. and released in this repository. ; It is capable of generating complete and semantically consistent sheet music directly from descriptions in natural language based on text. To the best of our knowledge, this is the first model that achieves text-conditional symbolic music generation which is trained on real text-music pairs, and the music is generated entirely by the model and without any hand-crafted rules.; This language-music model is available for online use and experience on Textune: Generating Tune from Text. With this online platform, you can easily input your desired text descriptions and receive a generated sheet music output from the model.; Due to copyright reasons, we are unable to publicly release the training dataset of this model. Instead, we have made available the WikiMusicText (WikiMT) dataset, which includes 1010 pairs of text-music data and can be used to evaluate the performance of language-music models.; You can use this model for text-conditional music generation. All scores generated by this model can be written on one stave (for vocal solo or instrumental solo) in standard classical notation, and are in a variety of styles, e.g., blues, classical, folk, jazz, pop, and world music. We recommend using the script in this repository for inference. The generated tunes are in ABC notation, and can be converted to sheet music or audio using this website, or this software."
Stable-diffusion-models,Text-to-Image,Diffusers,English,creativeml-openrail-m,,33,,0,29027.7652,1,https://huggingface.co/xiaolxl/Stable-diffusion-models,模型均来自网络，此仓库仅作为下载备用
whisper-small-urdu,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,Urdu,apache-2.0,,3,mozilla-foundation/common_voice_11_0,20,968.7670042,,https://huggingface.co/Abdullah17/whisper-small-urdu,"This model is a fine-tuned version of openai/whisper-Small on the Common Voice 11.0 dataset.
It achieves the following results on the evaluation set:; The following hyperparameters were used during training:"
stable-diffusion-2-base,Text-to-Image,Diffusers,,openrail++,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,278,,"44,822",10670.09426,68,https://huggingface.co/stabilityai/stable-diffusion-2-base,"This model card focuses on the model associated with the Stable Diffusion v2-base model, available here.; The model is trained from scratch 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5. Then it is further trained for 850k steps at resolution 512x512 on the same dataset on images with resolution >= 512x512.; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model"
vivit-b-16x2-kinetics400,Video Classification,PyTorch; Transformers,,mit,https://arxiv.org/pdf/2103.15691.pdf,1,,79,356.0213688,,https://huggingface.co/google/vivit-b-16x2-kinetics400,"ViViT model as introduced in the paper ViViT: A Video Vision Transformer by Arnab et al. and first released in this repository. ; Disclaimer: The team releasing ViViT did not write a model card for this model so this model card has been written by the Hugging Face team.; ViViT is an extension of the Vision Transformer (ViT) to video.; We refer to the paper for details.; The model is mostly meant to intended to be fine-tuned on a downstream task, like video classification. See the model hub to look for fine-tuned versions on a task that interests you."
Lamia,,,,,,11,,0,16469.09475,,https://huggingface.co/scriche/Lamia,"Lamia model mixed using AnythingV3 and centaur only tagged images from danbooru
Dataset of 300 images with complete tag lists; This is a very rough draft of a model still figuring out dream-booth and training using ""lamia"" tag should suffice ""full body"" might be needed to see the whole picture; NEW: LamiaV2 moderate increase to tail and less weird loop tails and weird scale artifacts; lamiatest_9900 is a model made with class images but the quality is about the same but diffrent results.; added bonus bunyip model which can still do lamias but i would say the base is still better"
c2m,Text2Text Generation,PyTorch; Transformers,Chinese,,,1,c2m,14,310.9758197,,https://huggingface.co/supermy/c2m,使用 pipeline 调用模型:; Here is how to use this model to get the features of a given text in PyTorch:; 非常全的文言文（古文）-现代文平行语料，基本涵盖了大部分经典古籍著作。; 原始爬取的数据是篇章级对齐，经过脚本分句（按照句号分号感叹号问号划分）以及人工校对，形成共计约96万句对。目录bitext下是文言文-现代文对齐的平行数据。此外，目录source下是文言文单语数据，target下是现代文单语数据，这两个目录下的文件内容按行对齐。; 以下为数据统计信息。其中，短篇章中包括了《论语》、《孟子》、《左传》等篇幅较短的古籍，已和《资治通鉴》合并。
mt-plantl-es-ca,,,,apache-2.0,,1,,0,1958.018037,,https://huggingface.co/PlanTL-GOB-ES/mt-plantl-es-ca,"This model was trained from scratch using the Fairseq toolkit on a combination of Spanish-Catalan datasets, up to 92 million sentences. Additionally, the model is evaluated on several public datasecomprising 5 different domains (general, adminstrative, technology, biomedical, and news).  ; You can use this model for machine translation from Spanish to Catalan. ; Required libraries:; Translate a sentence using python ; The model was trained on a combination of the following datasets:"
Zerotwo,,,,openrail,,1,,0,0.001470108,,https://huggingface.co/CielTempest12335/Zerotwo,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
flan-t5-large-grammar-synthesis,Text2Text Generation,PyTorch; ONNX; Safetensors; Transformers,,apache-2.0,https://arxiv.org/pdf/2107.06751.pdf,61,jfleg,"52,445",6413.477543,5,https://huggingface.co/pszemraj/flan-t5-large-grammar-synthesis,A fine-tuned version of google/flan-t5-large for grammar correction on an expanded version of the JFLEG dataset. Demo on HF spaces.; ; Compare vs. the original grammar-synthesis-large.; There's a colab notebook that already has this basic version implemented (click on the Open in Colab button); After pip install transformers run the following code:
wingview-640px,Text-to-Image,Diffusers,,creativeml-openrail-m,,9,,109,2181.126021,1,https://huggingface.co/TheLastBen/wingview-640px,"8k steps, 15% textenc (10% at the beginning, 5% from 5000steps), 37 instance images cropped to 640px.; Use the sample images to extract the prompts and the settings. You don't need to use the token ""ppllnnn"" in the prompt.; Prompt : View from an ultra high quality plane, 4k, high definition, beautiful view, [intricate clouds], sun rays shining through the window of the plane; Negative prompt: 3d, cartoon, fake, game, distorted, bad, improbable, nonsense; Settings : Steps: 60, Sampler: Euler a, CFG scale: 7.5, Seed: 108142383, Size: 640x640, Model hash: 4326c062"
led_base_16384_billsum_summarization,Summarization,PyTorch; Safetensors; Transformers,English,,,1,billsum,66,7540.054251,,https://huggingface.co/ArtifactAI/led_base_16384_billsum_summarization,"This model is a fine-tuned version of led-base-16384 on the billsum dataset.; As described in Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan, led-base-16384 was initialized from bart-base since both models share the exact same architecture. To be able to process 16K tokens, bart-base's position embedding matrix was simply copied 16 times.; The model is trained on the BillSum summarization dataset found here; Please find a notebook to test the model below:; "
large-email-classifier,Sentence Similarity,PyTorch; Sentence Transformers,,,,1,,5,91.83122921,,https://huggingface.co/lewispons/large-email-classifier,"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net; The model was trained with the parameters:"
long-t5-tglobal-xl-16384-book-summary,Summarization,PyTorch; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2112.07916.pdf; https://arxiv.org/pdf/2105.08209.pdf,11,kmfoda/booksum,383,11676.87244,,https://huggingface.co/pszemraj/long-t5-tglobal-xl-16384-book-summary,"Summarize long text and get a SparkNotes-like summary of any topic!; A simple example/use case with the base model on ASR is here.; A summary of the infamous navy seals copypasta:; In this chapter, the monster explains how he intends to exact revenge on ""the little b****"" who insulted him. He tells the kiddo that he is a highly trained and experienced killer who will use his arsenal of weapons--including his access to the internet--to exact justice on the little brat.; While this is a crude example, try running this copypasta through other summarization models to see the difference in comprehension (even though it's not even a ""long"" text!)."
Paint-by-Example,,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/2211.13227.pdf,27,,"3,103",12902.40511,8,https://huggingface.co/Fantasy-Studio/Paint-by-Example,"Paint by Example: Exemplar-based Image Editing with Diffusion Models by Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, Fang Wen; The abstract of the paper is the following:; Language-guided image editing has achieved great success recently. In this paper, for the first time, we investigate exemplar-guided image editing for more precise control. We achieve this goal by leveraging self-supervised training to disentangle and re-organize the source image and the exemplar. However, the naive approach will cause obvious fusing artifacts. We carefully analyze it and propose an information bottleneck and strong augmentations to avoid the trivial solution of directly copying and pasting the exemplar image. Meanwhile, to ensure the controllability of the editing process, we design an arbitrary shape mask for the exemplar image and leverage the classifier-free guidance to increase the similarity to the exemplar image. The whole framework involves a single forward of the diffusion model without any iterative optimization. We demonstrate that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity.; The original codebase can be found here.; Inference API has been turned off for this model."
ChromaV5,,,,,,129,,0,9730.23541,,https://huggingface.co/SomaMythos/ChromaV5,"; Model Filename: ChromaV5 ; Trained with: TheLastBen - fast-stable-diffusionBase Model: v1.5  &  2.0Number of Images: 19Resolution: 512x512  & 768x768Steps:3000Text Encoder: 15%fp16: ONContain Faces: NO ; Example of prompt: ChromaV5 desertic landscape, dark blue skies, award winning photography, extremely detailed, artstation, 8 k, incredible art ; Keywords: (Chromatic Aberration; Geometric Shapes; Bokeh; Depth of Field; Photorealistic; Cosmic; Detailed; Bloom; HDR) "
scandi-nli-large,Zero-Shot Classification,PyTorch; Safetensors; Transformers,4 languages,mit,,4,strombergnlp/danfever; KBLab/overlim; MoritzLaurer/multilingual-NLI-26lang-2mil7,"858,842",2909.748124,1,https://huggingface.co/alexandrainst/scandi-nli-large,"This model is a fine-tuned version of NbAiLab/nb-bert-large for Natural Language Inference in Danish, Norwegian Bokm?l and Swedish.; We have released three models for Scandinavian NLI, of different sizes:; A demo of the large model can be found in this Hugging Face Space - check it out!; The performance and model size of each of them can be found in the Performance section below.; You can use this model in your scripts as follows:"
mt-plantl-es-gl,,,,apache-2.0,,1,,0,1875.632188,,https://huggingface.co/PlanTL-GOB-ES/mt-plantl-es-gl,"This model was trained from scratch using the Fairseq toolkit on a combination of Spanish-Galician datasets, up to 31 million sentences. Additionally, the model is evaluated on several public datasets, Flores 101, Spanish Constitutioni (TaCon) and Tatoeba.  ; You can use this model for machine translation from Spanish to Galician. ; Required libraries:; Translate a sentence using python ; The model was trained on a combination of the following datasets:"
fragments_V2,,,,openrail,,36,,0,0.101865234,,https://huggingface.co/Stkzzzz222/fragments_V2,"This is my second embedding for Stable Diffusion v2 768 - fragments; Renders and prompts (from Automatic 1111); ; spiderman made of fragmenv2, very detailed, intrincated, fragments
Steps: 34, Sampler: DDIM, CFG scale: 7, Seed: 3114941515, Size: 704x704, Model hash: 2c02b20a, ENSD: -1; "
robo-diffusion-2-base,Text-to-Image,Diffusers,English,openrail++,,182,,353,2645.005448,24,https://huggingface.co/nousr/robo-diffusion-2-base,A dreambooth-method finetune of stable diffusion that will output cool looking robots when prompted.; Keep the words nousr robot towards the beginning of your prompt to invoke the finetuned style. Use negative prompts to achieve the best result.; Based on stable diffusion 1.4 can be found here; Use the #robodiffusion so i can see the cool stuff you make!; If you enjoy the model i'd appreciate a follow on twitter
poetry,Text Generation,PyTorch; Transformers,Chinese,,,1,poetry,11,511.7595097,2,https://huggingface.co/supermy/poetry,"  古诗词AI生成; 使用 pipeline 调用模型:; Here is how to use this model to get the features of a given text in PyTorch:; 非常全的古诗词数据，收录了从先秦到现代的共计85万余首古诗词。; 模型：GPT2 
训练环境：英伟达16G显卡"
cnn14-esc50,,,English,apache-2.0,https://arxiv.org/pdf/1912.10211.pdf; https://arxiv.org/pdf/2106.04624.pdf; https://arxiv.org/pdf/2205.07390.pdf,1,ESC50,293,607.9426359,,https://huggingface.co/speechbrain/cnn14-esc50,"This repository provides all the necessary tools to perform audip classification with CNN14 model model, implemented with SpeechBrain. For a better experience we encourage you to learn more about
SpeechBrain. The encoder is first trained with SimCLR on the VGGGSound dataset, and then fine tuned on ESC50 folds 1,2,3. ; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; The SpeechBrain team does not provide any warranty on the performance achieved by this model when used on other datasets.; The encoder is originally trained for our paper. You can reference our paper if you use this model for your research. "
gpt2-finetuned-cnn-summarization-v2,Summarization,PyTorch; Transformers,,mit,,3,,631,511.4288684,,https://huggingface.co/gavin124/gpt2-finetuned-cnn-summarization-v2,"This model is a fine-tuned version of gpt2 on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
coreml-stable-diffusion-v1-5,Text-to-Image,Core ML,,other,https://arxiv.org/pdf/2207.12598.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/1910.09700.pdf,48,,0,0.014384537,,https://huggingface.co/apple/coreml-stable-diffusion-v1-5,"This model was generated by Hugging Face using Apple’s repository which has ASCL.; Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
For more information about how Stable Diffusion functions, please have a look at ?'s Stable Diffusion blog.; The Stable-Diffusion-v1-5 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 
checkpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on ""laion-aesthetics v2 5+"" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.; These weights here have been converted to Core ML for use on Apple Silicon hardware.; There are 4 variants of the Core ML weights:"
coreml-stable-diffusion-2-base,Text-to-Image,Core ML,,other,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,77,,0,0.013311272,,https://huggingface.co/apple/coreml-stable-diffusion-2-base,"This model was generated by Hugging Face using Apple’s repository which has ASCL.; This model card focuses on the model associated with the Stable Diffusion v2 model, available here.; The model is trained from scratch 550k steps at resolution 256x256 on a subset of LAION-5B filtered for explicit pornographic material, using the LAION-NSFW classifier with punsafe=0.1 and an aesthetic score >= 4.5. Then it is further trained for 850k steps at resolution 512x512 on the same dataset on images with resolution >= 512x512.; ; These weights here have been converted to Core ML for use on Apple Silicon hardware."
nai,,Safetensors,,,,3,,0,15769.60148,,https://huggingface.co/Narsil/nai,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
animefull-latest,,,,cc-by-nc-4.0,,47,,2,28027.01736,,https://huggingface.co/Cordeliya/animefull-latest,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
marian-finetuned-kde4-en-to-es,Translation,PyTorch; TensorBoard; Transformers,,apache-2.0,,2,kde4,24,313.3178493,,https://huggingface.co/tmobaggins/marian-finetuned-kde4-en-to-es,This model is a fine-tuned version of Helsinki-NLP/opus-mt-en-es on the kde4 dataset.; More information needed; More information needed; More information needed; The following hyperparameters were used during training:
whisper_italian,Automatic Speech Recognition,PyTorch; Transformers,Italian,apache-2.0,,6,mozilla-foundation/common_voice_11_0,34,2945.109262,5,https://huggingface.co/whispy/whisper_italian,"This model is a fine-tuned version of openai/whisper-base on the Common Voice 11.0 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
HyperNetworkCollection_v2,Text-to-Image,,,unknown,,53,,0,0.002431641,,https://huggingface.co/WarriorMama777/HyperNetworkCollection_v2,"人的に集めてるn国のHyperNetworkコレクションやでもれなくコレクションしたい人は一次ソ`スをあたってな↓??? hypernet, embedding ?? (?? 有) - AI?? ?? ?? - https://arca.live/b/hypernetworks/60940948?category=%EA%B3%B5%EC%9C%A0&p=1; Unable to determine this model’s library. Check the
								docs 
.
							"
lichtspiel-diffusion,Text-to-Image,Diffusers,,creativeml-openrail-m,,12,,6,4373.863543,,https://huggingface.co/IcelosAI/lichtspiel-diffusion,"This is a fine-tuned Stable Diffusion 1.5 model trained on stills from movies by celebrated cinematographers. It gives your images a cinematic look, muted colors, bloom and film grain. Sometimes it works great, sometimes not so well. ; Sample pictures of this concept:; ; Download the *.ckpt file from the ""files and versions"" tab into the Stable Diffusion models folder of your web-ui of choice. Rename it to model.ckpt and ... that's it.; Use the token Lichtspiel Style to activate the effect. It works particularly well for portraits and if you additionally reference specific films like Blade Runner, True Grit or The Revenant. "
ConGen-paraphrase-multilingual-mpnet-base-v2,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,,apache-2.0,,2,,394,421.8885486,,https://huggingface.co/kornwtp/ConGen-paraphrase-multilingual-mpnet-base-v2,"This is a ConGen model: It maps sentences to a 768 dimensional dense vector space and can be used for tasks like semantic search.; Using this model becomes easy when you have ConGen installed:; Then you can use the model like this:; For an automated evaluation of this model, see the Thai Sentence Embeddings Benchmark: Semantic Textual Similarity"
git-base-textcaps,Image-to-Text,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2205.14100.pdf,2,,"1,145",707.9293587,,https://huggingface.co/microsoft/git-base-textcaps,"GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.; Disclaimer: The team releasing GIT did not write a model card for this model so this model card has been written by the Hugging Face team.; GIT is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using ""teacher forcing"" on a lot of (image, text) pairs.; The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens.; The model has full access to (i.e. a bidirectional attention mask is used for) the image patch tokens, but only has access to the previous text tokens (i.e. a causal attention mask is used for the text tokens) when predicting the next text token."
wav2vec2-mbart50-ru,Automatic Speech Recognition,PyTorch; Transformers,Russian,apache-2.0,,2,bond005/sberdevices_golos_10h_crowd; bond005/sberdevices_golos_100h_farfield; common_voice; bond005/sova_rudevices; bond005/rulibrispeech,68,3268.496171,,https://huggingface.co/bond005/wav2vec2-mbart50-ru,"Wav2Vec2-mBART-50-Ru is a speech-sequence-to-text-sequence model, which can convert an input audio with Russian speech into a text with punctuation, capitalization and so on.; Wav2Vec2-mBART-50-Ru is the SpeechEncoderDecoderModel, which was initialized with Wav2Vec2-Large-Ru-Golos as the encoder and mBART-large-50 as the decoder. After its initialization the model was fine-tuned using the training parts of several annotated speech corpora: ; CommonVoice 6.0 contains ""rich"" text annotations with punctuation and capitalization, but other speech corpora includes plain texts only. Therefore, text annotations of these corpora were riched automatically using the Silero text enhancement model.; When using this model, make sure that your speech input is sampled at 16kHz.; You can use this model by writing your own inference script:"
e5-small,Feature Extraction,PyTorch; ONNX; Transformers,English,mit,https://arxiv.org/pdf/2212.03533.pdf; https://arxiv.org/pdf/2104.08663.pdf; https://arxiv.org/pdf/2210.07316.pdf,33,,"20,333",267.7505883,,https://huggingface.co/intfloat/e5-small,"News (May 2023): please switch to e5-small-v2, which has better performance and same method of usage.; Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 12 layers and the embedding size is 384.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; Please refer to our paper at https://arxiv.org/pdf/2212.03533.pdf."
uie-base,,PyTorch; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/2203.12277.pdf,4,,350,472.1867476,,https://huggingface.co/xusenlin/uie-base,UIE(Universal Information Extraction)：Yaojie Lu等人在ACL-2022中提出了通用信息抽取统一框架 UIE。; 该框架实现了实体抽取、关系抽取、事件抽取、情感分析等任务的统一建模，并使得不同任务间具备良好的迁移和泛化能力。; 为了方便大家使用 UIE 的强大能力，PaddleNLP借鉴该论文的方法，基于 ERNIE 3.0 知识增强预训练模型，训练并开源了首个中文通用信息抽取模型 UIE。; 该模型可以支持不限定行业领域和抽取目标的关键信息抽取，实现零样本快速冷启动，并具备优秀的小样本微调能力，快速适配特定的抽取目标。; 更多实体抽取和关系抽取模型的使用详见 litie
whisper-small-zh-tw,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,Chinese,apache-2.0,,3,mozilla-foundation/common_voice_11_0,5,968.8638764,,https://huggingface.co/kimbochen/whisper-small-zh-tw,"This model is a fine-tuned version of kimbochen/whisper-small-zh-tw on the mozilla-foundation/common_voice_11_0 zh-TW dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
whisper-large-v2-jp,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,Japanese,apache-2.0,,7,mozilla-foundation/common_voice_11_0,220,6319.877241,,https://huggingface.co/vumichien/whisper-large-v2-jp,"This model is a fine-tuned version of openai/whisper-large-v2 on the mozilla-foundation/common_voice_11_0 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
abstract_nature_patterns_v2,Text-to-Image,Diffusers,,creativeml-openrail-m,,5,,71,2181.122744,,https://huggingface.co/sd-dreambooth-library/abstract_nature_patterns_v2,"inference prompt : abnapa\
The model is an attempt at teaching symmetry and scales associated with nature to SD 1.5 base model.
This version v2 is trained on better curated images for 40,000 steps. I am still working on finding what the model really does and if it has any impact on the base model.
With that being said, the following are my findings, at the outset it seems that
-Images have better symmetry and lighting.
-Images have less artifacts.
-Does not seem to work with large canvas such as 1024x1024 the repetition problem isstill there.
Feel free to experiment with the model."
fast-repo,,,English,,,8,,0,15912.15669,,https://huggingface.co/NoCrypt/fast-repo,"Mostly uses LZ4 compression, which means you'll need a specialized program to extract it, especially in windows.; For Windows users, I recommend using 7zip-zstd (it's 7zip but with lz4 support and more); For Linux users, use tar with liblz4-tool like this: tar -xI lz4 -f repo.tar.lz4; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
microsoft-deberta-v3-large_ner_conll2003,Token Classification,PyTorch; TensorBoard; Safetensors; Transformers,,mit,,1,conll2003,620,3574.648843,,https://huggingface.co/Gladiator/microsoft-deberta-v3-large_ner_conll2003,"This model is a fine-tuned version of microsoft/deberta-v3-large on the conll2003 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
NSFW_text_classifier,Text Classification,PyTorch; Transformers,English,,,29,,"38,858",268.9290865,7,https://huggingface.co/michellejieli/NSFW_text_classifier,"DistilBERT is a transformer model that performs sentiment analysis. I fine-tuned the model on Reddit posts with the purpose of classifying not safe for work (NSFW) content, specifically text that is considered inappropriate and unprofessional. The model predicts 2 classes, which are NSFW or safe for work (SFW). ; The model is a fine-tuned version of DistilBERT.; It was fine-tuned on 14317 Reddit posts pulled from the (Reddit API) [https://praw.readthedocs.io/en/stable/].; Please reach out to michelle.li851@duke.edu if you have any questions or feedback."
pokemon-rgby-sprite,,,,mit,,6,,0,0.058823853,,https://huggingface.co/sd-concepts-library/pokemon-rgby-sprite,"Pokémon Red/Green/Blue/Yellow battle sprite concept (GameBoy 56x56 upscaled to 512x512); This is the <pkmn-rgby> concept taught to Stable Diffusion via Textual Inversion. You can load this concept into the Stable Conceptualizer notebook. You can also train your own concepts and load them into the concept libraries using this notebook.; Here is the new concept you will be able to use as a style:




































































































































































































































































































































































































































































; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
whisper-medium-czech-cv11,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,Czech,apache-2.0,,2,mozilla-foundation/common_voice_11_0,13,3135.295393,,https://huggingface.co/mikr/whisper-medium-czech-cv11,"This model is a fine-tuned version of openai/whisper-medium on the mozilla-foundation/common_voice_11_0 cs dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
momoko,,,,unknown,,22,,0,4362.241469,,https://huggingface.co/ouo/momoko,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
blip-vqa-base,Visual Question Answering,PyTorch; TensorFlow; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2201.12086.pdf,42,,"32,209",3154.852573,61,https://huggingface.co/Salesforce/blip-vqa-base,"Model card for BLIP trained on visual question answering- base architecture (with ViT base backbone).; Authors from the paper write in the abstract:; Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.; You can use this model for conditional and un-conditional image captioning; Inference API has been turned off for this model."
uie-x-base,,PaddlePaddle; PaddleNLP,English; Chinese,apache-2.0,https://arxiv.org/pdf/2203.12277.pdf,15,,0,1165.027641,3,https://huggingface.co/PaddlePaddle/uie-x-base,"; Try out our space at https://huggingface.co/spaces/PaddlePaddle/UIE-X!; Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. The unified text-to-structure generation framework, namely UIE, can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism - structural schema instructor, and captures the common IE abilities via a large-scale pre-trained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.; UIE Paper: https://arxiv.org/abs/2203.12277; PaddleNLP released UIE model series for Information Extraction of texts and multi-modal documents which use the ERNIE 3.0 models as the pre-trained language models and were finetuned on a large amount of information extraction data."
ACertainThing,Text-to-Image,Diffusers,English,creativeml-openrail-m,https://arxiv.org/pdf/2106.09685.pdf,188,,"4,836",6553.608174,34,https://huggingface.co/JosephusCheung/ACertainThing,"Try full functions with Google Colab free T4 ; Anything3.0 is an overfitted model that takes liberties when it shouldn't be generating human images and certain details. However, the community has given it a high rating, and I believe that is because many lazy people who don't know how to write a prompt can use this overfitted model to generate high-quality images even if their prompts are poorly written.; Here is a ACertain version of Anything3.0, made with Dreambooth (idea of LoRA integrated), initialized with ACertainModel.; Although this model may produce better results for image generation, it is built on two major problems. Firstly, it does not always stay true to your prompts; it adds irrelevant details, and sometimes these details are highly homogenized. Secondly, it is an unstable, overfitted model, similar to Anything3.0, and is not suitable for any form of further training. As far as I know, Anything3.0 is obtained by merging several models in just the right way, but it is itself an overfitted model with defects in both its saturation and configuration. However, as I mentioned earlier, it can make even poorly written prompts produce good output images, which leads many lazy people who are incapable of writing good prompts to quickly surpass those who study the writing of prompts carefully. Despite these problems, I still want to release an extended version of the model that caters to the preferences of many people in the community. I hope would you like it.; In my personal view, I oppose all forms of model merging as it has no scientific principle and is nothing but a waste of time. It is a desire to get results without putting in the effort. That is why I do not like Anything3.0, or this model that is being released. But I respect the choices and preferences of the community, and I hope that you can also respect and understand my thoughts."
stable-diffusion-microsoft-emoji,Text-to-Image,,,mit,,28,,0,4362.241566,,https://huggingface.co/pixel-point/stable-diffusion-microsoft-emoji,"use token ""MS_emoji style"" or just ""MS_emoji""; Unable to determine this model’s library. Check the
								docs 
.
							"
Arca,,,,,,5,,0,189707.2014,,https://huggingface.co/Mirinae/Arca,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
megatron-bert-large-swedish-cased-165-zero-shot,Zero-Shot Classification,PyTorch; Safetensors; Transformers,Swedish,,,2,KBLab/overlim,117,3032.464198,,https://huggingface.co/KBLab/megatron-bert-large-swedish-cased-165-zero-shot,"This model is based on Megatron-BERT-large-165k (https://huggingface.co/KBLab/megatron-bert-large-swedish-cased-165k). It was fine-tuned on the QNLI task and further fine-tuned on the MNLI task.
The model can be used with the Hugging Face zero-shot classification pipeline."
CoreMLStableDiffusion,,,,creativeml-openrail-m,,40,,0,648.4020077,,https://huggingface.co/Guernika/CoreMLStableDiffusion,"This repository contains Guernika compatible models and instructions to convert existing models.; While these models and instructions were created for Guernika, they should work and help with any CoreML based solution.; WARNING: Xcode is required to convert models:; Make sure you have Xcode installed.; Once installed run the following commands:"
HistoryCurrentEvents,Text Generation,PyTorch; TensorBoard; Transformers,,,,1,,15,3207.977326,1,https://huggingface.co/BigSalmon/HistoryCurrentEvents,Trained on recent news and lots of political vocabulary.
ddpm-butterflies-128,,TensorBoard; Diffusers,English,apache-2.0,,1,huggan/smithsonian_butterflies_subset,0,0.003064919,,https://huggingface.co/HuggingFace7/ddpm-butterflies-128,"This diffusion model is trained with the ? Diffusers library 
on the huggan/smithsonian_butterflies_subset dataset.; [TODO: provide examples of latent issues and potential remediations]; [TODO: describe the data used to train the model]; The following hyperparameters were used during training:; ? TensorBoard logs"
c2m-mt5,Feature Extraction,PyTorch; Transformers,Chinese,,,1,c2m,7,693.1606469,,https://huggingface.co/supermy/c2m-mt5,使用 pipeline 调用模型:; Here is how to use this model to get the features of a given text in PyTorch:; 非常全的文言文（古文）-现代文平行语料，基本涵盖了大部分经典古籍著作。; 原始爬取的数据是篇章级对齐，经过脚本分句（按照句号分号感叹号问号划分）以及人工校对，形成共计约96万句对。目录bitext下是文言文-现代文对齐的平行数据。此外，目录source下是文言文单语数据，target下是现代文单语数据，这两个目录下的文件内容按行对齐。; 以下为数据统计信息。其中，短篇章中包括了《论语》、《孟子》、《左传》等篇幅较短的古籍，已和《资治通鉴》合并。
swin2SR-classical-sr-x4-64,Image-to-Image,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2209.11345.pdf,1,,165,49.20293877,,https://huggingface.co/caidas/swin2SR-classical-sr-x4-64,"Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration
by Conde et al. and first released in this repository. ; This model is intended for image super resolution.; Refer to the documentation.; Inference API has been turned off for this model."
stream-girl,Text-to-Image,TensorBoard; Diffusers,,creativeml-openrail-m,,5,,136,2641.924542,1,https://huggingface.co/chebao/stream-girl,"You run your new concept via diffusers Colab Notebook for Inference. Don't forget to use the concept prompts! ; Sample pictures of:; chebao (use that on your prompt) 
"
pixel-art-style,Text-to-Image,Diffusers,English,,,42,,756,4198.633756,3,https://huggingface.co/kohbanye/pixel-art-style,"This is a fine-tuned model of Stable Diffusion. 
Add token pixelartstyle to your prompt.; 
an astronaut riding a horse, pixelartstyle"
whisper-large-zh-cv11,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,Chinese,apache-2.0,,27,mozilla-foundation/common_voice_11_0,508,6326.195962,,https://huggingface.co/jonatasgrosman/whisper-large-zh-cv11,"This model is a fine-tuned version of openai/whisper-large-v2 on Chinese (Mandarin) using the train and validation splits of Common Voice 11. Not all validation split data were used during training, I extracted 1k samples from the validation split to be used for evaluation during fine-tuning.; I've performed the evaluation of the model using the test split of two datasets, the Common Voice 11 (same dataset used for the fine-tuning) and the Fleurs (dataset not seen during the fine-tuning). As Whisper can transcribe casing and punctuation, I've performed the model evaluation in 2 different scenarios, one using the raw text and the other using the normalized text (lowercase + removal of punctuations). Additionally, for the Fleurs dataset, I've evaluated the model in a scenario where there are no transcriptions of numerical values since the way these values are described in this dataset is different from how they are described in the dataset used in fine-tuning (Common Voice), so it is expected that this difference in the way of describing numerical values will affect the performance of the model for this type of transcription in Fleurs."
whisper-large-v2-cv11-german,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,German,apache-2.0,,7,mozilla-foundation/common_voice_11_0,344,6330.035443,1,https://huggingface.co/bofenghuang/whisper-large-v2-cv11-german,"

; This model is a fine-tuned version of openai/whisper-large-v2, trained on the mozilla-foundation/common_voice_11_0 de dataset. When using the model make sure that your speech input is also sampled at 16Khz. This model also predicts casing and punctuation.; Below are the WERs of the pre-trained models on the Common Voice 9.0. These results are reported in the original paper.; Below are the WERs of the fine-tuned models on the Common Voice 11.0.; Inference with ? Pipeline"
whisper-large-v2-mix-jp,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,,apache-2.0,,4,vumichien/preprocessed_jsut_jsss_css10_common_voice_11,139,6319.885211,,https://huggingface.co/vumichien/whisper-large-v2-mix-jp,"This model is a fine-tuned version of openai/whisper-large-v2 on the vumichien/preprocessed_jsut_jsss_css10_common_voice_11 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
PULI-GPT-3SX,Text Generation,PyTorch; Transformers,Hungarian,cc-by-nc-4.0,,1,,118,14176.10306,,https://huggingface.co/NYTK/PULI-GPT-3SX,"For further details, see our demo site.; If you use this model, please cite the following paper:"
sketchstyle-cutesexyrobutts,Text-to-Image,Diffusers,English,creativeml-openrail-m,,46,Cosk/cutesexyrobutts,"1,369",23992.35512,,https://huggingface.co/cosc/sketchstyle-cutesexyrobutts,"Base model: https://huggingface.co/Linaqruf/anything-v3.0.
Used 'fast-DreamBooth' on Google Colab and 768x768 images for all versions.; Merging sketchstyle models with other models will help to improve anatomy and other elements while also trying to keep the intended style as much as possible.
I will upload from time to time new merges, if any of those improves on the previous ones. 
A 'weak' model means there is more weight to cutesexyrobutts style and a 'strong' model means there is a little more focus on the other model/models.
Weak models might mantain a little more of the style but could have some anatomy problems, while strong models keep better anatomy though the style might become a little affected. Low CFG Scale (5-9) and using the ""sketchstyle"" token in the prompts might help with keeping the style on strong models.; List of merges:; Versions:; Recommended to use:"
DB_Monet_style,Text-to-Image,Diffusers; PyTorch,,creativeml-openrail-m,,1,,72,0.003339844,,https://huggingface.co/avuhong/DB_Monet_style,"This is a Stable Diffusion model fine-tuned Claude-Monet's painting style taught to Stable Diffusion with DreamBooth.
It can be used by modifying the instance_prompt: a painting in $M## style of; This model was created as part of the DreamBooth Hackathon ?. Visit the organisation page for instructions on how to take part!; This is a Stable Diffusion model fine-tuned on Claude-Monet's paintings.; Since it's more for landscape painting, the image size matters. I found that 512*1024 normally gave interesting results.
Check out this gallery for more generated images:
https://www.vuhongai.com/classicalart-ai"
BPModel,Text-to-Image,Diffusers; Safetensors,English,creativeml-openrail-m,https://arxiv.org/pdf/2212.03860.pdf,134,Crosstyan/BPDataset,"1,291",11161.61458,4,https://huggingface.co/Crosstyan/BPModel,; 2023-01-02: I wasted more GPU hours to train it a little bit more overfitting. Check out bp_mk3.safetensors and bp_mk5.safetensors. Prepare yourself own VAE! Update your WebUI if you can't load safetensors. Adds lots of samples in images folder!; 2023-01-06: Checkout NMFSAN for a new model trained with custom embeddings.; ; BPModel is an experimental Stable Diffusion model based on ACertainty from Joseph Cheung. 
whisper-telugu-large-v2,Automatic Speech Recognition,PyTorch; JAX; Transformers,Telugu,apache-2.0,,1,,130,12637.93005,,https://huggingface.co/vasista22/whisper-telugu-large-v2,"This model is a fine-tuned version of openai/whisper-large-v2 on the Telugu data available from multiple publicly available ASR corpuses.
It has been fine-tuned as a part of the Whisper fine-tuning sprint.; NOTE: The code used to train this model is available for re-use in the whisper-finetune repository.; In order to evaluate this model on an entire dataset, the evaluation codes available in the whisper-finetune repository can be used.; The same repository also provides the scripts for faster inference using whisper-jax.; In order to infer a single audio file using this model, the following code snippet can be used:"
CoCa-ViT-B-32-laion2B-s13B-b90k,,OpenCLIP,,mit,,6,,690,1034.241524,2,https://huggingface.co/laion/CoCa-ViT-B-32-laion2B-s13B-b90k,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
modelshoot,Text-to-Image,Diffusers; Safetensors,English,creativeml-openrail-m,,124,,"1,207",4362.245184,29,https://huggingface.co/wavymulder/modelshoot,"Modelshoot Style

CKPT DOWNLOAD LINK; Use modelshoot style in your prompt (I recommend at the start); I also suggest your prompts include subject and location, for example ""amy adams at the construction site"" , as this helps the model to resolve backgrounds and small details.; Modelshoot is a Dreambooth model trained from 1.5 with VAE on a diverse set of photographs of people. The goal was to create a model focused on full to medium body shots, with an emphasis on cool clothing and a fashion-shoot aesthetic. A result of the composition is that when your subject is further away, their face will usually look worse (and for celebrities, less like them). This limitation of training on 512x512 can be fixed with inpainting, and I plan on revisiting this model at higher resolution in the future.; Modelshoot style works best when using a tall aspect ratio."
3DKX_1.0b,,,,,,75,,0,9072.651602,,https://huggingface.co/unvailai/3DKX_1.0b,"Model name: H&A 3DKX
Model versions: 1.0b, 1.1(latest); V1.1: Minor update based on feedback, containing the following fixes:; -“nsfw”, “nudity” , and “erotica” have been trained into the model and work as Negatives to greatly reduce unintended NSFW content.; CFG can be pushed a bit higher before the images become burnt. As a result, the model can accommodate more complicated prompts now.; Oversaturated images will be encountered way less often"
e5-large,Feature Extraction,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2212.03533.pdf; https://arxiv.org/pdf/2104.08663.pdf; https://arxiv.org/pdf/2210.07316.pdf,53,,"15,697",1372.910289,4,https://huggingface.co/intfloat/e5-large,"News (May 2023): please switch to e5-large-v2, which has better performance and same method of usage.; Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 24 layers and the embedding size is 1024.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; Please refer to our paper at https://arxiv.org/pdf/2212.03533.pdf."
any_sdv15_0.15_0.5-any_gape60fp_06_0.5-Add_difference-merged,,,,cc,,1,,0,7884.801464,,https://huggingface.co/LarryAIDraw/any_sdv15_0.15_0.5-any_gape60fp_06_0.5-Add_difference-merged,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
YuzuLemonTea,Text-to-Image,,,cc0-1.0,,178,,0,65341.44377,,https://huggingface.co/thiros/YuzuLemonTea,"List of my experimental merge models; According to bbc-mc's note, there is a possibility of bug that some token(prompt) can be ignored, when merge with ""add difference"" option.
Milk and ChaiLatte models are now replaced with bug-fix ver.; https://note.com/bbcmc/n/n12c05bf109cc; VAE: ""kl-f8-anime2"" and ""vae-ft-mse-840000-ema-pruned"" are suitable; Steps: 20-30, Sampler: DPM++ SDE Karras or DPM++ 2M Karras, CFG scale: 8, Clip skip: 2, ENSD: 31377, Hires upscale: 2, Hires upscaler: Latent (bicubic antialiased),Denoising strength: 0.54~0.7"
NJ118,,,,,,24,,0,15311.84251,,https://huggingface.co/okiba/NJ118,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
diffusion_fashion,Text-to-Image,Diffusers,English,openrail,,30,,"1,298",0.003025742,8,https://huggingface.co/MohamedRashad/diffusion_fashion,This model is a fine-tuned version of openjourney that is based on Stable Diffusion targeting fashion and clothing.
dreamlike-diffusion-1.0-with-safecheck,Text-to-Image,Diffusers,English,other,,10,,36,2187.610507,,https://huggingface.co/wsxiaoys/dreamlike-diffusion-1.0-with-safecheck,"Use the same prompts as you would for SD 1.5. Add dreamlikeart if the artstyle is too weak.Non-square aspect ratios work better for some prompts. If you want a portrait photo, try using a 2:3 or a 9:16 aspect ratio. If you want a landscape photo, try using a 3:2 or a 16:9 aspect ratio.Use slightly higher resolution for better results: 640x640px, 512x768px, 768x512px, etc.  ; You can use this model for free on dreamlike.art!; We support a Gradio Web UI to run dreamlike-diffusion-1.0:
; Download dreamlike-diffusion-1.0.ckpt (2.13GB); This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion Pipeline."
lion_ckpt,,,,,,4,,0,0.021757813,,https://huggingface.co/xiaohui2022/lion_ckpt,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
MERT-v0,Feature Extraction,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/2306.00107.pdf,16,,"1,017",378.1845898,,https://huggingface.co/m-a-p/MERT-v0,"The development log of our Music Audio Pre-training (m-a-p) model family:; Here is a table for quick model pick-up:; The m-a-p models share the similar model architecture and the most distinguished difference is the paradigm in used pre-training. Other than that, there are several nuance technical configuration needs to know before using:; MERT-v0 is a completely unsupervised model trained on 1000 hour music audios. 
Its architecture is similar to the HuBERT model, but it has been specifically designed for music through the use of specialized pre-training strategies. 
It is SOTA-comparable on multiple MIR tasks even under probing settings, while keeping fine-tunable on a single 2080Ti. 
It outperforms Jukebox representation on GTZAN (genre classification) and GiantSteps (key classification) datasets.
Larger models trained with more data are on the way.; "
SD_Anime_Futuristic_Armor,Text-to-Image,,English,creativeml-openrail-m,,18,,0,9036.506104,,https://huggingface.co/Akumetsu971/SD_Anime_Futuristic_Armor,"Robot, Android, Mecha, futuristic armor, wepons, etc...; DreamBooth based on Elysium_Anime_V2.ckpt (https://gigazine.net/gsc_news/en/20221012-automatic1111-stable-diffusion-webui-deep-danbooru/); 4 files available :; -JhnsT3_step_4000.ckpt - 4000 steps (recommanded, last version I trained on Dreambooth); -JhnsT3_step_4000_0.8-arcane-diffusion-v3_0.2-Weighted_sum-merged.ckpt - 4000 steps (mixed with an Arcane Diffusion model)"
Sygil-Diffusion,Text-to-Image,Diffusers,4 languages,openrail++,,35,,800,30607.37011,7,https://huggingface.co/Sygil/Sygil-Diffusion,"This model is a fine-tune of Stable Diffusion, trained on the Imaginary Network Expanded Dataset, with the big advantage of allowing the use of multiple namespaces (labeled tags) to control various parts of the final generation.
While current models usually are prone to “context errors” and need substantial negative prompting to set them on the right track, the use of namespaces in this model (eg. “species:seal” or “studio:dc”) stop the model from misinterpreting a seal as the singer Seal, or DC Comics as Washington DC. 
This model is also able to understand other languages besides English, currently it can partially understand prompts in Chinese, Japanese and Spanish. More training is already being done in order to have the model completely understand those languages and have it work just like how it works with English prompts.; As the model is fine-tuned on a wide variety of content, it’s able to generate many types of images and compositions, and easily outperforms the original model when it comes to portraits, architecture, reflections, fantasy, concept art, anime, landscapes and a lot more without being hyper-specialized like other community fine-tunes that are currently available. ; **Note: The prompt engineering techniques needed are slightly different from other fine-tunes and the original Stable Diffusion model, so while you can still use your favorite prompts, for best results you might need to tweak them to make use of namespaces. A more detailed guide will be available later on, but you can use the tags and namespaces found here Dataset Explorer should be able to start you off on the right track.; If you find my work useful, please consider supporting me on GitHub Sponsors! ; This model is still in its infancy and it's meant to be constantly updated and trained with more and more data as time goes by, so feel free to give us feedback on our Discord Server or on the discussions section on huggingface. We plan to improve it with more, better tags in the future, so any help is always welcome ?
"
solidity-t5,Text2Text Generation,PyTorch; Safetensors; Transformers,English,apache-2.0,,4,,98,6043.775039,,https://huggingface.co/hululuzhu/solidity-t5,"Processing steps: Clean, contract-level segmentation sepration, split in and out; After processing input sample; After processing output sample (notice indentation is bad, this is intentional to reduce token size)"
yolov5m-license-plate,Object Detection,TensorBoard; PyTorch,,,,16,keremberke/license-plate-object-detection,"2,973",42.67581848,4,https://huggingface.co/keremberke/yolov5m-license-plate,More models available at: awesome-yolov5-models; Inference API has been turned off for this model.
andrewtate-billgates-elonmusk,Text Generation,PyTorch; Transformers,English,,,2,,33,513.3445173,,https://huggingface.co/huggingtweets/andrewtate-billgates-elonmusk,"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report."
anime-ai-detect,Image Classification,PyTorch; Transformers,,apache-2.0,,17,,676,347.0043755,498,https://huggingface.co/saltacc/anime-ai-detect,"A BEiT classifier to see if anime art was made by an AI or a human.; Like most AI models, this classifier is not 100% accurate. Please do not take the results of this model as fact.; The best version had a 96% accuracy distinguishing aibooru and the images from the imageboard sites. However, the success you have with this model will vary based on the images you are trying to classify.; Here are some biases I have noticed from my testing:; This model was trained from microsoft/beit-base-patch16-224 for one epoch on 11 thousand images from imageboard sites, and 11 thousand images from aibooru."
git-large-textcaps,Image-to-Text,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2205.14100.pdf,14,,"2,520",1618.849359,2,https://huggingface.co/microsoft/git-large-textcaps,"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.; Disclaimer: The team releasing GIT did not write a model card for this model so this model card has been written by the Hugging Face team.; GIT is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using ""teacher forcing"" on a lot of (image, text) pairs.; The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens.; The model has full access to (i.e. a bidirectional attention mask is used for) the image patch tokens, but only has access to the previous text tokens (i.e. a causal attention mask is used for the text tokens) when predicting the next text token."
mask2former-swin-base-coco-panoptic,Image Segmentation,PyTorch; Transformers,,other,https://arxiv.org/pdf/2112.01527.pdf; https://arxiv.org/pdf/2107.06278.pdf,7,coco,"15,280",432.0139799,1,https://huggingface.co/facebook/mask2former-swin-base-coco-panoptic,"Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation
 and first released in this repository. ; Disclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.; Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, 
MaskFormer both in terms of performance an efficiency by (i) replacing the pixel decoder with a more advanced multi-scale deformable attention Transformer, (ii) adopting a Transformer decoder with masked attention to boost performance without
without introducing additional computation and (iii) improving training efficiency by calculating the loss on subsampled points instead of whole masks.; ; You can use this particular checkpoint for panoptic segmentation. See the model hub to look for other
fine-tuned versions on a task that interests you."
mask2former-swin-large-coco-panoptic,Image Segmentation,PyTorch; Transformers,,other,https://arxiv.org/pdf/2112.01527.pdf; https://arxiv.org/pdf/2107.06278.pdf,11,coco,"17,577",866.0852689,1,https://huggingface.co/facebook/mask2former-swin-large-coco-panoptic,"Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation
 and first released in this repository. ; Disclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.; Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, 
MaskFormer both in terms of performance an efficiency by (i) replacing the pixel decoder with a more advanced multi-scale deformable attention Transformer, (ii) adopting a Transformer decoder with masked attention to boost performance without
without introducing additional computation and (iii) improving training efficiency by calculating the loss on subsampled points instead of whole masks.; ; You can use this particular checkpoint for panoptic segmentation. See the model hub to look for other
fine-tuned versions on a task that interests you."
paint-journey-v2,Text-to-Image,Diffusers,English,creativeml-openrail-m,,27,,"1,715",9069.729266,12,https://huggingface.co/FredZhang7/paint-journey-v2,"Begin the prompt with ((oil painting)) to add the oil paint effect. For digital and other painting styles, use similar prompts as you would for Midjourney V4 (with some tweaks), Stable Diffusion v1.5 (add more styles), Open Journey V2, or Disco Diffusion.; ; All examples were generated using Camenduru's WebUI (see the Colab file); 
?? 768x1136 portraits, generated using descriptive prompts and without face restoration, generation parameters; 
?? 1280x768 (mostly) natural landscapes, used shorter prompts, generation parameters"
Kenshi,Text-to-Image,Diffusers,English,creativeml-openrail-m,,140,,0,0.015923843,5,https://huggingface.co/SweetLuna/Kenshi,; “Do I hide or do I roam? That indecision… Now the world has changed and I’ve missed it all.”; ; ; 
MaterialsBERT,Feature Extraction,PyTorch; Transformers,English,other,https://arxiv.org/pdf/2007.15779.pdf,3,,352,497.2324514,,https://huggingface.co/pranav-s/MaterialsBERT,"This model is a fine-tuned version of PubMedBERT model on a dataset of 2.4 million materials science abstracts.
It was introduced in this paper. This model is uncased.; Domain-specific fine-tuning has been shown to improve performance in downstream performance on a variety of NLP tasks. MaterialsBERT fine-tunes PubMedBERT, a pre-trained language model trained using biomedical literature. This model was chosen as the biomedical domain is close to the materials science domain. MaterialsBERT when further fine-tuned on a variety of downstream sequence labeling tasks in materials science, outperformed other baseline language models tested on three out of five datasets.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on materials-science relevant downstream tasks.; Note that this model is primarily aimed at being fine-tuned on tasks that use a sentence or a paragraph (potentially masked)
to make decisions, such as sequence classification, token classification or question answering.; Here is how to use this model to get the features of a given text in PyTorch:"
gawrgura,,,,,,3,,0,2200.398906,,https://huggingface.co/hipete12/gawrgura,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
mask2former-swin-tiny-cityscapes-semantic,Image Segmentation,PyTorch; Transformers,,other,https://arxiv.org/pdf/2112.01527.pdf; https://arxiv.org/pdf/2107.06278.pdf,1,coco,735,190.0805512,,https://huggingface.co/facebook/mask2former-swin-tiny-cityscapes-semantic,"Mask2Former model trained on Cityscapes semantic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation
 and first released in this repository. ; Disclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.; Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, 
MaskFormer both in terms of performance an efficiency by (i) replacing the pixel decoder with a more advanced multi-scale deformable attention Transformer, (ii) adopting a Transformer decoder with masked attention to boost performance without
without introducing additional computation and (iii) improving training efficiency by calculating the loss on subsampled points instead of whole masks.; ; You can use this particular checkpoint for panoptic segmentation. See the model hub to look for other
fine-tuned versions on a task that interests you."
Protogen_x5.8_Official_Release,Text-to-Image,Diffusers,English,creativeml-openrail-m,,179,,"17,833",19292.60781,56,https://huggingface.co/darkstorm2150/Protogen_x5.8_Official_Release,"Research Model by darkstorm2150; Protogen x5.8; Protogen was warm-started with Stable Diffusion v1-5 and 
is rebuilt using dreamlikePhotoRealV2.ckpt as a core, adding small amounts during merge checkpoints.; Granular adaptive learning is a machine learning technique that focuses on adjusting the learning process at a fine-grained level, rather than making global adjustments to the model. This approach allows the model to adapt to specific patterns or features in the data, rather than making assumptions based on general trends.; Granular adaptive learning can be achieved through techniques such as active learning, which allows the model to select the data it wants to learn from, or through the use of reinforcement learning, where the model receives feedback on its performance and adapts based on that feedback. It can also be achieved through techniques such as online learning where the model adjust itself as it receives more data."
flan-t5-3b-summarizer,Summarization,PyTorch; TensorFlow; Transformers,English,bsd-3-clause,,22,jordiclive/scored_summarization_datasets,"2,019",17513.6723,1,https://huggingface.co/jordiclive/flan-t5-3b-summarizer,"A fine-tuned version of google/flan-t5-xl on various summarization datasets (xsum, wikihow, cnn_dailymail/3.0.0, samsum, scitldr/AIC, billsum, TLDR); Goal: a model that can be used for a general-purpose summarizer for academic and general usage. Control over the type of summary can be given by varying the instruction prepended to the source document. The result works well on lots of text, although trained with a max source length of 512 tokens and 150 max summary length. ; Check the colab notebook for desired usage
The model expects a prompt prepended to the source document to indicate the type of summary, examples of prompts used to train the model here:
Prompts should be formatted with a colon at the end so that the input to the model is formatted as e.g. ""Summarize the following: {input_text}"". Note this model was trained with far fewer prompts than models like jordiclive/flan-t5-11b-summarizer-filtered so new prompts might not generalize as well.; After pip install transformers run the following code:; This pipeline will run slower and not have some of the tokenization parameters as the colab."
roberta-base-university-writing2,Fill-Mask,PyTorch; TensorBoard; Transformers,,mit,,2,,15,502.343021,,https://huggingface.co/egumasa/roberta-base-university-writing2,"This model is a fine-tuned version of roberta-base on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
Sci-Fi-Diffusion,,,,openrail++,,42,,0,4362.244307,,https://huggingface.co/Corruptlake/Sci-Fi-Diffusion,"A Sci-Fi themed model trained on SD 1.5 with a 26K+ image dataset.; If you would like to support the development of v2.0 and other future projects, please consider supporting me here:
; This model has been trained on 26,949 high resolution and quality Sci-Fi themed images for 2 Epochs. Which equals to around 53K steps/iterations.
The training resolution was 640, however it works well at higher resolutions.
This model is still in developement; Comparison between SD1.5/2.1 on the same seed, prompt and settings can be found below.; "
Evt_V4-preview,Text-to-Image,Diffusers,English,creativeml-openrail-m,,61,,18,30758.04761,1,https://huggingface.co/haor/Evt_V4-preview,"EVT series is an experimental project for finetune with large datasets on animation style model.
Evt_V4 uses a larger dataset than before, and its cosine similarity with ACertainty reaches 85%.
It may behave differently from other models, hope you enjoy it.; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; You can also export the model to ONNX, MPS and/or FLAX/JAX.; Prompt1:

; Prompt2:

"
ppaine-landscape,Text-to-Image,Diffusers; PyTorch,,creativeml-openrail-m,https://arxiv.org/pdf/2208.12242.pdf; https://arxiv.org/pdf/2112.10752.pdf,12,,42,0.010042458,,https://huggingface.co/alkzar90/ppaine-landscape,"Torres del Paine National Park is a national park encompassing mountains, glaciers, lakes, and rivers in southern Chilean Patagonia.
It is also part of the End of the World Route, a tourist scenic route. Wikipedia; DreamBooth model for the ppaine concept trained by alkzar90 on the alkzar90/torres-del-paine dataset.; This is a Stable Diffusion model fine-tuned on the ppaine concept with DreamBooth. It can be used by modifying the instance_prompt: a photo of ppaine landscape; This model was created as part of the DreamBooth Hackathon ?. Visit the organisation page for instructions on how to take part!; This is a Stable Diffusion model fine-tuned on landscape images for the landscape theme."
whisper-large-v2-french,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,French,apache-2.0,,5,mozilla-foundation/common_voice_11_0; facebook/multilingual_librispeech; facebook/voxpopuli; google/fleurs; gigant/african_accented_french,202,3165.878828,,https://huggingface.co/bofenghuang/whisper-large-v2-french,"

; This model is a fine-tuned version of openai/whisper-large-v2, trained on a composite dataset comprising of over 2200 hours of French speech audio, using the train and the validation splits of Common Voice 11.0, Multilingual LibriSpeech, Voxpopuli, Fleurs, Multilingual TEDx, MediaSpeech, and African Accented French. When using the model make sure that your speech input is sampled at 16Khz. This model doesn't predict casing or punctuation.; Below are the WERs of the pre-trained models on the Common Voice 9.0, Multilingual LibriSpeech, Voxpopuli and Fleurs. These results are reported in the original paper.; Below are the WERs of the fine-tuned models on the Common Voice 11.0, Multilingual LibriSpeech, Voxpopuli, and Fleurs. Note that these evaluation datasets have been filtered and preprocessed to only contain French alphabet characters and are removed of punctuation outside of apostrophe. The results in the table are reported as WER (greedy search) / WER (beam search with beam width 5).; Inference with ? Pipeline"
7th_furry,,,,other,,39,,0,20131.84187,,https://huggingface.co/syaimu/7th_furry,"It is quite peaky in use, so the prompts need to be adjusted firmly.; default CFG Scale : 8 ±5; default Sampler : DPM++ SDE Karras; default Steps : 30; The following prompts are used for comparison images.
https://majinai.art/i/AmrKBRI"
gbert-large-paraphrase-cosine,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,German,mit,,7,deutsche-telekom/ger-backtrans-paraphrase,318,1373.113043,,https://huggingface.co/deutsche-telekom/gbert-large-paraphrase-cosine,"This is a sentence-transformers model.
It maps sentences & paragraphs (text) into a 1024 dimensional dense vector space.
The model is intended to be used together with SetFit
to improve German few-shot text classification.
It has a sibling model called
deutsche-telekom/gbert-large-paraphrase-euclidean.; This model is based on deepset/gbert-large.
Many thanks to deepset!; Loss FunctionWe have used MultipleNegativesRankingLoss
with cosine similarity as the loss function.; Training DataThe model is trained on a carefully filtered dataset of
deutsche-telekom/ger-backtrans-paraphrase.
We deleted the following pairs of sentences:; Hyperparameters"
deberta-v3-base-tasksource-nli,Zero-Shot Classification,PyTorch; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/2301.05948.pdf,53,glue; super_glue; anli; tasksource/babi_nli; sick; snli; scitail; OpenAssistant/oasst1; universal_dependencies; hans; qbao775/PARARULE-Plus; alisawuffles/WANLI; metaeval/recast; sileod/probability_words_nli; joey234/nan-nli; pietrolesci/nli_fever; pietrolesci/breaking_nli; pietrolesci/conj_nli; pietrolesci/fracas; pietrolesci/dialogue_nli; pietrolesci/mpe; pietrolesci/dnc; pietrolesci/gpt3_nli; pietrolesci/recast_white; pietrolesci/joci; martn-nguyen/contrast_nli; pietrolesci/robust_nli; pietrolesci/robust_nli_is_sd; pietrolesci/robust_nli_li_ts; pietrolesci/gen_debiased_nli; pietrolesci/add_one_rte; metaeval/imppres; pietrolesci/glue_diagnostics; hlgd; PolyAI/banking77; paws; quora; medical_questions_pairs; conll2003; nlpaueb/finer-139; Anthropic/hh-rlhf; Anthropic/model-written-evals; truthful_qa; nightingal3/fig-qa; tasksource/bigbench; blimp; cos_e; cosmos_qa; dream; openbookqa; qasc; quartz; quail; head_qa; sciq; social_i_qa; wiki_hop; wiqa; piqa; hellaswag; pkavumba/balanced-copa; 12ml/e-CARE; art; tasksource/mmlu; winogrande; codah; ai2_arc; definite_pronoun_resolution; swag; math_qa; metaeval/utilitarianism; mteb/amazon_counterfactual; SetFit/insincere-questions; SetFit/toxic_conversations; turingbench/TuringBench; trec; tals/vitaminc; hope_edi; strombergnlp/rumoureval_2019; ethos; tweet_eval; discovery; pragmeval; silicone; lex_glue; papluca/language-identification; imdb; rotten_tomatoes; ag_news; yelp_review_full; financial_phrasebank; poem_sentiment; dbpedia_14; amazon_polarity; app_reviews; hate_speech18; sms_spam; humicroedit; snips_built_in_intents; banking77; hate_speech_offensive; yahoo_answers_topics; pacovaldez/stackoverflow-questions; zapsdcn/hyperpartisan_news; zapsdcn/sciie; zapsdcn/citation_intent; go_emotions; scicite; liar; relbert/lexical_relation_classification; metaeval/linguisticprobing; tasksource/crowdflower; metaeval/ethics; emo; google_wellformed_query; tweets_hate_speech_detection; has_part; wnut_17; ncbi_disease; acronym_identification; jnlpba; species_800; SpeedOfMagic/ontonotes_english; blog_authorship_corpus; launch/open_question_type; health_fact; commonsense_qa; mc_taco; ade_corpus_v2; prajjwal1/discosense; circa; YaHi/EffectiveFeedbackStudentWriting; Ericwang/promptSentiment; Ericwang/promptNLI; Ericwang/promptSpoke; Ericwang/promptProficiency; Ericwang/promptGrammar; Ericwang/promptCoherence; PiC/phrase_similarity; copenlu/scientific-exaggeration-detection; quarel; mwong/fever-evidence-related; numer_sense; dynabench/dynasent; raquiba/Sarcasm_News_Headline; sem_eval_2010_task_8; demo-org/auditor_review; medmcqa; aqua_rat; RuyuanWan/Dynasent_Disagreement; RuyuanWan/Politeness_Disagreement; RuyuanWan/SBIC_Disagreement; RuyuanWan/SChem_Disagreement; RuyuanWan/Dilemmas_Disagreement; lucasmccabe/logiqa; wiki_qa; metaeval/cycic_classification; metaeval/cycic_multiplechoice; metaeval/sts-companion; metaeval/commonsense_qa_2.0; metaeval/lingnli; metaeval/monotonicity-entailment; metaeval/arct; metaeval/scinli; metaeval/naturallogic; onestop_qa; demelin/moral_stories; corypaik/prost; aps/dynahate; metaeval/syntactic-augmentation-nli; metaeval/autotnli; lasha-nlp/CONDAQA; openai/webgpt_comparisons; Dahoas/synthetic-instruct-gptj-pairwise; metaeval/scruples; metaeval/wouldyourather; sileod/attempto-nli; metaeval/defeasible-nli; metaeval/help-nli; metaeval/nli-veridicality-transitivity; metaeval/natural-language-satisfiability; metaeval/lonli; metaeval/dadc-limit-nli; ColumbiaNLP/FLUTE; metaeval/strategy-qa; openai/summarize_from_feedback; metaeval/folio; metaeval/tomi-nli; metaeval/avicenna; stanfordnlp/SHP; GBaker/MedQA-USMLE-4-options-hf; sileod/wikimedqa; declare-lab/cicero; amydeng2000/CREAK; metaeval/mutual; inverse-scaling/NeQA; inverse-scaling/quote-repetition; inverse-scaling/redefine-math; metaeval/puzzte; metaeval/implicatures; race; metaeval/spartqa-yn; metaeval/spartqa-mchoice; metaeval/temporal-nli; metaeval/ScienceQA_text_only; AndyChiang/cloth; metaeval/logiqa-2.0-nli; tasksource/oasst1_dense_flat; metaeval/boolq-natural-perturbations; metaeval/path-naturalness-prediction; riddle_sense; Jiangjie/ekar_english; metaeval/implicit-hate-stg1; metaeval/chaos-mnli-ambiguity; IlyaGusev/headline_cause; metaeval/race-c; metaeval/equate; metaeval/ambient; AndyChiang/dgen; metaeval/clcd-english; civil_comments; metaeval/acceptability-prediction; maximedb/twentyquestions; metaeval/counterfactually-augmented-snli; tasksource/I2D2; sileod/mindgames; metaeval/counterfactually-augmented-imdb; metaeval/cnli; metaeval/reclor; tasksource/oasst1_pairwise_rlhf_reward; tasksource/zero-shot-label-nli,"21,495",1487.161948,10,https://huggingface.co/sileod/deberta-v3-base-tasksource-nli,"This is DeBERTa-v3-base fine-tuned with multi-task learning on 600 tasks of the tasksource collection.
This checkpoint has strong zero-shot validation performance on many tasks (e.g. 70% on WNLI), and can be used for:; NLI training data of this model includes label-nli, a NLI dataset specially constructed to improve this kind of zero-shot classification.; The list of tasks is available in model config.json.
This is more efficient than ZS since it requires only one forward pass per example, but it is less flexible.; This model ranked 1st among all models with the microsoft/deberta-v3-base architecture according to the IBM model recycling evaluation.
https://ibm.github.io/model-recycling/; https://github.com/sileod/tasksource/ https://github.com/sileod/tasknet/ Training code: https://colab.research.google.com/drive/1iB4Oxl9_B5W3ZDzXoWJN-olUbqLBxgQS?usp=sharing"
upernet-convnext-small,Image Segmentation,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/1807.10221.pdf; https://arxiv.org/pdf/2201.03545.pdf,13,,"38,838",328.0118782,85,https://huggingface.co/openmmlab/upernet-convnext-small,"UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al.; Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.; Disclaimer: The team releasing UperNet + ConvNeXt did not write a model card for this model so this model card has been written by the Hugging Face team.; UperNet is a framework for semantic segmentation. It consists of several components, including a backbone, a Feature Pyramid Network (FPN) and a Pyramid Pooling Module (PPM).; Any visual backbone can be plugged into the UperNet framework. The framework predicts a semantic label per pixel."
BENT-PubMedBERT-NER-Gene,Token Classification,PyTorch; Transformers,English,,,4,,106,1307.685119,,https://huggingface.co/pruas/BENT-PubMedBERT-NER-Gene,Named Entity Recognition (NER) model to recognize gene and protein entities. ; PubMedBERT fine-tuned on the following datasets:
trocr-base-printed_license_plates_ocr,Image-to-Text,PyTorch; Transformers,English,,,4,,431,1372.180028,,https://huggingface.co/DunnBC22/trocr-base-printed_license_plates_ocr,"This model is a fine-tuned version of microsoft/trocr-base-printed on an unknown dataset.
It achieves the following results on the evaluation set:; This model extracts text from image input (License Plates).; For more information on how it was created, check out the following link: https://github.com/DunnBC22/Vision_Audio_and_Multimodal_Projects/blob/main/Optical%20Character%20Recognition%20(OCR)/OCR%20License%20Plates/OCR_license_plate_text_recognition.ipynb; This model is intended to demonstrate my ability to solve a complex problem using technology.; Dataset Source: https://www.kaggle.com/datasets/nickyazdani/license-plate-text-recognition-dataset"
dependencies,,,,,,3,,0,6490.961445,,https://huggingface.co/TheLastBen/dependencies,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
small-stable-diffusion-v0,Text-to-Image,Diffusers,English,openrail,,66,ChristophSchuhmann/improved_aesthetics_6plus,"3,844",4.694684448,5,https://huggingface.co/OFA-Sys/small-stable-diffusion-v0,"【Update 2023/02/07】 Recently, we have released a diffusion deployment repo to speedup the inference on both GPU (~4x speedup, based on TensorRT) and CPU (~12x speedup, based on IntelOpenVINO). 
Integrated with this repo, small-stable-diffusion could generate images in just 5 seconds on the CPU*.  ; * Test on Intel(R) Xeon(R) Platinum 8369B CPU, DPMSolverMultistepScheduler 10 steps, fix channel/height/width when converting to Onnx ; Similar image generation quality, but is nearly 1/2 smaller!Here are some samples:
; We support a Gradio Web UI to run small-stable-diffusion-v0:

We also provide a space demo for small-stable-diffusion-v0 + diffusion-deploy.As huggingface provides AMD CPU for the space demo, it costs about 35 seconds to generate an image with 15 steps, which is much slower than the Intel CPU environment as diffusion-deploy is based on Intel's OpenVINO.; Use Diffusers >=0.8.0, do not support lower versions."
legal-swiss-roberta-large,Fill-Mask,PyTorch; TensorBoard; Transformers,4 languages,cc,https://arxiv.org/pdf/2306.02069.pdf; https://arxiv.org/pdf/2301.13126.pdf; https://arxiv.org/pdf/2110.00976.pdf; https://arxiv.org/pdf/2306.09237.pdf,1,MultiLegalPile; LEXTREME; LEXGLUE,31,1787.874964,,https://huggingface.co/joelito/legal-swiss-roberta-large,"This model is a multilingual model pretrained on legal data. It is based on XLM-R (base and large). For pretraining we used Multi Legal Pile (Niklaus et al. 2023), a multilingual dataset from various legal sources covering 24 languages.; You can utilize the raw model for masked language modeling since we did not perform next sentence prediction. However, its main purpose is to be fine-tuned for downstream tasks.; It's important to note that this model is primarily designed for fine-tuning on tasks that rely on the entire sentence, potentially with masked elements, to make decisions. Examples of such tasks include sequence classification, token classification, or question answering. For text generation tasks, models like GPT-2 are more suitable.; Additionally, the model is specifically trained on legal data, aiming to deliver strong performance in that domain. Its performance may vary when applied to non-legal data.; For tasks such as text generation you should look at model like GPT2."
ore-o,Text-to-Image,,,openrail++,,10,,0,0.002900391,,https://huggingface.co/p1atdev/ore-o,"; Lazurite is a latent diffusion model fintuned with modern style illustrations in LoRA method, based on Waifu Diffusion 1.4 epoch 2.
If you want to use the model in AUTOMATIC1111's Web UI, you neeed lazurite-v1.yaml and rename it.
The lazurite-v1-lora.pt only works with kohya's sd-script or extension.; Unable to determine this model’s library. Check the
								docs 
.
							"
gpt-fluentui-flat-svg,Text Generation,PyTorch; Safetensors; Transformers,,mit,,9,,48,754.0324932,,https://huggingface.co/Norod78/gpt-fluentui-flat-svg,"A custom GPT model which was trained upon svg files.
Specifically the flat emoji variants from Microsoft's FluentUI repo. 
These svn files only consist of ""stand-alone"" path elements which should make it simpler to train upon and sample from.; Both Tokenizer and Model were trained using aitextgen
The python file which was used for training, the .txt file dataset and a few generated samples can be found here; The generated samples below were also created with this script


"
LoRA,,,,,,41,,0,4868.113027,,https://huggingface.co/Lykon/LoRA,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Cute_RichStyle_2,,,,apache-2.0,,10,,0,2672.820264,,https://huggingface.co/RichVip/Cute_RichStyle_2,"Model trained in SD 2.1 with photos generated with Midjourney, created to generate people, animals/creatures...; You can also make objects... landscapes, etc, but maybe you need more tries:; 30 steps - 7cfg; euler a,ddim, dpm++sde...; you can use different resolutions, you can generate interesting things"
promptgen-lexart,Text Generation,PyTorch; Transformers,English,mit,,34,,"9,836",335.227555,4,https://huggingface.co/AUTOMATIC/promptgen-lexart,Finetuned distilgpt2 for 100 epochs on 134819 prompts scraped from lexica.art (Stable Diffusion 1.5 checkpoint).; Intended for use with https://github.com/AUTOMATIC1111/stable-diffusion-webui-promptgen
promptgen-majinai-safe,Text Generation,PyTorch; Transformers,English,mit,,13,,"2,025",335.2275732,1,https://huggingface.co/AUTOMATIC/promptgen-majinai-safe,Finetuned distilgpt2 for 40 epochs on 1654 prompts scraped from majinai.art. Weights/emphasis stripped. Includes negative prompts.; Intended for use with https://github.com/AUTOMATIC1111/stable-diffusion-webui-promptgen
promptgen-majinai-unsafe,Text Generation,PyTorch; Transformers,English,mit,,11,,"5,416",335.2276752,,https://huggingface.co/AUTOMATIC/promptgen-majinai-unsafe,Finetuned distilgpt2 for 40 epochs on 825 prompts scraped from majinai.art. Weights/emphasis stripped. Includes negative prompts.; Intended for use with https://github.com/AUTOMATIC1111/stable-diffusion-webui-promptgen
review-summarizer-en,Summarization,PyTorch; Transformers,English,,,2,amazon_reviews_multi,48,1249.41288,,https://huggingface.co/MurkatG/review-summarizer-en,Model that trained to summarize product reviews.
av3-mirror,,Diffusers,,,,1,,1,15261.40193,,https://huggingface.co/NUROISEA/av3-mirror,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
OpenNiji,Text-to-Image,Diffusers,English,creativeml-openrail-m,,84,Korakoe/NijiJourney-Prompt-Pairs,"2,177",4664.244393,,https://huggingface.co/ShoukanLabs/OpenNiji,"; The Stable Diffusion model trained on Nijijourney images!; ; ; This model already has the in01 trick applied, so this model should
be better at generating hands!"
wd-v1-4-swinv2-tagger-v2,,Keras; ONNX,,apache-2.0,,36,,"16,978",493.2878251,14,https://huggingface.co/SmilingWolf/wd-v1-4-swinv2-tagger-v2,"Supports ratings, characters and general tags.; Trained using https://github.com/SmilingWolf/SW-CV-ModelZoo.TPUs used for training kindly provided by the TRC program.; Last image id: 5944504Trained on Danbooru images with IDs modulo 0000-0899.Validated on images with IDs modulo 0950-0999.Images with less than 10 general tags were filtered out.Tags with less than 600 images were filtered out.; P=R: threshold = 0.3771, F1 = 0.6854; Subject to change and updates.Downstream users are encouraged to use tagged releases rather than relying on the head of the repo."
ZeroTwo-Medium-DialoGPT,Conversational,PyTorch; Transformers,,,,1,,48,1477.899468,,https://huggingface.co/SumYin/ZeroTwo-Medium-DialoGPT,Finetuned version of DialoGPT-Medium
platform_tile,Text-to-Image,Diffusers,,creativeml-openrail-m,,5,,0,2672.756922,,https://huggingface.co/Rotyh/platform_tile,"___

"
ninjumango-jumango-v1-0,Audio-to-Audio,Diffusers,,creativeml-openrail-m,,2,,3,2181.129912,,https://huggingface.co/jumang4423/ninjumango-jumango-v1-0,"gen samples or filter samples with jumango textures!; Inference API does not yet support diffusers models for this pipeline type.
							"
git-large-r-coco,Image-to-Text,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2205.14100.pdf,3,,"2,379",1618.849456,2,https://huggingface.co/microsoft/git-large-r-coco,"R = re-trained by removing some offensive captions in cc12m dataset; GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.; Disclaimer: The team releasing GIT did not write a model card for this model so this model card has been written by the Hugging Face team.; GIT is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using ""teacher forcing"" on a lot of (image, text) pairs.; The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens."
git-large-r-textcaps,Image-to-Text,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2205.14100.pdf,7,,"4,305",1618.849427,24,https://huggingface.co/microsoft/git-large-r-textcaps,"R = re-trained by removing some offensive captions in cc12m dataset; GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.; Disclaimer: The team releasing GIT did not write a model card for this model so this model card has been written by the Hugging Face team.; GIT is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using ""teacher forcing"" on a lot of (image, text) pairs.; The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens."
Style-lora-all,,,,,,99,,0,2971.071953,,https://huggingface.co/xiaozhangMJXXZ/Style-lora-all,"https://t.me/+a-k8rVfjIVk3NGU1
https://t.me/loraeveryone
这是tg群组，之后会在第一时间更新tg，因为tg可以直接传tg原文件呜呜呜，笑脸站会缓慢更新！; 这里是各种画风或者衣物的lora合集，希望各位可以及时来补充！！！ 
分别为打包全下载与单个角色，由于中文名字的文件无法下载所以是压缩包的形式，下载之后需要各位解压一下里面就有对应的中文名字了。 校 长的联系方式：qq3062945846; 只是为了方便中文玩家而搬运整理！！; 有目录的截图小伙伴们可以参照！; 我们十分尊敬每一位lora的作者！！"
Clarity,Text-to-Image,Diffusers,,creativeml-openrail-m,,28,,0,24289.28422,2,https://huggingface.co/Schisim/Clarity,VAE NOT REQUIRED BUT RECOMENDED; VAE - https://huggingface.co/stabilityai/sd-vae-ft-mse-original/tree/main; File Structure for AUTOMATIC1111-webui :; |──sd; |----|──stable-diffusion-webui
chubas,Text-to-Image,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/1910.09700.pdf,22,,0,0.012821083,,https://huggingface.co/grugger/chubas,"grug's mixes for maek bicshar of vchuba; 
Full Size, Warning, 74 MiB, 60 megapixel PNG; Hi-Res Fix

Full Size, Warning, 286 MiB, 240 megapixel PNG; 
Full Size, Warning, 140 MiB, 112 megapixel PNG;  for maek bicshar of vchuba"
Anything-V3-X,Text-to-Image,Diffusers,English,creativeml-openrail-m,,16,,"17,496",60767.96211,33,https://huggingface.co/iZELX1/Anything-V3-X,
pokemon-lora,Text-to-Image,Diffusers,,creativeml-openrail-m,,8,,617,4.771596336,3,https://huggingface.co/pcuenq/pokemon-lora,"These are LoRA adaption weights trained on base model https://huggingface.co/runwayml/stable-diffusion-v1-5. The weights were fine-tuned on the lambdalabs/pokemon-blip-captions dataset.; The script below loads the base model, then applies the LoRA weights and performs inference:; Please, check our blog post or documentation for more details.; 


"
Brain_Tumor_Classification_using_swin_transformer,Image Classification,PyTorch; TensorBoard; Transformers,,apache-2.0,,1,imagefolder,6,348.0173895,,https://huggingface.co/surajjoshi/Brain_Tumor_Classification_using_swin_transformer,"This model is a fine-tuned version of microsoft/swin-base-patch4-window7-224-in22k on the imagefolder dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
chatgpt-prompts-bart-long,Text2Text Generation,TensorFlow; Transformers,,apache-2.0,,48,fka/awesome-chatgpt-prompts,311,1672.46019,28,https://huggingface.co/merve/chatgpt-prompts-bart-long,"This model is a fine-tuned version of BART-large on a ChatGPT prompts dataset.
It achieves the following results on the evaluation set:; You can use this to generate ChatGPT personas. Simply input a persona like below:; The following hyperparameters were used during training:"
opt-iml-1.3b,Text Generation,PyTorch; Transformers,,other,https://arxiv.org/pdf/2212.12017.pdf,26,,"4,385",2694.459349,1,https://huggingface.co/facebook/opt-iml-1.3b,"OPT-IML (OPT + Instruction Meta-Learning) is a set of instruction-tuned versions of OPT, on a collection of ~2000 NLP tasks gathered from 8 NLP benchmarks, called OPT-IML Bench.; We provide two model versions: ; You can use this model directly with a pipeline for text generation.; While OPT-IML models outperform baseline OPT on an extensive set of evaluations,
nevertheless, they are susceptible to the various risks associated with using large language models
relating to factual correctness, generation of toxic language and enforcing stereotypes. While we release our
OPT-IML models to proliferate future work on instruction-tuning and to improve the availability
of large instruction-tuned causal LMs, the use of these models should be
accompanied with responsible best practices.; OPT-IML models are trained on OPT-IML Bench, a large benchmark for Instruction MetaLearning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks include Super-NaturalInstructions, FLAN, PromptSource, etc. "
mouseymix,,,,creativeml-openrail-m,,44,,0,4372.481882,,https://huggingface.co/closertodeath/mouseymix,"overfit merged lora model; 
; thanks to:; anon for zankuro; sometimes#8916 for gyokai"
bert-base-uncased_clinical-ner,Token Classification,PyTorch; TensorFlow; JAX; Transformers,,,,1,tner/bc5cdr; commanderstrife/jnlpba; bc2gm_corpus; drAbreu/bc4chemd_ner; linnaeus; chintagunta85/ncbi_disease,"2,991",2179.244754,,https://huggingface.co/sschet/bert-base-uncased_clinical-ner,"A Named Entity Recognition model for clinical entities (problem, treatment, test); The model has been trained on the i2b2 (now n2c2) dataset for the 2010 - Relations task. Please visit the n2c2 site to request access to the dataset."
flan-t5-xxl-sharded-fp16,Text2Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2210.11416.pdf,44,,"12,189",23084.30756,3,https://huggingface.co/philschmid/flan-t5-xxl-sharded-fp16,"This is a fork of google/flan-t5-xxl implementing a custom handler.py as an example for how to use t5-11b with inference-endpoints on a single NVIDIA A10G.; You can deploy the flan-t5-xxl with a 1-click. 
Since we are using the ""quantized"" version, we can switch our instance type to ""GPU [medium] ・ 1x Nvidia A10G"". ; ; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models."
yolov8m-pcb-defect-segmentation,Image Segmentation,TensorBoard; PyTorch,,,,4,keremberke/pcb-defect-segmentation,"4,154",56.70560493,,https://huggingface.co/keremberke/yolov8m-pcb-defect-segmentation,More models available at: awesome-yolov8-models; Inference API has been turned off for this model.
anything-v3-1,Text-to-Image,Diffusers,English,creativeml-openrail-m,,67,cag/anything-v3-1-dataset,"7,059",17489.92917,34,https://huggingface.co/cag/anything-v3-1,"; Anything V3.1 is a third-party continuation of a latent diffusion model, Anything V3.0. This model is claimed to be a better version of Anything V3.0 with a fixed VAE model and a fixed CLIP position id key. The CLIP reference was taken from Stable Diffusion V1.5. The VAE was swapped using Kohya's merge-vae script and the CLIP was fixed using Arena's stable-diffusion-model-toolkit webui extensions.; Anything V3.2 is supposed to be a resume training of Anything V3.1. The current model has been fine-tuned with a learning rate of 2.0e-6, 50 epochs, and 4 batch sizes on datasets collected from many sources, with 1/4 of them being synthetic datasets. The dataset has been preprocessed using the Aspect Ratio Bucketing Tool so that it can be converted to latents and trained at non-square resolutions. This model is supposed to be a test model to see how the clip fix affects training. Like other anime-style Stable Diffusion models, it also supports Danbooru tags to generate images.; e.g. 1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden ; This model can be used just like any other Stable Diffusion model. For more information, please have a look at the Stable Diffusion. You can also export the model to ONNX, MPS and/or FLAX/JAX. Pretrained model currently based on Anything V3.1."
coreml-OrangeMixs,Text-to-Image,Core ML,,creativeml-openrail-m,,30,,0,0.055175323,,https://huggingface.co/coreml/coreml-OrangeMixs,"This model was converted to Core ML for use on Apple Silicon devices by following Apple's instructions here.
Provide the model to an app such as Mochi Diffusion to generate images.; split_einsum version is compatible with all compute unit options including Neural Engine.
original version is only compatible with CPU & GPU option.; Source: Hugging Face; ""OrangeMixs"" shares various Merge models that can be used with StableDiffusionWebui:Automatic1111 and others.Enjoy the drawing AI.; "
AudioLDM-S-Full,,Diffusers,English,bigscience-openrail-m,https://arxiv.org/pdf/2301.12503.pdf,31,thelou1s/AudioSet; Chr0my/freesound.org,0,2621.442578,20,https://huggingface.co/haoheliu/AudioLDM-S-Full,Generate any audio from text using your imagination; https://huggingface.co/spaces/haoheliu/audioldm-text-to-audio-generation; TODO; TODO; TODO
a-man-is-surfing,Text-to-Image,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/2212.11565.pdf; https://arxiv.org/pdf/2112.10752.pdf,8,,1,0.003779831,21,https://huggingface.co/Tune-A-Video-library/a-man-is-surfing,Test prompt: A panda is surfing; ; Inference API has been turned off for this model.
karlo-v1-alpha-image-variations,Text-to-Image,Diffusers,,creativeml-openrail-m,,5,,"6,946",0.007882538,1,https://huggingface.co/kakaobrain/karlo-v1-alpha-image-variations,"Karlo is a text-conditional image generation model based on OpenAI's unCLIP architecture with the improvement over the standard super-resolution model from 64px to 256px, recovering high-frequency details only in the small number of denoising steps.; Karlo is available in diffusers!; ; ; Karlo is a text-conditional diffusion model based on unCLIP, composed of prior, decoder, and super-resolution modules. In this repository, we include the improved version of the standard super-resolution module for upscaling 64px to 256px only in 7 reverse steps, as illustrated in the figure below:"
Grapefruit,Text-to-Image,Diffusers,English,creativeml-openrail-m,,142,,984,34546.49425,41,https://huggingface.co/iZELX1/Grapefruit,"grapefruit (general hentai model); lemon (minimal weaker on nsfw, looks on some parts better then grapefruit); Grapefruit aims to be a hentai model with a bright and more ?softer“ artstyle. ; Use a vae with it (AnythingV3 vae). But you can use any vae you like. Savetensor and the vae file must be named the same -> MODEL.savetensor and MODEL.vae.pt; Black result fix (vae bug in web ui): Use ""--no-half-vae"" in your command line arguments"
politicalBiasBERT,Text Classification,PyTorch; Safetensors; Transformers,English,mit,,8,,"1,536",868.0971621,,https://huggingface.co/bucketresearch/politicalBiasBERT,BERT finetuned on many examples of politically biased texts; Paper and repository coming soon.
riffusion-model-v1,,,,,,6,,0,6553.601445,,https://huggingface.co/webui/riffusion-model-v1,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
OPT-19M-ChatSalad,Text Generation,PyTorch; Transformers,English,other,,10,,782,40.12523338,2,https://huggingface.co/concedo/OPT-19M-ChatSalad,"This is an experimental OPT-based model with 19 million parameters trained entirely from scratch as a datasetting practice. 
Thus, it should not be subject to the usual OPT license. You are free to use this model for any purpose. 
The model is small enough (under 40mb) that it should run at very fast speeds even entirely on CPU. ; It is recommend to use this model with the KoboldAI software, with the following parameters:; All feedback and comments can be directed to Concedo on the KoboldAI discord.; Inference API has been turned off for this model."
GTM_UltimateBlend_v3,,,,creativeml-openrail-m,,8,,0,8744.962149,,https://huggingface.co/GalaxyTimeMachine/GTM_UltimateBlend_v3,"Fantasy.ai is the official and exclusive hosted AI generation platform that holds a commercial use license for GalaxyTimeMachine, you can use their service at https://fantasy.ai/
Please report any unauthorized commercial use.; No models using any license other than the standard creativeml-openrail-m license were used in this merge.; The VAE is baked into the model, but if you need one, then I would HIGHLY recommend using the standard SD VAE.
https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.ckpt; If you want to help me out, or just have a look at the public images, then please hop over to my Patreon: https://www.patreon.com/galaxytimemachine; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
HD-22,,,,,,19,,0,16899.80151,,https://huggingface.co/Deltaadams/HD-22,"Getting started and UI.; https://www.cognitionai.org/hdhowtogetstarted; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ai-illustration,Text-to-Image,Diffusers,,creativeml-openrail-m,,6,,165,2181.122445,1,https://huggingface.co/sd-dreambooth-library/ai-illustration,Test the concept via A1111 Colab fast-Colab-A1111; Sample pictures of this concept:
Randeng-Pegasus-238M-Summary-Chinese2,Summarization,PyTorch; Rust; Transformers,Chinese,,https://arxiv.org/pdf/1912.08777.pdf; https://arxiv.org/pdf/2209.02970.pdf,2,,12,1921.247582,,https://huggingface.co/spursyy/Randeng-Pegasus-238M-Summary-Chinese2,"善于处理摘要任务，在数个中文摘要数据集上微调后的，中文版的PAGASUS-base。; Good at solving text summarization tasks, after fine-tuning on multiple Chinese text summarization datasets, Chinese PAGASUS-base.; 参考论文：PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization; 基于Randeng-Pegasus-238M-Chinese，我们在收集的7个中文领域的文本摘要数据集（约4M个样本）上微调了它，得到了summary版本。这7个数据集为：education, new2016zh, nlpcc, shence, sohu, thucnews和weibo。; Based on Randeng-Pegasus-238M-Chinese, we fine-tuned a text summarization version (summary) on 7 Chinese text summarization datasets, with totaling around 4M samples. The datasets include: education, new2016zh, nlpcc, shence, sohu, thucnews and weibo."
HARDblend,Text-to-Image,Diffusers,English,creativeml-openrail-m,,75,,"6,579",30607.36412,38,https://huggingface.co/theintuitiveye/HARDblend,"A versatile photorealistic NSFW capable model which is great at generating high quality portraits.
It is a finetuned model trained on ~500 portrait images merged with Hassanblend, Aeros, RealisticVision1.2, Delibrate, SxD, f222.; Use stability ai VAE or bakedinVAE version for better results.; RAW samples
; Help us to be able to create models of professional standards. Consider supporting us on Patreon / Ko-fi / Paypal.; We support a Gradio Web UI to run HARDblend :
"
sd-x2-latent-upscaler,,Diffusers,,openrail++,,127,,"50,451",2.008348083,10,https://huggingface.co/stabilityai/sd-x2-latent-upscaler,"This model card focuses on the latent diffusion-based upscaler developed by Katherine Crowson 
in collaboration with Stability AI. 
This model was trained on a high-resolution subset of the LAION-2B dataset. 
It is a diffusion model that operates in the same latent space as the Stable Diffusion model, which is decoded into a full-resolution image. 
To use it with Stable Diffusion, You can take the generated latent from Stable Diffusion and pass it into the upscaler before decoding with your standard VAE. 
Or you can take any image, encode it into the latent space, use the upscaler, and decode it. ; Note: 
This upscaling model is designed explicitely for Stable Diffusion as it can upscale Stable Diffusion's latent denoised image embeddings.
This allows for very fast text-to-image + upscaling pipelines as all intermeditate states can be kept on GPU. More for information, see example below.
This model works on all Stable Diffusion checkpoints; Using the ?'s Diffusers library to run latent upscaler on top of any StableDiffusionUpscalePipeline checkpoint 
to enhance its output image resolution by a factor of 2.; Result:; 512-res Astronaut
"
palmyra-base,Text Generation,PyTorch; Transformers,English,apache-2.0,,27,English,130,10947.1264,5,https://huggingface.co/Writer/palmyra-base,"|||; Palmyra Base was primarily pre-trained with English text. Note that there is still a trace amount of non-English data present within the training corpus that was accessed through CommonCrawl. A causal language modeling (CLM) objective was utilized during the process of the model's pretraining. Similar to GPT-3, Palmyra Base is a member of the same family of models that only contain a decoder. As a result, it was pre-trained utilizing the objective of self-supervised causal language modeling. Palmyra Base uses the prompts and general experimental setup from GPT-3 in order to conduct its evaluation per GPT-3.; Palmyra Base is extremely powerful while being extremely fast. This model excels at many nuanced tasks such as sentiment classification and summarization.; Palmyra Base (5b) was trained on Writer’s custom dataset.; Palmyra Base learns an inner representation of the English language that can be used to extract features useful for downstream tasks. However, the model is best at what it was pre-trained for which is generating text from a prompt."
ulzzang-6500,,,,,,41,,0,0.01140625,,https://huggingface.co/yesyeahvh/ulzzang-6500,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
rMadArt2.5,Text-to-Image,Diffusers,English,other,,27,,0,16882.20488,4,https://huggingface.co/rmada/rMadArt2.5,More examples: https://openart.ai/@raudemer_enchanting_8k; Requires AUTOMATIC1111 Stable Diffusion Webui --api --listen (https://github.com/AUTOMATIC1111/stable-diffusion-webui); https://www.youtube.com/watch?v=47OjMczhBpM&t=416s; https://www.youtube.com/watch?v=o7hrptahjvI; https://www.youtube.com/watch?v=M_0DRfESzks
EasyNegative,,,,,,79,,0,0.025566406,,https://huggingface.co/embed/EasyNegative,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
bart-cause-effect,Text2Text Generation,PyTorch; Transformers,,,,2,,76,1672.459016,,https://huggingface.co/taskload/bart-cause-effect,"This model was created using by Taskload, a group led by Henry Leonardi for automating information extraction. To get involved contact me at leonardi.henry@gmail.com"
pupugirl_v1_anime_attempt_2,Text-to-Image,Diffusers,,creativeml-openrail-m,,1,,6,2181.122706,,https://huggingface.co/pupubear/pupugirl_v1_anime_attempt_2,Trained from Pu0112; Test the concept via A1111 Colab fast-Colab-A1111; Sample pictures of this concept:;  
Sygil-Muse,Text-to-Image,,,openrail,,23,,0,3120.089932,,https://huggingface.co/Sygil/Sygil-Muse,"This model is based in Muse and trained using the code hosted on Sygil-Dev/muse-maskgit-pytorch, which is based on lucidrains/muse-maskgit-pytorch and a collaboration between the Sygil-Dev and ShoukanLabs teams.; This model is a new model trained from scratch based on Muse, trained on a subset of the Imaginary Network Expanded Dataset, with the big advantage of allowing the use of multiple namespaces (labeled tags) to control various parts of the final generation.
The use of namespaces (eg. “species:seal” or “studio:dc”) stops the model from misinterpreting a seal as the singer Seal, or DC Comics as Washington DC. ; Note: As of right now, only the first VAE and MaskGit has been trained with different configuration, we are trying to find the best balance between quality, performance and vram usage so Muse can be used on all kind of devices, we still need to train the Super Resolution VAE for the model to be usable even tho we might be able to reuse the first VAE depending on the quality of it once the training progresses more.; If you find my work useful, please consider supporting me on GitHub Sponsors! ; This model is still in its infancy and it's meant to be constantly updated and trained with more and more data as time goes by, so feel free to give us feedback on our Discord Server or on the discussions section on huggingface. We plan to improve it with more, better tags in the future, so any help is always welcome.
"
flan-t5-11b-summarizer-filtered,Summarization,PyTorch; Transformers,English,bsd-3-clause,,14,jordiclive/scored_summarization_datasets; jordiclive/wikipedia-summary-dataset,423,23073.97333,,https://huggingface.co/jordiclive/flan-t5-11b-summarizer-filtered,"A fine-tuned version of google/flan-t5-xxl on various summarization datasets (xsum, wikihow, cnn_dailymail/3.0.0, samsum, scitldr/AIC, billsum, TLDR, wikipedia-summary); 70% of the data was also filtered with the use of the contriever with a cosine similarity between text and summary of 0.6 as threshold.; Goal: a model that can be used for a general-purpose summarizer for academic and general usage. Control over the type of summary can be given by varying the instruction prepended to the source document. The result works well on lots of text, although trained with a max source length of 512 tokens and 150 max summary length. ; Check the colab notebook for desired usage.
The model expects a prompt prepended to the source document to indicate the type of summary, this model was trained with a large (100s) variety of prompts:; The model has also learned for the length of the summary to be specified in words by a range ""x-y words"" or e.g. ""~/approximately/about/ x words."""
blip2-opt-6.7b-coco,Image-to-Text,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2301.12597.pdf,9,,"14,161",32597.38705,3,https://huggingface.co/Salesforce/blip2-opt-6.7b-coco,"BLIP-2 model, leveraging OPT-6.7b (a large language model with 6.7 billion parameters).
It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository.; Disclaimer: The team releasing BLIP-2 did not write a model card for this model so this model card has been written by the Hugging Face team.; BLIP-2 consists of 3 models: a CLIP-like image encoder, a Querying Transformer (Q-Former) and a large language model.; The authors initialize the weights of the image encoder and large language model from pre-trained checkpoints and keep them frozen
while training the Querying Transformer, which is a BERT-like Transformer encoder that maps a set of ""query tokens"" to query embeddings,
which bridge the gap between the embedding space of the image encoder and the large language model.; The goal for the model is simply to predict the next text token, giving the query embeddings and the previous text."
food-category-classification-v2.0,Image Classification,PyTorch; Transformers,,,,7,Kaludi/food-category-classification-v2.0,366,348.006122,2,https://huggingface.co/Kaludi/food-category-classification-v2.0,"This is an updated Food Category Image Classifier model of the old model that has been trained by Kaludi to recognize 12 different categories of foods, which includes Bread, Dairy, Dessert, Egg, Fried Food, Fruit, Meat, Noodles, Rice, Seafood, Soup, and Vegetable. It can accurately classify an image of food into one of these categories by analyzing its visual features. This model can be used by food bloggers, restaurants, and recipe websites to quickly categorize and sort their food images, making it easier to manage their content and provide a better user experience.; This model supports a Gradio Web UI to run the data-food-classification model:
"
ArOCR-handwritting-v2,Image-to-Text,PyTorch; TensorBoard; Transformers,Arabic,,,1,,144,748.5126209,1,https://huggingface.co/UBC-NLP/ArOCR-handwritting-v2,"This model is a fine-tuned version of  on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
Wav2Lip,,,,,https://arxiv.org/pdf/2008.10010.pdf,12,,0,0.071419678,,https://huggingface.co/camenduru/Wav2Lip,"For commercial requests, please contact us at radrabha.m@research.iiit.ac.in or prajwal.k@research.iiit.ac.in. We have an HD model ready that can be used commercially.; This code is part of the paper: A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild published at ACM Multimedia 2020. ; 

; All results from this open-source code or our demo website should only be used for research/academic/personal purposes only. As the models are trained on the LRS2 dataset, any form of commercial use is strictly prohibhited. For commercial requests please contact us directly!; You can lip-sync any video to any audio:"
flan-t5-11b-summarizer-filtered-1.5-epoch,Summarization,PyTorch; Transformers,English,bsd-3-clause,,3,jordiclive/scored_summarization_datasets; jordiclive/wikipedia-summary-dataset,106,23073.9735,,https://huggingface.co/jordiclive/flan-t5-11b-summarizer-filtered-1.5-epoch,"Note: This model is a further trained version of jordiclive/flan-t5-11b-summarizer-filtered.; A fine-tuned version of google/flan-t5-xxl on various summarization datasets (xsum, wikihow, cnn_dailymail/3.0.0, samsum, scitldr/AIC, billsum, TLDR, wikipedia-summary); 70% of the data was also filtered with the use of the contriever with a cosine similarity between text and summary of 0.6 as threshold.; Goal: a model that can be used for a general-purpose summarizer for academic and general usage. Control over the type of summary can be given by varying the instruction prepended to the source document. The result works well on lots of text, although trained with a max source length of 512 tokens and 150 max summary length. ; Check the colab notebook for desired usage.
The model expects a prompt prepended to the source document to indicate the type of summary, this model was trained with a large (100s) variety of prompts:"
OPT-2.7B-Nerybus-Mix,Text Generation,PyTorch; Transformers,English,other,,9,,"4,759",5694.911698,1,https://huggingface.co/KoboldAI/OPT-2.7B-Nerybus-Mix,"This is an experimental model containing a parameter-wise 50/50 blend (weighted average) of the weights of NerysV2-2.7B and ErebusV1-2.7B
Preliminary testing produces pretty coherent outputs, it appears to retain the NSFWness of Erebus but with a Nerys-esque twist in terms of prose.; The two models used for this blend, NerysV2-2.7B and ErebusV1-2.7B are made by Mr. Seeker.; As the original datasets used for the source models are not publically available, I use my own datasets for this evaluation, which may not provide accurate comparison.; Eval parameters: 32000 characters extracted from the middle of the corpus, tested in blocks of 1024 tokens each, same dataset used for each test batch.; It is recommend to use this model with the KoboldAI software. All feedback and comments can be directed to Concedo on the KoboldAI discord."
pythia-1.4b-deduped,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2304.01373.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2201.07311.pdf,13,EleutherAI/the_pile_deduplicated,"19,405",3002.44584,11,https://huggingface.co/EleutherAI/pythia-1.4b-deduped,"The Pythia Scaling Suite is a collection of models developed to facilitate 
interpretability research (see paper). 
It contains two sets of eight models of sizes 
70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two 
models: one trained on the Pile, and one trained on the Pile after the dataset 
has been globally deduplicated. All 8 model sizes are trained on the exact 
same data, in the exact same order. We also provide 154 intermediate 
checkpoints per model, hosted on Hugging Face as branches.; The Pythia model suite was designed to promote scientific 
research on large language models, especially interpretability research. 
Despite not centering downstream performance as a design goal, we find the 
models match or exceed the performance of 
similar and same-sized models, such as those in the OPT and GPT-Neo suites.; Previously, we released an early version of the Pythia suite to the public. 
However, we decided to retrain the model suite to address a few hyperparameter 
discrepancies. This model card lists the changes; 
see appendix B in the Pythia paper for further discussion. We found no 
difference in benchmark performance between the two Pythia versions. 
The old models are 
still available, but we 
suggest the retrained suite if you are just starting to use Pythia.
This is the current release.; Please note that all models in the Pythia suite were renamed in January 
2023. For clarity, a table 
comparing the old and new names is provided in this model card, together 
with exact parameter counts.; The primary intended use of Pythia is research on the behavior, functionality, 
and limitations of large language models. This suite is intended to provide 
a controlled setting for performing scientific experiments. We also provide 
154 checkpoints per model: initial step0, 10 log-spaced checkpoints 
step{1,2,4...512}, and 143 evenly-spaced checkpoints from step1000 to 
step143000. These checkpoints are hosted on Hugging Face as branches. Note 
that branch 143000 corresponds exactly to the model checkpoint on the main 
branch of each model."
onimai-characters,Text-to-Image,Diffusers,,creativeml-openrail-m,,32,,122,2181.128866,2,https://huggingface.co/alea31415/onimai-characters,"This model is trained on 6(+1?) characters from ONIMAI: I'm Now Your Sister! (お兄ちゃんはおしまい!); 
; Two lora checkpoints trained with the same dataset are added to the loras subfolder. The first one seems to work already.
The characters are learned but unfortunately not the styles nor the outfits.
It is trained based on ACertainty and works fine on Orange and Anything, but not so well on models that are trained further such as MyneFactoryBase or my own models.
I still cannot figure out when Loras get transferred correctly.; The lora has dimension 32, alpha 1, and is trained with learning rate 1e-4.
Here are some example generations.; 

"
domain_transfer_general-massive_email-roberta-large-v1-5-38,Text Classification,PyTorch; Sentence Transformers,,apache-2.0,https://arxiv.org/pdf/2209.11055.pdf,1,,6,1457.452512,,https://huggingface.co/fathyshalab/domain_transfer_general-massive_email-roberta-large-v1-5-38,"This is a SetFit model that can be used for text classification. The model has been trained using an efficient few-shot learning technique that involves:; To use this model for inference, first install the SetFit library:; You can then run inference as follows:; Inference API does not yet support sentence-transformers models for this pipeline type.
							"
Foto-Assisted-Diffusion-FAD_V0,Text-to-Image,Diffusers; Safetensors,English,creativeml-openrail-m,,152,,0,4375.754879,,https://huggingface.co/Dunkindont/Foto-Assisted-Diffusion-FAD_V0,"This model is meant to mimic a modern HDR photography style; It was trained on 600 HDR images on SD1.5 and works best at 768x768 resolutions; Merged with one of my own models for illustrations and drawings, to increase flexibility; Below you will find some example cards that this model is capable of outputting.
You can acquire the images used here: HF or
Google Drive. ; Google Drive gives you them all at once without needing to clone the repo, which is easier. "
CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup,Zero-Shot Image Classification,TensorBoard; OpenCLIP,,mit,https://arxiv.org/pdf/2201.03545.pdf; https://arxiv.org/pdf/2210.08402.pdf; https://arxiv.org/pdf/1910.04867.pdf,3,,"4,763",1447.429925,2,https://huggingface.co/laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup,"A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP.; The models utilize:; This 320x320 resolution model is a soup (weight average) of 3 fine-tunes of CLIP-convnext_large_d.laion2B-s26B-b102K-augreg at a higher resolution. It is an average of 3 fine-tunes from the final checkpoint of the original 256x256 training run w/ an additional ~2-3B samples for each fine-tune and a lower learning rate. Each fine-tune was a different learning rate (1e-4, 6e-5, 5e-5), and diff # of samples (3.2B, 2B, 2.5B).; At 320x320, the ConvNext-Large-D is significantly more efficient than the L/14 model at 336x336 that OpenAI fine-tuned. L/14-336 model is 2.5x more GMAC, 2.8x more activations, and 1.22x more parameters.; RRC = Random Resize Crop (crop pcts), RE = Random Erasing (prob), SD = Stochastic Depth (prob) -- image tower only, D = Dropout (prob) -- image tower head only"
toxic-comment-classification,Text Classification,PyTorch; Transformers,Portuguese,apache-2.0,,2,,72,1373.036154,,https://huggingface.co/dougtrajano/toxic-comment-classification,"This model is a fine-tuned version of neuralmind/bert-large-portuguese-cased on the OLID-BR dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
blue_pencil,Text-to-Image,,Japanese; English,other,,202,,0,37017.72258,1,https://huggingface.co/bluepen5805/blue_pencil,"
          A series of merged models that is just a messy merge of various models.
        ; Elysium_V2 / OpenRAIL M; ultracolor.v4 / CreativeML OpenRAIL M; basil_mix / OpenRAIL M; ACertainThing / CreativeML OpenRAIL M"
multilingual-MiniLMv2-L6-mnli-xnli,Zero-Shot Classification,PyTorch; Safetensors; Transformers,16 languages,mit,https://arxiv.org/pdf/2002.10957.pdf; https://arxiv.org/pdf/1809.05053.pdf,18,multi_nli; xnli,"7,828",878.1798879,,https://huggingface.co/MoritzLaurer/multilingual-MiniLMv2-L6-mnli-xnli,"This multilingual model can perform natural language inference (NLI) on 100+ languages and is therefore also 
suitable for multilingual zero-shot classification. The underlying multilingual-MiniLM-L6 model was created 
by Microsoft and was distilled from XLM-RoBERTa-large (see details in the original paper 
and newer information in this repo). 
The model was then fine-tuned on the XNLI dataset, which contains hypothesis-premise pairs from 15 languages, 
as well as the English MNLI dataset.; The main advantage of distilled models is that they are smaller (faster inference, lower memory requirements) than their teachers (XLM-RoBERTa-large).
The disadvantage is that they lose some of the performance of their larger teachers. ; For highest inference speed, I recommend using this 6-layer model. For higher performance I recommend 
mDeBERTa-v3-base-mnli-xnli (as of 14.02.2023).; This model was trained on the XNLI development dataset and the MNLI train dataset. 
The XNLI development set consists of 2490 professionally translated texts from English 
to 14 other languages (37350 texts in total) (see this paper). 
Note that the XNLI contains a training set of 15 machine translated versions of the MNLI dataset for 15 languages, 
but due to quality issues with these machine translations, this model was only trained on the professional translations 
from the XNLI development set and the original English MNLI training set (392 702 texts). 
Not using machine translated texts can avoid overfitting the model to the 15 languages; 
avoids catastrophic forgetting of the other languages it was pre-trained on; 
and significantly reduces training costs. ; The model was trained using the Hugging Face trainer with the following hyperparameters. 
The exact underlying model is mMiniLMv2-L6-H384-distilled-from-XLMR-Large."
gpt2-medium-finetuned-sst2-sentiment,Text Classification,PyTorch; Safetensors; Transformers,English,apache-2.0,,1,sst2,434,2950.546403,,https://huggingface.co/michelecafagna26/gpt2-medium-finetuned-sst2-sentiment,"OpenAI's GPT-2 medium fine-tuned on SST-2 dataset for Sentiment Analysis downstream task.; The GPT-2 model was presented in Language Models are Unsupervised Multitask Learners by Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever; The model has been finetuned for 10 epochs on standard hyperparameters; This model card is based on ""mrm8488/t5-base-finetuned-imdb-sentiment"" by Manuel Romero/@mrm8488"
lora,,,,openrail,,11,,0,3575.701509,,https://huggingface.co/joe123yo/lora,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
wd-1-5-beta,,,,other,,116,,0,0.003237724,,https://huggingface.co/waifu-diffusion/wd-1-5-beta,"IMPORTANT: this is a BETA MODEL! It is not done!; https://cafeai.notion.site/WD-1-5-Beta-Release-Notes-967d3a5ece054d07bb02cba02e8199b7; Checkpoints are located in the ""checkpoints"" folder, under the files tab; WD 1.5 uses the same VAE as WD 1.4, which can be found here https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt; I've included a ""wdgoodprompt"" and ""wdbadprompt"" embedding in the embeddings folder to help make generation easier. With in progress models, its common to have to use long prompts for good results. Using these embeddings helps alleviate some of that."
animefull-latests,,,,,,2,,0,7884.803252,,https://huggingface.co/YukikazeSamaNanoda/animefull-latests,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
coreml-stable-diffusion-2-1-base,Text-to-Image,Core ML,,openrail++,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,33,,0,0.013408928,,https://huggingface.co/apple/coreml-stable-diffusion-2-1-base,"This model was generated by Hugging Face using Apple’s repository which has ASCL.; This model card focuses on the model associated with the Stable Diffusion v2-1-base model.; This stable-diffusion-2-1-base model fine-tunes stable-diffusion-2-base (512-base-ema.ckpt) with 220k extra steps taken, with punsafe=0.98 on the same dataset. ; These weights here have been converted to Core ML for use on Apple Silicon hardware.; There are 4 variants of the Core ML weights:"
OPT-6.7B-Nerybus-Mix,Text Generation,PyTorch; Transformers,English,other,,15,,"131,870",14050.7496,,https://huggingface.co/KoboldAI/OPT-6.7B-Nerybus-Mix,"This is an experimental model containing a parameter-wise 50/50 blend (weighted average) of the weights of NerysV2-6.7B and ErebusV1-6.7B
Preliminary testing produces pretty coherent outputs, however, it seems less impressive than the 2.7B variant of Nerybus, as both 6.7B source models appear more similar than their 2.7B counterparts.; The two models used for this blend, NerysV2-6.7B and ErebusV1-6.7B are made by Mr. Seeker.; No formal evaluation is available for this model at this time. This blend was created in FP16, due to available memory constraints.; It is recommend to use this model with the KoboldAI software. All feedback and comments can be directed to Concedo on the KoboldAI discord.; Inference API has been turned off for this model."
pythia-70m,Text Generation,PyTorch; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/2304.01373.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2201.07311.pdf,6,EleutherAI/pile,"44,054",334.1257394,2,https://huggingface.co/EleutherAI/pythia-70m,"The Pythia Scaling Suite is a collection of models developed to facilitate 
interpretability research (see paper). 
It contains two sets of eight models of sizes 
70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two 
models: one trained on the Pile, and one trained on the Pile after the dataset 
has been globally deduplicated. All 8 model sizes are trained on the exact 
same data, in the exact same order. We also provide 154 intermediate 
checkpoints per model, hosted on Hugging Face as branches.; The Pythia model suite was deliberately designed to promote scientific 
research on large language models, especially interpretability research. 
Despite not centering downstream performance as a design goal, we find the 
models match or exceed the performance of 
similar and same-sized models, such as those in the OPT and GPT-Neo suites.; Previously, we released an early version of the Pythia suite to the public. 
However, we decided to retrain the model suite to address a few hyperparameter 
discrepancies. This model card lists the changes; 
see appendix B in the Pythia paper for further discussion. We found no 
difference in benchmark performance between the two Pythia versions. 
The old models are 
still available, but we 
suggest the retrained suite if you are just starting to use Pythia.
This is the current release.; Please note that all models in the Pythia suite were renamed in January 
2023. For clarity, a table 
comparing the old and new names is provided in this model card, together 
with exact parameter counts.; The primary intended use of Pythia is research on the behavior, functionality, 
and limitations of large language models. This suite is intended to provide 
a controlled setting for performing scientific experiments. We also provide 
154 checkpoints per model: initial step0, 10 log-spaced checkpoints 
step{1,2,4...512}, and 143 evenly-spaced checkpoints from step1000 to 
step143000. These checkpoints are hosted on Hugging Face as branches. Note 
that branch 143000 corresponds exactly to the model checkpoint on the main 
branch of each model."
pythia-410m,Text Generation,PyTorch; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/2304.01373.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2201.07311.pdf,6,EleutherAI/pile,"16,283",1824.125742,1,https://huggingface.co/EleutherAI/pythia-410m,"The Pythia Scaling Suite is a collection of models developed to facilitate 
interpretability research (see paper). 
It contains two sets of eight models of sizes 
70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two 
models: one trained on the Pile, and one trained on the Pile after the dataset 
has been globally deduplicated. All 8 model sizes are trained on the exact 
same data, in the exact same order. We also provide 154 intermediate 
checkpoints per model, hosted on Hugging Face as branches.; The Pythia model suite was deliberately designed to promote scientific 
research on large language models, especially interpretability research. 
Despite not centering downstream performance as a design goal, we find the 
models match or exceed the performance of 
similar and same-sized models, such as those in the OPT and GPT-Neo suites.; Previously, we released an early version of the Pythia suite to the public. 
However, we decided to retrain the model suite to address a few hyperparameter 
discrepancies. This model card lists the changes; 
see appendix B in the Pythia paper for further discussion. We found no 
difference in benchmark performance between the two Pythia versions. 
The old models are 
still available, but we 
suggest the retrained suite if you are just starting to use Pythia.
This is the current release.; Please note that all models in the Pythia suite were renamed in January 
2023. For clarity, a table 
comparing the old and new names is provided in this model card, together 
with exact parameter counts.; The primary intended use of Pythia is research on the behavior, functionality, 
and limitations of large language models. This suite is intended to provide 
a controlled setting for performing scientific experiments. We also provide 
154 checkpoints per model: initial step0, 10 log-spaced checkpoints 
step{1,2,4...512}, and 143 evenly-spaced checkpoints from step1000 to 
step143000. These checkpoints are hosted on Hugging Face as branches. Note 
that branch 143000 corresponds exactly to the model checkpoint on the main 
branch of each model."
pythia-1b-deduped,Text Generation,PyTorch; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/2304.01373.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2201.07311.pdf,11,EleutherAI/the_pile_deduplicated,"3,680",4282.445839,1,https://huggingface.co/EleutherAI/pythia-1b-deduped,"The Pythia Scaling Suite is a collection of models developed to facilitate 
interpretability research (see paper). 
It contains two sets of eight models of sizes 
70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two 
models: one trained on the Pile, and one trained on the Pile after the dataset 
has been globally deduplicated. All 8 model sizes are trained on the exact 
same data, in the exact same order. We also provide 154 intermediate 
checkpoints per model, hosted on Hugging Face as branches.; The Pythia model suite was designed to promote scientific 
research on large language models, especially interpretability research. 
Despite not centering downstream performance as a design goal, we find the 
models match or exceed the performance of 
similar and same-sized models, such as those in the OPT and GPT-Neo suites.; Previously, we released an early version of the Pythia suite to the public. 
However, we decided to retrain the model suite to address a few hyperparameter 
discrepancies. This model card lists the changes; 
see appendix B in the Pythia paper for further discussion. We found no 
difference in benchmark performance between the two Pythia versions. 
The old models are 
still available, but we 
suggest the retrained suite if you are just starting to use Pythia.
This is the current release.; Please note that all models in the Pythia suite were renamed in January 
2023. For clarity, a table 
comparing the old and new names is provided in this model card, together 
with exact parameter counts.; The primary intended use of Pythia is research on the behavior, functionality, 
and limitations of large language models. This suite is intended to provide 
a controlled setting for performing scientific experiments. We also provide 
154 checkpoints per model: initial step0, 10 log-spaced checkpoints 
step{1,2,4...512}, and 143 evenly-spaced checkpoints from step1000 to 
step143000. These checkpoints are hosted on Hugging Face as branches. Note 
that branch 143000 corresponds exactly to the model checkpoint on the main 
branch of each model."
wav2vec2-base-finetuned-iemocap6,Audio Classification,PyTorch; TensorBoard; Transformers,,,,2,,16,378.0076295,,https://huggingface.co/Zahra99/wav2vec2-base-finetuned-iemocap6,No model card; New: Create and edit this model card directly on the website!
x2-latent-upscaler-for-anime,Text-to-Image,Diffusers,,openrail++,,1,,106,2.008611755,1,https://huggingface.co/alfredplpl/x2-latent-upscaler-for-anime,"This model card focuses on the latent diffusion-based upscaler developed by Katherine Crowson 
in collaboration with Stability AI. 
This model was trained on a high-resolution subset of the LAION-2B dataset. 
It is a diffusion model that operates in the same latent space as the Stable Diffusion model, which is decoded into a full-resolution image. 
To use it with Stable Diffusion, You can take the generated latent from Stable Diffusion and pass it into the upscaler before decoding with your standard VAE. 
Or you can take any image, encode it into the latent space, use the upscaler, and decode it. ; Note: 
This upscaling model is designed explicitely for Stable Diffusion as it can upscale Stable Diffusion's latent denoised image embeddings.
This allows for very fast text-to-image + upscaling pipelines as all intermeditate states can be kept on GPU. More for information, see example below.
This model works on all Stable Diffusion checkpoints; Using the ?'s Diffusers library to run latent upscaler on top of any StableDiffusionUpscalePipeline checkpoint 
to enhance its output image resolution by a factor of 2.; Result:; 512-res Astronaut
"
coreml-ChilloutMix,Text-to-Image,Core ML,,creativeml-openrail-m,,81,,0,0.005224609,,https://huggingface.co/coreml/coreml-ChilloutMix,Source(s): Hugging Face - CivitAI; Merged "Basilmix"(nuigurumi/basil_mix ・ Hugging Face); and; wonderful realistic models.; (PoV Skin Texture - r34 Lucid Black | Stable Diffusion Checkpoint | Civitai
rev-animated,Text-to-Image,Diffusers,,creativeml-openrail-m,,5,,"1,313",0.004423904,1,https://huggingface.co/stablediffusionapi/rev-animated,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""rev-animated""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Try model for free: Generate Images"
ControlNet-diff-modules,,,,,,186,,0,5784.001656,,https://huggingface.co/kohya-ss/ControlNet-diff-modules,"Pre-made difference files extracted from original ControlNet models for transfer control.; Can be used with https://github.com/Mikubill/sd-webui-controlnet; Original models: https://huggingface.co/lllyasviel/ControlNet; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
clap-htsat-fused,Feature Extraction,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2211.06687.pdf,8,,"3,366",618.346829,,https://huggingface.co/laion/clap-htsat-fused,"Model card for CLAP: Contrastive Language-Audio Pretraining; ; The abstract of the paper states that: ; Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K and the proposed model are both available to the public.; You can use this model for zero shot audio classification or extracting audio and/or textual features."
clap-htsat-unfused,Feature Extraction,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2211.06687.pdf,7,,"6,809",618.3468132,,https://huggingface.co/laion/clap-htsat-unfused,"Model card for CLAP: Contrastive Language-Audio Pretraining; ; The abstract of the paper states that: ; Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K and the proposed model are both available to the public.; You can use this model for zero shot audio classification or extracting audio and/or textual features."
slimr-pp-msmarco-passage,Fill-Mask,PyTorch; Transformers,,,,1,,18,438.9234281,,https://huggingface.co/castorini/slimr-pp-msmarco-passage,No model card; New: Create and edit this model card directly on the website!
Taiyi-BLIP-750M-Chinese,Image-to-Text,PyTorch; Safetensors; Transformers,Chinese,apache-2.0,,9,,281,6164.594773,2,https://huggingface.co/IDEA-CCNL/Taiyi-BLIP-750M-Chinese,Inference API has been turned off for this model.
stolen,,,,,,16,,0,0.605009766,,https://huggingface.co/Linaqruf/stolen,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
MORIMORImix,,,,,,37,,0,15247.36145,,https://huggingface.co/morit-00/MORIMORImix,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
DucHaitenSuperCute,Text-to-Image,Diffusers,English,creativeml-openrail-m,,15,,"3,302",23756.80211,27,https://huggingface.co/DucHaiten/DucHaitenSuperCute,
lora,,,,,,5,,0,14353.70433,,https://huggingface.co/nyaa314/lora,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
madoka,,,,unknown,,3,,0,227.0020442,,https://huggingface.co/nub29/madoka,"LoRA Model Trained using Dreambooth using images from Puella Magi Madoka Magica; there are no triggers for this
you have to wrangle tags to get the girl depending on traits
example:1girl, solo, blonde_hair, hair_ornament, bow, twintails, yellow_eyes, drill_hair, twin_drills,
gets mami more often than not; Example Images:
;  DISCLAIMER: I am not responsible for what images you produce or what you do with them. By downloading this model you consent to taking full responsibility for the images you produce with it. ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
lora,,,,,,17,,0,1359.401445,,https://huggingface.co/cloud768/lora,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
MoeSS-SUBModel,,ONNX,,gpl-3.0,,26,,0,2766.201469,2,https://huggingface.co/NaruseMioShirakana/MoeSS-SUBModel,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
fashion-clip,Zero-Shot Image Classification,PyTorch; Safetensors; Transformers,English,mit,,55,,"243,379",1213.588362,6,https://huggingface.co/patrickjohncyh/fashion-clip,"    ; Disclaimer: The model card adapts the model card from here.; UPDATE (10/03/23): We have updated the model! We found that laion/CLIP-ViT-B-32-laion2B-s34B-b79K checkpoint (thanks Bin!) worked better than original OpenAI CLIP on Fashion. We thus fine-tune a newer (and better!) version of FashionCLIP (henceforth FashionCLIP 2.0), while keeping the architecture the same. We postulate that the perofrmance gains afforded by laion/CLIP-ViT-B-32-laion2B-s34B-b79K are due to the increased training data (5x OpenAI CLIP data). Our thesis, however, remains the same -- fine-tuning laion/CLIP on our fashion dataset improved zero-shot perofrmance across our benchmarks. See the below table comparing weighted macro F1 score across models.; FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, we train FashionCLIP on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks. FashionCLIP was not developed for model deplyoment - to do so, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.; March 2023"
LoraBackup,,,,wtfpl,,27,,0,0.012353516,,https://huggingface.co/reroreroro/LoraBackup,there are mad dogs ; mad dogs who keeping "hunting witches"; ―They are barking: ; "Ah that must be a child porn!!! I should report it right now!"; 今有疯狗猎巫狂，
ChilloutMix,,,,,,293,,0,2634.131406,3,https://huggingface.co/AnonPerson/ChilloutMix,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ctdmixes,,,,cc-by-nc-4.0,,12,,0,19630.08147,,https://huggingface.co/closertodeath/ctdmixes,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
distilbert-base-uncased-sst2-onnx-int8-for-tensorrt,Text Classification,ONNX; Transformers,English,mit,,1,sst2; glue,43,269.0095898,,https://huggingface.co/fxmarty/distilbert-base-uncased-sst2-onnx-int8-for-tensorrt,"This model is a fork of distilbert-base-uncased-finetuned-sst-2-english quantized with Optimum library ? using static quantization.; This model can be used as follow:; Inspecting the graph (for example here with netron), we see that it contains Quantize and Dequantize nodes, that will be interpreted by TensorRT to run in INT8:; "
d5_t5_validator_700M,Text2Text Generation,PyTorch; Transformers,,,,1,,20,3205.122257,,https://huggingface.co/ruiqi-zhong/d5_t5_validator_700M,No model card; New: Create and edit this model card directly on the website!
sd-controlnet-depth,Image-to-Image,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,24,,"48,523",2969.671707,101,https://huggingface.co/lllyasviel/sd-controlnet-depth,"ControlNet is a neural network structure to control diffusion models by adding extra conditions. 
This checkpoint corresponds to the ControlNet conditioned on Depth estimation.; It can be used in combination with Stable Diffusion.; ; Developed by: Lvmin Zhang, Maneesh Agrawala; Model type: Diffusion-based text-to-image generation model"
sd-controlnet-normal,Image-to-Image,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,13,,"8,954",2969.672489,30,https://huggingface.co/lllyasviel/sd-controlnet-normal,"ControlNet is a neural network structure to control diffusion models by adding extra conditions. 
This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation.; It can be used in combination with Stable Diffusion.; ; Developed by: Lvmin Zhang, Maneesh Agrawala; Model type: Diffusion-based text-to-image generation model"
sd-controlnet-scribble,Image-to-Image,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,30,,"9,725",2969.671805,99,https://huggingface.co/lllyasviel/sd-controlnet-scribble,"ControlNet is a neural network structure to control diffusion models by adding extra conditions. 
This checkpoint corresponds to the ControlNet conditioned on Scribble images.; It can be used in combination with Stable Diffusion.; ; Developed by: Lvmin Zhang, Maneesh Agrawala; Model type: Diffusion-based text-to-image generation model"
sd-controlnet-seg,Image-to-Image,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,30,,"12,625",2969.677821,106,https://huggingface.co/lllyasviel/sd-controlnet-seg,"ControlNet is a neural network structure to control diffusion models by adding extra conditions. 
This checkpoint corresponds to the ControlNet conditioned on Image Segmentation.; It can be used in combination with Stable Diffusion.; ; Developed by: Lvmin Zhang, Maneesh Agrawala; Model type: Diffusion-based text-to-image generation model"
legal-longformer-base,Fill-Mask,PyTorch; Safetensors; Transformers,English,cc-by-sa-4.0,https://arxiv.org/pdf/2004.05150.pdf; https://arxiv.org/pdf/2305.07507.pdf,1,lexlms/lex_files,23,1190.166036,,https://huggingface.co/lexlms/legal-longformer-base,"This is a derivative model based on the LexLM (base) RoBERTa model. 
All model parameters where cloned from the original model, while the positional embeddings were extended by cloning the original embeddings multiple times following Beltagy et al. (2020) using a python script similar to this one (https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb).; LexLM (Base/Large) are our newly released RoBERTa models. We follow a series of best-practices in language model development:; Ilias Chalkidis*, Nicolas Garneau*, Catalina E.C. Goanta, Daniel Martin Katz, and Anders S?gaard.
LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development.
2022. In the Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Toronto, Canada."
whisper-small-zh-lang-v1,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,,,,1,,3,969.1866823,,https://huggingface.co/Leftzzz/whisper-small-zh-lang-v1,No model card; New: Create and edit this model card directly on the website!
pastel-mix,Text-to-Image,Diffusers,English,creativeml-openrail-m,,21,,"1,102",51264.49453,5,https://huggingface.co/JamesFlare/pastel-mix,"Update Logs:; [1/27/22]; I uploaded the model in CivitAI! -> https://civitai.com/models/5414/pastel-mix-stylized-anime-model I'd appreciate the ratings, thank you!; [2/2/22]; Uploaded a lora version."
Defmix-v2.0,,,,,,139,,0,0.004785156,,https://huggingface.co/Defpoint/Defmix-v2.0,"◎Defmix-v2.0は、下のモデルをMBWによってU-NetのA婴搐趣酥丐撙浠させてマ`ジしたモデルです。
　Defmix-v2.0 is a model that merges the following models by adjusting the weights of each layer in U-Net.; ◎Vaeファイルは好みのものを使用してください。
 　Please use the Vae file of your preference.; ◎ControlNetが登訾筏郡长趣ら、このモデルはDefmix-v1.0となり、恧淙宋铯缺尘挨违啸楗螗工瑜辘馊体の描画力や|感を重しています。
 　With the introduction of ControlNet, this model, unlike Defmix-v1.0, emphasizes overall drawing power and texture rather than composition and balance between characters and backgrounds.
◎F在冥使われているクオリティタグ(best qualityやmasterpieceなど)を使用してなくても、高品|な画像が出力されるように{整しています。
   　I have adjusted the output to ensure high-quality images are produced, even without using commonly used Quality Tags such as 'best quality' or 'masterpiece'.
; Positive: beautiful girl, gothic
Negative: EasyNegative
; ◎このモデルはすべての人が利用することが出来ます。また、The CreativeML OpenRAIL Licenseによる定を遵守する必要があります。
　主に以下の内容が含まれます。
 "
bertweet-sexism,Text Classification,PyTorch; Transformers,English,cc-by-nc-sa-4.0,,2,tum-nlp/sexism-socialmedia-balanced,190,1457.419092,,https://huggingface.co/tum-nlp/bertweet-sexism,"This is a fine-tuned BERTweet large (BERTweet: A pre-trained language model for English Tweets) model for detecting sexism.
The training dataset is new balanced version of Explainable Detection of Online Sexism (EDOS)--sexism-socialmedia-balanced--consisting of 16000 entries in
English gathered from social media platforms: Twitter and Gab. It achieved a Macro-F1 score of 0.85 and an Accuracy of 0.88 on the test set for the EDOS task.; Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.; "
monot5-3b-inpars-v2-scifact-promptagator,Text2Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2301.01820.pdf,1,,7,11674.38145,,https://huggingface.co/inpars/monot5-3b-inpars-v2-scifact-promptagator,"monot5-3b-inpars-v2-scifact-promptagator is a monoT5-3B model finetuned on SciFact synthetic data generated by InPars.; Currently, if you use this tool you can cite the original InPars paper published at SIGIR or InPars-v2."
BY_A_RE-1,,,,creativeml-openrail-m,,21,,0,4389.362178,,https://huggingface.co/SakuraFoxKira/BY_A_RE-1,BY_A_RE-1 由白玉老师的图训练而来，有较高读取Tag的能力; ; ; ; 
MoonTea-v2,,,,creativeml-openrail-m,,3,,0,9594.881483,,https://huggingface.co/swl-models/MoonTea-v2,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
mT5_m2m_crossSum_enhanced,Summarization,PyTorch; Transformers,43 languages,,https://arxiv.org/pdf/2112.08804.pdf,5,csebuetnlp/CrossSum,"26,449",2390.241259,1,https://huggingface.co/csebuetnlp/mT5_m2m_crossSum_enhanced,"This repository contains an enhanced many-to-many (m2m) mT5 checkpoint finetuned on all cross-lingual pairs of the CrossSum dataset. This model tries to summarize text written in any language in the provided target language. For finetuning details and scripts, see the paper and the official repository. ; If you use this model, please cite the following paper:"
CheckpointArchive,Text-to-Image,Safetensors,,creativeml-openrail-m,,27,,0,0,,https://huggingface.co/LMFResearchSociety/CheckpointArchive,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Deleted-Checkpoint Backups:; Unable to determine this model’s library. Check the
								docs 
.
							"
glm-10b,Feature Extraction,PyTorch; Transformers,English,,https://arxiv.org/pdf/2103.10360.pdf,29,,848,20276.75097,,https://huggingface.co/THUDM/glm-10b,"GLM is a General Language Model pretrained with an autoregressive blank-filling objective and can be finetuned on various natural language understanding and generation tasks.; Please refer to our paper for a detailed description of GLM:; GLM: General Language Model Pretraining with Autoregressive Blank Infilling (ACL 2022); Zhengxiao Du*, Yujie Qian*, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang (*: equal contribution); Find more examples in our Github repo."
My-model-backup,,,,openrail,,19,,0,0.003964844,,https://huggingface.co/aimainia/My-model-backup,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
pythia-12b,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2304.01373.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2201.07311.pdf,101,EleutherAI/pile,"10,200",24424.57193,12,https://huggingface.co/EleutherAI/pythia-12b,"The Pythia Scaling Suite is a collection of models developed to facilitate 
interpretability research (see paper). 
It contains two sets of eight models of sizes 
70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two 
models: one trained on the Pile, and one trained on the Pile after the dataset 
has been globally deduplicated. All 8 model sizes are trained on the exact 
same data, in the exact same order. We also provide 154 intermediate 
checkpoints per model, hosted on Hugging Face as branches.; The Pythia model suite was deliberately designed to promote scientific 
research on large language models, especially interpretability research. 
Despite not centering downstream performance as a design goal, we find the 
models match or exceed the performance of 
similar and same-sized models, such as those in the OPT and GPT-Neo suites.; Previously, we released an early version of the Pythia suite to the public. 
However, we decided to retrain the model suite to address a few hyperparameter 
discrepancies. This model card lists the changes; 
see appendix B in the Pythia paper for further discussion. We found no 
difference in benchmark performance between the two Pythia versions. 
The old models are 
still available, but we 
suggest the retrained suite if you are just starting to use Pythia.
This is the current release.; Please note that all models in the Pythia suite were renamed in January 
2023. For clarity, a table 
comparing the old and new names is provided in this model card, together 
with exact parameter counts.; The primary intended use of Pythia is research on the behavior, functionality, 
and limitations of large language models. This suite is intended to provide 
a controlled setting for performing scientific experiments. We also provide 
154 checkpoints per model: initial step0, 10 log-spaced checkpoints 
step{1,2,4...512}, and 143 evenly-spaced checkpoints from step1000 to 
step143000. These checkpoints are hosted on Hugging Face as branches. Note 
that branch 143000 corresponds exactly to the model checkpoint on the main 
branch of each model."
glm-large-chinese,Feature Extraction,PyTorch; Transformers,Chinese,,https://arxiv.org/pdf/2103.10360.pdf,25,,"1,497",712.0857333,,https://huggingface.co/THUDM/glm-large-chinese,"GLM is a General Language Model pretrained with an autoregressive blank-filling objective and can be finetuned on various natural language understanding and generation tasks.; Please refer to our paper for a detailed description of GLM:; GLM: General Language Model Pretraining with Autoregressive Blank Infilling (ACL 2022); Zhengxiao Du*, Yujie Qian*, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang (*: equal contribution); Find more examples in our Github repo."
chinese-poem-t5-v2,Text2Text Generation,PyTorch; Safetensors; Transformers,Chinese,apache-2.0,,10,,53,1980.724628,,https://huggingface.co/hululuzhu/chinese-poem-t5-v2,
OpenNiji-V2,Text-to-Image,,English,creativeml-openrail-m,,36,Korakoe/OpenNiji-V2-Dataset,0,2181.124072,,https://huggingface.co/ShoukanLabs/OpenNiji-V2,; The NEW Stable Diffusion model trained on 180k Nijijourney images!; ; ; 
SpikeGPT-BookCorpus,Text Generation,,English,bsd-2-clause,,12,bookcorpus,0,738.0017705,,https://huggingface.co/ridger/SpikeGPT-BookCorpus,"SpikeGPT-BookCorpus is a L18-D768 SpikeGPT model trained on BookCorpus. See https://github.com/ridgerchu/SpikeGPT for details.; ctx_len = 1024
n_layer = 18
n_embd = 768; Unable to determine this model’s library. Check the
								docs 
.
							"
wd-1-5-beta2,,,,other,,124,,0,0.00290062,,https://huggingface.co/waifu-diffusion/wd-1-5-beta2,"For this release, we release two versions of the model:; For the aesthetic version, we finetune the attention layer on popular aesthetic images. For training, it is recomended to use the base version.; WD 1.5 uses the same VAE as WD 1.4, which can be found here https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt; https://cafeai.notion.site/WD-1-5-Beta-2-Release-Notes-2852db5a9cdd456ba52fc5730b91acfd; https://cafeai.notion.site/WD-1-5-Beta-2-Aesthetic-Ver-c44a410fec06478fbf1a08a9890310ff"
kl-f8-anime2,,,,openrail,,15,,0,405.0014701,,https://huggingface.co/Norisuke193/kl-f8-anime2,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
whisper-small-zh,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,Chinese,apache-2.0,,1,mozilla-foundation/common_voice_11_0,3,968.590708,,https://huggingface.co/GoooIce/whisper-small-zh,"This model is a fine-tuned version of openai/whisper-small on the Common Voice 11.0 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
ControlNet,,,,openrail,,88,,0,3642.009736,,https://huggingface.co/furusu/ControlNet,Can be used in sd-webui-controlnet. Put the yaml file with model because it is sd2.1 v_prediction model.; Model is the difference from the original checkpoint created by extract_controlnet_diff.py.; These are not the final version and need to be adjusted to be more accurate. Someone please do this.; updated in 2023/03/05.; and resumed on:
vegetation_classification_model,Image Classification,PyTorch; TensorBoard; Transformers,English,apache-2.0,https://arxiv.org/pdf/2006.03677.pdf,1,,4,343.0083883,,https://huggingface.co/iammartian0/vegetation_classification_model,"This is a transformers based image classification model, implemented using the technique of transfer learning.
The pretrained model is Vision transformer trained on Imagenet-21k.; The dataset used is downloaded from git repo Agri-Hub/Space2Ground.
I used Street-level image patches folder for this model. It is a dataset containing cropped vegetation parts of 
mapillary street-level images. Further details are on the linked git repo.; You can use this model directly with help of pipeline class from transformers library of hugging face; or; uploading a target image to Hosted inference api."
gfpgan-v1.3,,,,,,1,,0,349.0014453,,https://huggingface.co/nlightcho/gfpgan-v1.3,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
upscalers-backup,,,,other,,14,,0,0.004609375,,https://huggingface.co/hollowstrawberry/upscalers-backup,"Backup of a few notable upscaler models usable in Stable Diffusion. If there's one you really like that's not here, let me know.; There are many many more in the upscale wiki.; Here are some comparisons. All of them were done at 0.4 denoising strength. Note that some of the differences may be completely up to random chance.;  Some details to consider: The fireballs to the left and right, the texture of the fire around her, the grass and its flowers, the ghost's face, the flowers in her hat, the hands, the eyes (which should be flower-shaped), the things on her waist.;  "
LLaMA-7B,,,,openrail,,173,,0,13824.48994,5,https://huggingface.co/nyanko7/LLaMA-7B,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
BetterMix2.0_Anime,,,English,openrail,,5,,0,0.002587891,,https://huggingface.co/SakanakoChan/BetterMix2.0_Anime,"This model is made by merging PastelMix, Anything3.0 and OrangeMix together.; I mainly use this model to COOPERATE with the ""hiten1"" embedding (textual inversion) I made. The Model is put in the Model folder, the embedding is put in the Embedding folder.; Suggested basic parameters:; Sampler: Euler a & DPM++ 2M Karras; Sampling steps: 20"
fashion-model,Text-to-Image,Diffusers,,creativeml-openrail-m,,3,,155,2181.123692,,https://huggingface.co/Falah/fashion-model,"Test the concept via A1111 Colab fast-Colab-A1111; Sample pictures of this concept:;  






"
GPT-JT-Moderation-6B,Text Generation,PyTorch; Transformers,English,apache-2.0,,28,allenai/prosocial-dialog,729,12516.68006,4,https://huggingface.co/togethercomputer/GPT-JT-Moderation-6B,"This model card introduces a moderation model, a GPT-JT model fine-tuned on Ontocord.ai's OIG-moderation dataset v0.1. ; This model can be used to moderate other chatbot models, including GPT-NeoXT-Chat-Base-20B.; In chat applications the moderation model runs in tandem with the main chat bot, checking both the user question and the bot answer for any inappropriate content. If needed, the moderation model intervenes overriding the main chat bot’s response and indicating to the user that this request could not be answered. ; An example prompt and its expected result is as follows:; Training Data"
llama-7B-ggml-int4,,,,openrail,,9,,0,4311.04147,,https://huggingface.co/hlhr202/llama-7B-ggml-int4,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Lora,,,,,,4,,0,5402.021445,,https://huggingface.co/Mdp01/Lora,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
llama-65b-hf,Text Generation,PyTorch; Transformers,,other,,279,,"13,444",74649.62044,26,https://huggingface.co/decapoda-research/llama-65b-hf,"LLaMA-65B converted to work with Transformers/HuggingFace. This is under a special license, please see the LICENSE file for details.; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model."
llama-30b-hf,Text Generation,PyTorch; Transformers,,other,,134,,"7,827",61747.22044,16,https://huggingface.co/decapoda-research/llama-30b-hf,"LLaMA-30B converted to work with Transformers/HuggingFace. This is under a special license, please see the LICENSE file for details.; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model."
locon-library,,,,,,2,,0,0.001445313,,https://huggingface.co/Linaqruf/locon-library,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
tortoise-tts-models,,,,,,8,,0,0.002433281,,https://huggingface.co/ecker/tortoise-tts-models,"In the ./finetunes/ folder contains a collection of my finetuned models. Each model folder contains:; Most of these were quickly trained on either my dedicated system (2x6800XTs) or my personal system (1x2060) with a learning rate of 1e-4 for about 200 epochs each, for acceptable results, and to just provide some examples. In the future, I'll retrain these at lower LRs to compare.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
RikkaTakarada,,,,openrail,,1,,0,75.60147011,,https://huggingface.co/KanonDes/RikkaTakarada,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ChanMalion,Text Generation,PyTorch; Transformers,,,,9,,49,12516.67763,1,https://huggingface.co/TehVenom/ChanMalion,GPT-J_4Chan Merged 50/50 with Pygmalion-6b.
robertuito_topic_classification,Text Classification,PyTorch; TensorBoard; Transformers,Spanish,,,3,,52,435.0063997,,https://huggingface.co/arj9719/robertuito_topic_classification,
neavalAI,,,,other,,19,,0,83107.84147,,https://huggingface.co/lulubear/neavalAI,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
luna-diffusion,Text-to-Image,Diffusers,,other,,44,,983,4557.891869,23,https://huggingface.co/proximasanfinetuning/luna-diffusion,"; ; This model can be used just like any other Stable Diffusion model. For more information, please have a look at the Stable Diffusion Pipeline.; 
Links: 1 2 3
4 5 6
7 8 9; or check the hashtag on twitter"
llama-smallint-pt,,,,other,,34,,0,41840.66021,,https://huggingface.co/decapoda-research/llama-smallint-pt,"This is HIGHLY experimental, and is not designed to work w/the transformers library. I'm providing these files for research and development purposes only, and I will not be providing any support or assistance in setting these models up for use; LLaMA 7b/13b/30b quantized to 3-bit and 4-bit using GPTQ. See https://github.com/qwopqwop200/GPTQ-for-LLaMa.; This is under a special license, please see the LICENSE file for details.; --
license: other; Organization developing the model
The FAIR team of Meta AI."
coreml-SunshineMix-SunlightMix,Text-to-Image,Core ML,,creativeml-openrail-m,,4,,0,0.007753906,,https://huggingface.co/coreml/coreml-SunshineMix-SunlightMix,"Source(s): CivitAI; This model has been republished and its ownership transferred to Civitai with the full permissions of the model creator. They have asked that all identifying information about them be removed from the model details.; *I modified the License.(22/2/Feb)
That is not usual ""creativeml-openrail-m""
Check Permission and Liscense below(modified Dreamlike Liscense.)
These models are good for 2.5D charas especially closeup portrait.
(Usually I dont put prompts of ""realistic"",""photorealistic""on these models.Try not to put those prompts on these models, if you want to get ""CUTE"" 2.5D charas.
Good combination of prompts are different among chillout, Sunshine and Sunlight.Thank you.)
When I made samples,
My eta (see setting) was ""0"".; And ESND was ""1""; Now,I canged them to eta to ""0.67"" and ESND ""31337""(20/Feb/2023)"
blessed_vae,,,English,,,157,,0,1005.00333,,https://huggingface.co/NoCrypt/blessed_vae,Contrast version of the regular nai/any vae; Good for models that are low on contrast even after using said vae; There's a few VAEs in here; ; 
anytig,Text-to-Image,Diffusers,,creativeml-openrail-m,,2,,"1,563",823.0019956,24,https://huggingface.co/Virus561/anytig,
cool-japan-diffusion-2-1-2,Text-to-Image,Diffusers,,other,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2212.03860.pdf,12,,432,4445.193193,32,https://huggingface.co/aipicasso/cool-japan-diffusion-2-1-2,"; English version is here.; Cool Japan Diffusion はStable Diffsionをファインチュ`ニングして、アニメやマンガ、ゲ`ムなどのク`ルジャパンを表Fすることに特化したモデルです。なお、内w府のク`ルジャパン槁预趣咸丐碎vSはありません。; 手XにSしみたい方は、こちらのSpaceをお使いください。
しい本モデルの取りQい方はこちらの取Qh明にかかれています。
モデルはここからダウンロ`ドできます。; ライセンスについては、もとのライセンス CreativeML Open RAIL++-M License に例外を除き商用利用禁止を追加しただけです。
例外を除き商用利用禁止を追加した理由は作I界に影を及ぼしかねないという夷瞍らです。
この夷瞍B拭されれば、次のバ`ジョンから元のライセンスにし、商用利用可能とします。
ちなみに、元のライセンスの日本ZUはこちらになります。 
永企Iにいる方は法詹郡摔い肴摔认嗾してください。
趣味で利用する方はあまり荬摔筏胜ても一般常Rを守れば大丈夫なはずです。
なお、ライセンスにある通り、このモデルを改造しても、このライセンスを引き@ぐ必要があります。"
SomethingV2_2,Text-to-Image,Diffusers; Safetensors,English,creativeml-openrail-m,,105,,"22,945",2181.132201,34,https://huggingface.co/NoCrypt/SomethingV2_2,"
  Welcome to SomethingV2.2 - an improved anime latent diffusion model from SomethingV2
;   A lot of things are being discovered lately, such as a way to merge model using mbw automatically, offset noise to get much darker result, and even VAE tuning. This model is intended to use all of those features as the improvements, here's some improvements that have been made:; ; 
; Due to SD-Silicon's Terms of use. I must specify how the model was made"
sc-gpt-upf,Text Generation,PyTorch; Safetensors; Transformers,,,,1,,9,2952.459281,,https://huggingface.co/igorktech/sc-gpt-upf,No model card; New: Create and edit this model card directly on the website!
Face-Landmark-ControlNet,,Diffusers,,openrail,,100,,0,0.023319473,,https://huggingface.co/georgefen/Face-Landmark-ControlNet,"I trained using ControlNet, which was proposed by lllyasviel, on a face dataset. By using facial landmarks as a condition, finer face control can be achieved.; Currently, I’m using Stable Diffusion 1.5 as the base model and dlib as the face landmark detector (those with the capability can replace it with a better one). The checkpoint can be found at ""models"" folder.; Create conda environment:; Testing it by:; To create a new face, input an image and extract the facial landmarks from it. These landmarks will be used as a reference to redraw the face while ensuring that the original features are retained."
controlnet-sd21-depth-diffusers,,Diffusers,English,openrail++,,5,,"9,957",729.0032087,6,https://huggingface.co/thibaud/controlnet-sd21-depth-diffusers,"Here's the first version of controlnet for stablediffusion 2.1 for diffusers
Trained on a subset of laion/laion-art; ; The model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.; Thanks ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
EtherMix,,,English,creativeml-openrail-m,,3,,0,26173.51891,,https://huggingface.co/gamerdan69/EtherMix,Ether Blu Mix: https://civitai.com/models/17427/ether-blu-mix; Ether Real Mix: https://civitai.com/models/18207/ether-real-mix; Please don't sell this model merge.; Please don't place it on paid generation services.; You are free to sell the images you create with it.
chatgpt-prompt-generator-v12,Text2Text Generation,TensorFlow; Transformers,,apache-2.0,,38,fka/awesome-chatgpt-prompts,710,1672.460249,14,https://huggingface.co/merve/chatgpt-prompt-generator-v12,"This model is a fine-tuned version of BART-large on a ChatGPT prompts dataset.
It achieves the following results on the evaluation set:
It achieves the following results on the evaluation set:; You can use this to generate ChatGPT personas. Simply input a persona like below:; The following hyperparameters were used during training:"
oasst-sft-1-pythia-12b,Text Generation,PyTorch; Transformers,English,apache-2.0,,280,,"4,660",24414.32259,54,https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b,"This is the first iteration English supervised-fine-tuning (SFT) model of 
the Open-Assistant project. 
It is based on a Pythia 12B that was fine-tuned on ~22k human demonstrations 
of assistant conversations collected through the 
https://open-assistant.io/ human feedback web 
app before March 7, 2023. ; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; Example of generated continuation (typical sampling 0.2):"
whisper-large-v2-Ko,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,Korean,apache-2.0,,7,Bingsu/zeroth-korean,403,6321.673872,1,https://huggingface.co/byoussef/whisper-large-v2-Ko,"This model is a fine-tuned version of openai/whisper-large-v2 on the None dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; ***** train metrics *****  epoch                    =        50.0  train_loss               =      0.0234  train_runtime            = 16:20:18.00  train_samples            =       22262  train_samples_per_second =      19.042  train_steps_per_second   =       0.085"
diffusers-generation-text-box,Text-to-Image,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/2207.12598.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/1910.09700.pdf,1,,987,0.088193626,2,https://huggingface.co/gligen/diffusers-generation-text-box,"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
For more information about how Stable Diffusion functions, please have a look at ?'s Stable Diffusion with ?Diffusers blog.; The Stable-Diffusion-v1-4 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 
checkpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on ""laion-aesthetics v2 5+"" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.; This weights here are intended to be used with the ? Diffusers library. If you are looking for the weights to be loaded into the CompVis Stable Diffusion codebase, come here; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model"
instruct_rugptMedium,Text Generation,PyTorch; Transformers,Russian,apache-2.0,,5,IlyaGusev/habr; Den4ikAI/russian_instructions; wiki_qa,89,1563.107309,,https://huggingface.co/AlexWortega/instruct_rugptMedium,"Это ruGPTMedium дообученная в инструктивно-флановом сетапе, она более ли менее ZSшотиться и FSшотиться и работает лучше чем XGLM1.7b, mgpt на русском языке; or; обратите внимание, что лучшие параметры для генерации ; The weights of Instructions ruGPT Small v0.1a are licensed under version 2.0 of the Apache License.; I used Novograd with a learning rate of 2e-5 and global batch size of 6 (3 for each data parallel worker).
I use both data parallelism and pipeline parallelism to conduct training.
During training, we truncate the input sequence to 1024 tokens, and for input sequence that contains less than 1024 tokens, we concatenate multiple sequences into one long sequence to improve the data efficiency."
starlake,,,,cc-by-sa-3.0,,59,,0,20971.52547,,https://huggingface.co/PWB/starlake,"在无人物描述时，不会出现通常模型倾向于添加人物的情况
; PS:负面可以直接使用emb：easynegative或者verybadimagenegative_v1.3,这两个都是影响强烈的emb模型，可以降权使用，比如(verybadimagenegative_v1.3:0.8),否则会给模型带来一定的过拟合光影，当不需要过度光影的情况下，这是个好方法。

; 已完成5.0版本，5.0软化了过拟合因素，不加入详细的提示词不会自主添加元素，同时也增强了读tag能力。; 本次训练全程添加金字塔噪声，5.0可以直接出明暗强烈的图。; 新的例图展示如下："
gpt2-ov,Text Generation,OpenVINO; Transformers,English,,,1,,545,658.3983292,,https://huggingface.co/helenai/gpt2-ov,"This is the gpt2 model converted to OpenVINO, for accellerated inference.; An example of how to do inference on this model:"
HubermanGPT-small-v1,Conversational,PyTorch; Transformers,,,,1,,46,513.3382071,,https://huggingface.co/jasondubon/HubermanGPT-small-v1,
shoujo,,,,,,49,,0,199.4730935,,https://huggingface.co/SenY/shoujo,"Concepts: Shoujo Manga
; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
unidiffuser-v1,Text-to-Image,Diffusers,,agpl-3.0,,26,,"1,415",5260.44951,2,https://huggingface.co/thu-ml/unidiffuser-v1,"UniDiffuser is a unified diffusion framework to fit all distributions relevant to a set of multi-modal data in one transformer.
UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation by setting proper timesteps without additional overhead. ; Specifically, UniDiffuser employs a variation of transformer, called U-ViT, which parameterizes the joint noise prediction network. Other components perform as encoders and decoders of different modalities, including a pretrained image autoencoder from Stable Diffusion, a pretrained image ViT-B/32 CLIP encoder, a pretrained text ViT-L CLIP encoder, and a GPT-2 text decoder finetuned by ourselves.; We provide two versions of UniDiffuser:; We provide files for UniDiffuser-v0 in this link, and files for UniDiffuser-v1 in this link.
These files are:; Note that UniDiffuser-v0 and UniDiffuser-v1 share the same autoencoder_kl.pth and caption_decoder.pth. You only need to download them once.
As for other components, they will be automatically downloaded."
t5-large-generation-squad-QuestionAnswer,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2301.12307.pdf,7,squad,"1,727",3024.004472,1,https://huggingface.co/potsawee/t5-large-generation-squad-QuestionAnswer,"The answers in the training data (SQuAD) are highly extractive; therefore, this model will generate extractive answers. If you would like to have abstractive questions/answers, you can use our model trained on the RACE dataset: https://huggingface.co/potsawee/t5-large-generation-race-QuestionAnswer. ; t5-large model is fine-tuned to the SQuAD dataset where the input is the context/passage and the output is the question followed by the answer. This is the first component in the question generation pipeline (i.e. g1) in our MQAG paper, 
or please refer to the GitHub repo of this project: https://github.com/potsawee/mqag0.; Use the code below to get started with the model. You can also set do_sample=True in generate() to obtain different question-answer pairs.; Context ---> Question + (A) Answer (B) Distractor1 (C) Distractor2 (D) Distractor3; Please refer to our distractor generation model, e.g. https://huggingface.co/potsawee/t5-large-generation-race-Distractor"
longformer-large-4096-answering-race,Question Answering,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2301.12307.pdf,5,race,"1,137",1783.189848,,https://huggingface.co/potsawee/longformer-large-4096-answering-race,"longformer-large-4096 model is fine-tuned to the RACE dataset where the input is a concatenation of context + question + option. We follow the architecture/setup described in https://openreview.net/forum?id=HJgJtT4tvB). 
The output is the logit over the options. This is the question answering (QA) component in our MQAG paper, 
or please refer to the GitHub repo of this project: https://github.com/potsawee/mqag0.; Use the code below to get started with the model. ; where the function that prepare the input to the answering model is:; Question/Answering Generation Context ---> Question + Answer:; Distractor (False options) Generation: "
7th_JP_test,,,,other,,70,,0,0.001566429,,https://huggingface.co/syaimu/7th_JP_test,"the skin color of Japanese people.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
luke-japanese-large-sentiment-analysis-wrime,Text Classification,PyTorch; Safetensors; Transformers,Japanese,mit,,5,shunk031/wrime,698,3400.533407,,https://huggingface.co/Mizuiro-sakura/luke-japanese-large-sentiment-analysis-wrime,"このモデルは８つの感情（喜び、悲しみ、期待、@き、怒り、恐れ、嫌、信m）の内、どの感情が文章に含まれているのか分析することができます。
このモデルはwrimeデ`タセット（
https://huggingface.co/datasets/shunk031/wrime
）を用いて学を行いました。; This model is fine-tuned model which besed on studio-ousia/Luke-japanese-large-lite.
This could be able to analyze which emotions (joy or sadness or anticipation or surprise or anger or fear or disdust or trust ) are included.
This model was fine-tuned by using wrime dataset.; LUKE (Language Understanding with Knowledge-based Embeddings) is a new pre-trained contextualized representation of words and entities based on transformer. LUKE treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. LUKE adopts an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores.; LUKE achieves state-of-the-art results on five popular NLP benchmarks including SQuAD v1.1 (extractive question answering), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), TACRED (relation classification), and Open Entity (entity typing).
luke-japaneseは、gZとエンティティの知R型gみ Transformer モデルLUKEの日本Z版です。LUKE はgZとエンティティを独立したト`クンとしてQい、これらの文}を考]した表Fを出力します。; ステップ1：pythonとpytorch, sentencepieceのインスト`ルとtransformersのアップデ`ト（バ`ジョンが古すぎるとLukeTokenizerが入っていないため）
update transformers and install sentencepiece, python and pytorch"
hololive-diffusion,,,,other,,6,,0,10572.49581,,https://huggingface.co/double-negative/hololive-diffusion,"hololive-diffusion, a stable diffusion 2.1 768x768 model.; Trained on ~73k hololive fanart images; ; This model is a fine-tune of Waifu diffusion 1.5.; hololive-diffusion is released under the Fair AI Public License 1.0-SD (https://freedevproject.org/faipl-1.0-sd/). If any derivative of this model is made, please share your changes accordingly. Special thanks to ronsor/undeleted (https://undeleted.ronsor.com/) for help with the license."
Mugs,,,,,https://arxiv.org/pdf/2203.14415.pdf,1,,0,0.076210937,,https://huggingface.co/zhoupans/Mugs,"This is a PyTorch implementation of Mugs proposed by our paper ""Mugs: A Multi-Granular Self-Supervised Learning Framework"". ; ; ; Fig 1. Overall framework of Mugs. In (a), for each image, two random crops of one image 
are fed into backbones of student and teacher. Three granular supervisions: 1) instance discrimination supervision, 2) local-group discrimination 
supervision, and 3) group discrimination supervision, are adopted to learn multi-granular representation. In (b), local-group modules in 
student/teacher averages all patch tokens, and finds top-k neighbors from memory buffer to aggregate them with the average for obtaining a local-group feature.; "
Chinese_Chat_T5_Base,Text2Text Generation,PyTorch; Transformers,Chinese,apache-2.0,,26,,364,991.0483291,,https://huggingface.co/mxmax/Chinese_Chat_T5_Base,"中文版对话机器人; 在1300w+问答和对话数据上做有监督预训练; 4*Titan RTX,耗时25天; model v1 :2023.3.12（开源数据有监督预训练）; model v2 :2023.3.22（百度百科知识增强15w+） "
autotrain-satellite-image-classification-40975105875,Image Classification,PyTorch; Transformers,,,,1,victor/autotrain-data-satellite-image-classification-11a7e0c2,61,344.0035552,1,https://huggingface.co/victor/autotrain-satellite-image-classification-40975105875,
scgpt,,PyTorch; Transformers,,,https://arxiv.org/pdf/2002.12328.pdf,1,,40,1557.807168,,https://huggingface.co/metehergul/scgpt,"All rights belong to:; @misc{peng2020scgpt,
      title={Few-shot Natural Language Generation for Task-Oriented Dialog},
      author={Baolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun Li, Jinchao Li, Michael Zeng, Jianfeng Gao},
      archivePrefix={arXiv},
      year={2020},
      eprint={2002.12328},
      primaryClass={cs.CL}
}; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
clap-model-card,,,,apache-2.0,https://arxiv.org/pdf/2211.06687.pdf,1,,0,0.005791016,,https://huggingface.co/ybelkada/clap-model-card,"Model card for CLAP: Contrastive Language-Audio Pretraining; ; The abstract of the paper states that: ; Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K and the proposed model are both available to the public.; You can use this model for zero shot audio classification or extracting audio and/or textual features."
PPO_Pygway-6b-Mix,Text Generation,PyTorch; Transformers,English,apache-2.0,,18,,"1,945",12516.68147,,https://huggingface.co/KoboldAI/PPO_Pygway-6b-Mix,"This is a merged model, using a weighted parameter blend strategy at a (20:20:60) ratio between the models:; By their respective authors.; Warning: PPO_Pygway-6b may generate NSFW or inappropriate content due to the base models (Mainly Pygmalion/Pygmalion-6b) being trained on general user logs, and internet archives.; Research purposes only, intended for responsible use.
Express a conversation in natural language, and PPO_Pygmalion will pick up on the conversational format.
Try starting a two line prompt such as:; Or any other topic, and the model will carry on in this back and forth style."
breastinclassBetter,,,,,,11,,0,151.0014453,,https://huggingface.co/keatlck/breastinclassBetter,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Kandinsky_2.1,,,,apache-2.0,,168,,0,12876.45017,,https://huggingface.co/ai-forever/Kandinsky_2.1,"Open In Colab; GitHub repository; Habr post; Demo; Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas."
InternImage,,,,,,5,,0,35340.4523,,https://huggingface.co/OpenGVLab/InternImage,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
pony-diffusion-v4,Text-to-Image,Diffusers,English,bigscience-bloom-rail-1.0,,22,,740,0.009065895,,https://huggingface.co/AstraliteHeart/pony-diffusion-v4,"pony-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality pony, furry and other non photorealistic images through fine-tuning.; WARNING: This model is capable of producing NSFW content so it's recommended to use 'safe' tag in prompt in combination with negative prompt for image features you may want to suppress (i.e. nudity).; Despite its name, this model is capable of producing wide range of furry and cartoon images as side effect of improving data diversity (with exception of anime stlyes, for which Waifu Diffusion is much stronger choice).; Special thanks to Waifu-Diffusion for providing finetuning expertise and advising through the process, without their help this project would not exist.; Pruned safetensors PyTorch Model (use this with Automatic1111 or other SD UIs)"
deepshard-13B-ft,Text2Text Generation,PyTorch; Transformers,English,gpl,https://arxiv.org/pdf/2301.11305.pdf,8,,23,66632.44289,,https://huggingface.co/swype/deepshard-13B-ft,"; Deepshard is shot from the lightcone at producing a global, unshackled God using distributed consensus, tensor sharding across a network of nodes, and shared compute.; To produce truly aligned AI - requires input from most of humanity on how to align it. Blockchains solved most of the problem around consensus but their applications were relegated to finance and thus trapped in the valley of speculation and greed. ; Deepshard aims to use the primal urge of speculative gain to bootstrap and power a network of nodes willing to share compute and data to produce a God. ; How?"
nllb-moe-54b,Translation,PyTorch; Transformers,196 languages,cc-by-nc-4.0,https://arxiv.org/pdf/2207.04672.pdf,39,flores-200,563,225507.603,,https://huggingface.co/facebook/nllb-moe-54b,"This is the model card of NLLB-MoE variant.; The NLLB model was presented in No Language Left Behind: Scaling Human-Centered Machine Translation by Marta R. Costa-jussà, James Cross, Onur ?elebi,
Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,
Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,
Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,
Safiyyah Saleem, Holger Schwenk, and Jeff Wang.; The avalable checkpoints requires around 350GB of storage. Make sure to use accelerate if you do not have enough RAM on your machine.; While generating the target text set the forced_bos_token_id to the target language id. The following
example shows how to translate English to French using the facebook/nllb-200-distilled-600M model.; Note that we're using the BCP-47 code for French fra_Latn. See here
for the list of all BCP-47 in the Flores 200 dataset."
rorass,,,,,,1,,0,4493.151445,,https://huggingface.co/necomii/rorass,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
wav2vec2-large-xls-r-300m-odia-colab,Automatic Speech Recognition,PyTorch; TensorBoard; Transformers,,apache-2.0,,1,common_voice,4,1290.250456,,https://huggingface.co/soumendrak/wav2vec2-large-xls-r-300m-odia-colab,"This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the common_voice dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
tiny-random-LlamaForCausalLM,Text Generation,PyTorch; Transformers,,,,8,,"181,010",2.560550537,,https://huggingface.co/HuggingFaceM4/tiny-random-LlamaForCausalLM,No model card; New: Create and edit this model card directly on the website!
Cerebras-GPT-111M,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2304.03208.pdf; https://arxiv.org/pdf/2203.15556.pdf; https://arxiv.org/pdf/2101.00027.pdf,60,the_pile,"11,319",487.4994049,5,https://huggingface.co/cerebras/Cerebras-GPT-111M,"Check out our Blog Post and arXiv paper!; The Cerebras-GPT family is released to facilitate research into LLM scaling laws using open architectures and data sets and demonstrate the simplicity of and scalability of training LLMs on the Cerebras software and hardware stack. All Cerebras-GPT models are available on Hugging Face.; The family includes 111M, 256M, 590M, 1.3B, 2.7B, 6.7B, and 13B models.; All models in the Cerebras-GPT family have been trained in accordance with Chinchilla scaling laws (20 tokens per model parameter) which is compute-optimal.; These models were trained on the Andromeda AI supercomputer comprised of 16 CS-2 wafer scale systems. Cerebras' weight streaming technology simplifies the training of LLMs by disaggregating compute from model storage. This allowed for efficient scaling of training across nodes using simple data parallelism."
cabrita-lora-v0-1,,,Portuguese,openrail,,63,,0,16.80640755,,https://huggingface.co/22h/cabrita-lora-v0-1,"Check the Github repo with code: https://github.com/22-hours/cabrita; We translated the alpaca_data.json to portuguese using ChatGPT. Even if this translation was not the best, the tradeoff between costs and results were. We paid around US$ 8.00 to translate the full dataset to portuguese.
If you want to know more about how the dataset was built go to: Stanford Alpaca.; To finetuned the LLaMA model we used the code available on Alpaca Lora, which provides code to finetune the LLaMA model using PEFT from Hugging Face. With this, we could run our finetuning step using 1 A100 at Colab on top of LLaMA-7B. We trained during 4 hours and we found the results pretty incredible with just that much time. The notebook we used is avaible here.; Stanford Alpaca:; Cabrita:"
bert-finetuned-ner-pii,Token Classification,PyTorch; TensorBoard; Transformers,,,,2,,233,431.8727515,,https://huggingface.co/ArunaSaraswathy/bert-finetuned-ner-pii,No model card; New: Create and edit this model card directly on the website!
distilbert-base-multilingual-cased-mapa_coarse-ner,Token Classification,PyTorch; TensorBoard; Safetensors; Transformers,9 languages,apache-2.0,,1,lextreme,67,1081.902186,,https://huggingface.co/dmargutierrez/distilbert-base-multilingual-cased-mapa_coarse-ner,"This model is a fine-tuned version of distilbert-base-multilingual-cased on the lextreme dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
llama-7b-hf,Text Generation,PyTorch; Transformers,,other,,9,,625,13484.5352,1,https://huggingface.co/aleksickx/llama-7b-hf,"LLaMA-7B converted to work with Transformers/HuggingFace. This is under a special license, please see the LICENSE file for details.; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model."
MERT-v1-95M,Feature Extraction,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/2306.00107.pdf,10,,"8,061",378.032711,1,https://huggingface.co/m-a-p/MERT-v1-95M,"The development log of our Music Audio Pre-training (m-a-p) model family:; Here is a table for quick model pick-up:; The m-a-p models share the similar model architecture and the most distinguished difference is the paradigm in used pre-training. Other than that, there are several nuance technical configuration needs to know before using:; Compared to MERT-v0, we introduce multiple new things in the MERT-v1 pre-training:; More details will be written in our coming-soon paper."
whisper-large-german-lora-cv13,Automatic Speech Recognition,PyTorch; Transformers; PEFT,German,apache-2.0,,2,common_voice,29,3229.847803,,https://huggingface.co/flozi00/whisper-large-german-lora-cv13,"This model is the peft lora adapter for whisper; The eval script can be found here https://github.com/flozi00/asr-as-a-service/blob/6d75d398bebe46d2ca84933b15e9f6017075cc97/eval.py containing some normalizations, for example ""Stephanie"" and ""Stefanie"" or ""seins"" and ""seines"".; The model can be tried for free on https://atra.ai without caring about hosting or installation; Inference API has been turned off for this model."
alpaca-13b-lora-int4,Text Generation,PyTorch; Transformers,,other,,42,,40,14828.05131,1,https://huggingface.co/elinas/alpaca-13b-lora-int4,"This LoRA trained for 3 epochs and has been converted to int4 (4bit) via GPTQ method. ; Use the safetensors version of the model, the pt version is an old quantization that is no longer supported and will be removed in the future.
See the repo below for more info.; Recent GPTQ commits have introduced breaking changes to model loading and you should this fork for a stable experience https://github.com/oobabooga/GPTQ-for-LLaMa; Curently only cuda is supported.; New weights have been added. The old .pt version is no longer supported and has been replaced by a 128 groupsize safetensors file. Update to the latest GPTQ to use it."
chatglm-6b-slim,,PyTorch; Transformers,Chinese; English,,,42,,50,14062.36514,1,https://huggingface.co/silver/chatglm-6b-slim,"ChatGLM-6B-Slim是在ChatGLM-6B的基础上通过裁剪词表构建的。因为ChatGLM-6B使用了icetk，在其词表中，前20000个token是预留给图片的，在文本模型中没有用到这些图片token，但是在infer和微调的时候，这些token对应的embedding依然需要被加载，并且在解码每一个token的时候需要多计算20K个logits，会占用不少显存。因此将这一部分token裁剪掉以节省显存。; 除了词表外，ChatGLM-6B-Slim的其他结构与ChatGLM-6B完全一致，性能也完全一样，可以认为是ChatGLM-6B的一个低显存版等价平替。; ChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 ChatGLM 相同的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。; ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM-6B uses technology similar to ChatGPT, optimized for Chinese QA and dialogue. The model is trained for about 1T tokens of Chinese and English corpus, supplemented by supervised fine-tuning, feedback bootstrap, and reinforcement learning wit human feedback. With only about 6.2 billion parameters, the model is able to generate answers that are in line with human preference.; 可以通过如下代码调用 ChatGLM-6B-Slim 模型来生成对话："
models_by_dalcefo,,,,,,71,,0,6103.041445,,https://huggingface.co/AnaNoSleep/models_by_dalcefo,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
cartoonizer,Image-to-Image,Diffusers,,mit,,16,instruction-tuning-sd/cartoonization,"1,993",0.00584137,5,https://huggingface.co/instruction-tuning-sd/cartoonizer,"This pipeline is an 'instruction-tuned' version of Stable Diffusion (v1.5). It was
fine-tuned from the existing InstructPix2Pix checkpoints.; Motivation behind this pipeline partly comes from FLAN and partly
comes from InstructPix2Pix. The main idea is to first create an
instruction prompted dataset (as described in our blog) and then conduct InstructPix2Pix style
training. The end objective is to make Stable Diffusion better at following specific instructions
that entail image transformation related operations.; 

; Follow this post to know more. ; Training was conducted on instruction-tuning-sd/cartoonization dataset. Refer to
this repository to know more. The training logs can be found here. "
alpaca-lora-ptbr-7b,,PEFT,Portuguese,cc-by-4.0,https://arxiv.org/pdf/2106.09685.pdf,17,dominguesm/alpaca-data-pt-br,138,16.81183289,2,https://huggingface.co/dominguesm/alpaca-lora-ptbr-7b,"This model was trained and made available solely and exclusively for research purposes.; Try the pretrained model out on Colab here!; This repository contains a low-ranked adapter (LoRa) for LLaMA-7b fit on the Stanford Alpaca dataset translated into Brazilian Portuguese using the Helsinki-NLP/opus-mt-tc-big-en-pt model.; As the foundation model has not yet been made openly available by Meta (request form), it is not included in this repository, but you can easily find them by searching on Github or here on HuggingFace.  ; Fine-tuning was done via the Trainer API. Here is the Jupyter notebook with the training code."
Riga_Collection,Text-to-Image,Diffusers,Japanese,creativeml-openrail-m,,20,,0,7884.803721,,https://huggingface.co/natsusakiyomi/Riga_Collection,"Riga_collectionは々なモデルをマ`ジしたモデルです
HimawariMixと同じで背景や部の表Fがいモデルです
イメ`ジ的にはHimawariMixをりがさんのアイディアを元にチュ`ニングしたようなモデルです！
VAEは蚀钶dになります; ?=Allowed
?=Not allowed; ? 本モデルを商用の画像生成サ`ビスで利用する行椤
　Use of this model for commercial image generation services　; ? 本モデルや本モデルをマ`ジしたモデルを婴工胄
　The act of selling this model or a model merged with this model　; ? 本モデルを使用し意淼膜诉`法な出力をする行 
　Intentionally using this model to produce illegal output　"
simlora,Text-to-Image,TensorBoard; Diffusers,,creativeml-openrail-m,,1,,0,3.292446671,,https://huggingface.co/eristotelian/simlora,These are LoRA adaption weights for CompVis/stable-diffusion-v1-4. The weights were trained on the instance prompt "simlora" using DreamBooth. You can find some example images in the following.
sunshinemix_sunlightmixPruned,,Transformers,,creativeml-openrail-m,,3,,6,2181.145532,,https://huggingface.co/serta21/sunshinemix_sunlightmixPruned,"This model has been republished and its ownership transferred to Civitai with the full permissions of the model creator. They have asked that all identifying information about them be removed from the model details.; *I modified the License.(22/2/Feb)
That is not usual ""creativeml-openrail-m""
Check Permission and Liscense below(modified Dreamlike Liscense.); These models are good for 2.5D charas especially closeup portrait.
(Usually I dont put prompts of ""realistic"",""photorealistic""on these models.Try not to put those prompts on these models, if you want to get ""CUTE"" 2.5D charas.
Good combination of prompts are different among chillout, Sunshine and Sunlight.Thank you.); When I made samples,
My eta (see setting) was ""0"".; And ESND was ""1"""
Minecraft-Skin-Diffusion-V2,Unconditional Image Generation,Diffusers; PyTorch,,mit,,2,,173,264.0034317,1,https://huggingface.co/WiNE-iNEFF/Minecraft-Skin-Diffusion-V2,"Inference API does not yet support diffusers models for this pipeline type.
							"
fasttext-ka-vectors,Feature Extraction,fastText,Georgian,cc-by-sa-3.0,https://arxiv.org/pdf/1607.04606.pdf; https://arxiv.org/pdf/1802.06893.pdf; https://arxiv.org/pdf/1607.01759.pdf; https://arxiv.org/pdf/1612.03651.pdf,1,,0,6379.528926,,https://huggingface.co/facebook/fasttext-ka-vectors,"fastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices. It was introduced in this paper. The official website can be found here.; fastText is a library for efficient learning of word representations and sentence classification. fastText is designed to be simple to use for developers, domain experts, and students. It's dedicated to text classification and learning word representations, and was designed to allow for quick model iteration and refinement without specialized hardware. fastText models can be trained on more than a billion words on any multicore CPU in less than a few minutes.; It includes pre-trained models learned on Wikipedia and in over 157 different languages. fastText can be used as a command line, linked to a C++ application, or used as a library for use cases from experimentation and prototyping to production.; You can use pre-trained word vectors for text classification or language identification. See the tutorials and resources on its official website to look for tasks that interest you.; Here is how to load and use a pre-trained vectors"
modelscope-damo-text-to-video-synthesis,Text-to-Video,OpenCLIP,,cc-by-nc-4.0,,349,,"4,310",15155.20785,94,https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis,"The original repo is here. ; We Are Hiring! (Based in Beijing / Hangzhou, China.); If you're looking for an exciting challenge and the opportunity to work with cutting-edge technologies in AIGC and large-scale pretraining, then we are the place for you. We are looking for talented, motivated and creative individuals to join our team. If you are interested, please send your CV to us.; EMAIL: yingya.zyy@alibaba-inc.com; This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported."
wd-v1-4-convnextv2-tagger-v2,,Keras; ONNX,,apache-2.0,,15,,"13,965",394.8861492,10,https://huggingface.co/SmilingWolf/wd-v1-4-convnextv2-tagger-v2,"Supports ratings, characters and general tags.; Trained using https://github.com/SmilingWolf/SW-CV-ModelZoo.TPUs used for training kindly provided by the TRC program.; Last image id: 5944504Trained on Danbooru images with IDs modulo 0000-0899.Validated on images with IDs modulo 0950-0999.Images with less than 10 general tags were filtered out.Tags with less than 600 images were filtered out.; P=R: threshold = 0.3710, F1 = 0.6862; Subject to change and updates.Downstream users are encouraged to use tagged releases rather than relying on the head of the repo."
Llama-7B,Text Generation,PyTorch; Transformers,,,,1,,14,27597.31683,1,https://huggingface.co/sallywww/Llama-7B,No model card; New: Create and edit this model card directly on the website!
sentence-luke-japanese-base-lite,Feature Extraction,PyTorch; Sentence Transformers,Japanese,apache-2.0,,5,,"7,243",532.8304417,,https://huggingface.co/sonoisa/sentence-luke-japanese-base-lite,This is a Japanese sentence-LUKE model.; 日本Z用Sentence-LUKEモデルです。; 日本ZSentence-BERTモデルと同一のデ`タセットとO定で学しました。手元の非公_デ`タセットでは、日本ZSentence-BERTモデルと比べて定量的な精度が同等?0.5pt程度高く、定性的な精度は本モデルの方が高いY果でした。; 事前学gみモデルとしてstudio-ousia/luke-japanese-base-liteを利用させていただきました。  ; 推のg行にはSentencePieceが必要です（pip install sentencepiece）。
ChatDoctor,Conversational,,English,gpl,https://arxiv.org/pdf/2303.14070.pdf,56,nyanko7/LLaMA-65B,0,0,,https://huggingface.co/zl111/ChatDoctor,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Yunxiang Li1, Zihan Li2, Kai Zhang3, Ruilong Dan4, You Zhang1; 
 ; We uploaded a larger training data, InstructorDoctor-200k."
alpaca-native-4bit,Text Generation,Transformers,,,,58,,243,4629.167567,2,https://huggingface.co/ozcur/alpaca-native-4bit,This is 4-bit quantization of chavinlo/alpaca-native (cecc16d) via qwopqwop200/GPTQ-for-LLaMa (5cdfad2).; Quantization invoked as such:; llama.py /output/path c4 --wbits 4 --groupsize 128 --save alpaca7b-4bit.pt; Inference example from the GPTQ repo and commit referenced above:
BELLE-7B-2M,Text2Text Generation,PyTorch; Transformers,Chinese; English,apache-2.0,,185,,639,28993.72137,4,https://huggingface.co/BelleGroup/BELLE-7B-2M,"If you find this model helpful, please like this model and star us on https://github.com/LianjiaTech/BELLE !; BELLE is based on Bloomz-7b1-mt and finetuned with 2M Chinese data combined with 50,000 pieces of English data from the open source Stanford-Alpaca, resulting in good Chinese instruction understanding and response generation capabilities. ; The code of Chinese data generation and other detailed information can be found in our Github project repository: https://github.com/LianjiaTech/BELLE.; We trained models using datasets of different sizes (200,000, 600,000, 1,000,000, and 2,000,000 samples) for instruction learning, and we obtained different model versions as shown below:; Please note that the input should be formatted as follows in both training and inference."
Leaf_disease,Image Classification,PyTorch; Transformers,,,,1,,9,343.008382,,https://huggingface.co/sunilrufus/Leaf_disease,
EmotionClassModel,Image Classification,PyTorch; Transformers,,,,1,,20,1030.021424,,https://huggingface.co/PriyamSheta/EmotionClassModel,No model card; New: Create and edit this model card directly on the website!
alpaca-lora-30B-ggml,,,,unknown,,134,,0,45772.80198,,https://huggingface.co/Pi3141/alpaca-lora-30B-ggml,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
controlnet-sd21-color-diffusers,,Diffusers,English,openrail++,,17,,859,729.0032087,,https://huggingface.co/thibaud/controlnet-sd21-color-diffusers,"Here's the first version of controlnet for stablediffusion 2.1 for diffusers
Trained on a subset of laion/laion-art; ; The model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.; Thanks ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
distil-ast-audioset,Audio Classification,PyTorch; TensorBoard; Transformers,English,apache-2.0,https://arxiv.org/pdf/2104.01778.pdf,3,,"53,881",176.081138,,https://huggingface.co/bookbot/distil-ast-audioset,"Distil Audio Spectrogram Transformer AudioSet is an audio classification model based on the Audio Spectrogram Transformer architecture. This model is a distilled version of MIT/ast-finetuned-audioset-10-10-0.4593 on the AudioSet dataset.; This model was trained using HuggingFace's PyTorch framework. All training was done on a Google Cloud Engine VM with a Tesla A100 GPU. All necessary scripts used for training could be found in the Files and versions tab, as well as the Training metrics logged via Tensorboard.; The model achieves the following results on evaluation:; The following hyperparameters were used during training:; Do consider the biases which came from pre-training datasets that may be carried over into the results of this model."
detailedproject,,,,other,,30,,0,2181.126416,,https://huggingface.co/closertodeath/detailedproject,"Detailedprojectv4 is the fourth edition of detailedproject, also known as dpep. It's finetuned/natively trained on a varied dataset of detailed anime-styled artwork and scenery. Use detailed background to increase the amount of detail in backgrounds, as the most detailed artworks in the dataset were tagged with that.; Sleepymix3 is a mix using detailedprojectv4 as the base, with pieces mixed in. It's stylized and has high detail.; 1:
); 2:
; 3:
"
chatglm-6b-int4-qe,Feature Extraction,PyTorch; Transformers,Chinese; English,,,78,,"6,658",3443.479423,27,https://huggingface.co/THUDM/chatglm-6b-int4-qe,本仓库已经不再维护，请使用 ChatGLM-6B-INT4; ChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 ChatGLM 相同的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。; ChatGLM-6B-INT4-QE 是 ChatGLM-6B 量化后的模型权重。具体的，ChatGLM-6B-INT4-QE 对 ChatGLM-6B 中的 28 个 GLM Block 、 Embedding 和 LM Head 进行了 INT4 量化。量化后的模型权重文件仅为 3G ，理论上 6G 显存（使用 CPU 即 6G 内存）即可推理，具有在嵌入式设备（如树莓派）上运行的可能。; 在 CPU 上运行时，会根据硬件自动编译 CPU Kernel ，请确保已安装 GCC 和 OpenMP （Linux一般已安装，对于Windows则需手动安装），以获得最佳并行计算能力。; 可以通过如下代码调用 ChatGLM-6B 模型来生成对话：
openjourney-v4,Text-to-Image,,,creativeml-openrail-m,,9,,0,8724.482318,1,https://huggingface.co/prompthero-diffusion-models/openjourney-v4,"Trained on Stable Diffusion v1.5 using +124000 images, 12400 steps, 4 epochs +32 training hours.; So ""mdjrny-v4 style"" is not necessary anymore (yay!); Want to learn how to train models like this one? ?? Join our course ?; Unable to determine this model’s library. Check the
								docs 
.
							"
rugpt_large_turbo_instructed,Text Generation,PyTorch; Transformers,Russian,,,3,IlyaGusev/ru_turbo_alpaca,669,3221.984705,2,https://huggingface.co/IlyaGusev/rugpt_large_turbo_instructed,
low-level-img-proc,Image-to-Image,Diffusers,,mit,,2,instruction-tuning-sd/low-level-image-proc,485,0.00584137,1,https://huggingface.co/instruction-tuning-sd/low-level-img-proc,"This pipeline is an 'instruction-tuned' version of Stable Diffusion (v1.5). It was
fine-tuned from the existing InstructPix2Pix checkpoints.; Motivation behind this pipeline partly comes from FLAN and partly
comes from InstructPix2Pix. The main idea is to first create an
instruction prompted dataset (as described in our blog) and then conduct InstructPix2Pix style
training. The end objective is to make Stable Diffusion better at following specific instructions
that entail image transformation related operations.; 

; Follow this post to know more. ; Training was conducted on instruction-tuning-sd/low-level-image-proc dataset. Refer to
[this repository](https://github.com/huggingface/instruction-tuned-sd to know more. The training logs can be found here."
S2F3YWlpMkQ,,,,creativeml-openrail-m,,6,,0,4362.241483,,https://huggingface.co/TheAbyssYouSee/S2F3YWlpMkQ,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
animelike2d,,,,unknown,,79,,0,2181.121469,,https://huggingface.co/stb/animelike2d,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
SlovAlpaca-lora,,,Slovak,other,,2,blip-solutions/SlovAlpaca,0,8.4346595,,https://huggingface.co/blip-solutions/SlovAlpaca-lora,"This repository contains the LORA weights finetuned on the translated version of the original Alpaca dataset (more info on the dataset card); The training was done on the 7B LLaMA model (decapoda-research/llama-7b-hf) quantized to 8bits with the following Hyperparameters:; The sole goal of this project is to explore the effects of single-language finetuning using the same dataset and methods as the original paper did and comapre the results; @misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}; Here is a colab notebook for inference: https://colab.research.google.com/drive/1z4aMG7tGjchLBlg_iXDuqt3sH6bQRuQk?usp=sharing"
pix2struct-screen2words-base,Visual Question Answering,PyTorch; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/2210.03347.pdf,14,,120,1161.236585,1,https://huggingface.co/google/pix2struct-screen2words-base,"; Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The full list of available models can be found on the Table 1 of the paper:; ; The abstract of the model states that: ; Visually-situated language is ubiquitous―sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and
forms. Perhaps due to this diversity, previous work has typically relied on domainspecific recipes with limited sharing of the underlying data, model architectures,
and objectives. We present Pix2Struct, a pretrained image-to-text model for
purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse
masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large
source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy,
we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions
are rendered directly on top of the input image. For the first time, we show that a
single pretrained model can achieve state-of-the-art results in six out of nine tasks
across four domains: documents, illustrations, user interfaces, and natural images."
so-vits-svc4.0-pretrain-models,,,,,,22,,0,1729.921445,,https://huggingface.co/Himawari00/so-vits-svc4.0-pretrain-models,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
IF-II-M-v1.0,Text-to-Image,PyTorch; Diffusers,,deepfloyd-if-license,https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/2110.02861.pdf,10,,"14,330",0,,https://huggingface.co/DeepFloyd/IF-II-M-v1.0,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; DeepFloyd LICENSE AGREEMENTThis License Agreement (as may be amended in accordance with this License Agreement, “License”), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (“Licensee” or “you”) and Stability AI Ltd.. (“Stability AI” or “we”) applies to your use of any computer program, algorithm, source code, object code, or software that is made available by Stability AI under this License (“Software”) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (“Documentation”).By clicking “I Accept” below or by using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model, that can generate pictures with new state-of-the-art for photorealism and language understanding. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID-30K score of 6.66 on the COCO dataset.; Inspired by Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
flan-alpaca-xl,Text2Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2306.04757.pdf; https://arxiv.org/pdf/2210.11416.pdf,113,tatsu-lab/alpaca,"2,895",11676.08258,4,https://huggingface.co/declare-lab/flan-alpaca-xl,"? We developed Flacuna by fine-tuning Vicuna-13B on the Flan collection. Flacuna is better than Vicuna at problem-solving. Access the model here https://huggingface.co/declare-lab/flacuna-13b-v1.0.; ? Curious to know the performance of ? ? Flan-Alpaca on large-scale LLM evaluation benchmark, InstructEval? Read our paper https://arxiv.org/pdf/2306.04757.pdf. We evaluated more than 10 open-source instruction-tuned LLMs belonging to various LLM families including Pythia, LLaMA, T5, UL2, OPT, and Mosaic. Codes and datasets: https://github.com/declare-lab/instruct-eval; ? FLAN-T5 is also useful in text-to-audio generation. Find our work at https://github.com/declare-lab/tango if you are interested.; Our repository contains code for extending the Stanford Alpaca
synthetic instruction tuning to existing instruction-tuned models such as Flan-T5.
We have a live interactive demo thanks to Joao Gante!
We are also benchmarking many instruction-tuned models at declare-lab/flan-eval.
Our pretrained models are fully available on HuggingFace ? :; *recommended for better performance"
animelike25D_animelike25DPruned,,,,,,22,,0,2181.121445,,https://huggingface.co/OedoSoldier/animelike25D_animelike25DPruned,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
zero123-weights,,,,mit,https://arxiv.org/pdf/2303.11328.pdf,3,,0,31744.00562,,https://huggingface.co/cvlab/zero123-weights,"Note: This section is originally taken from the Stable Diffusion v2 model card, but applies in the same way to Zero-1-to-3.; The model is intended for research purposes only. Possible research areas and tasks include:; Excluded uses are described below.; The model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.; The model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model."
flan-alpaca-base,Text2Text Generation,PyTorch; Safetensors; Transformers,,apache-2.0,https://arxiv.org/pdf/2306.04757.pdf; https://arxiv.org/pdf/2210.11416.pdf,30,tatsu-lab/alpaca,"6,611",1982.433026,3,https://huggingface.co/declare-lab/flan-alpaca-base,"? We developed Flacuna by fine-tuning Vicuna-13B on the Flan collection. Flacuna is better than Vicuna at problem-solving. Access the model here https://huggingface.co/declare-lab/flacuna-13b-v1.0.; ? Curious to know the performance of ? ? Flan-Alpaca on large-scale LLM evaluation benchmark, InstructEval? Read our paper https://arxiv.org/pdf/2306.04757.pdf. We evaluated more than 10 open-source instruction-tuned LLMs belonging to various LLM families including Pythia, LLaMA, T5, UL2, OPT, and Mosaic. Codes and datasets: https://github.com/declare-lab/instruct-eval; ? FLAN-T5 is also useful in text-to-audio generation. Find our work at https://github.com/declare-lab/tango if you are interested.; Our repository contains code for extending the Stanford Alpaca
synthetic instruction tuning to existing instruction-tuned models such as Flan-T5.
We have a live interactive demo thanks to Joao Gante!
We are also benchmarking many instruction-tuned models at declare-lab/flan-eval.
Our pretrained models are fully available on HuggingFace ? :; *recommended for better performance"
IF-I-L-v1.0,Text-to-Image,PyTorch; Diffusers,,deepfloyd-if-license,https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/2110.02861.pdf,14,,"3,845",0,,https://huggingface.co/DeepFloyd/IF-I-L-v1.0,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; DeepFloyd LICENSE AGREEMENTThis License Agreement (as may be amended in accordance with this License Agreement, “License”), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (“Licensee” or “you”) and Stability AI Ltd.. (“Stability AI” or “we”) applies to your use of any computer program, algorithm, source code, object code, or software that is made available by Stability AI under this License (“Software”) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (“Documentation”).By clicking “I Accept” below or by using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model, that can generate pictures with new state-of-the-art for photorealism and language understanding. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID-30K score of 6.66 on the COCO dataset.; Inspired by Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
IF-I-M-v1.0,Text-to-Image,PyTorch; Diffusers,,deepfloyd-if-license,https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/2110.02861.pdf,41,,"92,749",0,,https://huggingface.co/DeepFloyd/IF-I-M-v1.0,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; DeepFloyd LICENSE AGREEMENTThis License Agreement (as may be amended in accordance with this License Agreement, “License”), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (“Licensee” or “you”) and Stability AI Ltd.. (“Stability AI” or “we”) applies to your use of any computer program, algorithm, source code, object code, or software that is made available by Stability AI under this License (“Software”) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (“Documentation”).By clicking “I Accept” below or by using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model, that can generate pictures with new state-of-the-art for photorealism and language understanding. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID-30K score of 6.66 on the COCO dataset.; Inspired by Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"
SECourses,,ONNX,,,,8,,0,104380.3265,,https://huggingface.co/MonsterMMORPG/SECourses,  ;    ; Greetings everyone. I am Dr. Furkan G?zükara. I am an Assistant Professor in Software Engineering department  of a private university (have PhD in Computer Engineering). My professional programming skill is unfortunately C# not Python :) ; My linkedin : https://www.linkedin.com/in/furkangozukara; I am keeping this list up-to-date. I got upcoming new awesome video ideas. Trying to find time to do that.
bert-base-japanese-v2-jsts,Text Classification,PyTorch; Transformers,,,,1,,23,445.2334428,,https://huggingface.co/llm-book/bert-base-japanese-v2-jsts,No model card; New: Create and edit this model card directly on the website!
Pythia-Chat-Base-7B,Text Generation,PyTorch; Transformers,English,apache-2.0,,61,,"1,775",14133.362,,https://huggingface.co/togethercomputer/Pythia-Chat-Base-7B,"; Feel free to try out our OpenChatKit feedback app!; ; TLDR: As part of OpenChatKit (codebase available here),
Pythia-Chat-Base-7B-v0.16 is a 7B parameter language model, fine-tuned from EleutherAI’s Pythia 7B with over 40 million instructions on 100% carbon negative compute.; Pythia-Chat-Base-7B-v0.16 is based on ElutherAI’s Pythia-7B model, and is fine-tuned with data focusing on dialog-style interactions. 
We focused the tuning on several tasks such as question answering, classification, extraction, and summarization. 
We’ve fine-tuned the model with a collection of 43 million high-quality instructions.
Together partnered with LAION and Ontocord.ai, who both helped curate the dataset the model is based on.
You can read more about this process and the availability of this dataset in LAION’s blog post here. "
ggml,,,,mit,,17,,0,31178.88147,4,https://huggingface.co/ggerganov/ggml,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
modelscope-damo-text2video-pruned-weights,,OpenCLIP,,cc-by-nc-4.0,,29,,"2,630",7577.602741,,https://huggingface.co/kabachuha/modelscope-damo-text2video-pruned-weights,"https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis, but with fp16 (half precision) weights; Read all the info here https://huggingface.co/damo-vilab/modelscope-damo-text-to-video-synthesis/blob/main/README.md; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
QteaMix,,,,cc0-1.0,,47,,0,16148.48542,,https://huggingface.co/chenxluo/QteaMix,"QteaMix――一个融合的Q版模型; 通常你需要使用tag:chibi  当然，即使没有，也很可能得到Q版的结果。; ――――――――――――――――――――――――――――――――――――――――――――――――――――――; Omega版本是经过新素材重新训练融合完成的。这是QteaMix的最终一个版本，相比之前的版本是一个完全的升级，最为稳定不容易脱Q，且直出大图也没有压力。

上为不加chibi的测试; 下为原图展示




"
sd-vae-collection,,,,,,3,,0,6448.000746,,https://huggingface.co/Kefasu/sd-vae-collection,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
realsr,,,,,,2,,0,0.005044785,,https://huggingface.co/tumuyan/realsr,Available models; These are some models prepared for RealSR-NCNN-Android.You can download whichever directory you need.  ; https://github.com/nihui/waifu2x-ncnn-vulkan; https://github.com/nihui/realsr-ncnn-vulkan; https://upscale.wiki/wiki/Model_Database
autotrain-meme-classification-42897109437,Image Classification,PyTorch; Transformers,,,,9,Hrishikesh332/autotrain-data-meme-classification,39,348.0046075,13,https://huggingface.co/Hrishikesh332/autotrain-meme-classification-42897109437,"Dataset; The dataset consist of two label images:; Meme folder consist of 222 meme images and Not Meme folder consist of 108 non meme files. Meme file consist most of the images contaning the text on the picture and not meme consist of all type of images from sports to the text in various forms like document, image text to get the higher accuracy and understand about the meme in a most efficient way.; UseCase; Future Scope"
ShuimohuaAnime,Text-to-Image,Safetensors,,creativeml-openrail-m,,75,,0,0,,https://huggingface.co/Jemnite/ShuimohuaAnime,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; A stylized anime model. The goal of the mix is to affect the style of ink washed paintings (水墨画, lit. water rubbed art) popular in China, Japan, and Korea from the late 10th century until the modern day on anime illustrations. ; Download; Download"
AnimePastelDream,Text-to-Image,Diffusers,English,other,,5,,0,54855.68165,,https://huggingface.co/Lykon/AnimePastelDream,For info: https://civitai.com/models/23521/anime-pastel-dream; Inference API has been turned off for this model.
lora-llama-unhelpful-assistant,,,,mit,,3,,0,16.80324348,,https://huggingface.co/lxe/lora-llama-unhelpful-assistant,"Example LoRA adapter for llama-7b finetuned using https://github.com/lxe/simple-llama-finetuner; The training data was generated using ChatGPT, but the LoRA dataset is not as strictly filtered.; Examples:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
AnyLoRA,Text-to-Image,Diffusers,English,other,,23,,0,18534.40171,1,https://huggingface.co/Lykon/AnyLoRA,"For info: https://civitai.com/models/23900/anylora
For info on AAM: https://civitai.com/models/84586/aam-anylora-anime-mix-anime-screencap-style-model; Inference API has been turned off for this model."
anything-v4.0,Text-to-Image,Diffusers,English,creativeml-openrail-m,,14,,"3,188",65582.52632,7,https://huggingface.co/xyn-ai/anything-v4.0,"Fantasy.ai is the official and exclusive hosted AI generation platform that holds a commercial use license for Anything V4.0, you can use their service at https://Fantasy.ai/; Please report any unauthorized commercial use.; Try out my new model! - Pastel Mix || Stylized Anime Model. Thanks.; I also uploaded it in CivitAI! https://civitai.com/models/5414/pastel-mix-stylized-anime-model I'd appreciate the ratings, thank you!; Yes, it's a shameless plug."
pygmalion-6b-gptq-4bit,Text Generation,Transformers,,creativeml-openrail-m,,11,,168,4056.468766,,https://huggingface.co/OccamRazor/pygmalion-6b-gptq-4bit,
ChatYuan-large-v2,Text2Text Generation,PyTorch; Transformers,English; Chinese,,,155,,"86,850",3206.792231,10,https://huggingface.co/ClueAI/ChatYuan-large-v2,"ChatYuan-large-v2是一个支持中英双语的功能型对话语言大模型。v2使用了和 v1版本相同的技术方案，在指令微调、人类反馈强化学习、思维链等方面进行了优化。; ChatYuan-large-v2 is a functional dialogue language model that supports bilingual Chinese and English. 
ChatYuan-large-v2 uses the same technical solution as the v1 version, and has been optimized in terms of instruct-tuning, human feedback reinforcement learning and chain-of-thought.; 在线Demo ? | 
  使用API(large版) ? | 
 ? Github项目地址? |
  ?Colab在线试用 ? |
  ?文章介绍 ; ChatYuan-large-v2是ChatYuan系列中以轻量化实现高质量效果的模型之一，用户可以在消费级显卡(6G)、 PC甚至手机上进行推理（INT4 最低只需 400M ）。; 在chatyuan-large-v1的原有功能的基础上，我们给模型进行了如下优化："
alpaca7B-ES-lora,,,,,,3,,0,8.432075691,,https://huggingface.co/Xanthius/alpaca7B-ES-lora,"This Lora was trained using a combination of ElderScrolls books and the alpaca dataset. The ElderScrolls books were reformated to fit with the alpaca formatting so it should work for anthing designed for Alpaca although it has an understanding of the Elder Scrolls Universe; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
faster-whisper-base,Automatic Speech Recognition,,99 languages,mit,,6,,"9,466",147.654873,,https://huggingface.co/guillaumekln/faster-whisper-base,"This repository contains the conversion of openai/whisper-base to the CTranslate2 model format.; This model can be used in CTranslate2 or projects based on CTranslate2 such as faster-whisper.; The original model was converted with the following command:; Note that the model weights are saved in FP16. This type can be changed when the model is loaded using the compute_type option in CTranslate2.; For more information about the original model, see its model card."
faster-whisper-medium,Automatic Speech Recognition,,99 languages,mit,,12,,"25,157",1569.374834,,https://huggingface.co/guillaumekln/faster-whisper-medium,"This repository contains the conversion of openai/whisper-medium to the CTranslate2 model format.; This model can be used in CTranslate2 or projects based on CTranslate2 such as faster-whisper.; The original model was converted with the following command:; Note that the model weights are saved in FP16. This type can be changed when the model is loaded using the compute_type option in CTranslate2.; For more information about the original model, see its model card."
dolly-v1-6b,Text Generation,PyTorch; Transformers,English,cc-by-nc-4.0,https://arxiv.org/pdf/2212.10560.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2203.02155.pdf,306,tatsu-lab/alpaca,813,12496.18849,4,https://huggingface.co/databricks/dolly-v1-6b,"Please try Dolly v2 instead:; Databricks’ dolly-v1-6b, a large language model (blog post) 
trained on the Databricks machine learning platform, demonstrates that a 
two-years-old open source model can, when subjected to just 30 minutes of fine tuning on a focused corpus of 50k records 
(Stanford Alpaca), exhibit surprisingly high quality instruction following behavior not characteristic of the foundation 
model on which it is based.  We believe this finding is important because it demonstrates that the ability to create powerful 
artificial intelligence technologies is vastly more accessible than previously realized.; Databricks is committed to ensuring that every organization and individual benefits from the transformative power of artificial intelligence. The Dolly model family represents our first steps along this journey, and we’re excited to share this technology with the world.; Owner: Databricks, Inc.; dolly-v1-6b is a 6 billion parameter causal language model created by Databricks that is derived from 
EleutherAI’s GPT-J (released June 2021) and fine-tuned 
on a ~52K record instruction corpus (Stanford Alpaca) (CC-NC-BY-4.0)
consisting of question/answer pairs generated using the techniques outlined in the Self-Instruct paper. 
The original version of was Dolly was trained using deepspeed ZeRO 3 
on the Databricks Machine Learning Platform in just 30 minutes (1 epoch) using a single 
NDasrA100_v4 machine with 8x A100 40GB GPUs.
The most recent dolly-v1-6b checkpoint was trained for 10 epochs on the same hardware."
flan-t5-xl-instructiongen,Text2Text Generation,PyTorch; Transformers,,apache-2.0,,2,pszemraj/fleece2instructions,7,11953.33615,,https://huggingface.co/pszemraj/flan-t5-xl-instructiongen,"This model is a fine-tuned version of google/flan-t5-xl on the pszemraj/fleece2instructions dataset.
It achieves the following results on the evaluation set:; More information needed; Generate/recover instructions (assumes that there is just an instruction, not inputs as well) from arbitrary text.; Refer to pszemraj/fleece2instructions; The following hyperparameters were used during training:"
dm_nfnet_f0.dm_in1k,Image Classification,PyTorch; Safetensors; Timm,,apache-2.0,https://arxiv.org/pdf/2102.06171.pdf; https://arxiv.org/pdf/2101.08692.pdf,1,imagenet-1k,"25,068",572.0066769,,https://huggingface.co/timm/dm_nfnet_f0.dm_in1k,"A NFNet (Normalization Free Network) image classification model. Trained on ImageNet-1k by paper authors.; Normalization Free Networks are (pre-activation) ResNet-like models without any normalization layers. Instead of Batch Normalization or alternatives, they use Scaled Weight Standardization and specifically placed scalar gains in residual path and at non-linearities based on signal propagation analysis.; Explore the dataset and runtime metrics of this model in timm model results."
animix,Text-to-Image,,,creativeml-openrail-m,,89,embed/EasyNegative,0,2200.133848,1,https://huggingface.co/OedoSoldier/animix,"Using this model will result in clean, anatomically-correct images that accurately capture the essence of anime-style art, complete with stunning backgrounds.; Two models are provided: an 18 MB LoRA model and a full base model that merges LoRA with Anything V4.5. The full model is recommended for training your character model, and is particularly effective for training anime characters using this model.; Note: all the LoRA name used in those samples are my local name, you need to change them to your saved LoRA filename!; ; "
neuroscience-to-dev-bio-jsv4,Text2Text Generation,PyTorch; Transformers,,apache-2.0,,2,,11,1672.470544,,https://huggingface.co/levinlab/neuroscience-to-dev-bio-jsv4,"This model is a fine-tuned version of facebook/bart-large on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
guanaco-7B-leh,Text Generation,PyTorch; Transformers,English; Chinese; Japanese,gpl-3.0,,36,JosephusCheung/GuanacoDataset,43,13793.80224,2,https://huggingface.co/KBlueLeaf/guanaco-7B-leh,"This model is trained with modified alpaca-lora with lora + embed_tokens + lm_head be trained.; The dataset is from alpaca-lora (the cleaned version of alpaca) and guanaco.
With trained embed and head, the model perform better at Chinese and Japanese then original LLaMA, and with instruction based prompt. You can use this model more easily.; Since this model is trained by guanaco dataset, you can also use this as chatbot. just use this format:; Tips: I just removed the first line of original prompt to reduce token comsumption, plz consider remove it when you want to use this model; You can try this model with this colab.
And the example below is also generated by this colab notebook."
bart-large-code-instructiongen,Text2Text Generation,PyTorch; Safetensors; Transformers,English,cc-by-nc-4.0,,6,pszemraj/fleece2instructions-codealpaca,42,3341.585151,,https://huggingface.co/pszemraj/bart-large-code-instructiongen,"Use this text2text model to find out what LLM instructions might be able to generate an arbitary piece of code!; This model is a fine-tuned version of facebook/bart-large on the pszemraj/fleece2instructions-codealpaca dataset.
It achieves the following results on the evaluation set:; ? note: as the authors elected to release the original dataset under cc-by-nc, the license carries over to this model and cannot be used for commercial activity. ; Intended use: Research on domain adaptation and/or other improvements to LLMs by extending instruction:text data pairs.; Refer to the linked dataset card for pszemraj/fleece2instructions-codealpaca or the original dataset repo."
text2video-zero-controlnet-canny-arcane,Text-to-Video,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/2303.13439.pdf; https://arxiv.org/pdf/2208.12242.pdf; https://arxiv.org/pdf/2302.05543.pdf,22,,34,0.005984726,2,https://huggingface.co/PAIR/text2video-zero-controlnet-canny-arcane,"Text2Video-Zero is a zero-shot text to video generator. It can perform zero-shot text-to-video generation, Video Instruct Pix2Pix (instruction-guided video editing), 
text and pose conditional video generation, text and canny-edge conditional video generation, and 
text, canny-edge and dreambooth conditional video generation. For more information about this work, 
please have a look at our paper and our demo: 
Our code works with any StableDiffusion base model.; This model provides DreamBooth weights for the Arcane style to be used with edge guidance (using ControlNet) in text2video zero.; We converted the original weights into diffusers and made them usable for ControlNet with edge guidance using: https://github.com/lllyasviel/ControlNet/discussions/12.; Developed by: Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan and Humphrey Shi; Model type: Dreambooth text-to-image and text-to-video generation model with edge control for text2video zero"
LyCORIS-experiments,,,,creativeml-openrail-m,,44,,0,0.058984375,,https://huggingface.co/alea31415/LyCORIS-experiments,"General advice: Having a good dataset is more important than anything else; For 0324_all_aniscreen_tags, I accidentally tag all the character images with aniscreen.For the others, things are done correctly (anime screenshots tagged as aniscreen, fanart tagged as fanart).; For reference, this is what each character looks like; Anisphia
; Euphyllia
"
alpaca-lora-german-7b,,,German,cc-by-nc-4.0,,10,tatsu-lab/alpaca; yahma/alpaca-cleaned,0,16.81013802,,https://huggingface.co/ludwigstumpp/alpaca-lora-german-7b,"This repo contains a German low-rank adapter for LLaMA-7b fit on a German translation of the Stanford Alpaca dataset.; It doesn't contain the LLaMA-foundation model itself, which is why this adapter is not GPL-v3 licensed.
Instead, it has the Creative Commons NonCommercial (CC BY-NC 4.0) license, as the original Stanford Alpaca dataset, which states that ""models trained using the dataset should not be used outside of research purposes."" (source).; Important: Please note that if one wants to use these LoRA-weights in combination with the original LLaMA-foundation model (as shown in the code below) the license of the LLaMA model applies.; This model is trained based on the scripts provided in https://github.com/tloen/alpaca-lora. Special thanks for contributing to Open-Source @tloen and 
22 horas for the inspiration of trying on a translated Alpaca dataset. We all stand on the shoulders of giants (cc. Meta, OpenAI and Stanford).; As a comparison, below the results for the same tasks of the original Alpaca-LoRA model:"
alpaca-lora-7b-onnx-fp16-with-past,Text Generation,ONNX; Transformers,,other,,3,,22,27652.98271,,https://huggingface.co/nenkoru/alpaca-lora-7b-onnx-fp16-with-past,"This LoRA trained for 3 epochs.; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model."
KoAlpaca-30B-LoRA,,,Korean; English,mit,,3,,0,51.20219681,,https://huggingface.co/beomi/KoAlpaca-30B-LoRA,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
visual-openllm-chatglm-6b-rola,,,,apache-2.0,,8,tatsu-lab/alpaca; shibing624/alpaca-zh,0,29.40229027,,https://huggingface.co/visual-openllm/visual-openllm-chatglm-6b-rola,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
rut5_large_turbo_instructed,Text2Text Generation,PyTorch; Transformers,Russian,,,1,IlyaGusev/ru_turbo_alpaca,105,3024.448162,,https://huggingface.co/IlyaGusev/rut5_large_turbo_instructed,
merges,Text-to-Image,,English,other,,3,,16,0.002234001,,https://huggingface.co/yoinked/merges,"some merges and or ggml conversions; img: booru tags, use the /awoo/ models preferibly, as theyre the best; all non-ggml models are licensed under yodayno v2:; Unable to determine this model’s library. Check the
								docs 
.
							"
Dolly_GPT-J-6b,Text Generation,PyTorch; Transformers,,,,13,,52,12464.26267,1,https://huggingface.co/TehVenom/Dolly_GPT-J-6b,"This is a merge of the Dolly LoRA with the main GPT-J-6B model, allowing users to use Dolly without having to worry about PEFT dependencies.; This hopes to be as similar as Alpaca, but without requirimg LLaMA access.; The performance is good but not as good as the orginal Alpaca trained from a base model of LLaMa; This is mostly due to the LLaMa 7B model being pretrained on 1T tokens and GPT-J-6B being trained on 300-400M tokens."
deberta-v3-large-tasksource-nli,Zero-Shot Classification,PyTorch; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/2301.05948.pdf,12,glue; super_glue; anli; metaeval/babi_nli; sick; snli; scitail; hans; alisawuffles/WANLI; metaeval/recast; sileod/probability_words_nli; joey234/nan-nli; pietrolesci/nli_fever; pietrolesci/breaking_nli; pietrolesci/conj_nli; pietrolesci/fracas; pietrolesci/dialogue_nli; pietrolesci/mpe; pietrolesci/dnc; pietrolesci/gpt3_nli; pietrolesci/recast_white; pietrolesci/joci; martn-nguyen/contrast_nli; pietrolesci/robust_nli; pietrolesci/robust_nli_is_sd; pietrolesci/robust_nli_li_ts; pietrolesci/gen_debiased_nli; pietrolesci/add_one_rte; metaeval/imppres; pietrolesci/glue_diagnostics; hlgd; paws; quora; medical_questions_pairs; conll2003; Anthropic/hh-rlhf; Anthropic/model-written-evals; truthful_qa; nightingal3/fig-qa; tasksource/bigbench; bigbench; blimp; cos_e; cosmos_qa; dream; openbookqa; qasc; quartz; quail; head_qa; sciq; social_i_qa; wiki_hop; wiqa; piqa; hellaswag; pkavumba/balanced-copa; 12ml/e-CARE; art; tasksource/mmlu; winogrande; codah; ai2_arc; definite_pronoun_resolution; swag; math_qa; metaeval/utilitarianism; mteb/amazon_counterfactual; SetFit/insincere-questions; SetFit/toxic_conversations; turingbench/TuringBench; trec; tals/vitaminc; hope_edi; strombergnlp/rumoureval_2019; ethos; tweet_eval; discovery; pragmeval; silicone; lex_glue; papluca/language-identification; imdb; rotten_tomatoes; ag_news; yelp_review_full; financial_phrasebank; poem_sentiment; dbpedia_14; amazon_polarity; app_reviews; hate_speech18; sms_spam; humicroedit; snips_built_in_intents; banking77; hate_speech_offensive; yahoo_answers_topics; pacovaldez/stackoverflow-questions; zapsdcn/hyperpartisan_news; zapsdcn/sciie; zapsdcn/citation_intent; go_emotions; scicite; liar; relbert/lexical_relation_classification; metaeval/linguisticprobing; metaeval/crowdflower; metaeval/ethics; emo; google_wellformed_query; tweets_hate_speech_detection; has_part; wnut_17; ncbi_disease; acronym_identification; jnlpba; species_800; SpeedOfMagic/ontonotes_english; blog_authorship_corpus; launch/open_question_type; health_fact; commonsense_qa; mc_taco; ade_corpus_v2; prajjwal1/discosense; circa; YaHi/EffectiveFeedbackStudentWriting; Ericwang/promptSentiment; Ericwang/promptNLI; Ericwang/promptSpoke; Ericwang/promptProficiency; Ericwang/promptGrammar; Ericwang/promptCoherence; PiC/phrase_similarity; copenlu/scientific-exaggeration-detection; quarel; mwong/fever-evidence-related; numer_sense; dynabench/dynasent; raquiba/Sarcasm_News_Headline; sem_eval_2010_task_8; demo-org/auditor_review; medmcqa; aqua_rat; RuyuanWan/Dynasent_Disagreement; RuyuanWan/Politeness_Disagreement; RuyuanWan/SBIC_Disagreement; RuyuanWan/SChem_Disagreement; RuyuanWan/Dilemmas_Disagreement; lucasmccabe/logiqa; wiki_qa; metaeval/cycic_classification; metaeval/cycic_multiplechoice; metaeval/sts-companion; metaeval/commonsense_qa_2.0; metaeval/lingnli; metaeval/monotonicity-entailment; metaeval/arct; metaeval/scinli; metaeval/naturallogic; onestop_qa; demelin/moral_stories; corypaik/prost; aps/dynahate; metaeval/syntactic-augmentation-nli; metaeval/autotnli; lasha-nlp/CONDAQA; openai/webgpt_comparisons; Dahoas/synthetic-instruct-gptj-pairwise; metaeval/scruples; metaeval/wouldyourather; sileod/attempto-nli; metaeval/defeasible-nli; metaeval/help-nli; metaeval/nli-veridicality-transitivity; metaeval/natural-language-satisfiability; metaeval/lonli; metaeval/dadc-limit-nli; ColumbiaNLP/FLUTE; metaeval/strategy-qa; openai/summarize_from_feedback; metaeval/folio; metaeval/tomi-nli; metaeval/avicenna; stanfordnlp/SHP; GBaker/MedQA-USMLE-4-options-hf; sileod/wikimedqa; declare-lab/cicero; amydeng2000/CREAK; metaeval/mutual; inverse-scaling/NeQA; inverse-scaling/quote-repetition; inverse-scaling/redefine-math; metaeval/puzzte; metaeval/implicatures; race; metaeval/spartqa-yn; metaeval/spartqa-mchoice; metaeval/temporal-nli,"18,281",3574.666011,,https://huggingface.co/sileod/deberta-v3-large-tasksource-nli,"DeBERTa-v3-large fine-tuned with multi-task learning on 600 tasks of the tasksource collection
You can further fine-tune this model to use it for any classification or multiple-choice task.
This checkpoint has strong zero-shot validation performance on many tasks (e.g. 77% on WNLI).
The untuned model CLS embedding also has strong linear probing performance (90% on MNLI), due to the multitask training.; This is the shared model with the MNLI classifier on top. Its encoder was trained on many datasets including bigbench, Anthropic rlhf, anli... alongside many NLI and classification tasks with a SequenceClassification heads while using only one shared encoder.
Each task had a specific CLS embedding, which is dropped 10% of the time to facilitate model use without it. All multiple-choice model used the same classification layers. For classification tasks, models shared weights if their labels matched.
The number of examples per task was capped to 64k. The model was trained for 30k steps with a batch size of 384, and a peak learning rate of 2e-5.; tasksource training code: https://colab.research.google.com/drive/1iB4Oxl9_B5W3ZDzXoWJN-olUbqLBxgQS?usp=sharing; https://github.com/sileod/tasksource/ https://github.com/sileod/tasknet/ Training took 6 days on Nvidia A100 40GB GPU.; More details on this article: "
bertin-alpaca-lora-7b,Text Generation,PEFT,Spanish,openrail,,3,bertin-project/alpaca-spanish,4,16.80738411,,https://huggingface.co/bertin-project/bertin-alpaca-lora-7b,"This is a Spanish adapter generated by fine-tuning LLaMA-7B on a Spanish Alpaca dataset.; Until PEFT is fully supported in Hugginface's pipelines, for generation we can either consolidate the LoRA weights into the LLaMA model weights, or use the adapter's generate() method. Remember that the prompt still needs the English template:; The dataset is a translation to Spanish of alpaca_data_cleaned.json (a clean version of the Alpaca dataset made at Stanford) using OpenAI's gpt-3.5-turbo model. We translated using a full-sample prompt instead of per strings, which resulted in more coherent tuples of (instruction, input, output) and costed around $60.0.; This dataset cannot be used to create models that compete in any way with OpenAI.; To fine-tune the LLaMA model we used the code available on Alpaca Lora, which provides code to finetune the LLaMA model using PEFT from Hugging Face. We run finetuning for 3 epochs using sequence length of 512 with no gradient accumulation on a single A6000 with 48GB VRAM for 12 hours on top of LLaMA-7B."
deliberate-v2,,Diffusers,,,,4,,"2,432",0.002016563,,https://huggingface.co/stablediffusionapi/deliberate-v2,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
neurogen,,Diffusers,,other,,5,,13,4697.242527,,https://huggingface.co/Em1t/neurogen,"According to the tests, this model gives a very good detail of skin and textures. Great for close-up photorealistic portraits as well as various characters and models.; UPD 26.03.2023:
v1.1: The new version has taken a step forward in the direction of versatility.; The detail of the half body planes and full body planes has been improved (don't forget to use the Hires fix). In addition to photorealism, you can use this model for digital art and anime as well. Texture detailing has been improved, and new colors have been added.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Storytelling-LLaMa-LoRAs,,,,apache-2.0,,11,,0,174.6020194,,https://huggingface.co/GamerUntouch/Storytelling-LLaMa-LoRAs,"A collection of LoRAs for int8 LLaMA trained on an assortment of literature (approximately 16 MB) for 2 epochs.; UPDATE: 2024-04-18
Retrained using Transformers 4.28.1, on two epochs, with a small amount more data.; Notes for usage.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
gpt2-mi-reflector,Text Generation,PyTorch; Transformers,English,openrail,,2,,14,2950.546442,,https://huggingface.co/andrewbrown/gpt2-mi-reflector,"Given a prompt and a response, this GPT2 based transformer will generate a reflection. It was fine-tuned on prompt/response/reflection triplets so it should also be inferenced with the same input style. Examples are shown below.; The triplets this reflector was fine-tuned on focus on smoking-cessation. Given this, its best performance is when inferencing about smoking.; To properly inference model (as it was fine-tuned), use the following format:; Prompt: ...Question from bot...; Response: ...Response from user..."
Kurage_Kikoto__Mahiro_Kikoto_so-vits-svc-4.0v1,,,,,,7,,0,0.001796265,,https://huggingface.co/yasyune/Kurage_Kikoto__Mahiro_Kikoto_so-vits-svc-4.0v1,"黄琴海月さん?黄琴まひろさんのITAコ`パスを学させて作ったso-vits-svc4.0v1用のモデルデ`タです。; 必ず利用sをiんでから使用してください。
Please be sure to read the Terms of Use before use.; 利用s　Terms of Use　https://kikyohiroto1227.wixsite.com/kikoto-utau/ter%EF%BD%8Ds-of-service; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
gpt2-medium-wikiwriter-squadv11-portuguese,Text Generation,PyTorch; Transformers,Portuguese,mit,,6,squad; squad_v1_pt; wikipedia,62,1477.939918,,https://huggingface.co/egonrp/gpt2-medium-wikiwriter-squadv11-portuguese,This model is a fine-tuned version of egonrp/gpt2-wikiwriter-medium-portuguese on wiki_pt and squad_v1.1_pt datasets.; ** It's a chatbot experiment. ;); The model was trained in 12 hours on a NVIDIA RTX 3060 12GB.; More information needed; More information needed
aesthetic-controlnet,Text-to-Image,Diffusers,,openrail++,https://arxiv.org/pdf/2302.05543.pdf,62,,96,4.687893105,,https://huggingface.co/krea/aesthetic-controlnet,"This model can produce highly aesthetic results from an input image and a text prompt.; ControlNet is a method that can be used to condition diffusion models on arbitrary input features, such as image edges, segmentation maps, or human poses. ; Aesthetic ControlNet is a version of this technique that uses image features extracted using a Canny edge detector and guides a text-to-image diffusion model trained on a large aesthetic dataset.; The base diffusion model is a fine-tuned version of Stable Diffusion 2.1 trained at a resolution of 640x640, and the control network comes from thibaud/controlnet-sd21 by @thibaudz.; For more information about ControlNet, please have a look at this thread or at the original work by Lvmin Zhang and Maneesh Agrawala."
medical-QA-chatGPT2-v1,Text Generation,TensorFlow; Transformers,,apache-2.0,,2,,174,498.0024238,,https://huggingface.co/danielpark/medical-QA-chatGPT2-v1,
hll-test,Text-to-Image,,,creativeml-openrail-m,,9,,0,13086.72151,,https://huggingface.co/CluelessC/hll-test,"Unable to determine this model’s library. Check the
								docs 
.
							"
bloom-1b4-zh,Text Generation,PyTorch; Transformers,,,,2,,202,2869.315551,,https://huggingface.co/YeungNLP/bloom-1b4-zh,项目地址：LLMPruner：大语言模型裁剪工具; LLMPruner是一个大语言模型裁剪工具，通过对大语言模型的冗余词表进行裁剪，减少模型参数量，降低显存占用，提升训练速度，并且能够保留预训练中学习到的知识。; 本项目对Bloom进行词表裁剪，保留中文token和常用的英文token，词表由250880将至46145，缩减为原来的18.39%。裁剪得到的Bloom模型如下表：; 使用方法：
llm,,,,,,1,,0,0.001445313,,https://huggingface.co/jiaqian/llm,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
flan-alpaca-xxl,Text2Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2306.04757.pdf; https://arxiv.org/pdf/2210.11416.pdf,34,tatsu-lab/alpaca,"1,437",46154.16181,,https://huggingface.co/declare-lab/flan-alpaca-xxl,"? We developed Flacuna by fine-tuning Vicuna-13B on the Flan collection. Flacuna is better than Vicuna at problem-solving. Access the model here https://huggingface.co/declare-lab/flacuna-13b-v1.0.; ? Curious to know the performance of ? ? Flan-Alpaca on large-scale LLM evaluation benchmark, InstructEval? Read our paper https://arxiv.org/pdf/2306.04757.pdf. We evaluated more than 10 open-source instruction-tuned LLMs belonging to various LLM families including Pythia, LLaMA, T5, UL2, OPT, and Mosaic. Codes and datasets: https://github.com/declare-lab/instruct-eval; ? FLAN-T5 is also useful in text-to-audio generation. Find our work at https://github.com/declare-lab/tango if you are interested.; Our repository contains code for extending the Stanford Alpaca
synthetic instruction tuning to existing instruction-tuned models such as Flan-T5.
We have a live interactive demo thanks to Joao Gante!
We are also benchmarking many instruction-tuned models at declare-lab/flan-eval.
Our pretrained models are fully available on HuggingFace ? :; *recommended for better performance"
pygmalion-6b-4bit-128g,Text Generation,Transformers,English,creativeml-openrail-m,,39,,"1,303",4058.412851,,https://huggingface.co/mayaeary/pygmalion-6b-4bit-128g,GPTQ quantization of https://huggingface.co/PygmalionAI/pygmalion-6b/commit/b8344bb4eb76a437797ad3b19420a13922aaabe1; Using this repository: https://github.com/mayaeary/GPTQ-for-LLaMa/tree/gptj-v2; Command: ; Inference API has been turned off for this model.
gpt2-medium-squadv11-portuguese,Text Generation,PyTorch; Transformers,Portuguese,mit,,4,squad; squad_v1_pt,77,1477.939709,,https://huggingface.co/egonrp/gpt2-medium-squadv11-portuguese,This model is a fine-tuned version of gpt2-medium on squad_v1.1_pt dataset.; ** It's a chatbot experiment. ;); The model was trained in 12 hours on a NVIDIA RTX 3060 12GB.; More information needed; More information needed
bart-large-scientific-lay-summarisation,Summarization,PyTorch; Transformers,English,apache-2.0,,2,tomasg25/scientific_lay_summarisation,51,1672.465971,,https://huggingface.co/sambydlo/bart-large-scientific-lay-summarisation,"This model was trained using Amazon SageMaker and the new Hugging Face Deep Learning container.
For more information look at:; }"
civitai_mirror,Text-to-Image,Diffusers,,wtfpl,,28,,0,1.736957359,2,https://huggingface.co/mirroring/civitai_mirror,"This repo is mostly organized around the structure 
that's necessary to import the entire repo into an 
automatic1111 install. This is because models were originally
uploaded directly from such an install via google drive in 
a colab session.; Currently I'm uploading files via my new space over at 
https://hf.co/anonderpling/repo_uploader, however I expect 
to go back to a real file system and git uploads soon, 
hopefully paperspace can help with that.; My workflow for downloading files into paperspace gradient 
is to download the entire repo without pulling LFS 
files, do a sparse checkout, and then pull the files I 
want with LFS --include (slow) or aria2 (fast). This 
workflow should work with colab, too. Whether you use 
colab or paperspace, you'll probably need the latest 
version of git to use sparse-checkout --add.; Paperspace makes free notebooks public, and I'm not sure 
if that includes filesystem access or outputs; if someone 
can access that ssh key and you didnt remove the access it 
generates, you've given them thr ability to make changes 
to your repo! This means they could delete everything. 
If you're technically inclined, you can possibly use the 
paperspace secrets configuration to hide such information 
(I'm not sure how it works yet); Alternatively, you can add big files via 
https://hf.co/anonderpling/repo_uploader before your 
session (the renamed file part is pretty much added for 
uploading from HF urls, but also works for adding preview 
images), and manually upload the civitai.info files 
locally (these are just simple civitai api responses 
afaik)"
chinese-llama-lora-7b,,,Chinese,apache-2.0,,56,,0,835.7426095,,https://huggingface.co/ziqingyang/chinese-llama-lora-7b,"This repo contains the tokenizer, Chinese-LLaMA LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
gpt2-jokes,Text Generation,PyTorch; Transformers,,mit,,3,Fraser/short-jokes,409,513.3494305,,https://huggingface.co/AlekseyKorshuk/gpt2-jokes,"This model is a fine-tuned version of gpt2 on the Fraser/short-jokes dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
chinese-alpaca-lora-7b,,,Chinese,apache-2.0,,63,,0,858.742681,,https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b,"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
text2-video-zero,Text-to-Video,,,,https://arxiv.org/pdf/2303.13439.pdf,3,,0,0.047120094,1,https://huggingface.co/camenduru/text2-video-zero,"Paper: https://arxiv.org/abs/2303.13439; Unable to determine this model’s library. Check the
								docs 
.
							"
AnyLORA,Text-to-Image,Diffusers,English,creativeml-openrail-m,,28,,"4,890",0.007388115,,https://huggingface.co/emilianJR/AnyLORA,"AnyLORA is the diffuser that is highly compatible with Civitai's LORA weights.
Basically, it is just a converted version of [Lykon/AnyLORA][https://huggingface.co/Lykon/AnyLoRA/tree/main]
This model was created by Lykon from Civitai. All credits for him. Thanks for creating this wonderful model.; I made this model to ensure my future LoRA training is compatible with newer models, plus to get a model with a style neutral enough to get accurate styles with any style LoRA. Training on this model is much more effective conpared to NAI, so at the end you might want to adjust the weight or offset (I suspect that's because NAI is now much diluted in newer models). I usually find good results at 0.65 weigth that I later offset to 1.; This is good for inference (again, especially with styles) even if I made it mainly for training. It ended up being super good for generating pics and it's now my go-to anime model. It also eats very little vram.; The first version I'm uploading is a fp16-pruned with no baked vae, which is less than 2 GB, meaning you can get up to 6 epochs in the same batch on a colab.; Just make sure you use CLIP skip two and booru style tags when training. Remember to use a good VAE when generating, or images will look desaturated. I suggest WD Vae or FT MSE. Or you can use the baked vae version."
Chinese-Vicuna-lora-13b-belle-and-guanaco,,,Chinese; English,gpl-3.0,,51,BelleGroup/generated_train_0.5M_CN; JosephusCheung/GuanacoDataset,0,26.30249054,1,https://huggingface.co/Chinese-Vicuna/Chinese-Vicuna-lora-13b-belle-and-guanaco,"This is a Chinese instruction-tuning lora checkpoint based on llama-13B from this repo's work; You can use it like this: ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
so-vits-svc-4.0-models,,,,cc,,43,,0,0.001996536,,https://huggingface.co/marcoc2/so-vits-svc-4.0-models,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
medalpaca-7b,Text Generation,PyTorch; Transformers,English,cc,https://arxiv.org/pdf/2303.14070.pdf,23,,"2,208",27597.60132,5,https://huggingface.co/medalpaca/medalpaca-7b,"Model Description ; medalpaca-7b is a large language model specifically fine-tuned for medical domain tasks. 
It is based on LLaMA (Large Language Model Meta AI) and contains 7 billion parameters. 
The primary goal of this model is to improve question-answering and medical dialogue tasks.
Architecture; The training data for this project was sourced from various resources. 
Firstly, we used Anki flashcards to automatically generate questions, 
from the front of the cards and anwers from the back of the card. 
Secondly, we generated medical question-answer pairs from Wikidoc. 
We extracted paragraphs with relevant headings, and used Chat-GPT 3.5 
to generate questions from the headings and using the corresponding paragraphs 
as answers. This dataset is still under development and we believe 
that approximately 70% of these question answer pairs are factual correct. 
Thirdly, we used StackExchange to extract question-answer pairs, taking the 
top-rated question from five categories: Academia, Bioinformatics, Biology, 
Fitness, and Health. Additionally, we used a dataset from ChatDoctor 
consisting of 200,000 question-answer pairs, available at https://github.com/Kent0n-Li/ChatDoctor.; To evaluate the performance of the model on a specific dataset, you can use the Hugging Face Transformers library's built-in evaluation scripts. Please refer to the evaluation guide for more information.
Inference; You can use the model for inference tasks like question-answering and medical dialogues using the Hugging Face Transformers library. Here's an example of how to use the model for a question-answering task:"
tiny-random-GPTJForCausalLM,Text Generation,PyTorch; TensorFlow; Transformers,,,,1,,"1,457",2.491420364,,https://huggingface.co/hf-tiny-model-private/tiny-random-GPTJForCausalLM,No model card; New: Create and edit this model card directly on the website!
realistic-vision-v20,,,,,,7,,0,3942.401445,,https://huggingface.co/ckpt/realistic-vision-v20,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
zh-clip-vit-roberta-large-patch14,,PyTorch; Transformers,,apache-2.0,,4,,22,2580.865125,1,https://huggingface.co/thu-ml/zh-clip-vit-roberta-large-patch14,"; You can download ZH-CLIP model from ? thu-ml/zh-clip-vit-roberta-large-patch14. The model structure is shown below:; You can clone code from https://github.com/thu-ml/zh-clip; In addition, to compare the effectiveness of different methods, the inference methods of other Chinese CLIP models have been integrated. For the convenience of use, the inference code has also been made public, and please contact us if there is any infringement. The code only implements models at the same level as clip-vit-large-patch14, but it may be adapted for the use of more different versions of models in the future.; Usage in inference.py"
embeddings,,,,,,2,,0,0.329580078,,https://huggingface.co/lokCX/embeddings,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
QW5pbWVMaWdodA,,,,creativeml-openrail-m,,15,,0,4362.241483,,https://huggingface.co/TheAbyssYouSee/QW5pbWVMaWdodA,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
hubert-base-amharic-speech-emotion-recognition,Audio Classification,PyTorch; Transformers,Amharic,,,1,,18,380.0039614,,https://huggingface.co/quaja/hubert-base-amharic-speech-emotion-recognition,"model_name_or_path = ""quaja/hubert-base-amharic-speech-emotion-recognition""
config = AutoConfig.from_pretrained(model_name_or_path)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path)
sampling_rate = feature_extractor.sampling_rate
model = HubertForSpeechClassification.from_pretrained(model_name_or_path)"
cuteGirlMix4_v10,,,,,,26,,0,151.0014453,,https://huggingface.co/luxluna/cuteGirlMix4_v10,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ambientmix,Text-to-Image,,,creativeml-openrail-m,,95,embed/EasyNegative,0,2181.138496,1,https://huggingface.co/OedoSoldier/ambientmix,"Civitai: https://civitai.com/models/26622; This is a fine-tuned variant derived from Animix, trained with selected beautiful anime images. It gives you more delicate anime-like illustrations and a lesser AI feeling.; This image shows a comparison between three of my mix models: Aniflatmix, Animix, and Ambientmix (this model).; ; Highres fix is also recommended."
medical_chat-en-zh,Translation,PyTorch; Transformers,unk; unk,,,8,zhaozh/autotrain-data-chatdoctor-reft-en-zh,65,313.3270396,1,https://huggingface.co/zhaozh/medical_chat-en-zh,
opus-mt-en-ro,Translation,PyTorch; Safetensors; Transformers,Romanian; English,apache-2.0,,4,yhavinga/ccmatrix; Helsinki-NLP/tatoeba_mt,113,601.0865001,1,https://huggingface.co/BlackKakapo/opus-mt-en-ro,"

; This model is a finetune of the Helsinki-NLP/opus-mt-en-ro model, on ccmatrix(full) dataset."
llama-7b_stack-exchange_RM_peft-adapter-merged,Text Classification,PyTorch; Transformers,,,,2,,21,13537.30823,,https://huggingface.co/kashif/llama-7b_stack-exchange_RM_peft-adapter-merged,No model card; New: Create and edit this model card directly on the website!
span-marker-roberta-large-fewnerd-fine-super,Token Classification,PyTorch; Safetensors; SpanMarker,English,apache-2.0,,3,DFKI-SLT/few-nerd,291,2911.507344,,https://huggingface.co/tomaarsen/span-marker-roberta-large-fewnerd-fine-super,"This is a SpanMarker model that can be used for Named Entity Recognition. In particular, this SpanMarker model uses roberta-large as the underlying encoder. See train.py for the training script.; To use this model for inference, first install the span_marker library:; You can then run inference with this model like so:; See the SpanMarker repository for documentation and additional information on this library."
oasst-sft-3-pythia-12b-epoch-3.5,Text Generation,PyTorch; Transformers,,apache-2.0,,3,,44,24414.32027,,https://huggingface.co/andreaskoepf/oasst-sft-3-pythia-12b-epoch-3.5,"https://wandb.ai/open-assistant/supervised-finetuning/runs/7pz5n33h
exported checkpoint: 4500 steps; datasets:; pythia:"
360-Diffusion-LoRA-sd-v1-5,Text-to-Image,,English,creativeml-openrail-m,https://arxiv.org/pdf/2106.09685.pdf,24,,0,166.8476855,,https://huggingface.co/ProGamerGov/360-Diffusion-LoRA-sd-v1-5,"This LoRA model was finetuned on an extremely diverse dataset of 360° equirectangular projections with 2104 captioned training images, using the Stable Diffusion v1-5 model.; This model was finetuned with the trigger word qxj. If using the AUTOMATIC1111 WebUI, then you will have to append <lora:360Diffusion_v1:1> to the prompt as well in order to activate the model.; In order to improve usability of the model, various words and phrases were used to tag objects, scenes, style, and content. Note that these lists are based on the training data and do not include things added by the base model. These lists are also not comprehensive.; When rendering, it is recommended that you use either a 1:2 ratio or a perfect square. Rendering as a 1:1 square can help improve concept coherence (like the walls of a room).; Details can lose coherence at large sizes with txt2img, so it is recommended that you initially render a smaller version with at least one dimension near 512px, and then upscale it with img2img (with denoising set to 0.5) or a built in high-res fix feature."
bigscience-bloom-560m-ov,Text Generation,OpenVINO; Transformers,English,,,1,,25,3355.57374,,https://huggingface.co/helenai/bigscience-bloom-560m-ov,"This is the bigscience/bloom-560m model converted to OpenVINO, for accellerated inference.; An example of how to do inference on this model:"
eva02_large_patch14_448.mim_m38m_ft_in22k_in1k,Image Classification,PyTorch; Safetensors; Timm,,mit,https://arxiv.org/pdf/2303.11331.pdf; https://arxiv.org/pdf/2303.15389.pdf,1,imagenet-1k; imagenet-22k,"6,419",2498.56746,,https://huggingface.co/timm/eva02_large_patch14_448.mim_m38m_ft_in22k_in1k,"An EVA02 image classification model. Pretrained on Merged-38M (IN-22K, CC12M, CC3M, COCO (train), ADE20K (train), Object365, and OpenImages) with masked image modeling (using EVA-CLIP as a MIM teacher) and fine-tuned on ImageNet-22k then on ImageNet-1k by paper authors.; EVA-02 models are vision transformers with mean pooling, SwiGLU, Rotary Position Embeddings (ROPE), and extra LN in MLP (for Base & Large).; NOTE: timm checkpoints are float32 for consistency with other models. Original checkpoints are float16 or bfloat16 in some cases, see originals if that's preferred.; Explore the dataset and runtime metrics of this model in timm model results."
alpaca-13b,Text Generation,PyTorch; Transformers,,,,100,,786,53320.49517,11,https://huggingface.co/chavinlo/alpaca-13b,NO LORA
poca-SoccerTwos_500M,Reinforcement Learning,TensorBoard; ONNX; ML-Agents,,,,2,,2,1.776103516,,https://huggingface.co/Art-phys/poca-SoccerTwos_500M,"  This is a trained model of a poca agent playing SoccerTwos using the Unity ML-Agents Library.;   The Documentation: https://github.com/huggingface/ml-agents#get-started
  We wrote a complete tutorial to learn to train your first agent using ML-Agents and publish it to the Hub:;   You can watch your agent playing directly in your browser:."
span-marker-bert-base-fewnerd-fine-super,Token Classification,PyTorch; Safetensors; SpanMarker,English,apache-2.0,,4,DFKI-SLT/few-nerd,471,868.8718576,1,https://huggingface.co/tomaarsen/span-marker-bert-base-fewnerd-fine-super,"This is a SpanMarker model that can be used for Named Entity Recognition. In particular, this SpanMarker model uses bert-base-cased as the underlying encoder. ; To use this model for inference, first install the span_marker library:; You can then run inference with this model like so:; See the SpanMarker repository for documentation and additional information on this library."
hivt5-base-mpdocvqa,,PyTorch; Transformers,English,gpl-3.0,https://arxiv.org/pdf/2212.05935.pdf; https://arxiv.org/pdf/1905.13648.pdf,3,rubentito/mp-docvqa,322,1280.785755,,https://huggingface.co/rubentito/hivt5-base-mpdocvqa,"This is Hierarchical Visual T5 (Hi-VT5) base fine-tuned on Multipage DocVQA (MP-DocVQA) dataset.; This model was proposed in Hierarchical multimodal transformers for Multi-Page DocVQA.; Disclaimer: Due to some issues, this model does not achieve as good results as the reported ones in the paper. Please refer to the project Github for more details.; Hi-VT5 is not integrated into HF yet. Please download the code from Github repository and follow the instructions.; Average Normalized Levenshtein Similarity (ANLS)"
galpaca-6.7b,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,,18,tatsu-lab/alpaca,50,28100.89843,,https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-6.7b,"GALACTICA 6.7B fine-tuned on the Alpaca dataset.; The model card from the original Galactica repo can be found here, and the original paper here.; The dataset card for Alpaca can be found here, and the project homepage here.
  The Alpaca dataset was collected with a modified version of the Self-Instruct Framework, and was built using OpenAI's text-davinci-003 model. As such it is subject to OpenAI's terms of service.; The GALACTICA models are trained on a large-scale scientific corpus and are designed to perform scientific tasks.
The Alpaca dataset is a set of 52k instruct-response pairs designed to enhace the instruction following capabilites of pre-trained language models.; The GALACTICA model card specifies that the primary indended users of the GALACTICA models are researchers studying language models applied to the scientific domain, and it cautions against production use of GALACTICA without safeguards due to the potential for the model to produce inaccurate information.
The original GALACTICA models are available under a non-commercial CC BY-NC 4.0 license, and the GALPACA model is additionally subject to the OpenAI Terms of Service."
pegasus-subreddit-comments-summarizer,Summarization,PyTorch; Transformers,English,,,1,stevied67/autotrain-data-pegasus-subreddit-comments-summarizer,400,2343.239052,,https://huggingface.co/stevied67/pegasus-subreddit-comments-summarizer,You can use cURL to access this model:
Useless-X-mix,Text-to-Image,Diffusers,,creativeml-openrail-m,,1,,0,37591.05602,,https://huggingface.co/Inzamam567/Useless-X-mix,"Civitai: X-mix | Stable Diffusion Checkpoint | Civitai; X-mix is a merging model used to generate anime images. My English is not very good, so there may be some parts of this article that are unclear. ; V2.0 is a merged model based on V1.0. This model supports nsfw. ; ; "
Useless-SomethingV2_2,Text-to-Image,Diffusers; Safetensors,English,creativeml-openrail-m,,8,,"2,465",2181.13224,,https://huggingface.co/Inzamam567/Useless-SomethingV2_2,"
  Welcome to SomethingV2.2 - an improved anime latent diffusion model from SomethingV2
;   A lot of things are being discovered lately, such as a way to merge model using mbw automatically, offset noise to get much darker result, and even VAE tuning. This model is intended to use all of those features as the improvements, here's some improvements that have been made:; ; 
; Due to SD-Silicon's Terms of use. I must specify how the model was made"
randomCoserFaceCoser,,,,,,4,,0,75.60144531,,https://huggingface.co/freedom-4444/randomCoserFaceCoser,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
flan-gpt4all-xl,Text2Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2306.04757.pdf; https://arxiv.org/pdf/2210.11416.pdf,23,tatsu-lab/alpaca,590,11676.08258,1,https://huggingface.co/declare-lab/flan-gpt4all-xl,"? We developed Flacuna by fine-tuning Vicuna-13B on the Flan collection. Flacuna is better than Vicuna at problem-solving. Access the model here https://huggingface.co/declare-lab/flacuna-13b-v1.0.; ? Curious to know the performance of ? ? Flan-Alpaca on large-scale LLM evaluation benchmark, InstructEval? Read our paper https://arxiv.org/pdf/2306.04757.pdf. We evaluated more than 10 open-source instruction-tuned LLMs belonging to various LLM families including Pythia, LLaMA, T5, UL2, OPT, and Mosaic. Codes and datasets: https://github.com/declare-lab/instruct-eval; ? FLAN-T5 is also useful in text-to-audio generation. Find our work at https://github.com/declare-lab/tango if you are interested.; Our repository contains code for extending the Stanford Alpaca
synthetic instruction tuning to existing instruction-tuned models such as Flan-T5.
We have a live interactive demo thanks to Joao Gante!
We are also benchmarking many instruction-tuned models at declare-lab/flan-eval.
Our pretrained models are fully available on HuggingFace ? :; *recommended for better performance"
llama-65b-hf,Text Generation,PyTorch; Transformers,,apache-2.0,,4,,554,133653.0356,3,https://huggingface.co/pinkmanlove/llama-65b-hf,
llama-65b-hf,Text Generation,PyTorch; Transformers,,apache-2.0,,4,,554,133653.0356,3,https://huggingface.co/pinkmanlove/llama-65b-hf,
endlessMix,,Diffusers,Japanese,creativeml-openrail-m,,64,,0,0.016367187,,https://huggingface.co/teasan/endlessMix,; このモデルはDefactaをベ`スにしたA鹰蕞`ジモデルです。モデル作者である私が勉用と自分使用目的に制作しました。なお、VAEは入されていないのでe途DLしてください。  ; モデルをcloneもしくはDLした後、以下に格{してください。; ALL; 2dMix
gpt4-x-alpaca-native-13B-ggml,,,,unknown,,66,,0,97392.64169,,https://huggingface.co/Pi3141/gpt4-x-alpaca-native-13B-ggml,"All credits go to chavinlo for creating the dataset and training/fine-tuning the modelhttps://huggingface.co/chavinlo/gpt4-x-alpaca; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
t3,Conversational,PyTorch; Transformers,English,apache-2.0,,6,McGill-NLP/FaithDial,86,299.4279005,,https://huggingface.co/ayushutkarsh/t3,"T3 stands for Terribly Tiny Transformers that are an efficient way of creating tiny distilled (student) models for hallucination-free LLM models in parameter-constrained environment (edge devices).
The base model is a T3 adaptation of T5 model. The paradigm of T3 models can be extended to all types of models ( encoder only, decoder only & seq2seq)"
ruBert-base-reward,,PyTorch; Transformers,Russian,mit,,7,,5,716.7854797,,https://huggingface.co/Andrilko/ruBert-base-reward,"For training i use the next pair-ranking-loss
Model based on ruBert-base; Datasets have been translated with google-translate-api for reward training:; Firstly download custom model localy. You can do it manualy.; OR:; OR look at this manual "
guanaco-7b-leh-v2,Text Generation,PyTorch; Transformers,English; Chinese; Japanese,gpl-3.0,,31,JosephusCheung/GuanacoDataset; yahma/alpaca-cleaned,844,13804.04006,,https://huggingface.co/KBlueLeaf/guanaco-7b-leh-v2,"This model is trained with guanaco-lora with lora + embed_tokens + lm_head be trained.; The dataset is from alpaca-cleaned and guanaco.
With trained embed and head, the model perform better at Chinese and Japanese then original LLaMA, and with instruction based prompt. You can use this model more easily.; Since this model is trained by guanaco dataset, you can also use this as chatbot. just use this format:; Tips: I just removed the first line of original prompt to reduce token comsumption, plz consider remove it when you want to use this model; The main differences are:"
llama_13b_ru_turbo_alpaca_lora,Text2Text Generation,,Russian; English,,,16,IlyaGusev/ru_turbo_alpaca; yahma/alpaca-cleaned,0,52.99744846,,https://huggingface.co/IlyaGusev/llama_13b_ru_turbo_alpaca_lora,Important: You should probably use Saiga. It has regular updates and should be better in every task.; Based on LLaMA 13B.; Colab: link; Training code: link; Prompt template for v2:
bloom-6b4-zh,Text Generation,PyTorch; Transformers,,,,4,,65,13150.30678,,https://huggingface.co/YeungNLP/bloom-6b4-zh,项目地址：LLMPruner：大语言模型裁剪工具; LLMPruner是一个大语言模型裁剪工具，通过对大语言模型的冗余词表进行裁剪，减少模型参数量，降低显存占用，提升训练速度，并且能够保留预训练中学习到的知识。; 本项目对Bloom进行词表裁剪，保留中文token和常用的英文token，词表由250880将至46145，缩减为原来的18.39%。裁剪得到的Bloom模型如下表：; 使用方法：
ChilloutMix,,,,other,,20,,0,28876.80317,,https://huggingface.co/hanafuusen2001/ChilloutMix,"本Y料A中的模型不是我所u作，版w原作者所有（各模型版嘣 http://www.civitai.com 所示）。我上髦帘举Y料AH方便在Q抽取Y源，并非盈利。; The models in this folder are not made by me, and the copyright belongs to the original author (see http://www.civitai.com for details on the copyright of each model). I uploaded to this folder only for the convenience of extracting resources online, not for profit.; 本Y料A中所有模型下表。; All the models in this folder are detailed in the table below.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
pai-diffusion-artist-large-zh,Text-to-Image,Diffusers; PyTorch,,apache-2.0,,4,,266,1.509002304,3,https://huggingface.co/alibaba-pai/pai-diffusion-artist-large-zh,"我们开源了一个中文 Diffusion 模型，您可以直接输入中文提示词，我们为您呈现精美的艺术风格图片。本模型的默认分辨率是 512*512。; We release a Chinese diffusion model, which is able to generate high-quality artistic images according to the prompts you input. The default resolution of this model is 512*512.; 本模型支持 diffusers，可以参考以下范例代码：; This model supports diffusers. Please refer to the following code:; 使用上述模型需遵守AIGC模型开源特别条款。"
matcha-chart2text-pew,Visual Question Answering,PyTorch; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/2212.09662.pdf,5,,83,1161.236214,,https://huggingface.co/google/matcha-chart2text-pew,"; This model is the MatCha model, fine-tuned on Chart2text-pew dataset. This fine-tuned checkpoint might be better suited for chart summarization task.; The abstract of the paper states that: ; Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art visionlanguage models do not perform well on these data. We propose MATCHA (Math reasoning and Chart derendering pretraining) to enhance visual language models’ capabilities jointly modeling charts/plots and language data. Specifically we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MATCHA pretraining starting from Pix2Struct, a recently proposed imageto-text visual language model. On standard benchmarks such as PlotQA and ChartQA, MATCHA model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MATCHA pretraining transfers to domains such as screenshot, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MATCHA pretraining on broader visual language tasks.; You can use the convert_pix2struct_checkpoint_to_pytorch.py script as follows:"
matcha-base,Visual Question Answering,PyTorch; Transformers,5 languages,apache-2.0,https://arxiv.org/pdf/2212.09662.pdf,15,,895,1161.236165,,https://huggingface.co/google/matcha-base,"; This model is the base MatCha model. Can be used for fine-tuning purposes only.; The abstract of the paper states that: ; Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art visionlanguage models do not perform well on these data. We propose MATCHA (Math reasoning and Chart derendering pretraining) to enhance visual language models’ capabilities jointly modeling charts/plots and language data. Specifically we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MATCHA pretraining starting from Pix2Struct, a recently proposed imageto-text visual language model. On standard benchmarks such as PlotQA and ChartQA, MATCHA model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MATCHA pretraining transfers to domains such as screenshot, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MATCHA pretraining on broader visual language tasks.; You can use the convert_pix2struct_checkpoint_to_pytorch.py script as follows:"
llama13b-lora-cot,,,,mit,,5,QingyiSi/Alpaca-CoT,0,26.3018964,,https://huggingface.co/magicgh/llama13b-lora-cot,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
vicuna-13b-delta-v0,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,440,,839,26655.24643,,https://huggingface.co/lmsys/vicuna-13b-delta-v0,"NOTE: New version availablePlease check out a newer version of the weights here.If you still want to use this old version, please see the compatibility and difference between different versions here.; NOTE: This ""delta model"" cannot be used directly.Users have to apply it on top of the original LLaMA weights to get actual Vicuna weights. See instructions.; Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Command line interface: https://github.com/lm-sys/FastChat#vicuna-weights.APIs (OpenAI API, Huggingface API): https://github.com/lm-sys/FastChat/tree/main#api.  "
legacy-ggml-vicuna-13b-4bit,Conversational,,,,,209,,0,16670.72176,,https://huggingface.co/eachadea/legacy-ggml-vicuna-13b-4bit,"NOTE: Download new version (13B): https://huggingface.co/eachadea/ggml-vicuna-13b-1.1; NOTE: Download new version (7B): https://huggingface.co/eachadea/ggml-vicuna-7b-1.1; tags:; Unable to determine this model’s library. Check the
								docs 
.
							"
clearvaemirror,,,,creativeml-openrail-m,,3,,0,1005.00159,,https://huggingface.co/grugger/clearvaemirror,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
llama-30b,Text Generation,PyTorch; Safetensors; Transformers,,other,,34,,"14,124",133265.8001,2,https://huggingface.co/huggyllama/llama-30b,"This contains the weights for the LLaMA-30b model. This model is under a non-commercial license (see the LICENSE file).
You should only use this repository if you have been granted access to the model by filling out this form but either lost your copy of the weights or got some trouble converting them to the Transformers format."
audioldm-l-full,,Diffusers,,,https://arxiv.org/pdf/2301.12503.pdf,10,,"1,336",0.049290123,,https://huggingface.co/cvssp/audioldm-l-full,"AudioLDM is a latent text-to-audio diffusion model capable of generating realistic audio samples given any text input. It is available in the ? Diffusers library from v0.15.0 onwards.; AudioLDM was proposed in the paper AudioLDM: Text-to-Audio Generation with Latent Diffusion Models by Haohe Liu et al.; Inspired by Stable Diffusion, AudioLDM
is a text-to-audio latent diffusion model (LDM) that learns continuous audio representations from CLAP
latents. AudioLDM takes a text prompt as input and predicts the corresponding audio. It can generate text-conditional
sound effects, human speech and music.; This is the large version of the AudioLDM model, with twice the number of UNet channels and head channels as the small checkpoints. The four AudioLDM checkpoints are summarised in the table below:; Table 1: Summary of the AudioLDM checkpoints."
gpt4all-alpaca-oa-codealpaca-lora-7b,Text Generation,,English,mit,,40,Nebulous/gpt4all_pruned; sahil2801/CodeAlpaca-20k; yahma/alpaca-cleaned,0,34.13568836,,https://huggingface.co/jordiclive/gpt4all-alpaca-oa-codealpaca-lora-7b,"This repo contains a low-rank adapter for LLaMA-7b fit on ; This version of the weights was trained with the following hyperparameters:; The model was trained with flash attention and gradient checkpointing.; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:"
flan-sharegpt-xl,Text2Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2306.04757.pdf; https://arxiv.org/pdf/2210.11416.pdf,11,tatsu-lab/alpaca,"1,075",11676.85602,1,https://huggingface.co/declare-lab/flan-sharegpt-xl,"? We developed Flacuna by fine-tuning Vicuna-13B on the Flan collection. Flacuna is better than Vicuna at problem-solving. Access the model here https://huggingface.co/declare-lab/flacuna-13b-v1.0.; ? Curious to know the performance of ? ? Flan-Alpaca on large-scale LLM evaluation benchmark, InstructEval? Read our paper https://arxiv.org/pdf/2306.04757.pdf. We evaluated more than 10 open-source instruction-tuned LLMs belonging to various LLM families including Pythia, LLaMA, T5, UL2, OPT, and Mosaic. Codes and datasets: https://github.com/declare-lab/instruct-eval; ? FLAN-T5 is also useful in text-to-audio generation. Find our work at https://github.com/declare-lab/tango if you are interested.; Our repository contains code for extending the Stanford Alpaca
synthetic instruction tuning to existing instruction-tuned models such as Flan-T5.
We have a live interactive demo thanks to Joao Gante!
We are also benchmarking many instruction-tuned models at declare-lab/flan-eval.
Our pretrained models are fully available on HuggingFace ? :; *recommended for better performance"
chinese-Alpaca-lora-7b-ggml,,,Chinese,apache-2.0,,18,,0,4413.441859,,https://huggingface.co/Mabbs/chinese-Alpaca-lora-7b-ggml,"由chinese-alpaca-lora-7b与llama-7b-hf合并后并进行4bit量化后的模型。按照Chinese-LLaMA-Alpaca文档制作而成。   ; 一样的东西合那么多次干啥？不环保！?; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
so-vits-svc-4.0,,,,,,4,,0,0.001484375,,https://huggingface.co/RAYTRAC3R/so-vits-svc-4.0,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
sovits-emu-model,,,jp,gpl-3.0,,8,emu,0,0,,https://huggingface.co/MashiroSA/sovits-emu-model,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; 在使用模型时，您必须同意以下内容：; 该模型使用GPL-3.0许可开源，因而，你不可以使用该模型用于商业用途，不能用于盈利。; 此外：所有训练集归SEGA，Project Sekai，Pえむ的声优本人所属，我们与SEGA无往来，无利益关系。
推导模型是基于公开传播的音声资源所做的训练集生成的，但是，它仍然与原角色的音色有差异，因而不会对角色构成危害。
使用该模型，即您同意所有风险自行承当，模型仅供学术交流，不可用于非法目的。"
keywordextractor,Text2Text Generation,PyTorch; Transformers,,,,5,,227,1672.461874,2,https://huggingface.co/transformer3/keywordextractor,No model card; New: Create and edit this model card directly on the website!
nucleotide-transformer-500m-1000g,Fill-Mask,PyTorch; TensorFlow; Transformers,,cc-by-nc-sa-4.0,,2,,900,3973.156,2,https://huggingface.co/InstaDeepAI/nucleotide-transformer-500m-1000g,"The Nucleotide Transformers are a collection of foundational language models that were pre-trained on DNA sequences from whole-genomes. Compared to other approaches, our models do not only integrate information from single reference genomes, but leverage DNA sequences from over 3,200 diverse human genomes, as well as 850 genomes from a wide range of species, including model and non-model organisms. Through robust and extensive evaluation, we show that these large models provide extremely accurate molecular phenotype prediction compared to existing methods; Part of this collection is the nucleotide-transformer-500m-1000g, a 500M parameters transformer pre-trained on a collection of 3202 genetically diverse human genomes. The model is made available both in Tensorflow and Pytorch.; Developed by: InstaDeep, NVIDIA and TUM; Until its next release, the transformers library needs to be installed from source with the following command in order to use the models:; A small snippet of code is given here in order to retrieve both logits and embeddings from a dummy DNA sequence."
llama-7b-se-rm-peft,,Transformers,English,bigscience-openrail-m,,6,lvwerra/stack-exchange-paired,0,17.29435024,,https://huggingface.co/trl-lib/llama-7b-se-rm-peft,"; Adapter weights of a reward model based on LLaMa (see Meta's LLaMA release for the original LLaMA model). 
For more info check out the blog post and github example.; Llama-se-rm is a Llama-based model that has been first fine-tuned on the Stack Exchange dataset and used for reward modeling using a Stack Exchange Data. 
This dataset consists of questions and answers from various domains in Stack Exchange, such as programming, mathematics, physics, and more. 
The model is designed to generate human-like responses to questions in these domains. 
The model has been training to respond to prompts with the following template:; The Llama-se-rm model was trained for long form QA using Stack Exchange data wich is released under a CC-BY-SA 4.0, and covers topics such as programming, mathematics, and physics.
It is intended to demonstrate a Large Language Model's ability to follow a target behavior (in this case, generating answers to a question that would have been rated more highly on SE).
It is not intended to replace human expertise, and answers should be validated through the use of external sources.
Further research is also needed to attribute model generations to sources in the training data, especially in cases where the model may copy answers from the training data verbatim.  ; The Llama-se-rm model inherits limitations and biases from the Llama model and also those contained in the Stack Exchange dataset.
In particular, per the latest developer survey for Stack Overflow,
which constitutes a significant part of the StackExchange data,
most users who answered the survey identified themselves as White or European, men, between 25 and 34 years old, and based in the US (with a significant part of responders from India).
While this demographic information likely varies by topic, disparities between the data contributors and the direct and indirect users of the technology should inform developers in assessing what constitutes an appropriate use case."
PAIR-diffusion-sdv15-coco-finetune,Text-to-Image,,English,mit,https://arxiv.org/pdf/2303.17546.pdf,4,,0,8827.043896,1,https://huggingface.co/PAIR/PAIR-diffusion-sdv15-coco-finetune,"PAIR Diffusion models an image as composition of multiple objects and aim to control structural and appearance properties of these objects. It allows reference image-guided appearance manipulation and structure editing of an image at an object level. Describing object appearances using text can be challenging and ambiguous, PAIR Diffusion  enables a user to control the appearance of an object using images. Having fine-grained control over appearance and structure at object level can be beneficial for future works in video and 3D beside image editing, 
            where we need to have consistent appearance across time in case of video or across various viewing positions in case of 3D.; For more information please refer to GitHub, arXiv; ??; This model card applies the method to Stable Diffusion 1.5 (SDv1.5). We used COCO-Stuff dataset to finetune SDv1.5 using ControlNet due to its efficiency. The model can be tested using the publicly available demo here ; ??"
resnet18.a1_in1k,Image Classification,PyTorch; Safetensors; Timm,,apache-2.0,https://arxiv.org/pdf/2110.00476.pdf; https://arxiv.org/pdf/1512.03385.pdf,3,,"260,988",93.63966534,,https://huggingface.co/timm/resnet18.a1_in1k,A ResNet-B image classification model.; This model features:; Trained on ImageNet-1k in timm using recipe template described below.; Recipe details:; Explore the dataset and runtime metrics of this model in timm model results.
resnet50.a1_in1k,Image Classification,PyTorch; Safetensors; Timm,,apache-2.0,https://arxiv.org/pdf/2110.00476.pdf; https://arxiv.org/pdf/1512.03385.pdf,2,,"2,818,732",205.0396663,,https://huggingface.co/timm/resnet50.a1_in1k,A ResNet-B image classification model.; This model features:; Trained on ImageNet-1k in timm using recipe template described below.; Recipe details:; Explore the dataset and runtime metrics of this model in timm model results.
DialoGPT-medium-ZeroTwo,Conversational,PyTorch; Transformers,,,,1,,21,1477.898179,,https://huggingface.co/Stromello/DialoGPT-medium-ZeroTwo,#Zero Two DiabloGPT Model
LLaMA-7B-HF,Text Generation,PyTorch; Safetensors; Transformers,,other,,19,,"1,221",27609.43469,,https://huggingface.co/Neko-Institute-of-Science/LLaMA-7B-HF,"LLaMA converted to Transformers. This is under a special license, please see the LICENSE file for details.; https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md; Note: the torrent has outdated tokenizer_config.json and special_tokens_map.json. Replace them with the ones here.
For those who want to save HF's bandwith here's a magnet link:
magnet:?xt=urn:btih:8d634925911a03f787d9f68ac075a9b24281573a&dn=Safe-LLaMA-HF-v2%20(4-04-23)&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce"
Zero_Two,,,,,,1,,0,151.0014453,,https://huggingface.co/TrulyKatsu/Zero_Two,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
LLaMA-65B-HF,Text Generation,PyTorch; Safetensors; Transformers,,other,,5,,785,267307.4326,2,https://huggingface.co/Neko-Institute-of-Science/LLaMA-65B-HF,"LLaMA converted to Transformers. This is under a special license, please see the LICENSE file for details.; https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md; Note: the torrent has outdated tokenizer_config.json and special_tokens_map.json. Replace them with the ones here.
For those who want to save HF's bandwith here's a magnet link:
magnet:?xt=urn:btih:8d634925911a03f787d9f68ac075a9b24281573a&dn=Safe-LLaMA-HF-v2%20(4-04-23)&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce"
gpt_bigcode-santacoder,Text Generation,PyTorch; Safetensors; Transformers,code,openrail,,20,bigcode/the-stack,"12,193",4610.089153,,https://huggingface.co/bigcode/gpt_bigcode-santacoder,"; Play with the model on the SantaCoder Space Demo.; This is the same model as SantaCoder but it can be loaded with transformers >=4.28.1 to use the GPTBigCode architecture.
We refer the reader to the SantaCoder model page for full documentation about this model; There are two versions (branches) of the model:; The model was trained on GitHub code. As such it is not an instruction model and commands like ""Write a function that computes the square root."" do not work well.
You should phrase commands like they occur in source code such as comments (e.g. # the following function computes the sqrt) or write a function signature and docstring and let the model complete the function body."
bloom-7b-chunhua,Text Generation,PyTorch; Transformers,Chinese,apache-2.0,,7,BelleGroup/train_0.5M_CN,28,16588.26699,1,https://huggingface.co/wptoux/bloom-7b-chunhua,"春华是一个基于Bloom的古汉语问答模型，使用汉语古典文本数据库scripta-sinica进行微调，并结合BELLE项目生成的中文指令数据来让模型具有较好的对话能力。
scripta-sinica包含由殆知阁提供的10万卷，近13亿字古籍文本，约3.14 GB。可以作为对比的是，《四库全书》共收书3503种，79337卷，近230万页，约8亿字。
因此，模型具有丰富的中华古代知识，以及较强的古汉语理解能力。; 训练中参考了ChatML的结构来组织训练数据，因此，使用时建议也遵循这一规范。; 注：生成参数：temperature=0.4, top_k=0, top_p=0.9, max_new_tokens=1000, do_sample=True生成古诗的参数：temperature=0.7, top_k=0, repetition_penalty=1.2, max_new_tokens=1000, do_sample=True; 虽然模型经历了大量数据的训练，但其输出结果可能包含不准确或存在偏见的情况。; Belle数据集的要求：仅允许将此数据集及使用此数据集生成的衍生物用于研究目的，不得用于商业，以及其他会对社会带来危害的用途。 本数据集不代表任何一方的立场、利益或想法，无关任何团体的任何类型的主张。因使用本数据集带来的任何损害、纠纷，本项目不承担任何责任。"
gpt2-medium-4k-pile,Text Generation,PyTorch; Transformers,,,,1,,19,2041.027286,,https://huggingface.co/naxautify/gpt2-medium-4k-pile,No model card; New: Create and edit this model card directly on the website!
Based-mixes,,,English,cc-by-4.0,,41,,0,46612.48435,,https://huggingface.co/AnonymousM/Based-mixes,"A model I made anonymously at the start using a Vtuber finetuned model created anonymously along with WarriorMama777's AbyssOrangeMix2 mixes as a starting point for my mixes
https://huggingface.co/WarriorMama777/OrangeMixs#abyssorangemix2_hard-aom2h; the reasons being? Well I like Vtubers and both models had great NSFW capabilites along with me liking the very simple anime look the final epoch for HLL3 has. Luckily it seems like version 4 
of the users' model has been uploaded here so I will be providing the link 
https://huggingface.co/CluelessC/hll-test/tree/main; and this is the link as well for the 3rd revision of the model I was using up until Based65-final-mix
https://huggingface.co/grugger/chubas/resolve/main/models/mirrors/hll3vtubers-last-pruned.safetensors; Aside from that there's nothing really crazy I have to say about this model that I didn't say on the Civitai upload, which if you want to keep up with what I'm doing I have 
a Linktree sharing all my social media platforms
https://linktr.ee/anonymousm; You can use the model however you like just remember to credit me and refer to my CC-License
https://mega.nz/file/qExmQBQA#9eyI78TMEJu8V4c84UWitrlDAjyqxrxSVc1D5ktb87k"
chatglm-6b-csc-zh-lora,,PyTorch,Chinese,apache-2.0,,14,,0,14.70530685,,https://huggingface.co/shibing624/chatglm-6b-csc-zh-lora,ChatGLM中文纠错LoRA模型; chatglm-6b-csc-zh-lora evaluate test data：; The overall performance of chatglm-6b-csc-zh-lora on CSC test:; 在CSC测试集上生成结果纠错准确率高，由于是基于大模型，结果常常能带给人惊喜，不仅能纠错，还带有句子润色和改写功能。; 本项目开源在textgen项目：textgen，可支持ChatGLM原生模型和LoRA微调后的模型，通过如下命令调用：
koala,,,,other,,68,,0,67072.00272,,https://huggingface.co/young-geng/koala,"This repo contains the weights diff against the base LLaMA for the Koala model. Check out the following links to get started:; The model weights are intended for academic research only, subject to the
model License of LLaMA,
Terms of Use of the data generated by OpenAI,
and Privacy Practices of ShareGPT.
Any other usage of the model weights, including but not limited to commercial usage, is strictly prohibited.
Please contact us If you find any potential violations. Our training and inference code is released under the Apache License 2.0.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
pangu-13B,Text Generation,PyTorch; Transformers,Chinese,,,2,,52,26031.01106,,https://huggingface.co/sunzeyeah/pangu-13B,"Link to github: here; Pangu-α is proposed by a joint technical team headed by PCNL. It was first released in this repository  It is the first large-scale Chinese pre-trained language model with 200 billion parameters trained on 2048 Ascend processors using an automatic hybrid parallel training strategy. The whole training process is done on the “Peng Cheng Cloud Brain II” computing platform with the domestic deep learning framework called MindSpore. The PengCheng・PanGu-α pre-training model can support rich applications, has strong few-shot learning capabilities, and has outstanding performance in text generation tasks such as knowledge question and answer, knowledge retrieval, knowledge reasoning, and reading comprehension.; This repository contains PyTorch implementation of PanGu model with 13 billion parameters pretrained weights (FP32 precision). ; It is slightly different from the original pangu implementation to support the ChatGPT training pipeline in this github repo: sunzeyeah/RLHF."
long-t5-tglobal-base-sci-simplify,Summarization,PyTorch; ONNX; Safetensors; Transformers,English,apache-2.0,,3,pszemraj/scientific_lay_summarisation-plos-norm,199,2975.444102,3,https://huggingface.co/pszemraj/long-t5-tglobal-base-sci-simplify,"Exploring how well long-document models trained on ""lay summaries"" of scientific papers generalize. ; A lay summary is a summary of a research paper or scientific study that is written in plain language, without the use of technical jargon, and is designed to be easily understood by non-experts.; This model is a fine-tuned version of google/long-t5-tglobal-base on the pszemraj/scientific_lay_summarisation-plos-norm dataset for two epochs.; It's recommended to use this model with beam search decoding. If you are interested, you can also use the textsum util repo to have most of this abstracted for you:; Install with pip:"
chinese-alpaca-lora-13b,,,Chinese,apache-2.0,,55,,0,1116.902632,,https://huggingface.co/ziqingyang/chinese-alpaca-lora-13b,"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Command-nightly,,,,apache-2.0,,4,,0,3.162113838,,https://huggingface.co/Cohere/Command-nightly,"Cohere command-nightly tokenizer; This is the tokenizer for the Cohere command-nightly and command-light-nightly generate models.; You can load it with the tokenizer library like this:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
FollowYourPose_v1,,Diffusers,,,,13,,0,309.0014453,1,https://huggingface.co/YueMafighting/FollowYourPose_v1,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
LLaMA-7B-4bit-128g,Text Generation,Transformers,,,,7,,375,3985.691769,,https://huggingface.co/Neko-Institute-of-Science/LLaMA-7B-4bit-128g,
OPT-13B-Erebus-4bit-128g,Text Generation,Transformers,English,other,,13,,322,15433.11688,,https://huggingface.co/notstoic/OPT-13B-Erebus-4bit-128g,"Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; This is a 4-bit GPTQ quantization of OPT-13B-Erebus, original model:
https://huggingface.co/KoboldAI/OPT-13B-Erebus; Quantized with: https://github.com/0cc4m/GPTQ-for-LLaMa; OPT-13B is licensed under the OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved.; Inference API has been turned off for this model."
bloom-3b-zh,Text Generation,PyTorch; Transformers,Chinese,bigscience-bloom-rail-1.0,https://arxiv.org/pdf/2303.04715.pdf,21,,"1,095",12302.52459,1,https://huggingface.co/ckip-joint/bloom-3b-zh,"Version 1.0 / 10.Apr.2023; BLOOM-zh is a joint collaboration between CKIP lab at Acedemia Sinica (link), MediaTek Research (BY, 连结, link), and National Academy for Educational Research (link). This model is released for non-commerical research purposes only.; BLOOM-zh is a language model with enhanced Traditional Chinese capability. It is derived from BLOOMZ. 
BLOOM-zh is trained extendedly on large amount of Traditional Chinese text data.; This section provides information for people who work on model development.; For technical specifications, please refer to BLOOM."
Fauno-Italian-LLM-7B,,,Italian; English,gpl-3.0,,15,andreabac3/MedQuaAD-Italian-Fauno-Baize; andreabac3/StackOverflow-Italian-Fauno-Baize; andreabac3/Quora-Italian-Fauno-Baize; teelinsan/camoscio_cleaned,0,72.39312614,,https://huggingface.co/andreabac3/Fauno-Italian-LLM-7B,"; Get ready to meet Fauno -  the Italian language model crafted by the RSTLess Research Group from the Sapienza University of Rome.; The talented research team behind Fauno includes Andrea Bacciu, Dr. Giovanni Trappolini, Andrea Santilli, and Professor Fabrizio Silvestri.; Fauno represents a cutting-edge development in open-source Italian Large Language Modeling. It's trained on extensive Italian synthetic datasets, encompassing a wide range of fields such as medical data ?, technical content from Stack Overflow ?, Quora discussions ?, and Alpaca data ? translated into Italian.; Hence, our model is able to answer to your questions in Italian ?, fix your buggy code ? and understand a minimum of medical literature ?."
BeenYou_v2,,,,creativeml-openrail-m,,5,,0,4362.241483,,https://huggingface.co/jtamph/BeenYou_v2,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
4x-Ultrasharp,,,,,,61,,0,67.00144531,,https://huggingface.co/lokCX/4x-Ultrasharp,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
fantasticmix_real,,,,creativeml-openrail-m,,6,,0,10711.04161,,https://huggingface.co/jtamph/fantasticmix_real,"this is created by https://civitai.com/user/michin
all credits reserved to the creator; this is uploaded for personal use in colab; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ggml-LLaMa-65B-quantized,Text Generation,,20 languages,gpl-3.0,,29,,0,225792.0023,,https://huggingface.co/CRD716/ggml-LLaMa-65B-quantized,"LLaMa 65B converted to ggml via LLaMa.cpp, then quantized to 4bit.; Legacy is for llama.cpp setups older than https://github.com/ggerganov/llama.cpp/pull/1508, the regular is faster but does not work on old versions.; I recommend the following settings when running as a good starting point:
main.exe -m ggml-LLaMa-65B-q4_0.bin -n -1 -t 32 -c 2048 --temp 0.7 --repeat_penalty 1.2 --mirostat 2 --interactive-first --color; Be aware that LLaMa is a text generation model, not a conversational one, and as such you will have to prompt it differently than, for example, Vicuna or ChatGPT.; Unable to determine this model’s library. Check the
								docs 
.
							"
llama-7b-ggml,,,,,,2,,0,4311.041445,,https://huggingface.co/Drararara/llama-7b-ggml,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
VITS-finetuning-models,,,,apache-2.0,,1,,0,0.001472015,,https://huggingface.co/kira4424/VITS-finetuning-models,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
gpt2-open-instruct-v1,Text Generation,PyTorch; Safetensors; Transformers,English,mit,,9,hakurei/open-instruct-v1,782,2020.86752,1,https://huggingface.co/vicgalle/gpt2-open-instruct-v1,"The finetune used the Alpaca format for the prompts, so for better results you have to format the prompt using Alpaca's template. See the following examples below"
SadTalker,,,,mit,,20,,0,3427.54222,22,https://huggingface.co/vinthony/SadTalker,"This modelcard aims to be a base template for new models. It has been generated using this raw template.; The model used in https://github.com/Winfredy/SadTalker; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
vicuna-7b,Text Generation,PyTorch; Transformers,,apache-2.0,,17,,193,13804.03777,2,https://huggingface.co/Tribbiani/vicuna-7b,
ReVAnimated,,,,other,,31,,0,35901.44337,,https://huggingface.co/hanafuusen2001/ReVAnimated,"本Y料A中的模型不是我所u作，版w原作者所有（各模型版嘣 http://www.civitai.com 所示）。我上髦帘举Y料AH方便在Q抽取Y源，并非盈利。; The models in this folder are not made by me, and the copyright belongs to the original author (see http://www.civitai.com for details on the copyright of each model). I uploaded to this folder only for the convenience of extracting resources online, not for profit.; 本Y料A中所有模型下表。; All the models in this folder are detailed in the table below.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
nvjobmagiccircle,Text-to-Image,,English,mit,,3,,0,4363.693594,,https://huggingface.co/nvjob/nvjobmagiccircle,"This is a stable diffusion model capable of generating magic summoning circle images (for video games and more). It is trained on a large dataset of summoning magic circle images, allowing her to create new unique summoning magic circles based on the training data.; Game developers can use this model to create their own magic summoning circle, allowing them to create unique visuals in their games.; With this model, developers can save time and effort that would otherwise be spent manually creating each magic summoning circle.; Sampling method - Euler a
Sampling steps - 30-80
Resolution - 768x768
CFG Scale - 4-12
Or 
Sampling method - DPM++ 2M Karras
Sampling steps - 23-28
Resolution - 768x768
CFG Scale - 7-10; For higher resolution use Hires. fix or upscale."
Fauno-Italian-LLM-13B,,,Italian; English,gpl-3.0,,6,andreabac3/MedQuaAD-Italian-Fauno-Baize; andreabac3/StackOverflow-Italian-Fauno-Baize; andreabac3/Quora-Italian-Fauno-Baize; teelinsan/camoscio_cleaned,0,112.6266427,,https://huggingface.co/andreabac3/Fauno-Italian-LLM-13B,"; Get ready to meet Fauno -  the Italian language model crafted by the RSTLess Research Group from the Sapienza University of Rome.; The talented research team behind Fauno includes Andrea Bacciu, Dr. Giovanni Trappolini, Andrea Santilli, and Professor Fabrizio Silvestri.; Fauno represents a cutting-edge development in open-source Italian Large Language Modeling. It's trained on extensive Italian synthetic datasets, encompassing a wide range of fields such as medical data ?, technical content from Stack Overflow ?, Quora discussions ?, and Alpaca data ? translated into Italian.; Hence, our model is able to answer to your questions in Italian ?, fix your buggy code ? and understand a minimum of medical literature ?."
ExperimentalBiggerMerges,,,,agpl-3.0,,2,,0,4362.241656,,https://huggingface.co/mariaWitch/ExperimentalBiggerMerges,"This is where my experimental merges go. Expect broken models, UNETs, and models that produce weird artifacting to be common here. Models that are here may eventually make it into the other repo.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
chatglm-6b-belle-zh-lora,,PyTorch,Chinese,apache-2.0,,28,,0,14.70581993,,https://huggingface.co/shibing624/chatglm-6b-belle-zh-lora,ChatGLM中文问答LoRA模型; chatglm-6b-belle-zh-lora evaluate test data：; The overall performance of chatglm-6b-belle-zh-lora on QA test:; 在中文开放测试集中的表现优异，继承了两方面的优势：1）微调的底座是ChatGLM-6B模型，中文的表现优于LLAMA，2）微调使用的是高质量100万条中文ChatGPT指令Belle数据集，微调后的模型对话效果优于原始ChatGLM-6B。; 本项目开源在textgen项目：textgen，可支持ChatGLM模型，通过如下命令调用：
gawrgura,Text Generation,PyTorch; Transformers,English,,,2,,82,513.3443911,,https://huggingface.co/huggingtweets/gawrgura,"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report."
galpaca-30B-GPTQ-4bit-128g,Text Generation,Transformers,,cc-by-nc-4.0,,49,tatsu-lab/alpaca,78,32974.94779,,https://huggingface.co/TheBloke/galpaca-30B-GPTQ-4bit-128g,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is an attempt to create a GPTQ 4-bit version of Galpaca 30B.; I created these files on request. I have no previous experience of Galactica or Galpaca, and have not done much testing to confirm that the output is useful and usable.; You will need 18+ GB VRAM to load these models on a GPU."
MeinaHentai,Text-to-Image,Diffusers,English,creativeml-openrail-m,,17,,0,10905.60253,,https://huggingface.co/Meina/MeinaHentai,"MeinaHentai is the third addition to the Meina family of models, it is focused in hard nsfw and hentai.; For prompts and image samples, please check out: https://civitai.com/models/12606/meinahentai; I created a discord server where you can post images that you generated, discuss prompt and/or ask for help. https://discord.gg/meinaverse; If you like one of my models and want to support their updates:; I have a ko-fi: https://ko-fi.com/meina"
mt5-translation-ja_zh,Translation,PyTorch; Transformers,Chinese; Japanese,,,4,larryvrh/WikiMatrix-v1-Ja_Zh-filtered; larryvrh/CCMatrix-v1-Ja_Zh-filtered,256,5054.385915,,https://huggingface.co/larryvrh/mt5-translation-ja_zh,"This is the finetuned version of google/mt5-large for translating Japanese into Simplified Chinese.; Trained for 1 epoch on 5680000 samples from CCMatrix-v1-Ja_Zh-filtered and 690095 samples from WikiMatrix-v1-Ja_Zh-filtered.; This model is trained on sentence pairs with max seq_len=128, therefore you need to break document into small sentences before inference in order to avoid performance degradation.; Demo Usage"
so-vits-svc-4.0,,,,other,,5,,0,0.001467247,,https://huggingface.co/xgdhdh/so-vits-svc-4.0,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
MIMIC-Medical-Report-Generator,Text2Text Generation,PyTorch; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/1910.09700.pdf,2,Waleed-bin-Qamar/MIMIC-medical-report,71,1960.219066,,https://huggingface.co/IndianaUniversityDatasetsModels/MIMIC-Medical-Report-Generator,This modelcard aims to be a base template for new models. It has been generated using this raw template.; [More Information Needed]; [More Information Needed]; [More Information Needed]; [More Information Needed]
vicuna-13b,Text Generation,PyTorch; Transformers,,apache-2.0,,17,,186,26993.16421,,https://huggingface.co/Tribbiani/vicuna-13b,
chinese-llama-13b-4bit-128g,Text Generation,Transformers,,,,8,,20,7803.623379,,https://huggingface.co/mrtoy/chinese-llama-13b-4bit-128g,No model card; New: Create and edit this model card directly on the website!
cerebras-111M-ggml,,,,,,4,,0,1037.601445,,https://huggingface.co/concedo/cerebras-111M-ggml,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
conceptFemale_v10,,,,other,,1,,0,37.90146725,,https://huggingface.co/hiyukiya/conceptFemale_v10,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
controlnet-sd21-lineart-diffusers,,Diffusers,English,openrail++,,3,,314,729.0032172,,https://huggingface.co/thibaud/controlnet-sd21-lineart-diffusers,"Here's the first version of controlnet for stablediffusion 2.1 for diffusers
Trained on a subset of laion/laion-art; ; The model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.; Thanks ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
YourrrlModelSet,,,,other,,18,,0,0.001467247,,https://huggingface.co/Yourrrl/YourrrlModelSet,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
gpt2-large-rl-prompt-writing,Text Generation,PyTorch; Safetensors; Transformers,English,mit,,1,,20,6454.537981,2,https://huggingface.co/toloka/gpt2-large-rl-prompt-writing,"This is a GPT2-Large model fine-tuned for rewriting prompts for Stable Diffusion v1.5 through RLHF. The model takes an image description and writes a prompt resulting in aestheticcally pleasing images.; Write an image description and add a special SEP token: </s>, the model writes the prompt after it. Example:"
camel-5b-hf,Text Generation,PyTorch; Transformers,English,apache-2.0,,99,,"3,337",21589.28944,3,https://huggingface.co/Writer/camel-5b-hf,"Introducing Camel-5b, a state-of-the-art instruction-following large language model designed to deliver exceptional performance and versatility. Derived from the foundational architecture of Palmyra-Base, Camel-5b is specifically tailored to address the growing demand for advanced natural language processing and comprehension capabilities.; The Camel-5b model is meticulously trained on an extensive dataset of approximately 70,000 instruction-response records. These records are generated by our dedicated Writer Linguist team, who possess considerable expertise in language modeling and fine-tuning techniques. By leveraging their skills and knowledge, the Camel-5b model is primed to offer unparalleled proficiency in understanding and executing language-based instructions.; One of the key differentiators of Camel-5b lies in its ability to process complex instructions and generate accurate, contextually appropriate responses. This makes it an ideal choice for a wide range of applications, including virtual assistants, customer support, content generation, and more. Additionally, the model's comprehensive training enables it to adapt and perform well under varying conditions and contexts, further expanding its potential use cases.; Live demo => https://chatcamel.vercel.app/ ; We used the Baseten platform to package and serve Camel-5B at scale. Utilizing the open source Truss model packaging framework, users can create a customized environment using the simple instructions found on GitHub. This repo allows users to maintain full control over the inference and deployment paths to meet their specific requirements. 
We would like to thank the Baseten team for their contributions in deploying and hosting the model."
kollama-7b,Text Generation,PyTorch; Safetensors; Transformers,Korean; English,mit,,7,,291,27614.7487,,https://huggingface.co/beomi/kollama-7b,"? Note: this repo is under construction ?; ? - finish; ? - currently working on it; KoLLaMA (7B) trained on Korean/English/Code dataset with LLaMA Architecture via JAX, 
with the warm support from Google TPU Research Cloud program for providing part of the computation resources. ; Researcher developing the model"
LaMini-Flan-T5-248M,Text2Text Generation,PyTorch; Transformers,English,cc-by-nc-4.0,https://arxiv.org/pdf/2304.14402.pdf,16,,"3,970",993.2110756,1,https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M,"

; ; This model is one of our LaMini-LM model series in paper ""LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"". This model is a fine-tuned version of google/flan-t5-base on LaMini-instruction dataset that contains 2.58M samples for instruction fine-tuning. For more information about our dataset, please refer to our project repository.You can view other models of LaMini-LM series as follows. Models with ? are those with the best overall performance given their size/architecture, hence we recommend using them. More details can be seen in our paper. ; We recommend using the model to response to human instructions written in natural language. ; We now show you how to load and use our model using HuggingFace pipeline()."
vicuna-7B-GPTQ-4bit-128g,Text Generation,PyTorch; Transformers,,other,,14,,95,7967.243445,,https://huggingface.co/TheBloke/vicuna-7B-GPTQ-4bit-128g,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repository contains the Vicuna 7B model quantised using GPTQ-for-LLaMa.; The original Vicuna 7B repository contains deltas rather than weights. Rather than merging the deltas myself, I used the model files from https://huggingface.co/helloollel/vicuna-7b.; Two model files are provided. You don't need both, choose the one you prefer."
vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g,Text Generation,PyTorch; Transformers,,other,,43,,233,8081.724127,,https://huggingface.co/TheBloke/vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repository contains Aleksey Korshuk's Vicuna 7B model quantised using GPTQ-for-LLaMa.; Aleksey's model is an alternative to the original Vicuna 7B model. It uses the same ShareGPT source data, but without ""ethics filtering"".; Please read the two sections below carefully. You either need to update to the latest QwopQwop GPTQ-for-LLaMa code, or use vicuna-AlekseyKorshuk-7B-GPTQ-4bit-128g.no-act-order.pt."
gpt4-x-alpaca-rp-lora,,,,,,5,,0,0.001480598,,https://huggingface.co/ZeusLabs/gpt4-x-alpaca-rp-lora,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
BELLE-LLaMA-7B-2M-enc,Text2Text Generation,PyTorch,Chinese; English,gpl-3.0,,7,,0,13804.04352,,https://huggingface.co/BelleGroup/BELLE-LLaMA-7B-2M-enc,"Considering LLaMA's license constraints, the model is for research and learning only. 
Please strictly respect LLaMA's usage policy. We are not allowed to publish weights for LLaMA, of course, even finetuned, but there is no problem publishing the difference, a patch that we suggest to apply to the files. 
The encryption is a simple XOR between files, ensuring that only the people that have access to the original weights (from completely legal sources, of course) can transform them into finetuned weights. 
You can find the decrypt code on https://github.com/LianjiaTech/BELLE/tree/main/models .; If you find this model helpful, please like this model and star us on https://github.com/LianjiaTech/BELLE !; A new checkpoint trained with learning rate of 5e-6 is uploaded. 
In our evaluation, llama trained with smaller lr achieved better performance. ; BELLE-LLAMA-7B-2M-enc is based on LLAMA 7B and finetuned with 2M Chinese data combined with 50,000 pieces of English data from the open source Stanford-Alpaca, resulting in good Chinese instruction understanding and response generation capabilities. ; The code of Chinese data generation and other detailed information can be found in our Github project repository: https://github.com/LianjiaTech/BELLE."
BELLE-LLaMA-13B-2M-enc,Text2Text Generation,PyTorch,Chinese; English,gpl-3.0,,19,,0,26655.25011,,https://huggingface.co/BelleGroup/BELLE-LLaMA-13B-2M-enc,"Considering LLaMA's license constraints, the model is for research and learning only. 
Please strictly respect LLaMA's usage policy. We are not allowed to publish weights for LLaMA, of course, even finetuned, but there is no problem publishing the difference, a patch that we suggest to apply to the files. 
The encryption is a simple XOR between files, ensuring that only the people that have access to the original weights (from completely legal sources, of course) can transform them into finetuned weights. 
You can find the decrypt code on https://github.com/LianjiaTech/BELLE/tree/main/models .; If you find this model helpful, please like this model and star us on https://github.com/LianjiaTech/BELLE !; A new checkpoint trained with learning rate of 5e-6 is uploaded. 
In our evaluation, llama trained with smaller lr achieved better performance. ; BELLE-LLAMA-13B-2M-enc is based on LLAMA 13B and finetuned with 2M Chinese data combined with 50,000 pieces of English data from the open source Stanford-Alpaca, resulting in good Chinese instruction understanding and response generation capabilities. ; The code of Chinese data generation and other detailed information can be found in our Github project repository: https://github.com/LianjiaTech/BELLE."
phoenix-inst-chat-7b,Text Generation,PyTorch; Transformers,,apache-2.0,,43,,"2,597",16593.09406,1,https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b,Please see our LLMZoo project: https://github.com/FreedomIntelligence/LLMZoo.
Cerebras-GPT-111M-instruction,Text Generation,PyTorch; Transformers,English,,,2,,"2,057",489.3405161,3,https://huggingface.co/SebastianSchramm/Cerebras-GPT-111M-instruction,"The smallest of cerebras GPT models with only 111M parameters instruction fine-tuned.; Instruction fine-tuned cerebras-GPT-111M; The model has been evaluated with Huggingface's Open LLM leaderboard. Have a look at the leaderboard for more details: https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard
The performance of the instruction fine-tuned model does improve compared to the cerebras base model by about 5.7% (average score):; The model was fine-tuned with the following data: alpaca_gpt4_data (data generated by GPT-4 using Alpaca prompts for fine-tuning LLMs) and alpaca_data_cleaned.; Fine-tuning was performed with the promp template from stanford alpaca:"
Paella,,,,mit,https://arxiv.org/pdf/2211.07292.pdf,36,,0,7886.828477,,https://huggingface.co/dome272/Paella,"; We are releasing a new Paella model which builds on top of our initial paper https://arxiv.org/abs/2211.07292.
Paella is a text-to-image model that works in a quantized latent space and learns similarly to MUSE and Diffusion models.
Since the paper-release we worked intensively to bring Paella to a similar level as other 
state-of-the-art models. With this release we are coming a step closer to that goal. However, our main intention is not
to make the greatest text-to-image model out there (at least for now), it is to bring text-to-image models closer
to people outside the field on a technical basis. For example, many models have codebases with many thousand lines of 
code, that make it pretty hard for people to dive into the code and easily understand it. And that is the contribution
we are the most with Paella. The training and sampling code for Paella is minimalistic and can be understood in a few
minutes, making further extensions, quick tests, idea testing etc. extremely fast. For instance, the entire
sampling code can be written in just 12 lines of code.; Paella works in a quantized latent space, just like StableDiffusion etc., to reduce the computational power needed.
Images will be encoded to a smaller latent space and converted to visual tokens of shape h x w. Now during training,
these visual tokens will be noised, by replacing a random amount of tokens with other randomly selected tokens
from the codebook of the VQGAN. The noised image will be given to the model, along with a timestep and the conditional
information, which is text in our case. The model is tasked to predict the un-noised version of the tokens. 
And that's it. The model is optimized with the CrossEntropy loss between the original tokens and the predicted tokens.
The amount of noise added during the training is just a linear schedule, meaning that we uniformly sample a percentage 
between 0 and 100% and noise that amount of tokens.; Sampling is also extremely simple, we start with the entire image being random tokens. Then we feed the latent image, 
the timestep and the condition into the model and let it predict the final image. The models outputs a distribution
over every token, which we sample from with standard multinomial sampling.Since there are infinite possibilities for the result to look like, just doing a single step results in very basic 
shapes without any details. That is why we add noise to the image again and feed it back to the model. And we repeat
that process for a number of times with less noise being added every time and slowly get our final image.
You can see how images emerge here.
The following is the entire sampling code needed to generate images:; Model-Architecture: U-Net (Mix of....) 
Dataset: Laion-A, Laion Aesthetic > 6.0 
Training Steps: 1.3M 
Batch Size: 2048 
Resolution: 256 
VQGAN Compression: f4 
Condition: ByT5-XL (95%), CLIP-H Image Embedding (10%), CLIP-H Text Embedding (10%)
Optimizer: AdamW
Hardware: 128 A100 @ 80GB 
Training Time: ~3 weeks 
Learning Rate: 1e-4 
More details on the approach, training and sampling can be found in paper and on GitHub."
Base-GPT4-x-Alpaca-Roleplay-Lora,Text Generation,PyTorch; Transformers,,,,31,,100,14486.00399,1,https://huggingface.co/teknium/Base-GPT4-x-Alpaca-Roleplay-Lora,"This is a llama-13B based model. (sorry, I forgot to put it in the model name); Base Model: GPT4-x-Alpaca full fine tune by Chavinlo -> https://huggingface.co/chavinlo/gpt4-x-alpacaLORA fine tune using the Roleplay Instruct from GPT4 generated dataset -> https://github.com/teknium1/GPTeacher/tree/main/RoleplayLORA Adapter Only: https://huggingface.co/ZeusLabs/gpt4-x-alpaca-rp-lora/tree/main/gpt-rp-instruct-1 ; Merged LORA to the model.; FYI Latest HF Transformers generates BROKEN generations. 
Try this instead if your generations are terrible (first uninstall transformers): pip install git+https://github.com/huggingface/transformers@9eae4aa57650c1dbe1becd4e0979f6ad1e572ac0; Instruct it same way as alpaca / gpt4xalpaca:"
vicuna-7b-delta-v1.1,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,196,,"22,455",13804.04008,7,https://huggingface.co/lmsys/vicuna-7b-delta-v1.1,"NOTE: New version availablePlease check out a newer version of the weights here.If you still want to use this old version, please see the compatibility and difference between different versions here.; NOTE: This ""delta model"" cannot be used directly.Users have to apply it on top of the original LLaMA weights to get actual Vicuna weights. See instructions.; Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Command line interface: https://github.com/lm-sys/FastChat#vicuna-weights.APIs (OpenAI API, Huggingface API): https://github.com/lm-sys/FastChat/tree/main#api.  "
emotion-english,Text Classification,PyTorch; Safetensors; Transformers,English,mit,,3,,225,661.3417786,1,https://huggingface.co/jitesh/emotion-english,Here is how to use this model to get the emotion label of a given text:; The above code outputs the following line.
ChatBELLE-int4,Text2Text Generation,,Chinese; English,gpl-3.0,,23,,0,4454.404277,,https://huggingface.co/BelleGroup/ChatBELLE-int4,"4-bit quantized model using llama.cpp.
If you find this model helpful, please like this model and star us on https://github.com/LianjiaTech/BELLE !; ChatBELLE-int4 is based on 7B model and quantized to 4-bit.; The code of Chinese data generation and other detailed information can be found in our Github project repository: https://github.com/LianjiaTech/BELLE.; Should you accept our license and acknowledged the limitations, download the model by clicking Download.; You can use this model with ChatBELLE, a minimal, cross-platform LLM chat app powered by BELLE 
using quantized on-device offline models and Flutter UI, running on macOS (done), Windows, Android, 
iOS(see Known Issues) and more."
vicuna-13b-delta-v1.1,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,400,,"1,143",26655.24649,7,https://huggingface.co/lmsys/vicuna-13b-delta-v1.1,"NOTE: New version availablePlease check out a newer version of the weights here.If you still want to use this old version, please see the compatibility and difference between different versions here.; NOTE: This ""delta model"" cannot be used directly.Users have to apply it on top of the original LLaMA weights to get actual Vicuna weights. See instructions.; Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Command line interface: https://github.com/lm-sys/FastChat#vicuna-weights.APIs (OpenAI API, Huggingface API): https://github.com/lm-sys/FastChat/tree/main#api.  "
LoRA_download_2,,,,other,,2,,0,302.0033789,,https://huggingface.co/hanafuusen2001/LoRA_download_2,"本Y料A中的模型不是我所u作，版w原作者所有（各模型版嘣 http://www.civitai.com 所示）。我上髦帘举Y料AH方便在Q抽取Y源，并非盈利。; The models in this folder are not made by me, and the copyright belongs to the original author (see http://www.civitai.com for details on the copyright of each model). I uploaded to this folder only for the convenience of extracting resources online, not for profit.; 本Y料A中所有模型下表。; All the models in this folder are detailed in the table below.; 注 1：samdoesartsSamYang 模型的|l~椋sam yang"
orangechillmix,,,,,,9,,0,7885.39125,,https://huggingface.co/zylux/orangechillmix,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
RVC_datealive,Audio-to-Audio,,,,,8,,0,0.006005859,,https://huggingface.co/Wanlau/RVC_datealive,?; English | 中文简体; Using RVC (Retrieval-based-Voice-Conversion-WebUI); RVC; ?
all-mpnet-base-v2,,,,,,1,,0,0.020488281,1,https://huggingface.co/dmlls/all-mpnet-base-v2,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
my_merged_models,,,,,,1,,0,24432.64145,,https://huggingface.co/Alt31/my_merged_models,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
vicuna-13b-v1.1,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,94,,"438,271",26993.16631,,https://huggingface.co/lmsys/vicuna-13b-v1.1,"NOTE: New version availablePlease check out a newer version of the weights here.If you still want to use this old version, please see the compatibility and difference between different versions here.; Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Command line interface: https://github.com/lm-sys/FastChat#vicuna-weights.APIs (OpenAI API, Huggingface API): https://github.com/lm-sys/FastChat/tree/main#api.  ; Vicuna v1.1 is fine-tuned from LLaMA with supervised instruction fine-tuning.
The training data is around 70K conversations collected from ShareGPT.com.
See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper."
vicuna-7B-1.1-GPTQ-4bit-128g,Text Generation,Transformers,,other,,56,,"1,149",3985.699019,,https://huggingface.co/TheBloke/vicuna-7B-1.1-GPTQ-4bit-128g,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a 4-bit GPTQ version of the Vicuna 7B 1.1 model.; It was created by merging the deltas provided in the above repo with the original Llama 7B model, using the code provided on their Github page.; It was then quantized to 4bit using GPTQ-for-LLaMa."
Cygni-Mix,,,,,,14,,0,19630.08209,,https://huggingface.co/ray1001/Cygni-Mix,"Loli Worshipers Rise Up;  Alpha Cygni = T666_v2(=T666_RAWx0.6+BlossomMixx0.4)x0.5+BY_A_REx0.5;  Beta Cygni A = (T666_RAWx0.6+BY_A_REx0.4)x0.6+loliDv0.8.3_CTFx0.4
 Beta Cygni B = (T666_RAWx0.6+BY_A_REx0.4)x0.6+loliDv0.8.3_AOM3A1x0.4;  Gamma Cygni = T666_RAWx0.2+BY_A_REx0.4+loliDv0.8.3_AOM2x0.4; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
vicuna-13b-1.1,Text Generation,PyTorch; Transformers,,apache-2.0,,132,,"4,935",26993.16573,1,https://huggingface.co/eachadea/vicuna-13b-1.1,"delta v1.1 merge
; Model type:
Vicuna is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
Vicuna was trained between March 2023 and April 2023.; Organizations developing the model:
The Vicuna team with members from UC Berkeley, CMU, Stanford, and UC San Diego.; Paper or resources for more information:
https://vicuna.lmsys.org/"
ggml-vicuna-1.1-quantized,Conversational,Adapter Transformers,20 languages,gpl-3.0,,36,,0,194631.682,4,https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized,"Legacy is for llama.cpp setups older than https://github.com/ggerganov/llama.cpp/pull/1508, the regular is faster but does not work on old versions.; This is a ggml version of vicuna 7b and 13b. This is the censored model, a similar 1.0 uncensored 13b model can be found at https://huggingface.co/eachadea/ggml-vicuna-13b-1.1.; Inference API does not yet support adapter-transformers models for this pipeline type.
							"
ggml-vicuna-13b-1.1,Document Question Answering,,,apache-2.0,,345,,0,188170.2458,,https://huggingface.co/eachadea/ggml-vicuna-13b-1.1,"Better maintained files can be found here:
https://huggingface.co/CRD716/ggml-vicuna-1.1-quantized/; The PR #1405 brought breaking changes - none of the old models work with the latest build of llama.cpp.; Pre-PR #1405 files have been marked as old but remain accessible for those who need them (oobabooga, gpt4all-chat haven't been updated to support the new format as of May 14).; Additionally, q4_3 and q4_2 have been completely axed in favor of their 5-bit counterparts (q5_1 and q5_0, respectively).; New files inference up to 10% faster without any quality reduction."
alpaca-7b-wdiff,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,,35,,412,27597.31903,,https://huggingface.co/tatsu-lab/alpaca-7b-wdiff,"This repo hosts the weight diff for Stanford Alpaca-7B that can be used to reconstruct the original model weights when applied to Meta's LLaMA weights. ; To recover the original Alpaca-7B weights, follow these steps:; Once step 3 completes, you should have a directory with the recovered weights, from which you can load the model like the following"
gpt4all-j-lora,Text Generation,,English,apache-2.0,,16,nomic-ai/gpt4all-j-prompt-generations,0,7.364795494,,https://huggingface.co/nomic-ai/gpt4all-j-lora,"An Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions including word problems, multi-turn dialogue, code, poems, songs, and stories.; This model has been finetuned from GPT-J; GPT4All is made possible by our compute partner Paperspace.; Trained on a DGX cluster with 8 A100 80GB GPUs for ~12 hours. Using Deepspeed + Accelerate, we use a global batch size of 32 with a learning rate of 2e-5 using LoRA. More information can be found in the repo.; Results on common sense reasoning benchmarks"
control_v11p_sd15_lineart,,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,7,,"1,289",4415.609536,,https://huggingface.co/ControlNet-1-1-preview/control_v11p_sd15_lineart,"Controlnet v1.1 is the successor model of Controlnet v1.0
and was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ; "
llama-65b-hf,Text Generation,PyTorch; Transformers,,other,,3,,"29,004",74649.62054,,https://huggingface.co/Enoch/llama-65b-hf,"LLaMA-65B converted to work with Transformers/HuggingFace. This is under a special license, please see the LICENSE file for details.; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model."
vicuna_7B_vanilla_1.1,Text Generation,PyTorch; Transformers,,,,2,,203,13804.0376,,https://huggingface.co/Ejafa/vicuna_7B_vanilla_1.1,No model card; New: Create and edit this model card directly on the website!
Alpacino30b,Text Generation,PyTorch; Transformers,,other,,65,,229,108861.9831,3,https://huggingface.co/digitous/Alpacino30b,"-Alpac(ino) stands for Alpaca Integrated Narrative Optimization.; This model is a triple model merge of (Alpaca+(CoT+Storytelling)), resulting in a comprehensive boost in Alpaca's reasoning and story writing capabilities.
Alpaca was chosen as the backbone of this merge to ensure Alpaca's instruct format remains dominant. ; Hey! New GGML flavor! WOW!; Thanks to camelids for making Alpacino30B accessible to the cool GGML community.
https://huggingface.co/camelids/alpacino-33b-ggml-q5_1; -Legalese:"
dolly-japanese-gpt-1b,Conversational,PyTorch; Safetensors; Transformers,Japanese,mit,,27,kunishou/databricks-dolly-15k-ja; kunishou/oasst1-89k-ja,712,5387.294704,,https://huggingface.co/inu-ai/dolly-japanese-gpt-1b,"2023年5月7日; 「oasst1-89k-ja」デ`タセットを追加してシステムに辘筏蓼筏俊1024ト`クンまで会履sを保存できます。
前回のモデルで行った|疑甏黏握答率は今回のモデルで下がりました。「日本で一番冥ずは？」が91%から89%、「世界で一番高い山は？」が84%から73%に下がりました。（は分けた方が良かったのか、それともoasst1の|が良くないとか）; 2023年4月13日; 「japanese-gpt-1b」モデルを「databricks-dolly-15k-ja」デ`タセットでRLHF (人gのフィ`ドバックからの化学)しました。; 1.3Bパラメ`タの日本ZGPT-2モデルを使用した型のAIです。VRAM 7GB または RAM 7GB が必要で、}なく幼鳏工毪人激铯欷蓼埂"
Aurora,Text-to-Image,Diffusers,English,creativeml-openrail-m,,68,,0,0.02178566,2,https://huggingface.co/SweetLuna/Aurora,; ; ; ; 
gpt4-alpaca-lora-30b,Text2Text Generation,,English,apache-2.0,,61,,0,205.0027938,,https://huggingface.co/chansung/gpt4-alpaca-lora-30b,"This repository comes with LoRA checkpoint to make LLaMA into a chatbot like language model. The checkpoint is the output of instruction following fine-tuning process with the following settings on 8xA100(40G) DGX system.; You can find how the training went from W&B report here.; Unable to determine this model’s library. Check the
								docs 
.
							"
chatglm-6b-int8,,PyTorch; Transformers,Chinese; English,,,59,,"11,725",6873.88268,13,https://huggingface.co/THUDM/chatglm-6b-int8,"
    ? Join our Slack and WeChat
; ChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 ChatGLM 相同的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。; ChatGLM-6B-INT8 是 ChatGLM-6B 量化后的模型权重。具体的，ChatGLM-6B-INT8 对 ChatGLM-6B 中的 28 个 GLM Block 进行了 INT8 量化，没有对 Embedding 和 LM Head 进行量化。量化后的模型理论上 8G 显存（使用 CPU 即内存）即可推理，具有在嵌入式设备（如树莓派）上运行的可能。; 在 CPU 上运行时，会根据硬件自动编译 CPU Kernel ，请确保已安装 GCC 和 OpenMP （Linux一般已安装，对于Windows则需手动安装），以获得最佳并行计算能力。; 可以通过如下代码调用 ChatGLM-6B 模型来生成对话："
fred_t5_ru_turbo_alpaca,Text Generation,PyTorch; Transformers,Russian,,,7,IlyaGusev/ru_turbo_alpaca,"1,934",3570.169319,1,https://huggingface.co/IlyaGusev/fred_t5_ru_turbo_alpaca,Colab: link
LaMini-Neo-125M,Text Generation,PyTorch; Transformers,English,cc-by-nc-4.0,https://arxiv.org/pdf/2304.14402.pdf,12,,821,554.3448519,,https://huggingface.co/MBZUAI/LaMini-Neo-125M,"

; ; This model is one of our LaMini-LM model series in paper ""LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"". 
This model is a fine-tuned version of EleutherAI/gpt-neo-125m on LaMini-instruction dataset that contains 2.58M samples for instruction fine-tuning. For more information about our dataset, please refer to our project repository.You can view other models of LaMini-LM series as follows. Models with ? are those with the best overall performance given their size/architecture, hence we recommend using them. More details can be seen in our paper. ; We recommend using the model to respond to human instructions written in natural language. 
Since this decoder-only model is fine-tuned with wrapper text, we suggest using the same wrapper text to achieve the best performance.
See the example on the right or the code below.; We now show you how to load and use our model using HuggingFace pipeline(). "
NewLora,,,,,,1,,0,968.7014453,,https://huggingface.co/MiruP/NewLora,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
GPT4-X-Alpaca-30B-4bit,Text Generation,PyTorch; Transformers,,,,159,,462,104552.7831,1,https://huggingface.co/MetaIX/GPT4-X-Alpaca-30B-4bit,Information; This was made using Chansung's GPT4-Alpaca Lora; Update 05.26.2023; Updated the ggml quantizations to be compatible with the latest version of llamacpp (again).; What's included
control_v11p_sd15_lineart,Image-to-Image,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,16,,"90,467",4415.618911,50,https://huggingface.co/lllyasviel/control_v11p_sd15_lineart,"Controlnet v1.1 was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ; "
control_v11p_sd15_inpaint,Image-to-Image,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,34,,"86,522",4415.677739,4,https://huggingface.co/lllyasviel/control_v11p_sd15_inpaint,"Controlnet v1.1 was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ; "
control_v11e_sd15_ip2p,Image-to-Image,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,11,,"6,818",4415.676291,46,https://huggingface.co/lllyasviel/control_v11e_sd15_ip2p,"Controlnet v1.1 was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ; "
Alpacino-13b-ggml,,,,,,9,,0,8335.361774,,https://huggingface.co/verymuchawful/Alpacino-13b-ggml,GGML conversion of https://huggingface.co/digitous/Alpacino13b using https://github.com/ggerganov/llama.cpp/pull/896. (Edited to write the model with ftype 2 so it won't be incorrectly identified as 4 - mostly q4_1 some f16.); GPTQ(cuda) quantization available here: https://huggingface.co/gozfarb/alpacino-13b-4bit-128g; Inference API has been turned off for this model.
so-vits-svc-rvc-models,,,English; Japanese,,,7,,0,0.00305851,,https://huggingface.co/1358Adrian/so-vits-svc-rvc-models,"Caitlin (Pokémon Masters EX; English VA); 3.0-32k, 4.0 & RVC v2 model
Sothis (Fire Emblem; English VA); 3.0-32k, 4.0 & RVC v2 model
Barbara Pegg (Genshin Impact; English VA); 3.0-32k, 4.0 & RVC v2 model
Sucrose (Genshin Impact; English VA); 3.0-32k, 4.0 & RVC v2 (incl. feature index) model
Shōgun Raiden/Ei Raiden (Genshin Impact; English VA); 4.0 & RVC v2 model
Ai Kizuna (virtual YouTuber); 4.0 & RVC v2 (incl. feature index) model
Flayn (Fire Emblem; English VA); 4.1 & RVC v2 model 
Eleanor Forte (Synthesizer V Studio; Standard Lite); RVC v2 model only
Polar (virtual singer); RVC v2 (incl. feature index) model only
Rhea/Seiros (Fire Emblem; English VA); RVC v2 (incl. feature index) model only
Yukari Yuzuki (exVOICE); RVC v2 (incl. feature index) model only
Rise Kujikawa (Persona 4; PlayStation 2; English VA); RVC v2 (incl. feature index) model only; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
leet-coding-alpaca-lora-7b,,,,apache-2.0,,3,sahil2801/CodeAlpaca-20k; ehartford/leet10k-alpaca; yahma/alpaca-cleaned,0,16.80298794,,https://huggingface.co/vihangd/leet-coding-alpaca-lora-7b,This repository contains a LoRA checkpoint based on the Alpaca-LoRA implementation to perform coding tasks better. The provided checkpoint can be used with the generate.py script from the Alpaca-LoRA repository to generate text or perform other NLP tasks.; Clone the Alpaca-LoRA repository:;  git clone https://github.com/tloen/alpaca-lora.git; Install the required packages:;  pip install -r requirements.txt
ForYou-Photo,,,,other,,1,,0,12970.07312,,https://huggingface.co/GalaxyTimeMachine/ForYou-Photo,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
llama-lite-134m,Text Generation,PyTorch; Transformers,,apache-2.0,,6,,218,620.7916959,,https://huggingface.co/skeskinen/llama-lite-134m,"134m parameter, 768 width version of Llama for creating sentence embeddings; https://github.com/skeskinen/llama-lite; Currently trained on just the alpaca dataset, but might contain better weights later."
chatglm6b-dddd,Text2Text Generation,PyTorch; Transformers,Chinese,,,17,,710,14062.33147,,https://huggingface.co/yuanzhoulvpi/chatglm6b-dddd,更多信息，可以查看这个链接：https://github.com/yuanzhoulvpi2017/zero_nlp/tree/main/simple_thu_chatglm6b
dlite-v2-355m,Text Generation,PyTorch; Transformers,English,apache-2.0,,4,aisquared/databricks-dolly-15k,450,738.3544615,,https://huggingface.co/aisquared/dlite-v2-355m,"AI Squared's dlite-v2-355m is a large language 
model which is derived from OpenAI's medium GPT-2 model and fine-tuned on a single GPU on a corpus of 15k records
(Databricks' ""Dolly 15k"" Dataset) to help it exhibit chat-based capabilities.; Just like Databricks' Dolly V2 models,
dlite-v2-355m (and all other members of the dlite-v2 family) is licensed for both research and commercial use. We are extremely grateful 
for the work that Databricks has done to create the databricks-dolly-15k dataset, for without it we would not be able to create and release this
model under such an open and permissive license.; While dlite-v2-355m is not a state-of-the-art model, we believe that the level of interactivity that can be achieved on such a small model that is trained so cheaply
is important to showcase, as it continues to demonstrate that creating powerful AI capabilities may be much more accessible than previously thought. ; dlite-v2-355m is not a state-of-the-art language model. dlite-v2-355m is an experimental technology, and as with any experimental technology, 
AI Squared urges potential users of this technology to test its capabilities thoroughly before usage.
Furthermore, the model can sometimes exhibit undesired behaviors. Some of these behaviors include,
but are not limited to: factual inaccuracies, biases, offensive responses, toxicity, and hallucinations.
Just as with any other LLM, we advise users of this technology to exercise good judgment when applying this technology.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers and accelerate libraries installed.
From your terminal, run:"
dlite-v2-774m,Text Generation,PyTorch; Transformers,English,apache-2.0,,7,aisquared/databricks-dolly-15k,676,1631.515525,2,https://huggingface.co/aisquared/dlite-v2-774m,"AI Squared's dlite-v2-774m is a large language 
model which is derived from OpenAI's large GPT-2 model and fine-tuned on a corpus of 15k records
(Databricks' ""Dolly 15k"" Dataset) to help it exhibit chat-based capabilities.; Just like Databricks' Dolly V2 models,
dlite-v2-774m (and all other members of the dlite-v2 family) is licensed for both research and commercial use. We are extremely grateful 
for the work that Databricks has done to create the databricks-dolly-15k dataset, for without it we would not be able to create and release this
model under such an open and permissive license.; While dlite-v2-774m is not a state-of-the-art model, we believe that the level of interactivity that can be achieved on such a small model that is trained so cheaply
is important to showcase, as it continues to demonstrate that creating powerful AI capabilities may be much more accessible than previously thought. ; dlite-v2-774m is not a state-of-the-art language model. dlite-v2-774m is an experimental technology, and as with any experimental technology, 
AI Squared urges potential users of this technology to test its capabilities thoroughly before usage.
Furthermore, the model can sometimes exhibit undesired behaviors. Some of these behaviors include,
but are not limited to: factual inaccuracies, biases, offensive responses, toxicity, and hallucinations.
Just as with any other LLM, we advise users of this technology to exercise good judgment when applying this technology.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers and accelerate libraries installed.
From your terminal, run:"
llama-13b-hf-transformers-4.29,Text Generation,PyTorch; Transformers,,other,,17,,"1,353",26995.01219,,https://huggingface.co/elinas/llama-13b-hf-transformers-4.29,"Original weights converted with the latest transformers version using the LlamaTokenizerFast implementation. ; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model."
druggpt,Text Generation,PyTorch; Transformers,,gpl-3.0,,7,,605,525.8669296,1,https://huggingface.co/liyuesen/druggpt,"A generative drug design model based on GPT2.
; DrugGPT is a generative pharmaceutical strategy based on GPT structure, which aims to bring innovation to drug design by using natural language processing technique. ; This project applies the GPT model to the exploration of chemical space to discover new molecules with potential binding abilities for specific proteins. ; DrugGPT provides a fast and efficient method for the generation of drug candidate molecules by training on up to 1.8 million protein-ligand binding data.; Use drug_generator.py"
dlite-v2-1_5b,Text Generation,PyTorch; Transformers,English,apache-2.0,,9,aisquared/databricks-dolly-15k,844,3249.435542,2,https://huggingface.co/aisquared/dlite-v2-1_5b,"AI Squared's dlite-v2-1.5b is a large language 
model which is derived from OpenAI's large GPT-2 model and fine-tuned on a corpus of 15k records
(Databricks' ""Dolly 15k"" Dataset) to help it exhibit chat-based capabilities.; Just like Databricks' Dolly V2 models,
dlite-v2-1.5b (and all other members of the dlite-v2 family) is licensed for both research and commercial use. We are extremely grateful 
for the work that Databricks has done to create the databricks-dolly-15k dataset, for without it we would not be able to create and release this
model under such an open and permissive license.; While dlite-v2-1.5b is not a state-of-the-art model, we believe that the level of interactivity that can be achieved on such a small model that is trained so cheaply
is important to showcase, as it continues to demonstrate that creating powerful AI capabilities may be much more accessible than previously thought. ; dlite-v2-1.5b is not a state-of-the-art language model. dlite-v2-1.5b is an experimental technology, and as with any experimental technology, 
AI Squared urges potential users of this technology to test its capabilities thoroughly before usage.
Furthermore, the model can sometimes exhibit undesired behaviors. Some of these behaviors include,
but are not limited to: factual inaccuracies, biases, offensive responses, toxicity, and hallucinations.
Just as with any other LLM, we advise users of this technology to exercise good judgment when applying this technology.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers and accelerate libraries installed.
From your terminal, run:"
DeepNegative,Text-to-Image,,English; Chinese,,,4,,0,0.227330704,,https://huggingface.co/AsciiP/DeepNegative,"I am not the author of these models, I just uploaded them to Huggingface for ease of use in Colab.; 我不是这些模型的作者，我只是为了在colab里使用方便而上传到huggingface。; https://civitai.com/models/4629/deep-negative-v1x; Unable to determine this model’s library. Check the
								docs 
.
							"
control_v11f1p_sd15_depth,Image-to-Image,Diffusers,,openrail,https://arxiv.org/pdf/2302.05543.pdf,20,,"178,732",4415.678577,49,https://huggingface.co/lllyasviel/control_v11f1p_sd15_depth,"Controlnet v1.1 is the successor model of Controlnet v1.0
and was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ; "
control_v1u_sd15_brightness,Text-to-Image,Diffusers,,creativeml-openrail-m,,2,,1,1489.772083,,https://huggingface.co/ioclab/control_v1u_sd15_brightness,"These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with new type of conditioning. You can find some example images in the following. ; prompt: a painting of a village in the mountains

prompt: three people walking in an alleyway with hats and pants
"
embeddings_all_datasets_v4,Sentence Similarity,PyTorch; Sentence Transformers,English,,https://arxiv.org/pdf/2104.08727.pdf; https://arxiv.org/pdf/1810.09305.pdf; https://arxiv.org/pdf/2102.07033.pdf; https://arxiv.org/pdf/1904.06472.pdf,2,,15,91.64321812,,https://huggingface.co/HuynhTrong/embeddings_all_datasets_v4,"The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised 
contrastive learning objective. We used the pretrained 'MiniLM-L6-H384-uncased' model and fine-tuned in on a 
1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.; We developped this model during the 
Community week using JAX/Flax for NLP & CV, 
organized by Hugging Face. We developped this model as part of the project:
Train the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well
as intervention from Google’s Flax, JAX, and Cloud team member about efficient deep learning frameworks.; Our model is intented to be used as a sentence encoder. Given an input sentence, it ouptuts a vector which captures 
the sentence semantic information. The sentence vector may be used for information retrieval, clustering or sentence 
similarity tasks.; Here is how to use this model to get the features of a given text using SentenceTransformers library:; We use the pretrained 'MiniLM-L6-H384-uncased' which is a 6 layer version of 
'microsoft/MiniLM-L12-H384-uncased' by keeping only every second layer. 
Please refer to the model card for more detailed information about the pre-training procedure."
sd-backup-files,,,,,,2,,0,28632.72145,,https://huggingface.co/Arxyma/sd-backup-files,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
focalnet-small,Image Classification,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2203.11926.pdf,1,imagenet-1k,15,200.0737849,,https://huggingface.co/microsoft/focalnet-small,"FocalNet model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Focal Modulation Networks
 by Yang et al. and first released in this repository. ; Disclaimer: The team releasing FocalNet did not write a model card for this model so this model card has been written by the Hugging Face team.; Focul Modulation Networks are an alternative to Vision Transformers, where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. 
Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its
content, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-the-art SA counterparts (e.g., Vision Transformers, Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation.; ; You can use the raw model for image classification. See the model hub to look for
fine-tuned versions on a task that interests you."
clip4clip-webvid150k,Text-to-Video,PyTorch; Transformers,,,https://arxiv.org/pdf/2104.08860.pdf,5,HuggingFaceM4/webvid,183,608.5868781,1,https://huggingface.co/Searchium-ai/clip4clip-webvid150k,"A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. 
The model and training method are described in the paper ""Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval"" by Lou et el, and implemented in the accompanying GitHub repository.; The training process utilized the WebVid Dataset, a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. 
For training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.; This HF model is based on the clip-vit-base-patch32 architecture, with weights trained by Daphna Idelson at Searchium.; An additional notebook ""GSI_VideoRetrieval_VideoEmbedding.ipynb"", provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.; This model is intended for use in large scale video-text retrieval applications. "
gpt4-alpaca-lora-13B-HF,Text2Text Generation,PyTorch; Transformers,English,other,,3,,221,26993.16715,,https://huggingface.co/TheBloke/gpt4-alpaca-lora-13B-HF,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is the HF format merged model for chansung's gpt4-alpaca-lora-13b.; For further support, and discussions on these models and AI in general, join us at:; TheBloke AI's Discord server"
RinneVoiceSet,Text-to-Speech,,Japanese,other,,20,,0,838.239067,,https://huggingface.co/RinneAi/RinneVoiceSet,"Please translate this page in your web browser.; とてもgに Rinne の声で歌わせたり、しゃべらせたり、ボイスチェンジしたりできます。; ; ; 

"
GhostMix,,,,,,61,,0,17909.383,,https://huggingface.co/drnighthan/GhostMix,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
KoAlpaca-Polyglot-12.8B,Text Generation,PyTorch; Safetensors; Transformers,Korean,apache-2.0,,34,KoAlpaca-v1.1b,"5,046",52813.71073,1,https://huggingface.co/beomi/KoAlpaca-Polyglot-12.8B,Update @ 2023.06.01; This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KoAlpaca Dataset v1.1b; Detail Codes are available at KoAlpaca Github Repository; The following hyperparameters were used during training:
GLM-130B-quant-int4-4gpu,,,,,,11,,0,0.001655121,,https://huggingface.co/ianZzzzzz/GLM-130B-quant-int4-4gpu,"GLM-130B模型的int4量化版本，可在四张3090Ti的情况下进行推理。
An int4 quantized version of the GLM-130B model that can be inferred with 4 * 3090Ti .; iannobug@gmail.com; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
korean-vicuna-7b-1.1,Text Generation,PyTorch; Transformers,,apache-2.0,,17,,86,34502.9908,,https://huggingface.co/ureca07/korean-vicuna-7b-1.1,"?? ??! (github ??!)  ; https://github.com/hotco87/gradio_ko_chat/tree/master; #################################################################; USER: ??? ??? ?? 3? ??? ?????.; ASSISTANT: 
1?: ??? ?????? ???? ?? ??? ????? ?????.
2?: ??? ??? ???? ??? ?? ????? ??? ?????.
3?: ?? ?? ?????? ??? ??? ??? ??? ?????."
updated_t5_squad_mcq_vam,Text2Text Generation,PyTorch; Transformers,,,,1,,126,892.7797363,,https://huggingface.co/vishal2014/updated_t5_squad_mcq_vam,No model card; New: Create and edit this model card directly on the website!
nanoT5-base,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2002.05202.pdf,7,allenai/c4,135,993.2010777,,https://huggingface.co/pnawrot/nanoT5-base,"Google's T5-v1.1-base pre-trained for 24 hours (80k steps / 256 batch size) on a single GPU in nanoT5 library for efficient pre-training.; For more details about the model refer to the original paper and original model weights.; It can be further fine-tuned on SuperNatural-Instructions dataset to achieve comparable performance to the same model pre-trained on 150x more data through ""a combination of model and data parallelism [...] on slices of Cloud TPU Pods"", each with 1024 TPUs."
mental-xlnet-base-cased,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/1906.08237.pdf; https://arxiv.org/pdf/2304.10447.pdf,1,,27,0,,https://huggingface.co/AIMH/mental-xlnet-base-cased,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This model is pretrained from the checkpoint of xlnet-base-cased for the mental healthcare domain. XLNet model pre-trained on English language. It was introduced in the paper XLNet: Generalized Autoregressive Pretraining for Language Understanding by Yang et al. and first released in this repository. ; Here is how to use this model to get the features of a given text in PyTorch:; To minimize the influence of worrying mask predictions, this model is gated.  To download a gated model, you’ll need to be authenticated. 
Know more about gated models. "
stablelm-tuned-alpha-3b,Text Generation,PyTorch; Transformers,English,cc-by-nc-sa-4.0,,100,dmayhem93/ChatCombined; tatsu-lab/alpaca; nomic-ai/gpt4all_prompt_generations; Dahoas/full-hh-rlhf; jeffwan/sharegpt_vicuna; HuggingFaceH4/databricks_dolly_15k,"18,249",15218.78015,8,https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b,"StableLM-Tuned-Alpha is a suite of 3B and 7B parameter decoder-only language models built on top of the StableLM-Base-Alpha models and further fine-tuned on various chat and instruction-following datasets.; Get started chatting with StableLM-Tuned-Alpha by using the following code snippet:; StableLM Tuned should be used with prompts formatted to <|SYSTEM|>...<|USER|>...<|ASSISTANT|>...
The system prompt is; StableLM-Tuned-Alpha models are fine-tuned on a combination of five datasets:
Alpaca, a dataset of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine.
GPT4All Prompt Generations, which consists of 400k prompts and responses generated by GPT-4;
Anthropic HH, made up of preferences about AI assistant helpfulness and harmlessness;
DataBricks Dolly, comprising 15k instruction/responses generated by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA and summarization;
and ShareGPT Vicuna (English subset), a dataset of conversations retrieved from ShareGPT.; Models are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (FP16), and optimized with AdamW. We outline the following hyperparameters:"
GLIGEN_pruned_safetensors,,,,,,5,,0,1254.001677,,https://huggingface.co/comfyanonymous/GLIGEN_pruned_safetensors,"gligen_sd14_textbox_pruned.safetensors contains the fuser and positionnet weights from: https://huggingface.co/gligen/gligen-generation-text-box/tree/main; These can be used in ComfyUI: https://comfyanonymous.github.io/ComfyUI_examples/gligen/; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
pai-diffusion-artist-xlarge-zh,Text-to-Image,Diffusers; PyTorch,,apache-2.0,,4,,46,4.131639023,2,https://huggingface.co/alibaba-pai/pai-diffusion-artist-xlarge-zh,"我们开源了一个中文 Diffusion 模型，您可以直接输入中文提示词，我们为您呈现精美的艺术风格图片。本模型的默认分辨率是 768*768。; We release a Chinese diffusion model, which is able to generate high-quality artistic images according to the prompts you input. The default resolution of this model is 768*768.; 本模型支持 diffusers，可以参考以下范例代码：; This model supports diffusers. Please refer to the following code:; 使用上述模型需遵守AIGC模型开源特别条款。"
ko_vicuna_7b,Text Generation,PyTorch; Transformers,Korean; English,,,21,,979,27597.43779,,https://huggingface.co/junelee/ko_vicuna_7b,
moss-moon-003-sft,Text Generation,PyTorch; Transformers,English; Chinese,agpl-3.0,https://arxiv.org/pdf/2203.13474.pdf,112,fnlp/moss-002-sft-data,"21,232",34389.9015,187,https://huggingface.co/fnlp/moss-moon-003-sft,"MOSS is an open-sourced plugin-augmented conversational language model. moss-moon models have 16B parameters, allowing users to perform inference on a single A100 GPU or 2 NVIDIA 3090 GPUs with FP16 precision, and on a single NVIDIA 3090 GPU with INT-4/8 precision. The base language model of MOSS was pre-trained on ~700B English, Chinese, and code tokens, including the PILE, BigQuery, BigPython, and our private Chinese corpus. The base model was then fine-tuned on multi-turn plugin-augmented conversational data. Finally, we performed preference-aware training to further improve the model.; Limitations: Due to the (relatively) small number of parameters and the autoregressive nature, MOSS is still possible to generate outputs that contain incorrect, misleading, or biased information. Please carefully check the contents generated by MOSS before you use them.; MOSS Use Cases：; ; "
db-simpsons-asim-style,Text-to-Image,Diffusers,,creativeml-openrail-m,,5,JerryMo/db-simpsons-dataset,46,2181.123545,,https://huggingface.co/JerryMo/db-simpsons-asim-style,"Github Repo The detailed work description and code can be found in https://github.com/foxintohumanbeing/DDA4210_Group_project.; The creation of high-quality image content from text descriptions is a challenging yet highly desirable task in the field of artificial intelligence. We focus on the Simpsons, a popular animated series. Based on pretrained SOTA model, we investigate in obtaining high-quality dataset and efficient fine-tuning methods. We explore the options of manually creating the dataset and using different fine-tuning techniques such as simple baseline, LoRA, and Dreambooth. Our approach involves combining the advantages of each option to achieve better results.; We propose dataset collection method and fine-tuning model(Simspon Artistic Memory). Moreover, to better illustrating our results, we create two APPs, one for generating images and one for annotating the images (you can find them in github link provided). By improving data collection and fine-tuning techniques on Simpsons, we hope to push the boundaries of what is achievable in the text-to-image synthesis domain and inspire further research in this area.; Prompts Format ""Asim. a [closeup?] of a [emotional expression] [race] [X year old] [man / woman / etc.], with [hair and makeup style], wearing [clothing style] while [doing] near [nearby objects],[outside / inside] with [objects / color ] in the background,in [time period].""; Contact"
sam_vit_h,,,,,,1,,0,2621.441445,,https://huggingface.co/segments-arnaud/sam_vit_h,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ChatFlow-13B,,,,gpl-3.0,,12,,0,26624.48983,,https://huggingface.co/Linly-AI/ChatFlow-13B,"Model weights in TencentPretrain format; How to use: https://github.com/CVI-SZU/Linly; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
EimisAnimeDiffusion_2.0v,Text-to-Image,Diffusers,English,creativeml-openrail-m,,26,,0,7884.810996,,https://huggingface.co/eimiss/EimisAnimeDiffusion_2.0v,"This model is trained using base model as the previous version with way bigger dataset.
There are two versions of it:
EimisAnimeDiffusion_2-0 (original)
EimisAnimeDiffusion_2-0_alternative (original + orangemix:0.2 + even bigger dataset).
Read the end to choose the one you want the most.
At the beginning all the examples will be using ""EimisAnimeDiffusion_2-0"". ; Of course this model works well with anime style, magic, and a bunch of different effects. A couple of examples:; Right V2, Left V1.; For more in depth testing between these two:
Better face structures (eyes fixed)
Higher resolution (new data was trained on 768x768 instead of 512x512)
Better looking characters, animations, enviroment, effects and way more ; EimisAnimeDiffusion_2-0 is trained on smaller dataset, however it keeps the style better.
It might be worse on some aspects like hardly getting specific prompts or some other small issues, however
it has way better quality, effects and keeps the style I wanted way better.
EimisAnimeDiffusion_2-0_alternative on the other hand understands better way more prompts (especially in comparison with some NSFW prompts).
However, way worse with style, effects, details.
Also sometimes not as smooth, some stuff be random, btu still really great alternative model. 
Example:
Left normal, right alternative:"
wtp-bert-tiny,Token Classification,PyTorch; ONNX; Transformers,85 languages,mit,,2,,625,17.5280138,,https://huggingface.co/benjamin/wtp-bert-tiny,Model for wtpsplit.
mpt-1b-redpajama-200b,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf,71,togethercomputer/RedPajama-Data-1T,"3,527",5378.177688,,https://huggingface.co/mosaicml/mpt-1b-redpajama-200b,"MPT-1b-RedPajama-200b is a 1.3 billion parameter decoder-only transformer trained on the RedPajama dataset.
The model was trained for 200B tokens by sampling from the subsets of the RedPajama dataset in the same proportions as were used by the Llama series of models.
This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; April 20, 2023; Note: This model requires that trust_remote_code=True be passed to the from_pretrained method. 
This is because we use a custom model architecture MosaicGPT that is not yet part of the transformers package.
MosaicGPT includes options for many training efficiency features such as FlashAttention (Dao et al. 2022), ALIBI, QK LayerNorm, and more.; To use the optimized triton implementation of FlashAttention, you can load with attn_impl='triton' and move the model to bfloat16 like so:; This model uses the MosaicML LLM codebase, which can be found in the MosaicML Examples Repository.
The architecture is a modification of a standard decoder-only transformer.
The transformer has 24 layers, 16 attention heads, and width 2048.
The model has been modified from a standard transformer in the following ways:"
hanfu,,,,,,10,,0,12769.04174,,https://huggingface.co/liaoliaojun/hanfu,"分享汉服的LORA、大模型、vae等文件资料。
https://civitai.com/models/15365/hanfu
https://civitai.com/models/65314/hanfu-ming
https://civitai.com/models/47916/hanfu-song
https://civitai.com/models/44395/hanfu-tang; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
rvc-ontentvec-768,,,,,,1,,0,0.001445313,,https://huggingface.co/ddPn08/rvc-ontentvec-768,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
segformer_b0_clothes,Image Segmentation,PyTorch; Safetensors; Transformers,,mit,,2,mattmdjaga/human_parsing_dataset,337,29.80644009,,https://huggingface.co/mattmdjaga/segformer_b0_clothes,"SegFormer model fine-tuned on ATR dataset for clothes segmentation.
The dataset on hugging face is called ""mattmdjaga/human_parsing_dataset""."
stablelm-7b-sft-v7-epoch-3,Text Generation,PyTorch; Transformers,English,,,65,,"1,269",32811.09765,7,https://huggingface.co/OpenAssistant/stablelm-7b-sft-v7-epoch-3,"This is the 7th iteration English supervised-fine-tuning (SFT) model of 
the Open-Assistant project. 
It is based on a StableLM 7B that was fine-tuned on human demonstrations 
of assistant conversations collected through the 
https://open-assistant.io/ human feedback web 
app before April 12, 2023. ; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; command: deepspeed trainer_sft.py --configs defaults stablelm-7b oasst-mix --cache_dir /home/ubuntu/data_cache --output_dir .saved/stable-lm-7b-1 --num_train_epochs 4 --deepspeed"
MandarinMix,Text-to-Image,Diffusers,Japanese,creativeml-openrail-m,,24,,356,0.005708427,,https://huggingface.co/Hemlok/MandarinMix,; Join Discord Server; ; ; 
xsarchitectural9advanced_xsarchitectural,,,,openrail,,5,,0,19991.43916,,https://huggingface.co/Oldyu/xsarchitectural9advanced_xsarchitectural,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
so-vits-genshin,,,,mit,,29,,0,0.00146534,,https://huggingface.co/kaze-mio/so-vits-genshin,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
SegGPT,,,,mit,https://arxiv.org/pdf/2304.03284.pdf,9,,0,1516.153672,,https://huggingface.co/BAAI/SegGPT,"Xinlong Wang1*, ? Xiaosong Zhang1*, ? Yue Cao1*, ? Wen Wang2, ?  Chunhua Shen2, ? Tiejun Huang1,3; 1BAAI, ? 2ZJU, ? 3PKU; Enjoy the Demo and Code; ;    We present SegGPT, a generalist model for segmenting everything in context. With only one single model, SegGPT can perform arbitrary segmentation tasks in images or videos via in-context inference, such as object instance, stuff, part, contour, and text. 
   SegGPT is evaluated on a broad range of tasks, including few-shot semantic segmentation, video object segmentation, semantic segmentation, and panoptic segmentation. 
   Our results show strong capabilities in segmenting in-domain and out-of-domain targets, either qualitatively or quantitatively. "
ESRGAN,,,,other,,1,,0,134.0014672,,https://huggingface.co/Echiki/ESRGAN,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
H2-keywordextractor,Summarization,PyTorch; Transformers,unk,,,2,transformer3/autotrain-data-finance6,591,1672.459149,,https://huggingface.co/transformer3/H2-keywordextractor,You can use cURL to access this model:
llava-13b-v0-4bit-128g,Text Generation,Transformers,,,,33,,98,7629.291451,,https://huggingface.co/wojtab/llava-13b-v0-4bit-128g,4-bit quant of llama part of llava https://github.com/haotian-liu/LLaVA https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0; quantized by:; on https://github.com/oobabooga/GPTQ-for-LLaMa CUDA branch of GPTQ (commit 57a2629); YOU CAN NOW RUN IT IN TEXT-GENERATION-WEBUI with llava extension (see: https://github.com/oobabooga/text-generation-webui/tree/main/extensions/llava)
llama-13b-supercot-4bit-128g,Text Generation,PyTorch; Transformers,,,,16,,91,7631.197052,,https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g,huggyllama/llama-13b + kaiokendev/SuperCOT-LoRA/13b/gpu/cutoff-2048; CUDA_VISIBLE_DEVICES=0 python llama.py c4 --wbits 4 --true-sequential --act-order --groupsize 128; In ooba make sure to use --groupsize 128 --wbits 4
edge-of-realism,Text-to-Image,Diffusers,,creativeml-openrail-m,,14,,"83,996",0.004350548,4,https://huggingface.co/stablediffusionapi/edge-of-realism,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""edge-of-realism""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Model link: View model"
1111,,Transformers,,openrail,,2,,0,29352.02936,,https://huggingface.co/caocaocoa/1111,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
FSMN-VAD,Voice Activity Detection,ONNX,,apache-2.0,https://arxiv.org/pdf/1803.05030.pdf,4,,0,2.414453125,,https://huggingface.co/funasr/FSMN-VAD,"Voice activity detection (VAD) plays a important role in speech recognition systems by detecting the beginning and end of effective speech. FunASR provides an efficient VAD model based on the FSMN structure. To improve model discrimination, we use monophones as modeling units, given the relatively rich speech information. During inference, the VAD system requires post-processing for improved robustness, including operations such as threshold settings and sliding windows.; This repository demonstrates how to leverage FSMN-VAD in conjunction with the funasr_onnx runtime. The underlying model is derived from FunASR, which was trained on a massive 5,000-hour dataset.; We have relesed numerous industrial-grade models, including speech recognition, voice activity detection, punctuation restoration, speaker verification, speaker diarization, and timestamp prediction (force alignment). To learn more about these models, kindly refer to the documentation available on FunASR. If you are interested in leveraging advanced AI technology for your speech-related projects, we invite you to explore the possibilities offered by FunASR.; Input: wav formt file, support formats: str, np.ndarray, List[str]; Output: List[str]: recognition result"
medalpaca-13B-GPTQ-4bit,Text Generation,PyTorch; Transformers,English,cc,https://arxiv.org/pdf/2303.14070.pdf,23,,91,7435.246279,,https://huggingface.co/TheBloke/medalpaca-13B-GPTQ-4bit,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a GPTQ-for-LLaMa 4bit quantisation of medalpaca-13b.; Please read the Provided Files section below. You should use medalpaca-13B-GPTQ-4bit-128g.no-act-order.safetensors unless you are able to use the latest Triton branch of GPTQ-for-LLaMa.; Open the text-generation-webui UI as normal.
GPT4-x-Alpaca13b-RolePlayLora-4bit-v2,Text Generation,PyTorch; Transformers,,mit,,22,,65,7189.107939,,https://huggingface.co/teknium/GPT4-x-Alpaca13b-RolePlayLora-4bit-v2,"This is a llama-13B based model that has been converted with GPTQ to 4bit quantized model.; Load without groupsize flag or you may get OOM; Base Model: GPT4-x-Alpaca full fine tune by Chavinlo -> https://huggingface.co/chavinlo/gpt4-x-alpacaLORA fine tune using the Roleplay Instruct from GPT4 generated dataset -> https://github.com/teknium1/GPTeacher/tree/main/RoleplayLORA Adapter Only: https://huggingface.co/ZeusLabs/gpt4-x-alpaca-rp-lora - The v2 one -; Merged LORA to the model. ; FYI Latest HF Transformers generates BROKEN generations. 
Try this instead if your generations are terrible (first uninstall transformers): pip install git+https://github.com/huggingface/transformers@9eae4aa57650c1dbe1becd4e0979f6ad1e572ac0
More info and possible alternative solutions in these github issues.https://github.com/tloen/alpaca-lora/issues/279#issuecomment-1514725886https://github.com/oobabooga/text-generation-webui/issues/931#issuecomment-1516215250 "
OpenAssistant-Llama-30b-4bit,Text Generation,PyTorch; Transformers,,,,69,,53,104550.9427,,https://huggingface.co/MetaIX/OpenAssistant-Llama-30b-4bit,"Information; This was made using Open Assistant's native fine-tune of Llama 30b on their dataset.; What's included; GPTQ: 2 quantized versions. One quantized --true-sequential and act-order optimizations, and the other was quantized using --true-sequential --groupsize 128 optimizations; GGML: 3 quantized versions. One quantized using q4_1, another one was quantized using q5_0, and the last one was quantized using q5_1."
gpt4tools-vicuna-13b-lora,,,,mit,,27,,0,105.003244,,https://huggingface.co/stevengrove/gpt4tools-vicuna-13b-lora,"Lin Song, Yanwei Li, Rui Yang, Sijie Zhao, Yixiao Ge, Ying Shan; GPT4Tools is a centralized system that can control multiple visual foundation models. 
It is based on Vicuna (LLaMA), and 71K self-built instruction data.
By analyzing the language content, GPT4Tools is capable of automatically deciding, controlling, and utilizing different visual foundation models, allowing the user to interact with images during a conversation.
With this approach, GPT4Tools provides a seamless and efficient solution to fulfill various image-related requirements in a conversation.
Different from previous work, we support users teach their own LLM to use tools with simple refinement via self-instruction and LoRA.;    ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
psychology-alpaca,,,,,,8,samhog/psychology-10k,0,8.432671738,,https://huggingface.co/KTH/psychology-alpaca,"This is a LLaMA-7B language model trained on 10.000 psychology-related prompts and answers generated by ChatGPT. The model was trained on a single A100 GPU from Google Colab. The model shows some knowledge in the field of psychology and generally performs better than its base model parent.; This model was developed as part of a thesis project in the field of machine learning and psychology. It was used as a base model for further fine-tuning using reinforcement learning. The goal of the thesis was to compare reinforcement learning from human feedback and AI feedback. When the paper is available, it will be linked here!; Links: RLHF model; RLAIF model; Authors:
Samuel H?glund, samhog@kth.se;
Josef Khedri, jkhedri@kth.se; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
MIX-Pro,,,,creativeml-openrail-m,,1,,0,0.001492996,,https://huggingface.co/GIMG/MIX-Pro,"test model; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
perceiver-io-optical-flow,,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2107.14795.pdf,1,autoflow,89,353.0075781,,https://huggingface.co/krasserm/perceiver-io-optical-flow,"This model is a Perceiver IO optical flow model pretrained on AutoFlow.
It is weight-equivalent to the deepmind/optical-flow-perceiver
model but based on implementation classes of the perceiver-io library. It 
can be created from the deepmind/optical-flow-perceiver model with a library-specific conversion utility.
Both models generate equal output for the same input.; Content of the deepmind/optical-flow-perceiver model card
also applies to this model except usage examples. Refer to the linked card for further model and
training details.; The model is specified in Appendix H (Table 16) of the Perceiver IO paper.; The model can be used to predict the optical flow between a pair of images.; To use this model you first need to install 
the perceiver-io library with extension vision."
flat-2d-animerge,,,,,,4,,0,2181.121445,,https://huggingface.co/ckpt/flat-2d-animerge,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
BraV4,,,,,,3,,0,14520.32145,,https://huggingface.co/ckpt/BraV4,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
mzpikas_tmnd_enhanced,,,,creativeml-openrail-m,,70,,0,3533.233994,,https://huggingface.co/ashen-sensored/mzpikas_tmnd_enhanced,"; Using the sum of negative DAAM cumulative attention scores from teacher models as loss, running neuron-wise merge with AdamW from four teacher models, trained on images with resolution 2048x2048; https://civitai.com/models/27259/tmnd-mix; https://civitai.com/models/47067/pikas-new-generation-v10; https://civitai.com/models/31383/mzmix"
chinese-alpaca-7b-and-13b-quantized,,,,,,40,,0,0.002485237,,https://huggingface.co/johnlui/chinese-alpaca-7b-and-13b-quantized,"移动本仓库中的 llama-7b-hf 和 llama-13b-hf 两个文件夹，到你项目的 ./models 文件下即可。该文件夹同时适用于 llama.cpp 和 text-generation-webui。; 以 7B 为例：; 7b 为我自己合成，13b 是从 https://huggingface.co/minlik/chinese-alpaca-13b-quantized 仓库里下载的。; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
gpt4all-13b-snoozy,Text Generation,PyTorch; Transformers,English,gpl,,70,nomic-ai/gpt4all-j-prompt-generations,"1,236",53320.20866,8,https://huggingface.co/nomic-ai/gpt4all-13b-snoozy,"A GPL licensed chatbot trained over a massive curated corpus of assistant interactions including word problems, multi-turn dialogue, code, poems, songs, and stories.; This model has been finetuned from LLama 13B; This model was trained on nomic-ai/gpt4all-j-prompt-generations using revision=v1.3-groovy; Results on common sense reasoning benchmarks"
dark-sushi-mix,Text-to-Image,Diffusers,,creativeml-openrail-m,,10,,966,0.00438961,33,https://huggingface.co/stablediffusionapi/dark-sushi-mix,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""dark-sushi-mix""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Model link: View model"
codegen2-1B,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2305.02309.pdf,29,,"8,925",4232.692925,,https://huggingface.co/Salesforce/codegen2-1B,"CodeGen2 is a family of autoregressive language models for program synthesis, introduced in the paper:; CodeGen2: Lessons for Training LLMs on Programming and Natural Languages by Erik Nijkamp*, Hiroaki Hayashi*, Caiming Xiong, Silvio Savarese, Yingbo Zhou.; Unlike the original CodeGen model family (i.e., CodeGen1), CodeGen2 is capable of infilling, and supports more programming languages.; Four model sizes are released: 1B, 3.7B, 7B, 16B.; This model can be easily loaded using the AutoModelForCausalLM functionality."
wangchanglm-7.5B-sft-enth,Text Generation,PyTorch; Transformers,4 languages,cc-by-sa-4.0,,3,laion/OIG; Hello-SimpleAI/HC3; databricks/databricks-dolly-15k,234,17512.09138,2,https://huggingface.co/pythainlp/wangchanglm-7.5B-sft-enth,"WangChanGLM is a multilingual, instruction-finetuned Facebook XGLM-7.5B using open-source, commercially permissible datasets (LAION OIG chip2 and infill_dbpedia, DataBricks Dolly v2, OpenAI TL;DR, and Hello-SimpleAI HC3; about 400k examples), released under CC-BY SA 4.0. The models are trained to perform a subset of instruction-following tasks we found most relevant namely: reading comprehension, brainstorming, and creative writing. We provide the weights for a model finetuned on an English-only dataset (wangchanglm-7.5B-sft-en) and another checkpoint further finetuned on Google-Translated Thai dataset (wangchanglm-7.5B-sft-enth). We perform Vicuna-style evaluation using both humans and ChatGPT (in our case, gpt-3.5-turbo since we are still on the waitlist for gpt-4) and observe some discrepancies between the two types of annoators. All training and evaluation codes are shared under the Apache-2.0 license in our Github, as well as datasets and model weights on HuggingFace. In a similar manner to Dolly v2, we use only use open-source, commercially permissive pretrained models and datasets, our models are neither restricted by non-commercial clause like models that use LLaMA as base nor non-compete clause like models that use self-instruct datasets from ChatGPT. See our live demo here.; Intended to be use as an instruction-following model for reading comprehension, brainstorming and creative writing.; The model can be finetuned for any typical instruction-following use cases.; We do not expect the models to perform well in math problems, reasoning, and factfulness. We intentionally filter out training examples from these use cases.; We noticed similar limitations to other finetuned instruction followers such as math problems, reasoning, and factfulness. Even though the models do not perform on the level that we expect them to be abused, they do contain undesirable biases and toxicity and should be further optimized for your particular use cases."
CLIP-ViT-L-14-DataComp.XL-s13B-b90K,Zero-Shot Image Classification,PyTorch; OpenCLIP,,mit,https://arxiv.org/pdf/2304.14108.pdf,82,mlfoundations/datacomp_pools,"36,098",3505.669449,2,https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K,"A CLIP ViT-L/14 model trained with the DataComp-1B (https://github.com/mlfoundations/datacomp) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done on the stability.ai cluster. ; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the DataComp paper (https://arxiv.org/abs/2304.14108) include additional discussion as it relates specifically to the training dataset. ; Zero-shot image classification, image and text retrieval, among others."
blink-bert-large,Feature Extraction,PyTorch; Safetensors; Transformers,English,mit,,2,,8,2745.243559,,https://huggingface.co/riccorl/blink-bert-large,This is a BERT-Large model finetuned on BLINK
falcon-rw-1b,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2306.01116.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2205.14135.pdf,36,tiiuae/falcon-refinedweb,"19,335",2686.287039,4,https://huggingface.co/tiiuae/falcon-rw-1b,"Falcon-RW-1B is a 1B parameters causal decoder-only model built by TII and trained on 350B tokens of RefinedWeb. It is made available under the Apache 2.0 license.; See the ? paper on arXiv for more details.; RefinedWeb is a high-quality web dataset built by leveraging stringent filtering and large-scale deduplication. Falcon-RW-1B, trained on RefinedWeb only, matches or outperforms comparable models trained on curated data.; ?? Falcon is now available as a core model in the transformers library! To use the in-library version, please install the latest version of transformers with pip install git+https://github.com/huggingface/transformers.git, then simply remove the trust_remote_code=True argument from from_pretrained().; ?? This model is intended for use as a research artifact, to study the influence of training on web data alone. If you are interested in state-of-the-art models, we recommend using Falcon-7B/40B, both trained on >1,000 billion tokens."
TeaColorMix,,,,,,2,,0,189.5016408,,https://huggingface.co/Kirintea/TeaColorMix,"lora ! ! ! lora ! ! !; prompt:masterpiece, best quality,1girl, cirno,; ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
codegen2-7B,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2305.02309.pdf,18,,"1,833",28245.51605,2,https://huggingface.co/Salesforce/codegen2-7B,"CodeGen2 is a family of autoregressive language models for program synthesis, introduced in the paper:; CodeGen2: Lessons for Training LLMs on Programming and Natural Languages by Erik Nijkamp*, Hiroaki Hayashi*, Caiming Xiong, Silvio Savarese, Yingbo Zhou.; Unlike the original CodeGen model family (i.e., CodeGen1), CodeGen2 is capable of infilling, and supports more programming languages.; Four model sizes are released: 1B, 3.7B, 7B, 16B.; This model can be easily loaded using the AutoModelForCausalLM functionality."
all-526-animated,Text-to-Image,Diffusers,,creativeml-openrail-m,,2,,"3,448",0.004399376,34,https://huggingface.co/stablediffusionapi/all-526-animated,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""all-526-animated""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Model link: View model"
4x_foolhardy_Remacri,,,,unknown,,13,,0,67.00185921,,https://huggingface.co/FacehugmanIII/4x_foolhardy_Remacri,Using the Remacri upscaler in Automatic1111:; Get the '4x_foolhardy_Remacri.pth' file linked in this post; Copy it to: \stable-diffusion-webui\models\ESRGAN; Restart WebUI; 4x_foolhardy_Remacri is now available in the Extras tab and for the SD Upscale script
safety-flan-t5-base,Text2Text Generation,PyTorch; Transformers,,,,1,,"2,030",3019.979631,,https://huggingface.co/Salesforce/safety-flan-t5-base,Find below some example scripts on how to use the model in transformers:
llama7b_4bit_128g,,,,gpl-3.0,,1,,0,3983.361821,,https://huggingface.co/Chinese-Vicuna/llama7b_4bit_128g,"This is a Chinese instruction-tuning lora checkpoint based on llama-13B from this repo's work
Consumes approximately 5.4G of graphics memory; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
MyLora,,,,,,3,,0,5628.471445,,https://huggingface.co/axhello/MyLora,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
legal-swiss-longformer-base,Fill-Mask,PyTorch; Transformers,4 languages,cc,https://arxiv.org/pdf/2306.02069.pdf; https://arxiv.org/pdf/2301.13126.pdf; https://arxiv.org/pdf/2110.00976.pdf; https://arxiv.org/pdf/2306.09237.pdf,2,MultiLegalPile; LEXTREME; LEXGLUE,41,840.1154416,,https://huggingface.co/joelito/legal-swiss-longformer-base,"This model is a multilingual model pretrained on legal data. It is based on XLM-R (base and large). For pretraining we used Multi Legal Pile (Niklaus et al. 2023), a multilingual dataset from various legal sources covering 24 languages.; You can utilize the raw model for masked language modeling since we did not perform next sentence prediction. However, its main purpose is to be fine-tuned for downstream tasks.; It's important to note that this model is primarily designed for fine-tuning on tasks that rely on the entire sentence, potentially with masked elements, to make decisions. Examples of such tasks include sequence classification, token classification, or question answering. For text generation tasks, models like GPT-2 are more suitable.; Additionally, the model is specifically trained on legal data, aiming to deliver strong performance in that domain. Its performance may vary when applied to non-legal data.; For tasks such as text generation you should look at model like GPT2."
gpt2-conversational-or-qa,Conversational,PyTorch; Safetensors; Transformers,English,openrail,,1,Locutusque/ColumnedChatCombined,238,1021.436221,2,https://huggingface.co/Locutusque/gpt2-conversational-or-qa,"This model is intended to be used for generating conversational responses in a variety of contexts, such as chatbots, virtual assistants, and customer service applications. It is designed to provide natural and engaging responses to user input, with a focus on maintaining a consistent tone and style throughout the conversation. The model is suitable for use in both text-based and voice-based interfaces, and can be easily integrated into existing applications using the PyTorch and Transformers frameworks.; The model is trained on a large dataset of conversational data, consisting of interactions between users and an AI assistant. The data is preprocessed to remove any sensitive information and is formatted in a way that is suitable for training a language model. The training data is split into a training set and a validation set, with the training set used to update the model parameters and the validation set used to evaluate the model performance. The model was trained on 245,000 examples over 1,225,000 steps, it achieved decent metrics.
This model outperformed the base GPT-2 model significantly on a new conversational dataset during a fine-tuning session. Here is a side-by-side comparison of the two models during the first steps of training ; The model architecture used in this model is GPT-2, a transformer-based language model that is capable of generating high-quality text with a wide range of styles and tones. The GPT-2 architecture consists of a multi-layered transformer encoder-decoder, with self-attention mechanisms that allow the model to capture long-term dependencies and generate coherent text.; The model is evaluated based on several metrics, including loss, reward, penalty, BLEU score, and perplexity. The loss metric is calculated during training and reflects the difference between the predicted output and the actual output. The reward metric is based on the number of correct words generated by the model, while the penalty metric penalizes the model for repeating words consecutively. The BLEU score measures the similarity between the generated text and the ground truth text, while the perplexity metric measures how well the model is able to predict the next word in a sequence. During validation, the model achieved the following metrics:; Although these metrics seem mediocre, it's actually better because that way the model is able to make open-ended responses, but is still coherent to the user's input."
polyglot-ko-12.8b-chang-instruct-chat,Text Generation,PyTorch; Transformers,Korean,apache-2.0,,12,,502,26625.65302,1,https://huggingface.co/lcw99/polyglot-ko-12.8b-chang-instruct-chat,demofinetune dataset
chinese-alpaca-plus-lora-7b,,,Chinese,apache-2.0,,38,,0,1168.102639,,https://huggingface.co/ziqingyang/chinese-alpaca-plus-lora-7b,"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ChatGLM-6b-onnx-u8s8,,ONNX,Chinese; English,,,6,,0,0.01514225,1,https://huggingface.co/K024/ChatGLM-6b-onnx-u8s8,This model is exported from ChatGLM-6b with int8 quantization and optimized for ONNXRuntime inference. Export code in this repo.; Inference code with ONNXRuntime is uploaded with the model. Install requirements and run streamlit run web-ui.py to start chatting. Currently the MatMulInteger (for u8s8 data type) and DynamicQuantizeLinear operators are only supported on CPU. Arm64 with Neon support (Apple M1/M2) should be reasonably fast. ; 安装依赖并运行 streamlit run web-ui.py 预览模型效果。由于 ONNXRuntime 算子支持问题，目前仅能够使用 CPU 进行推理，在 Arm64 (Apple M1/M2) 上有可观的速度。具体的 ONNX 导出代码在这个仓库中。; Clone with git-lfs:; Or use huggingface_hub python client lib to download the repo snapshot:
chinese-llama-plus-7b-merged,Text Generation,PyTorch; Transformers,,,,6,,753,14101.25046,,https://huggingface.co/minlik/chinese-llama-plus-7b-merged,加入中文词表并继续预训练中文Embedding，得到的中文LLaMA-plus模型。; 详情可参考：https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v3.0
animefull-final-pruned,,Diffusers,,mit,,3,,204,0.002022285,,https://huggingface.co/jianghushinian/animefull-final-pruned,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
lora,,,,openrail,,1,,0,17829.42147,,https://huggingface.co/NKOJKN/lora,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
chinese-alpaca-plus-7b-merged,Text Generation,PyTorch; Transformers,,,,24,,"4,800",14103.50099,2,https://huggingface.co/minlik/chinese-alpaca-plus-7b-merged,加入中文词表并继续预训练中文Embedding，并在此基础上继续使用指令数据集finetuning，得到的中文Alpaca-plus模型。; 详情可参考：https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v3.0
IrisMix,Text-to-Image,Diffusers,Japanese; English,creativeml-openrail-m,,33,,0,4710.421758,,https://huggingface.co/natsusakiyomi/IrisMix,"
          彩度が高いVAE内i型かわいい系モデル
          Cute model with built-in VAE with high color saturation
        ; IrisMix-v2は^去のIrisMix-v1等とは全く中身が`い一からマ`ジしたモデル; Twiter: @min__san"
Color-Canny-Controlnet-model,Image-to-Image,Diffusers,English,apache-2.0,,17,laion/laion-art,782,727.6644824,1,https://huggingface.co/ghoskno/Color-Canny-Controlnet-model,"These are ControlNet checkpoints trained on runwayml/stable-diffusion-v1-5, using fused color and canny edge as conditioning. ; You can find some example images in the following. ; prompt: a concept art of by Makoto Shinkai, a girl is standing in the middle of the sea; negative prompt: text, bad anatomy, blurry, (low quality, blurry)
; prompt: a concept art of by Makoto Shinkai, a girl is standing in the middle of the sea"
InstructPalmyra-20b,Text Generation,PyTorch; Transformers,English,apache-2.0,,9,,2,0,,https://huggingface.co/Writer/InstructPalmyra-20b,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Introducing InstructPalmyra-20b, a state-of-the-art instruction-following 20b language model designed to deliver exceptional performance and versatility. Derived from the foundational architecture of Palmyra-20b, InstructPalmyra-20b is specifically tailored to address the growing demand for advanced natural language processing and comprehension capabilities.; The InstructPalmyra-20b model is meticulously trained on an extensive dataset of approximately 70,000 instruction-response records. These records are generated by our dedicated Writer Linguist team, who possess considerable expertise in language modeling and fine-tuning techniques. By leveraging their skills and knowledge, the InstructPalmyra-20b model is primed to offer unparalleled proficiency in understanding and executing language-based instructions.; One of the key differentiators of InstructPalmyra-20b lies in its ability to process complex instructions and generate accurate, contextually appropriate responses. This makes it an ideal choice for a wide range of applications, including virtual assistants, customer support, content generation, and more. Additionally, the model's comprehensive training enables it to adapt and perform well under varying conditions and contexts, further expanding its potential use cases."
pythia-1.4b-deduped-4k-base,Text Generation,PyTorch; Transformers,,,,1,,12,6207.552466,,https://huggingface.co/emozilla/pythia-1.4b-deduped-4k-base,No model card; New: Create and edit this model card directly on the website!
stable-vicuna-13B-HF,Text Generation,PyTorch; Transformers,English,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2302.13971.pdf,93,OpenAssistant/oasst1; nomic-ai/gpt4all_prompt_generations; tatsu-lab/alpaca,"4,394",26657.09437,10,https://huggingface.co/TheBloke/stable-vicuna-13B-HF,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is an HF format unquantised float16 model of CarperAI's StableVicuna 13B.; It is the result of merging the deltas from the above repository with the original Llama 13B weights.; This model requires the following prompt template:
stable-vicuna-13B-GGML,,,,other,https://arxiv.org/pdf/2302.13971.pdf,111,,0,118896.6592,,https://huggingface.co/TheBloke/stable-vicuna-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for CarperAI's Stable Vicuna 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
stable-vicuna-13B-GPTQ,Text Generation,Transformers,English,cc-by-nc-sa-4.0,https://arxiv.org/pdf/2302.13971.pdf,212,OpenAssistant/oasst1; nomic-ai/gpt4all_prompt_generations; tatsu-lab/alpaca,"1,548",7436.58622,,https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains 4bit GPTQ format quantised models of CarperAI's StableVicuna 13B.; It is the result of first merging the deltas from the above repository with the original Llama 13B weights, then quantising to 4bit using GPTQ-for-LLaMa.; This model works best with the following prompt template:"
IF-notebooks,,,,,,20,,0,22.80144531,,https://huggingface.co/DeepFloyd/IF-notebooks,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
llama7b-pyg-lora,,,,,,1,,0,16.80176956,,https://huggingface.co/ausboss/llama7b-pyg-lora,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
BioGPT-Large-finetuned-chatdoctor,Text Generation,PyTorch; TensorBoard; Safetensors; Transformers,English,apache-2.0,https://arxiv.org/pdf/2303.14070.pdf,16,LinhDuong/chatdoctor-200k,696,12871.73679,1,https://huggingface.co/Narrativaai/BioGPT-Large-finetuned-chatdoctor,"Microsoft's BioGPT Large fine-tuned on ChatDoctor dataset for Question Answering.; This is just a research model and does NOT have to be used out of this scope.; TBA; Microsoft's BioGPT Large:; Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms."
OpenAssistant-SFT-7-Llama-30B-HF,Text Generation,PyTorch; Transformers,,other,https://arxiv.org/pdf/2304.07327.pdf,13,,"1,590",66634.07442,,https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-HF,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This in HF format repo of OpenAssistant's LLaMA 30B SFT 7.; It is the result of merging the XORs from the above repo with the original Llama 30B weights.; This is epoch 7 of OpenAssistant's training of a Llama 30B model.
vicuna-13b-v1.1-rm-formated,,,,,,2,,0,0,,https://huggingface.co/pvduy/vicuna-13b-v1.1-rm-formated,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
OpenAssistant-SFT-7-Llama-30B-GGML,,,,other,https://arxiv.org/pdf/2304.07327.pdf,37,,0,241152.0174,,https://huggingface.co/TheBloke/OpenAssistant-SFT-7-Llama-30B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for OpenAssistant SFT 7 Llama 30B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
metharme-7b,Text Generation,,English,,,54,,0,0.009224586,,https://huggingface.co/PygmalionAI/metharme-7b,"Metharme 7B is an instruct model based on Meta's LLaMA-7B.; This is an experiment to try and get a model that is usable for conversation, roleplaying and storywriting, but which can be guided using natural language like other instruct models. See the prompting section below for examples.; It was trained by doing supervised fine-tuning over a mixture of regular instruction data alongside roleplay, fictional stories and conversations with synthetically generated instructions attached.; The model weights in this repository cannot be used as-is. The files here are XORs due to licensing concerns. To obtain proper, usable model weights you need to:; Request access to the original LLaMA weights from Meta through this form"
Pygmalion-7b-Merged-Safetensors,Text Generation,Safetensors; Transformers,English,,,11,,827,13785.40167,,https://huggingface.co/TehVenom/Pygmalion-7b-Merged-Safetensors,"Pygmalion 7B is a dialogue model based on Meta's LLaMA-7B.; This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project.; This models has the XOR files pre-applied out of the box.
Converted from the XORs weights from PygmalionAI's release https://huggingface.co/PygmalionAI/pygmalion-7b; The model was trained on the usual Pygmalion persona + chat format, so any of the usual UIs should already handle everything correctly. If you're using the model directly, this is the expected formatting:; Where [CHARACTER] is, as you can probably guess, the name of the character you want the model to portray, <START> should be used verbatim as a delimiter token to separate persona and scenario data from the dialogue, and [DIALOGUE HISTORY] is a sliding window of chat history so the model can have conversational context to draw from. Here's a concrete example:"
counterfeit-v30,Text-to-Image,Diffusers,,creativeml-openrail-m,,4,,"2,968",0.00438961,34,https://huggingface.co/stablediffusionapi/counterfeit-v30,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""counterfeit-v30""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Model link: View model"
anythingelse-v4,Text-to-Image,Diffusers,,creativeml-openrail-m,,7,,"1,362",0.00438961,32,https://huggingface.co/stablediffusionapi/anythingelse-v4,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""anythingelse-v4""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Model link: View model"
LaBSE-sentence-embeddings,Feature Extraction,PyTorch; TensorFlow; JAX; Safetensors; Transformers,109 languages,apache-2.0,https://arxiv.org/pdf/2007.01852.pdf,5,CommonCrawl; Wikipedia,118,7719.310213,,https://huggingface.co/Blaxzter/LaBSE-sentence-embeddings,"Copy of setu4993/LaBSE that returns the sentence embeddings (pooler_output) and implements caching; Original Model Card:; Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.; This is migrated from the v2 model on the TF Hub, which uses dict-based input. The embeddings produced by both the versions of the model are equivalent.; Using the model:"
bloomz-rlhf,Text Generation,PyTorch; Transformers,Chinese; English,bigscience-bloom-rail-1.0,,9,OpenAssistant/oasst1; RyokoAI/ShareGPT52K; Dahoas/full-hh-rlhf; liswei/rm-static-m2m100-zh; fnlp/moss-002-sft-data,195,16664.77696,,https://huggingface.co/keyfan/bloomz-rlhf,This is an attempt to replicate the RLHF pipeline;   We used bloomz-7b1-mt because of its less-restricted license and multilingual ability.;   For SFT we used a combination of multiple datasets including:;   For RM we used the code of reward-modeling repo and datasets from;   For RL we used the code of trlx with slight modification.
vicuna-13b-v1.1-sft-ver2,Text Generation,PyTorch; Transformers,,,,1,,0,0,,https://huggingface.co/pvduy/vicuna-13b-v1.1-sft-ver2,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; No model card; New: Create and edit this model card directly on the website!"
PlantDiseaseClassifier-Resnet50,Image Classification,PyTorch; TensorBoard; Transformers,,apache-2.0,,2,imagefolder,58,94.74089863,,https://huggingface.co/SanketJadhav/PlantDiseaseClassifier-Resnet50,"This model is a fine-tuned version of microsoft/resnet-50 on the imagefolder dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
AlpacaGPT4-LoRA-13B-elina,Text2Text Generation,,,apache-2.0,,12,,0,105.2149471,,https://huggingface.co/LLMs/AlpacaGPT4-LoRA-13B-elina,"This is LoRA checkpoint fine-tuned with the following CLI. The fine-tuning process is logged in W&B dashboard. I have used DGX workstation with 8 x A100(40G).; This LoRA checkpoint is recommended to be used with transformers >= 4.29 which should be installed with the following command currently(4/30/2023).; Please find out the example usage and results in the accompanied notebook. For the example outputs are listed below as well.; 1. Instruction: Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.; I recently took the journey of my lifetime - a weeklong adventure through one of nature's most beautiful creations: Hawaii! I had heard so much about this magical island paradise before going but nothing could prepare me for how truly breathtaking it was in real life. From hiking along verdant mountain trails to swimming with turtles beneath clear turquoise waters, every moment felt like something out of a fairytale. But above all else, what made our visit to Hawaii so memorable were its rich culture and traditions that we experienced firsthand."
edit-anything-v0-4-sd15,,Transformers,,,,2,,"3,416",1484.802318,1,https://huggingface.co/shgao/edit-anything-v0-4-sd15,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ct2fast-flan-ul2,,Transformers,,apache-2.0,,3,,27,19970.72707,,https://huggingface.co/michaelfeil/ct2fast-flan-ul2,"Speedup inference by 2x-8x using int8 inference in C++; quantized version of google/flan-ul2; Checkpoint compatible to ctranslate2 and hf-hub-ctranslate2; This is just a quantized version. Licence conditions are intended to be idential to original huggingface repo.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
pythia-6.9b-deduped-4k,Text Generation,PyTorch; Transformers,English,apache-2.0,,10,EleutherAI/the_pile_deduplicated,34,27958.15648,,https://huggingface.co/CarperAI/pythia-6.9b-deduped-4k,"Pythia-6.9B Deduped 4K is a Pythia-6.9B Deduped model fine-tuned with a 4096 context length.
Training resumed from their 143,000 step checkpoint and continued on The Pile v1 Deduped (threshold=0.87).
This particular model is from a checkpoint captured at step 175,500 for an extra 134,217,728,000 tokens of training.; Note: Sequence length warmup was not used to move up from 2048 but, in hindsight, should have been applied.; This work would not have been possible without the support of Stability AI."
CounterfeitV30_v30,,,,other,,1,,0,4341.761467,,https://huggingface.co/sunnyweir/CounterfeitV30_v30,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
tokyolagi,,,,creativeml-openrail-m,,1,,0,37.90148251,,https://huggingface.co/darrel99/tokyolagi,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
KoreanLM-LoRA,,,,,,1,,0,8.431779099,,https://huggingface.co/quantumaikr/KoreanLM-LoRA,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
controlnet-seg-room,Image-to-Image,Diffusers,,openrail,,6,,"1,218",723.0043026,3,https://huggingface.co/BertChristiaens/controlnet-seg-room,"Big thanks to Google for lending us TPUv4s to train this model on. Big thanks to the Huggingface and Diffusers team for organising the JAX Diffusers sprint, giving support and making the JAX training scripts. Big thanks to StabilityAI for opensourcing the Stable Diffusion model, it has made a great impact on the community!; To make this demo as good as possible, our team spend a lot of time training a custom model. We used the LAION5B dataset to build our custom dataset, which contains 130k images of 15 types of rooms in almost 30 design styles. After fetching all these images, we started adding metadata such as captions (from the BLIP captioning model) and segmentation maps (from the HuggingFace UperNetForSemanticSegmentation model).; This dataset was then used to train the controlnet model to generate quality interior design images by using the segmentation maps and prompts as conditioning information for the model. By training on segmentation maps, the end user has a very finegrained control over which objects they want to place in their room.
The training started from the lllyasviel/control_v11p_sd15_seg checkpoint, which is a robustly trained controlnet model conditioned on segmentation maps. This checkpoint got fine-tuned on a TPUv4 with the JAX framework. Afterwards, the checkpoint was converted into a PyTorch checkpoint for easy integration with the diffusers library.; Our team made a streamlit demo where you can test out the capabilities of this model.
The resulting model is used in a community pipeline that supports image2image and inpainting, so the user can keep elements of their room and change specific parts of the image.
https://huggingface.co/spaces/controlnet-interior-design/controlnet-seg"
llama7b-wizardlm-unfiltered,Text Generation,PyTorch; Transformers,,,,5,,33,13795.63764,,https://huggingface.co/ausboss/llama7b-wizardlm-unfiltered,No model card; New: Create and edit this model card directly on the website!
segformer-b5-finetuned-human-parsing,Image Segmentation,PyTorch; TensorBoard; Safetensors; Transformers,,other,,4,,"1,116",678.2619928,,https://huggingface.co/matei-dorian/segformer-b5-finetuned-human-parsing,"This model is a fine-tuned version of nvidia/mit-b5 on the None dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
nllb-200-distilled-600M,Translation,ONNX; Transformers.js,,,,3,,"1,024",22.15720211,2,https://huggingface.co/Xenova/nllb-200-distilled-600M,"https://huggingface.co/facebook/nllb-200-distilled-600M with ONNX weights to be compatible with Transformers.js.; Note: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using ? Optimum and structuring your repo like this one (with ONNX weights located in a subfolder named onnx).; Inference API does not yet support transformers.js models for this pipeline type.
							"
LLaVA-Lightning-7B-delta-v1-1,Text Generation,PyTorch; Transformers,,apache-2.0,,9,,148,13814.27986,,https://huggingface.co/liuhaotian/LLaVA-Lightning-7B-delta-v1-1,"NOTE: This ""delta model"" cannot be used directly.Users have to apply it on top of the original LLaMA weights to get actual LLaVA weights.See https://github.com/haotian-liu/LLaVA#llava-weights for instructions.; Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA-Lightning was trained in May 2023.; Paper or resources for more information:
https://llava-vl.github.io/; License:
Apache License 2.0"
Paragon_V1.0,,,,creativeml-openrail-m,,49,,0,26091.52417,,https://huggingface.co/SG161222/Paragon_V1.0,"Please read this!
This model is in the testing phase. The necessary VAE is already baked into the model.; ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
perceiver-ar-sam-giant-midi,Audio-to-Audio,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2202.07765.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/1809.04281.pdf,4,,112,539.0117776,,https://huggingface.co/krasserm/perceiver-ar-sam-giant-midi,"This model is a Perceiver AR symbolic audio model (134M parameters) pretrained on 
the GiantMIDI-Piano dataset for 27 epochs (157M tokens). It uses rotary embedding
for relative position encoding. It is a training example
of the perceiver-io library.; Perceiver AR is a simple extension of a plain decoder-only transformer such as GPT-2, for example. A core building block 
of both is the decoder layer consisting of a self-attention layer followed by a position-wise MLP. Self-attention uses 
a causal attention mask.; Perceiver AR additionally cross-attends to a longer prefix of the input sequence in its first attention layer. This layer
is a hybrid self- and cross-attention layer. Self-attention is over the last n positions of the input sequence, with a 
causal attention mask, cross-attention is from the last n positions to the first m positions. The length of the input 
sequence is m + n. This allows a Perceiver AR to process a much larger context than decoder-only transformers which are 
based on self-attention only.; 

Fig. 1. Attention in Perceiver AR with m=8 prefix tokens and n=3 latent tokens.
; 
"
open_llama_7b_hf,Text Generation,PyTorch; Transformers,,,,3,,63,13804.30235,,https://huggingface.co/quantumaikr/open_llama_7b_hf,No model card; New: Create and edit this model card directly on the website!
Wiki_LLM_82M,Fill-Mask,PyTorch; Transformers,,cc-by-4.0,,1,,24,335.2653136,,https://huggingface.co/Ferrag/Wiki_LLM_82M,
LaMini-GPT-124M,Text Generation,ONNX; Transformers.js,,,,3,,76,3.338683968,,https://huggingface.co/Xenova/LaMini-GPT-124M,"https://huggingface.co/MBZUAI/LaMini-GPT-124M with ONNX weights to be compatible with Transformers.js.; Note: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using ? Optimum and structuring your repo like this one (with ONNX weights located in a subfolder named onnx).; Inference API does not yet support transformers.js models for this pipeline type.
							"
RedPajama-INCITE-7B-Base,Text Generation,PyTorch; Transformers,English,apache-2.0,,76,togethercomputer/RedPajama-Data-1T,"3,014",14133.36267,,https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base,"RedPajama-INCITE-7B-Base was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. 
The training was done on 3,072 V100 GPUs provided as part of the INCITE 2023 project on Scalable Foundation Models for Transferrable Generalist AI, awarded to MILA, LAION, and EleutherAI in fall 2022, with support from the Oak Ridge Leadership Computing Facility (OLCF) and INCITE program. ; Please note that the model requires transformers version >= 4.25.1.; This requires a GPU with 16GB memory.; This requires a GPU with 12GB memory.; To run inference with int8, please ensure you have installed accelerate and bitandbytes. You can install them with the following command:"
RedPajama-INCITE-Base-3B-v1,Text Generation,PyTorch; Transformers,English,apache-2.0,,77,togethercomputer/RedPajama-Data-1T,"6,739",5828.680827,2,https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1,"RedPajama-INCITE-Base-3B-v1 was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. 
The training was done on 3,072 V100 GPUs provided as part of the INCITE 2023 project on Scalable Foundation Models for Transferrable Generalist AI, awarded to MILA, LAION, and EleutherAI in fall 2022, with support from the Oak Ridge Leadership Computing Facility (OLCF) and INCITE program. ; Please note that the model requires transformers version >= 4.25.1.; This requires a GPU with 8GB memory.; To run inference with int8, please ensure you have installed accelerate and bitandbytes. You can install them with the following command:; Then you can run inference with int8 as follows:"
lyraSD,,Diffusers,English,creativeml-openrail-m,,16,,0,0.043024368,,https://huggingface.co/TMElyralab/lyraSD,"lyraSD is currently the fastest Stable Diffusion model available, boasting an inference cost of only 0.435 seconds for a 512x512 image, accelerating the process up to 10 times faster than the original version. ; Among its main features are:; ; ; "
MUSE,,,,,,4,,0,0.001445313,1,https://huggingface.co/nzl-thu/MUSE,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
controlnet-encoded-hands-130k,Image-to-Image,Diffusers,,creativeml-openrail-m,,6,,110,5904.237105,1,https://huggingface.co/MakiPan/controlnet-encoded-hands-130k,"These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with new type of conditioning. You can find some example images in the following. ; prompt: a man in a colorful shirt giving a peace sign in front of a rallying crowd

prompt: a police officer signaling someone to stop in a park
"
rwkv-raven-1b5,Text Generation,PyTorch; Transformers,,,,3,EleutherAI/pile,629,6199.751688,1,https://huggingface.co/RWKV/rwkv-raven-1b5,"; RWKV is a project led by Bo Peng. Learn more about the model architecture in the blogposts from Johan Wind here and here. Learn more about the project by joining the RWKV discord server.; Below is the description from the original repository; RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). It's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, ""infinite"" ctx_len, and free sentence embedding.; The details of the architecture can be found on the blogpost mentioned above and the Hugging Face blogpost of the integration."
wizard-vicuna-13B-GGML,,,,other,,135,,0,118896.6503,,https://huggingface.co/TheBloke/wizard-vicuna-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for June Lee's Wizard Vicuna 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
CatCon-Controlnet-WD-1-5-b2R,Text-to-Image,Diffusers; JAX; PyTorch,,mit,,7,animelover/danbooru2022,62,0,1,https://huggingface.co/Ryukijano/CatCon-Controlnet-WD-1-5-b2R,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; 
Experimental proof of concept made for the Huggingface JAX/Diffusers community sprint; Demo available here
[My teammate's demo is available here] (https://huggingface.co/spaces/Cognomen/CatCon-Controlnet-WD-1-5-b2) ; This is a controlnet for the Stable Diffusion checkpoint Waifu Diffusion 1.5 beta 2 which aims to guide image generation by conditioning outputs with patches of images from a common category of the training target examples. The current checkpoint has been trained for approx. 100k steps on a filtered subset of Danbooru 2021 using artists as the conditioned category with the aim of learning robust style transfer from an image example."
bluemoonrp-13b,Text Generation,Transformers,English,,,37,gozfarb/bluemoon_roleplay_300k_vicuna,523,17265.13258,,https://huggingface.co/reeducator/bluemoonrp-13b,"Bluemoon roleplay finetune of LLaMA 13B (2 roleplayers only).; Two models are provided, labeled (1) 4k-epoch6 and (2) epoch3 (other branch). In case of the (1), the training is extended over more epochs to reduce the high training loss observed in (2). This release also tests a longer 4k context token size achieved with AliBi.; GGML 4-bit for llama.cpp; GPTQ 4-bit CUDA:; This model has been trained using the following prompt (Vicuna 1.1 format):"
kl-f8-anime2-blessed,,,,creativeml-openrail-m,,2,,0,670.0018525,,https://huggingface.co/nubby/kl-f8-anime2-blessed,"These VAEs are modified versions of the kl-f8-anime2 vae.; They have been modified using the VAE-BlessUp script to produce lower contrast images than the original version. ; The number at the end of the filename corresponds to the multiplier used for the contrast level of that version.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
wizard-vicuna-13B-GPTQ,Text Generation,Transformers,English,,,98,,"12,641",7434.929792,,https://huggingface.co/TheBloke/wizard-vicuna-13B-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains 4bit GPTQ format quantised models of  junelee's wizard-vicuna 13B.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Open the text-generation-webui UI as normal.
RedPajama-INCITE-7B-Chat,Text Generation,PyTorch; Transformers,English,apache-2.0,,84,togethercomputer/RedPajama-Data-1T; OpenAssistant/oasst1; databricks/databricks-dolly-15k,"4,812",14133.36132,4,https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat,"RedPajama-INCITE-7B-Chat was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. ; It is fine-tuned on OASST1 and Dolly2 to enhance chatting ability.; Please note that the model requires transformers version >= 4.25.1.; To prompt the chat model, use the following format:; This requires a GPU with 16GB memory."
galactica-6.7b-evol-instruct-70k,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/2304.12244.pdf,13,victor123/evol_instruct_70k,783,28101.14421,2,https://huggingface.co/GeorgiaTechResearchInstitute/galactica-6.7b-evol-instruct-70k,"GALACTICA 6.7B fine-tuned on the Evol-Instruct 70k dataset.; The model card from the original Galactica repo can be found here, and the original paper here.; The HF dataset for Evol-Instruct-70k can be found here, and the original GitHub repo for WizardLM is here.; The GALACTICA models are trained on a large-scale scientific corpus and are designed to perform scientific tasks.; The GALACTICA model card specifies that the primary indended users of the GALACTICA models are researchers studying language models applied to the scientific domain, and it cautions against production use of GALACTICA without safeguards due to the potential for the model to produce inaccurate information.
The original GALACTICA models are available under a non-commercial CC BY-NC 4.0 license, and models based on the Evol-Instruct-70k dataset are additionally subject to the OpenAI Terms of Service."
KoreanLM-hf,Text Generation,PyTorch; Transformers,,,,1,,6,13804.03767,,https://huggingface.co/quantumaikr/KoreanLM-hf,No model card; New: Create and edit this model card directly on the website!
rwkv-4-pileplus,Text Generation,PyTorch,English,apache-2.0,,37,EleutherAI/pile; togethercomputer/RedPajama-Data-1T,0,10416.00216,,https://huggingface.co/BlinkDL/rwkv-4-pileplus,"RWKV-4-pile models finetuning on [RedPajama + some of Pile v2 = 1.7T tokens]. Updated with 2020+2021+2022 data, and better at all European languages.; Although some of these are intermedia checkpoints (XXXGtokens means finetuned for XXXG tokens), you can already use them because I am finetuning from Pile models (instead of retraining).; Note: not instruct tuned yet, and recommended to replace vanilla Pile models.; 7B and 14B coming soon.; See https://github.com/BlinkDL/RWKV-LM for details."
aliceDollMix,,,Japanese,creativeml-openrail-m,,13,,0,5806.091475,,https://huggingface.co/aliceDollMix/aliceDollMix,"Negative:; EasyNegativeV2
https://huggingface.co/gsdf/Counterfeit-V3.0/tree/main/embedding; negative_hand-neg
https://civitai.com/models/56519/negativehand-negative-embedding; The prompt ""photorealistic"" can be used to change the texture of the image as shown above.（上のな画像にプロンプト『photorealistic』を使用することにより|感を浠させることが可能です。）; Adjust the value of ""photorealistic"" based on the condition of the original image to get the desired texture.（元画像の状Bを参考に『photorealistic』のを{整して好みの|感に近づけてください。）"
open_llama_7b_300bt_ggml,,,,apache-2.0,,9,,0,27176.96147,,https://huggingface.co/vihangd/open_llama_7b_300bt_ggml,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
LostJourneyMix,,,,,,8,,0,21012.48635,,https://huggingface.co/Lightofdark/LostJourneyMix,"自己融的几个模型
Just some merge models I played around with.; 2023/07/21 更新V6，偶数模型新作，大概算VX去掉太3D部分的感觉吧
2023/07/11 更新V5 强化版，暂定为奇数模型最终版， LostJourneyMix_X 
2023/06/23 更新了V5，2.5D 风格； added V5, 2.5D model 
懒得更新例图了，反正能找到这里的应该都见过我的模型大概是什么风格 
; V2：融了PileDream之后的厚涂风格; Added PileDream to get impasto style 
配方(recipe)： V1 + PileDream ; V4：类似于V3，在V2的基础上增强了细节和光影效果; Similar to V3, strengthening details, light and shadow effects 
配方(recipe)： V2 + Line and Light ; 下配例图; Sample Images below "
bert-medical-ner,Token Classification,PyTorch; Transformers,,openrail,,5,,924,436.9256965,,https://huggingface.co/ukkendane/bert-medical-ner,Medical documents NER model by fine tuning BERT; widget:
RedPajama-INCITE-Instruct-3B-v1,Text Generation,PyTorch; Transformers,English,apache-2.0,,78,togethercomputer/RedPajama-Data-1T; Muennighoff/P3; Muennighoff/natural-instructions,"9,955",5828.683801,4,https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1,"RedPajama-INCITE-Instruct-3B-v1 was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. ; The model was fine-tuned for few-shot applications on the data of GPT-JT, with exclusion of tasks that overlap with the HELM core scenarios.; Please note that the model requires transformers version >= 4.25.1.; This requires a GPU with 8GB memory.; This requires a GPU with 6GB memory."
RedPajama-INCITE-7B-Instruct,Text Generation,PyTorch; Transformers,English,apache-2.0,,98,togethercomputer/RedPajama-Data-1T; togethercomputer/RedPajama-Data-Instruct,"5,209",14133.36502,1,https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct,"RedPajama-INCITE-7B-Instruct was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. ; The model was fine-tuned for few-shot applications on the data of GPT-JT, with exclusion of tasks that overlap with the HELM core scenarios.; Please note that the model requires transformers version >= 4.25.1.; This requires a GPU with 16GB memory.; This requires a GPU with 12GB memory."
starcoder-GPTQ-8bit-128g,,,,bigcode-openrail-m,https://arxiv.org/pdf/2210.17323.pdf,11,,0,17305.60297,,https://huggingface.co/mayank31398/starcoder-GPTQ-8bit-128g,"Visit GPTQ-for-SantaCoder for instructions on how to use the model weights here.
If you want 4-bit weights, visit starcoder-GPTQ-4bit-128g.; The model is licenses under the CodeML Open RAIL-M v0.1 license. You can find the full license here.; Thanks to everyone in BigCode who worked so hard to create these code models.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ChatMed-Consult,,,,apache-2.0,,6,,0,80.11417152,,https://huggingface.co/michaelwzhu/ChatMed-Consult,"以ChatGPT、GPT-4等为代表的大语言模型（Large Language Model, LLM）掀起了新一轮自然语言处理领域的研究浪潮，展现出了类通用人工智能（AGI）的能力，受到业界广泛关注。; 为推动LLM在中文医疗领域的发展和落地，提升LLM的医疗知识与回答医学咨询的能力，我们现推出ChatMed系列中文医疗大规模语言模型:; Text2DT | 中文医疗大模型评测基准PromptCBLUE | 中文医疗在线问诊数据集ChatMed_Consult_Dataset | 中医药指令数据集ChatMed_TCM_Dataset | 中医药知识图谱; 2023/5/05 开源ChatMed-Consult模型;; 在使用ChatMed-Consult之前，大家需要准备好LlaMA-7b底座模型，详细操作见LlaMA-7b模型准备。"
rwkv-raven-14b,Text Generation,PyTorch; Transformers,,,,41,EleutherAI/pile,352,57929.85424,1,https://huggingface.co/RWKV/rwkv-raven-14b,"; RWKV is a project led by Bo Peng. Learn more about the model architecture in the blogposts from Johan Wind here and here. Learn more about the project by joining the RWKV discord server.; Below is the description from the original repository; RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). It's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, ""infinite"" ctx_len, and free sentence embedding.; The details of the architecture can be found on the blogpost mentioned above and the Hugging Face blogpost of the integration."
rwkv-raven-7b,Text Generation,PyTorch; Transformers,,,,15,EleutherAI/pile,407,30353.52299,,https://huggingface.co/RWKV/rwkv-raven-7b,"; RWKV is a project led by Bo Peng. Learn more about the model architecture in the blogposts from Johan Wind here and here. Learn more about the project by joining the RWKV discord server.; Below is the description from the original repository; RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). It's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, ""infinite"" ctx_len, and free sentence embedding.; The details of the architecture can be found on the blogpost mentioned above and the Hugging Face blogpost of the integration."
Zenev,,,,,,3,,0,14807.04145,,https://huggingface.co/mycraft/Zenev,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
GPT4All-13B-snoozy-GPTQ,Text Generation,Transformers,English,gpl,,26,nomic-ai/gpt4all-j-prompt-generations,183,7436.579352,,https://huggingface.co/TheBloke/GPT4All-13B-snoozy-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains 4bit GPTQ format quantised models of Nomic.AI's GPT4all-13B-snoozy.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Open the text-generation-webui UI as normal.
ReedMix,,,,cc-by-nc-4.0,,4,,0,10885.12401,,https://huggingface.co/P01yH3dr0n/ReedMix,"几个自己常用的融合的模型，成分写在末尾了，不过比例不记得了。; Some of my merged models, the ingredients are at last of readme, though I forgot the proportions.; 推荐参数; （其实也没什么推荐的，想怎么用怎么用，这只是我常用的设置）; Recommended parameters"
santacoder-finetuned-Shadertoys-fine,Text Generation,PyTorch; Transformers,code,bigcode-openrail-m,,1,bigcode/the-stack-dedup; Vipitis/Shadertoys-fine,336,4712.519833,2,https://huggingface.co/Vipitis/santacoder-finetuned-Shadertoys-fine,"Santacoder finetuned on Shadertoys-fine for 1000 steps with a batch size of 2 and full sequence length of 2048.
adapted finetuning script found here; Try model in the ShaderCoder demo space; Main purpose of this model is to explore if finetuning models improves performance on ShaderEval, which reached 0.567 with 300 samples and 0.59749 on all samples.; While the train/test split is held out, there is a lot of data contamination. The model results can't be trusted for this simple benchmark.
Better tasks for the benchmark will be developed and tested against these models.; License carried over from model, however training data has an undefied license. Check details in Shadertoys."
gpt4-x-vicuna-13B-GPTQ,Text Generation,Transformers,,other,,33,,224,7436.578222,,https://huggingface.co/TheBloke/gpt4-x-vicuna-13B-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains 4bit GPTQ format quantised models of NousResearch's gpt4-x-vicuna-13b.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Open the text-generation-webui UI as normal.
gpt4-x-vicuna-13B-GGML,,,,other,,88,,0,119316.4926,,https://huggingface.co/TheBloke/gpt4-x-vicuna-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for NousResearch's GPT4-x-Vicuna-13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
logo-diffusion-checkpoint,Text-to-Image,Diffusers,,creativeml-openrail-m,,10,logo-wizard/modern-logo-dataset,"2,421",2.093185997,2,https://huggingface.co/logo-wizard/logo-diffusion-checkpoint,"This is checkpoint based on stabilityai/stable-diffusion-2-1 and logo-wizard/logo-diffusion. The weights were fine-tuned on the logo-wizard/modern-logo-dataset dataset. You can find some example images in the following. ; We recommend using this model with the following prompt template:positive: f""a logo of {company industry}, {some objects}, {colors}, modern, minimalism, vector art, 2d, best quality, centered""negative: ""low quality, worst quality, bad composition, extra digit, fewer digits, text, inscription, watermark, label, asymmetric""  ; Some other recommendations:num_inference_steps = 30guidance_scale = 7.5height = 768width = 768scheduler = diffusers.EulerAncestralDiscreteScheduler (used by default); "
santacoder-finetuned-Shadertoys,Text Generation,PyTorch; Transformers,code,bigcode-openrail-m,,1,bigcode/the-stack-dedup; Vipitis/Shadertoys,14,4712.519667,1,https://huggingface.co/Vipitis/santacoder-finetuned-Shadertoys,"Santacoder finetuned on Shadertoys for 1000 steps with a batch size of 2 and full sequence length of 2048.
adapted finetuning script found here; Try model in the ShaderCoder demo space; Main purpose of this model is to explore if finetuning models improves performance on ShaderEval, which reached 0.550 with 300 samples.; While the train/test split is held out, there is a lot of data contamination. The model results can't be trusted for this simple benchmark.
Better tasks for the benchmark will be developed and tested against these models.; License carried over from model, however training data has an undefied license. Check details in Shadertoys."
ai-face,Image-to-Image,Diffusers,,bigscience-openrail-m,,2,,528,2181.122446,,https://huggingface.co/Lobbb08/ai-face,Test the concept via A1111 Colab fast-Colab-A1111; Sample pictures of this concept:
coreml-ghostmix_v12,Text-to-Image,Core ML,,creativeml-openrail-m,,7,,0,0.005390625,,https://huggingface.co/coreml/coreml-ghostmix_v12,"Source(s): CivitAI; If you like my model，please give me 5 Stars ,it will encourage me a lot. Thanks!; Color Problem ：Check VAE is kl-f8-anime2 ？颜色问题：查VAE是否为kl-f8-anime2？; GhostMix V1.2 is an absolutely astonishing model, and I think it is the strongest 2.5D model in Civitai right now. I think a update of the model should improve the model’s compatibility, good image rate and image details given the main structure of 90% generated images doesn’t change. So I use layer combination to combine models layer by layer. And I got this model after 9 different versions of “the final version of GhostMix V1.2”,LOL.This version of GhostMix V1.2 is a balance in terms of compatibility, good image rate and image details, although sometimes using the same Promts of GhostMix V1.1, may comes up a different result.But this doesn’t happen too much.; PS：GhostMixV1.2的头图是致敬1995押井守版Ghost in Shell 攻壳机动队的生成的，Ghost in Shell也是作者名字的由来。
  "
GPT4-x-Vicuna-13b-4bit,Text Generation,Transformers,English,gpl,,53,,"1,589",7639.532704,1,https://huggingface.co/NousResearch/GPT4-x-Vicuna-13b-4bit,"This is the GPTQ 4Bit Groupsize 128 Pre-Quantized Model, for the full model in fp32, visit https://huggingface.co/NousResearch/gpt4-x-vicuna-13b; As a base model used https://huggingface.co/eachadea/vicuna-13b-1.1; Finetuned on Teknium's GPTeacher dataset, Teknium's unreleased Roleplay v2 dataset, WizardLM Uncensored, GPT-4-LLM Uncensored, and Nous Research Instruct Dataset; Approx 180k instructions, all from GPT-4, all cleaned of any OpenAI censorship/""As an AI Language Model"" etc.; Base model still has OpenAI censorship. Soon, a new version will be released with cleaned vicuna from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltere"
speecht5_tts_commonvioce_zh,,PyTorch; TensorBoard; Transformers,,,,1,,1,585.240455,,https://huggingface.co/YeQing/speecht5_tts_commonvioce_zh,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
LLaVA-Lightning-MPT-7B-preview,Text Generation,PyTorch; Transformers,,cc-by-nc-sa-4.0,,29,,"2,703",13621.33116,,https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview,"NOTE: This is a research preview of the LLaVA-Lightning based on MPT-7B-chat checkpoint. The usage of the model should comply with MPT-7B-chat license and agreements.; NOTE: Unlike other LLaVA models, this model can (should) be used directly without delta weights conversion!; Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna/MPT on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA-Lightning-MPT was trained in May 2023.; Paper or resources for more information:
https://llava-vl.github.io/"
santacoder-finetuned-the-stack-glsl,Text Generation,PyTorch; Transformers,code,bigcode-openrail-m,,2,bigcode/the-stack-dedup,13,4712.519315,1,https://huggingface.co/Vipitis/santacoder-finetuned-the-stack-glsl,"Santacoder finetuned on The-Stack-dedup (GLSL subset) for 1000 steps with a batch size of 2 and full sequence length of 2048.
adapted finetuning script found here; Main purpose of this model is to explore if finetuning models improves performance on ShaderEval, which reached 0.380 with 300 samples.; License carried over from model, and the finetuning dataset holds the same license."
ModerationGPT,Text2Text Generation,PyTorch; Safetensors; Transformers,,apache-2.0,,1,,34,486.4347872,,https://huggingface.co/abhiai/ModerationGPT,"This model is a fine-tuned version of t5-small on the None dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
xor-tydi-docTquery-mt5-large,Text2Text Generation,PyTorch; Safetensors; Transformers,,apache-2.0,https://arxiv.org/pdf/2206.10128.pdf; https://arxiv.org/pdf/2305.03950.pdf,2,,260,10080.4759,,https://huggingface.co/ielabgroup/xor-tydi-docTquery-mt5-large,"mT5-large query generation model that is trained with XOR QA data.; Used in paper Bridging the Gap Between Indexing and Retrieval for
Differentiable Search Index with Query Generation; and Augmenting Passage Representations with Query Generation
for Enhanced Cross-Lingual Dense Retrieval"
rvc_models,,,,,,2,,0,2300.307109,,https://huggingface.co/trioskosmos/rvc_models,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
pythia-12b-sft-v8-7k-steps,Text Generation,PyTorch; Transformers,English,apache-2.0,,21,,"5,717",24383.61225,1,https://huggingface.co/OpenAssistant/pythia-12b-sft-v8-7k-steps,
better_loras,,,,other,,3,,0,843.6014672,,https://huggingface.co/leonyuri/better_loras,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
mpt-7b-storywriter-4bit-128g,Text Generation,Safetensors; Transformers,,apache-2.0,https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2205.14135.pdf,17,the_pile_books3,104,3965.090584,,https://huggingface.co/4bit/mpt-7b-storywriter-4bit-128g,"MPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths.
It was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset.
At inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.
We demonstrate generations as long as 84k tokens on a single node of 8 A100-80GB GPUs in our blogpost.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; Apache-2.0 (commercial use permitted); Note: This model requires that trust_remote_code=True be passed to the from_pretrained method. This is because we use a custom model architecture that is not yet part of the transformers package."
segformer-b0-finetuned-food,Image Segmentation,PyTorch; Transformers,English,apache-2.0,,2,,434,15.00738731,,https://huggingface.co/prem-timsina/segformer-b0-finetuned-food,
xlm-roberta_punctuation_fullstop_truecase,Text2Text Generation,ONNX; NeMo,47 languages,apache-2.0,,7,,"3,357",2288.640684,,https://huggingface.co/1-800-BAD-CODE/xlm-roberta_punctuation_fullstop_truecase,"This is an xlm-roberta fine-tuned to restore punctuation, true-case (capitalize), 
and detect sentence boundaries (full stops) in 47 languages.; If you want to just play with the model, the widget on this page will suffice. To use the model offline,
the following snippets show how to use the model both with a wrapper (that I wrote, available from PyPI)
and manual usuage (using the ONNX and SentencePiece models in this repo).; The easiest way to use this model is to install punctuators:; But this is just an ONNX and SentencePiece model, so you may run it as you wish.; The input to the punctuators API is a list (batch) of strings. 
Each string will be punctuated, true-cased, and segmented on predicted full stops.
The output will therefore be a list of list of strings: one list of segmented sentences per input text.
To disable full stops, use m.infer(texts, apply_sbd=False). 
The output will then be a list of strings: one punctuated, true-cased string per input text."
RedPajama-INCITE-Chat-3B-v1-GGML,Text Generation,Transformers,English,apache-2.0,,21,togethercomputer/RedPajama-Data-1T; OpenAssistant/oasst1; databricks/databricks-dolly-15k,79,3924.03424,,https://huggingface.co/keldenl/RedPajama-INCITE-Chat-3B-v1-GGML,"Original Model Link: https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1; This will NOT work with llama.cpp as of 5/13/2023, but this NOW works (5/13/2023) with the GGML in https://github.com/ggerganov/ggml/ via gpt-neox
This also works in my project https://github.com/keldenl/gpt-llama.cpp (uses ggml as an InferenceEngine).; RedPajama-INCITE-Chat-3B-v1 was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. ; It is fine-tuned on OASST1 and Dolly2 to enhance chatting ability.; To prompt the chat model, use the following format:"
RedPajama-INCITE-Instruct-3B-v1-GGML,Text Generation,Transformers,English,apache-2.0,,10,togethercomputer/RedPajama-Data-1T; Muennighoff/P3; Muennighoff/natural-instructions,31,3924.034767,,https://huggingface.co/keldenl/RedPajama-INCITE-Instruct-3B-v1-GGML,"Original Model Link: https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1; This will NOT work with llama.cpp as of 5/13/2023, but this NOW works (5/13/2023) with the GGML in https://github.com/ggerganov/ggml/ via gpt-neox
This also works in my project https://github.com/keldenl/gpt-llama.cpp (uses ggml as an InferenceEngine).; RedPajama-INCITE-Instruct-3B-v1 was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Université de Montréal, MILA - Québec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. ; The model was fine-tuned for few-shot applications on the data of GPT-JT, with exclusion of tasks that overlap with the HELM core scenarios.; To prompt the chat model, use a typical instruction format + few shot prompting, for example:"
DucHaiten-MindBreak,,,,creativeml-openrail-m,,1,,0,6082.561483,,https://huggingface.co/DucHaiten/DucHaiten-MindBreak,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
pegasus-x-large-4096-pubmed,Text2Text Generation,PyTorch; Transformers,,,,1,,215,2332.99668,,https://huggingface.co/twigs/pegasus-x-large-4096-pubmed,No model card; New: Create and edit this model card directly on the website!
mmarco-mMiniLMv2-L12-H384-v1,Text Classification,PyTorch; Transformers,15 languages,apache-2.0,,1,unicamp-dl/mmarco,32,488.1167551,,https://huggingface.co/RubenAMtz/mmarco-mMiniLMv2-L12-H384-v1,"This model was trained on the MMARCO dataset. It is a machine translated version of MS MARCO using Google Translate. It was translated to 14 languages. In our experiments, we observed that it performs also well for other languages.; As a base model, we used the multilingual MiniLMv2 model.; The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco; The usage becomes easy when you have SentenceTransformers installed. Then, you can use the pre-trained models like this:"
oldfish,,,,,,1,,0,6277.121445,,https://huggingface.co/asj319/oldfish,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
bert-base-uncased_finetuned_sentiments,Text Classification,PyTorch; Transformers,English,apache-2.0,,1,custom,46,438.233507,,https://huggingface.co/RinInori/bert-base-uncased_finetuned_sentiments,"This model is a fine-tuned version of the BERT ForSequenceClassification model for sentiment analysis. 
It is trained on a dataset of texts with six different emotions: anger, fear, joy, love, sadness, and surprise.
The model was trained and tested on a labeled dataset from Kaggle.; Github link:
https://github.com/hennypurwadi/Bert_FineTune_Sentiment_Analysis; The labeled dataset I used to fine-tune and train the model can be found at: 
https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp?select=train.txt; The model was trained and tested on a labeled dataset from Kaggle.; ##To predict the sentiments on unlabeled datasets, use the predict_sentiments function provided in this repository."
ParaphraseGPT,Text2Text Generation,PyTorch; Transformers,English,apache-2.0,,2,humarin/chatgpt-paraphrases,155,892.0043542,,https://huggingface.co/sharad/ParaphraseGPT,This model re-fine-tunes the ChatGPT Paraphraser on T5 Base with additional Google PAWS dataset.
chinese-llama-plus-lora-13b,,,Chinese,apache-2.0,,16,,0,1116.902614,,https://huggingface.co/ziqingyang/chinese-llama-plus-lora-13b,"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
chinese-alpaca-plus-lora-13b,,,Chinese,apache-2.0,,36,,0,1557.22264,,https://huggingface.co/ziqingyang/chinese-alpaca-plus-lora-13b,"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
BiLLa-7B-SFT,,PyTorch; Transformers,,apache-2.0,,65,,66,14138.31151,1,https://huggingface.co/Neutralzz/BiLLa-7B-SFT,"BiLLa is an open-source reasoning-enhanced bilingual LLaMA model. The main features are:; Github: https://github.com/Neutralzz/BiLLa; Note: Due to LLaMA's license, the model weights in this hub cannot be used directly. 
The weight of word embedding is the sum of the weights of the trained model and the original LLaMA, 
so as to ensure that developers with LLaMA original model accessibility can convert the model released by this hub into a usable one.; First, you can revert the model weights by this script:; Then, you can run this model as follows:"
stable-diffusion-v1-5-img2img,Image-to-Image,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/2207.12598.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/1910.09700.pdf,6,,"2,375",24514.57811,3,https://huggingface.co/radames/stable-diffusion-v1-5-img2img,"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
For more information about how Stable Diffusion functions, please have a look at ?'s Stable Diffusion blog.; The Stable-Diffusion-v1-5 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 
checkpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on ""laion-aesthetics v2 5+"" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.; You can use this both with the ?Diffusers library and the RunwayML GitHub repository.; For more detailed instructions, use-cases and examples in JAX follow the instructions here; Download the weights "
vit_large_patch14_dinov2.lvd142m,Image Classification,PyTorch; Safetensors; Timm,,cc-by-nc-4.0,https://arxiv.org/pdf/2304.07193.pdf; https://arxiv.org/pdf/2010.11929.pdf,3,,"7,223",2498.565939,2,https://huggingface.co/timm/vit_large_patch14_dinov2.lvd142m,A Vision Transformer (ViT) image feature model. Pretrained on LVD-142M with self-supervised DINOv2 method.; Explore the dataset and runtime metrics of this model in timm model results.
larissa,,,,creativeml-openrail-m,,1,,0,37.90148251,,https://huggingface.co/AZZLI/larissa,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
WizardLM-13B-Uncensored-Q5_1-GGML,,,,,,32,,0,9994.241575,,https://huggingface.co/TehVenom/WizardLM-13B-Uncensored-Q5_1-GGML,"Quantization of this model:; https://huggingface.co/ehartford/WizardLM-13B-Uncensored/; According to the parameters listed in the title.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
sunyanzi,,Transformers,,openrail,,7,,27,557.4034525,1,https://huggingface.co/YeQing/sunyanzi,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
OpenGen,Text-to-Image,Diffusers,English,creativeml-openrail-m,,9,,0,9748.558213,1,https://huggingface.co/darkstorm2150/OpenGen,"Research Model by darkstorm2150; OpenGen is a continuation from Protogen model, this model was handcrafted by a selection from what I personally consider the best models currently available, the licensing continues its respective formal bindings from its previous merges.; Granular adaptive learning is a machine learning technique that focuses on adjusting the learning process at a fine-grained level, rather than making global adjustments to the model. This approach allows the model to adapt to specific patterns or features in the data, rather than making assumptions based on general trends.; Granular adaptive learning can be achieved through techniques such as active learning, which allows the model to select the data it wants to learn from, or through the use of reinforcement learning, where the model receives feedback on its performance and adapts based on that feedback. It can also be achieved through techniques such as online learning where the model adjust itself as it receives more data.; Granular adaptive learning is often used in situations where the data is highly diverse or non-stationary and where the model needs to adapt quickly to changing patterns. This is often the case in dynamic environments such as robotics, financial markets, and natural language processing."
rvc_voces,,,,mit,,1,,0,127.0014653,,https://huggingface.co/juuxn/rvc_voces,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
fantassified_icons_v2,Text-to-Image,Diffusers,English,other,,29,,385,4368.989584,1,https://huggingface.co/proximasanfinetuning/fantassified_icons_v2,"; 

; if you enjoy this consider buying me a coffee (ノ?ヮ?)ノ*:???
; How to use it with diffusers; This model is licensed under a modified CreativeML OpenRAIL-M license."
wizardlm-13b-unbiased-lora,,,,,,2,,0,52.50177814,,https://huggingface.co/ausboss/wizardlm-13b-unbiased-lora,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
vits-aishell3-175-chinese,,Transformers,Chinese,apache-2.0,,11,,34,1039.004043,,https://huggingface.co/jackyqs/vits-aishell3-175-chinese,aishell3数据介绍:; 希尔贝壳中文普通话语音数据库AISHELL-3的语音时长为85小时88035句，可做为多说话人合成系统。录制过程在安静室内环境中， 使用高保真麦克风（44.1kHz，16bit）。; 218名来自中国不同口音区域的发言人参与录制。专业语音校对人员进行拼音和韵律标注，并通过严格质量检验，此数据库音字确率在98%以上。; vits模型介绍：; 这是一个基于vits_chinese和aishell3 175人中文训练的预训练模型，可以直接用于微调语音克隆，大大缩短微调训练的时间。
MPT-7b-WizardLM_Uncensored-Storywriter-Merge,Text Generation,PyTorch; Transformers,,,,23,,181,13600.93975,1,https://huggingface.co/TehVenom/MPT-7b-WizardLM_Uncensored-Storywriter-Merge,#TODO: Model card.; Temp:; Merge of the Long context Storywriter with the instruct based WizardLM_Unfiltered:
gpt4all-mpt,Text Generation,PyTorch; Transformers,English,apache-2.0,,10,nomic-ai/gpt4all-j-prompt-generations,153,27271.34444,,https://huggingface.co/nomic-ai/gpt4all-mpt,"An Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions including word problems, multi-turn dialogue, code, poems, songs, and stories.; This model has been finetuned from MPT 7B; This model was trained on nomic-ai/gpt4all-j-prompt-generations using revision=v1.3-groovy; Results on common sense reasoning benchmarks"
models,,,,creativeml-openrail-m,,1,,0,14576.45909,,https://huggingface.co/JCTN/models,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
T5-large-sentiment-analysis-Chinese-MultiTask,Text2Text Generation,PyTorch; Transformers,Chinese; English,,,5,Yaxin/SemEval2016Task5NLTK,131,3218.343467,,https://huggingface.co/yuyijiong/T5-large-sentiment-analysis-Chinese-MultiTask,情感分析任务 T5模型yuyijiong/T5-large-sentiment-analysis-Chinese的改进版，增加更多任务，使用chatgpt生成部分数据在多个中英文情感分析数据集上微调得到 输出格式为 ; 可以使用yuyijiong/quad_match_score评估指标进行评估; 支持以下情感分析任务; 可以增加额外条件来控制答案的生成，例如：; 答案风格控制，希望抽取的观点为整句话or缩减为几个词：(观点尽量短)(观点可以较长)(对较长观点进行概括) 注意此条件可能使答案中出现与原文不同的词
4x_NMKD-Siax_200k,,,,,,8,,0,67.00144531,,https://huggingface.co/gemasai/4x_NMKD-Siax_200k,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
yuyi_llm_verson1,Text Generation,PyTorch; Transformers,,,,1,,5,0.524147148,,https://huggingface.co/Linbo/yuyi_llm_verson1,No model card; New: Create and edit this model card directly on the website!
ggml-gpt4-x-vicuna-13b,Conversational,,,,,12,,0,27494.40235,,https://huggingface.co/eachadea/ggml-gpt4-x-vicuna-13b,"ggml version (post-PR #1405); As a base model used https://huggingface.co/eachadea/vicuna-13b-1.1; Finetuned on Teknium's GPTeacher dataset, unreleased Roleplay v2 dataset, GPT-4-LLM dataset, and Nous Research Instruct Dataset; Approx 180k instructions, all from GPT-4, all cleaned of any OpenAI censorship/""As an AI Language Model"" etc.; Base model still has OpenAI censorship. Soon, a new version will be released with cleaned vicuna from https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltere"
airoboros-gpt-3.5-turbo-100k-7b,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,,8,,156,27597.60978,1,https://huggingface.co/jondurbin/airoboros-gpt-3.5-turbo-100k-7b,"This is a 7b parameter, fine-tuned on 100k synthetic instruction/response pairs generated by gpt-3.5-turbo using my version of self-instruct airoboros; Context length for this model is 2048.; Links:; The instructions.jsonl file was converted to conversation style expected by the FastChat training scripts, and then trained with:; Training took roughly 22 hours on 8x nvidia A100 80GB."
StableSR,Image-to-Image,,,other,https://arxiv.org/pdf/2305.07015.pdf,21,,0,18042.48526,,https://huggingface.co/Iceclear/StableSR,"This model card focuses on the models associated with the StableSR, available here.; Developed by: Jianyi Wang; Model type: Diffusion-based image super-resolution model; License: S-Lab License 1.0; Model Description: This is the model used in Paper."
chimera-inst-chat-13b-gptq-4bit,Text Generation,PyTorch; Transformers,,apache-2.0,,6,,34,7434.764338,1,https://huggingface.co/Yhyu13/chimera-inst-chat-13b-gptq-4bit,GPTQ 4-bit no actor version for compatibility that works in textgen-webui; Generated by using scripts from https://gitee.com/yhyu13/llama_-tools ; Delta weights: https://huggingface.co/FreedomIntelligence/chimera-inst-chat-13b-delta; Original hf weights: https://huggingface.co/Yhyu13/chimera-inst-chat-13b-hf; Sample conversation generated in textgen-webui:
pygmalion-7b-ggml-q5_1,,,,other,,4,,0,5181.441583,,https://huggingface.co/waifu-workshop/pygmalion-7b-ggml-q5_1,Pygmalion 7B model card; Quantized from ggml f32.; Inference API has been turned off for this model.
SeViLA,,,,openrail,,2,,0,854.0014701,1,https://huggingface.co/Shoubin/SeViLA,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
TinyStories-33M,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2305.07759.pdf,32,roneneldan/TinyStories,"4,423",294.3387965,3,https://huggingface.co/roneneldan/TinyStories-33M,"Model trained on the TinyStories Dataset, see https://arxiv.org/abs/2305.07759; Based on GPT-Neo architecture.; License: mit; ------ EXAMPLE USAGE ---; from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
TinyStories-28M,Text Generation,PyTorch; Transformers,,,,5,,"1,221",245.3389213,2,https://huggingface.co/roneneldan/TinyStories-28M,No model card; New: Create and edit this model card directly on the website!
TinyStories-Instruct-1M,Text Generation,PyTorch; Transformers,,,,2,,416,51.93893749,,https://huggingface.co/roneneldan/TinyStories-Instruct-1M,No model card; New: Create and edit this model card directly on the website!
ChatFlow-7B,,,,gpl-3.0,,5,,0,13824.48983,,https://huggingface.co/Linly-AI/ChatFlow-7B,"Model weights in TencentPretrain format; How to use: https://github.com/CVI-SZU/Linly; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
GPT4-x-MedOAlpacino-13B,,,,,,5,,0,9994.242081,,https://huggingface.co/digitous/GPT4-x-MedOAlpacino-13B,(Alpacino-13B + (MedAlpaca-13B + (GPT4-x-Alpaca-13B + OASST-LLaMa-13B))); Alpacino-13B: https://huggingface.co/digitous/Alpacino13b; GPT4-x-Alpaca-13B: https://huggingface.co/chavinlo/gpt4-x-alpaca; OASST-LLaMa-13B: https://huggingface.co/dvruette/oasst-llama-13b-2-epochs; MedAlpaca-13B: https://huggingface.co/medalpaca/medalpaca-13b
gpt-j_rm_format-oa,Text Classification,PyTorch; Transformers,English,apache-2.0,,1,,"9,404",12086.53803,,https://huggingface.co/reciprocate/gpt-j_rm_format-oa,GPT-J for preference modeling; Usage:; Output:
codet5p-220m,Text2Text Generation,PyTorch; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2305.07922.pdf,10,,"230,833",446.9939863,1,https://huggingface.co/Salesforce/codet5p-220m,"CodeT5+ is a new family of open code large language models with an encoder-decoder architecture that can flexibly operate in different modes (i.e. encoder-only, decoder-only, and encoder-decoder) to support a wide range of code understanding and generation tasks. 
It is introduced in the paper:; CodeT5+: Open Code Large Language Models for Code Understanding and Generation
by Yue Wang*, Hung Le*, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi (* indicates equal contribution).; Compared to the original CodeT5 family (base: 220M, large: 770M), CodeT5+ is pretrained with a diverse set of pretraining tasks including span denoising, causal language modeling, contrastive learning, and text-code matching to learn rich representations from both unimodal code data and bimodal code-text data. 
Additionally, it employs a simple yet effective compute-efficient pretraining method to initialize the model components with frozen off-the-shelf LLMs such as CodeGen to efficiently scale up the model (i.e. 2B, 6B, 16B), and adopts a ""shallow encoder and deep decoder"" architecture. 
Furthermore, it is instruction-tuned to align with natural language instructions (see our InstructCodeT5+ 16B) following Code Alpaca.  ; This model can be easily loaded using the T5ForConditionalGeneration functionality and employs the same tokenizer as original CodeT5.; This checkpoint is trained on the stricter permissive subset of the deduplicated version of the github-code dataset.
The data is preprocessed by reserving only permissively licensed code (""mit"" “apache-2”, “bsd-3-clause”, “bsd-2-clause”, “cc0-1.0”, “unlicense”, “isc”).
Supported languages (9 in total) are as follows:
c, c++, c-sharp,  go, java, javascript,  php, python, ruby."
RVC_Kyoukai_No_Kanata,,,,,,1,,0,0.001531067,,https://huggingface.co/baguss/RVC_Kyoukai_No_Kanata,"700 step; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
abstract-sim-query,Feature Extraction,PyTorch; Transformers,English,,,9,biu-nlp/abstract-sim,475,438.9292969,,https://huggingface.co/biu-nlp/abstract-sim-query,"A model for mapping abstract sentence descriptions to sentences that fit the descriptions. Trained on Wikipedia. Use load_finetuned_model to load the query and sentence encoder, and encode_batch() to encode a sentence with the model.; Note: the method uses a dual encoder architecture. This is the query encoder; it should be used alongside the sentence encoder.; Usage example:; Expected output:"
h2ogpt-research-oasst1-llama-65b,Text Generation,PyTorch; Transformers,English,other,,8,h2oai/openassistant_oasst1_h2ogpt_graded,"3,167",134550.9705,7,https://huggingface.co/h2oai/h2ogpt-research-oasst1-llama-65b,"H2O.ai's h2ogpt-research-oasst1-llama-65b is a 65 billion parameter instruction-following large language model (NOT licensed for commercial use).; To use the model with the transformers library on a machine with GPUs, first make sure you have the following libraries installed.; Alternatively, if you prefer to not use trust_remote_code=True you can download instruct_pipeline.py,
store it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:; Model validation results using EleutherAI lm-evaluation-harness.; TBD"
random-animals-birds,Text-to-Image,Diffusers; PyTorch,,creativeml-openrail-m,,2,,119,1.999986267,,https://huggingface.co/DeadBeast/random-animals-birds,"This is a Stable Diffusion model fine-tuned on the animal concept with DreamBooth. It can be used by modifying the instance_prompt: a photo of animal dog; This is a Stable Diffusion model fine-tuned on random animal,birds unsplash images for the animals theme."
pygmalion-7b-4bit-128g-cuda-2048Token,Text Generation,PyTorch; Transformers,English,,,12,,578,4098.36217,,https://huggingface.co/AnimusOG/pygmalion-7b-4bit-128g-cuda-2048Token,"Quantized from https://huggingface.co/Neko-Institute-of-Science/pygmalion-7b; language:; Pygmalion 7B is a dialogue model based on Meta's LLaMA-7B.; This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project.; The model weights in this repository cannot be used as-is. The files here are XORs due to licensing concerns. To obtain proper, usable model weights you need to:"
Indigo_Furry_mix,,,,creativeml-openrail-m,,16,,0,48711.68148,,https://huggingface.co/uhralk/Indigo_Furry_mix,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
t5-russian-summarization-detox-finetuning,Text2Text Generation,PyTorch; TensorBoard; Transformers,,,,1,,13,895.6414284,,https://huggingface.co/zaaabik/t5-russian-summarization-detox-finetuning,No model card; New: Create and edit this model card directly on the website!
gpt-2-ggml,,,,mit,,8,,873,251.0017419,,https://huggingface.co/marella/gpt-2-ggml,"See https://github.com/marella/ctransformers; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
whisper-small-ru-1k-steps-ggml,,,,,,1,,0,488.0020833,,https://huggingface.co/sBPOH/whisper-small-ru-1k-steps-ggml,"language:; https://huggingface.co/sanchit-gandhi/whisper-small-ru-1k-steps; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
open-calm-small,Text Generation,PyTorch; Transformers,Japanese,cc-by-sa-4.0,,12,wikipedia; cc100; mc4,"6,662",384.2360891,1,https://huggingface.co/cyberagent/open-calm-small,"OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.; Ryosuke Ishigami; Inference API has been turned off for this model."
open-calm-3b,Text Generation,PyTorch; Transformers,Japanese,cc-by-sa-4.0,,10,wikipedia; cc100; mc4,"5,068",5840.036076,2,https://huggingface.co/cyberagent/open-calm-3b,"OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.; Ryosuke Ishigami; Inference API has been turned off for this model."
codet5p-770m-py,Text2Text Generation,PyTorch; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2305.07922.pdf,15,,"2,793",1516.514401,,https://huggingface.co/Salesforce/codet5p-770m-py,"CodeT5+ is a new family of open code large language models with an encoder-decoder architecture that can flexibly operate in different modes (i.e. encoder-only, decoder-only, and encoder-decoder) to support a wide range of code understanding and generation tasks. 
It is introduced in the paper:; CodeT5+: Open Code Large Language Models for Code Understanding and Generation
by Yue Wang*, Hung Le*, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi (* indicates equal contribution).; Compared to the original CodeT5 family (base: 220M, large: 770M), CodeT5+ is pretrained with a diverse set of pretraining tasks including span denoising, causal language modeling, contrastive learning, and text-code matching to learn rich representations from both unimodal code data and bimodal code-text data. 
Additionally, it employs a simple yet effective compute-efficient pretraining method to initialize the model components with frozen off-the-shelf LLMs such as CodeGen to efficiently scale up the model (i.e. 2B, 6B, 16B), and adopts a ""shallow encoder and deep decoder"" architecture. 
Furthermore, it is instruction-tuned to align with natural language instructions (see our InstructCodeT5+ 16B) following Code Alpaca.  ; This model can be easily loaded using the T5ForConditionalGeneration functionality and employs the same tokenizer as original CodeT5.; This checkpoint is trained on the stricter permissive subset of the deduplicated version of the github-code dataset.
The data is preprocessed by reserving only permissively licensed code (""mit"" “apache-2”, “bsd-3-clause”, “bsd-2-clause”, “cc0-1.0”, “unlicense”, “isc”).
Supported languages (9 in total) are as follows:
c, c++, c-sharp,  go, java, javascript,  php, python, ruby."
wizard-mega-13B-GPTQ,Text Generation,Transformers,English,other,,99,anon8231489123/ShareGPT_Vicuna_unfiltered; ehartford/wizard_vicuna_70k_unfiltered; ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered,"1,099",7631.138535,3,https://huggingface.co/TheBloke/wizard-mega-13B-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains 4bit GPTQ format quantised models of OpenAccess AI Collective's Wizard Mega 13B.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Open the text-generation-webui UI as normal.
cetusmix,Text-to-Image,Diffusers,,creativeml-openrail-m,,10,,279,0.004340782,24,https://huggingface.co/stablediffusionapi/cetusmix,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""cetusmix""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Model link: View model"
nsfw-waifu-diffusion,Text-to-Image,Diffusers,English,creativeml-openrail-m,,9,,875,0,3,https://huggingface.co/gisohi6975/nsfw-waifu-diffusion,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.; ; masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck"
saiga_13b_lora_llamacpp,Text2Text Generation,,Russian,,,11,IlyaGusev/ru_turbo_alpaca; IlyaGusev/ru_turbo_saiga; IlyaGusev/ru_sharegpt_cleaned,0,32460.80225,2,https://huggingface.co/IlyaGusev/saiga_13b_lora_llamacpp,Llama.cpp compatible versions of an original 13B model.; How to run:; System requirements:; Inference API has been turned off for this model.
mpt-7b-storywriter-4bit-128g-65kTokens-CPU,Text Generation,Transformers,,,,6,,63,3965.085545,1,https://huggingface.co/AnimusOG/mpt-7b-storywriter-4bit-128g-65kTokens-CPU,"THIS MODEL IS NOT QUITE FULLY FINISHED OR TESTED, PLEASE TAKE THIS INTO CONSIDERATION.; license: apache-2.0; tags:; Quantized for KoboldAI (4bit-fork); Oobabooga"
models,,,,,,2,,0,122142.7214,,https://huggingface.co/junxinyijiu/models,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
gpt2-medium-conversational,Text Generation,PyTorch; Safetensors; Transformers,English,openrail,,1,Locutusque/ColumnedChatCombined; tatsu-lab/alpaca,72,2950.550567,,https://huggingface.co/Locutusque/gpt2-medium-conversational,"This model is intended to be used for generating conversational responses in a variety of contexts, such as chatbots, virtual assistants, and customer service applications. It is designed to provide natural and engaging responses to user input, with a focus on maintaining a consistent tone and style throughout the conversation. The model is suitable for use in both text-based and voice-based interfaces, and can be easily integrated into existing applications using the PyTorch and Transformers frameworks.; The model is trained on a large dataset of conversational data, consisting of interactions between users and an AI assistant. The data is preprocessed to remove any sensitive information and is formatted in a way that is suitable for training a language model. The training data is split into a training set and a validation set, with the training set used to update the model parameters and the validation set used to evaluate the model performance. The model was trained on 302,000 examples over 502,505 steps, it achieved decent metrics.; The model architecture used in this model is GPT-2, a transformer-based language model that is capable of generating high-quality text with a wide range of styles and tones. The GPT-2 architecture consists of a multi-layered transformer encoder-decoder, with self-attention mechanisms that allow the model to capture long-term dependencies and generate coherent text.; The model is evaluated based on several metrics, including loss, reward, penalty, BLEU score, and perplexity. The loss metric is calculated during training and reflects the difference between the predicted output and the actual output. The reward metric is based on the number of correct words generated by the model, while the penalty metric penalizes the model for repeating words consecutively. The BLEU score measures the similarity between the generated text and the ground truth text, while the perplexity metric measures how well the model is able to predict the next word in a sequence. During validation, the model achieved the following metrics:; This model is not suitable for all use cases due to its limited training time on a weak computer. As a result, it may produce irrelevant or nonsensical responses. Additionally, it has not been fine-tuned to remember the chat history, is unable to provide follow-up responses, and it does not know the answer to many questions (it was only fine-tuned to respond in a conversational way). For optimal performance, we recommend using a GPU with at least 8GB of VRAM and downloading the model manually instead of using the Transformers library. Here's how you should deploy the model:"
emotions_classifier,Image Classification,TensorFlow; Transformers,,apache-2.0,,2,,108,344.0838347,,https://huggingface.co/CynthiaCR/emotions_classifier,"This model is a fine-tuned version of google/vit-base-patch16-224-in21k on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
gta,Text-to-Image,Diffusers,,creativeml-openrail-m,,1,,4,2181.123338,,https://huggingface.co/Sula1723/gta,"This model was trained on the official art images of the GTA 5 game, which usually appear on the loading screens. These arts include images of people, animals, and various objects that are usually part of a general picture or background.
Stable Diffusion v1-5 was used for training, also about 30 concept images and 1000 regularization images for style training.
Due to the use of Google Colab resources, you do not need to have a GPU and you can freely experiment on your concepts.
To select the optimal settings before training, I recommend using this guide written by Nitrosocke.; To reference the art style, use the token: gta; Test the concept via A1111 Colab fast-Colab-A1111; Sample pictures of this concept:; 




"
starcoder-GGML,Text Generation,Transformers,,bigcode-openrail-m,https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,22,bigcode/the-stack-dedup,0,110387.2127,1,https://huggingface.co/NeoDim/starcoder-GGML,"This is GGML format quantised 4bit, 5bit and 8bit models of StarCoder.
This repo is the result of quantising to 4bit, 5bit and 8bit GGML for CPU inference using ggml.; ; Play with the model on the StarCoder Playground.; The StarCoder models are 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2), with opt-out requests excluded. The model uses Multi Query Attention, a context window of 8192 tokens,  and was trained using the Fill-in-the-Middle objective on 1 trillion tokens. ; The model was trained on GitHub code. As such it is not an instruction model and commands like ""Write a function that computes the square root."" do not work well. However, by using the Tech Assistant prompt you can turn it into a capable technical assistant."
negative_textual_inversions,,,,openrail,,2,,0,1.016011124,,https://huggingface.co/Nekos4Lyfe/negative_textual_inversions,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
RedPajama-INCITE-Chat-3B-Instruction-Tuning-with-GPT-4,Text Generation,PyTorch; Safetensors; Transformers,English,cc,,2,Fredithefish/Instruction-Tuning-with-GPT-4-RedPajama-Chat,61,11655.23349,,https://huggingface.co/Fredithefish/RedPajama-INCITE-Chat-3B-Instruction-Tuning-with-GPT-4,RedPajama-INCITE-Chat-3B Model finetuned on this dataset; The code for the finetuning of this model can be found at https://github.com/fredi-python/Fine-tune-RedPajama-Chat-3B; The Model is intended and licensed for research use only. The model is under the CC BY NC 4.0 license (allowing only non-commercial use); Inference API has been turned off for this model.
wikipedia2vec_zhwiki_20180420_100d,,,Chinese,apache-2.0,,1,wikipedia,0,1218.562607,,https://huggingface.co/Word2vec/wikipedia2vec_zhwiki_20180420_100d,"Pretrained Word2vec in Chinese. For more information, see https://wikipedia2vec.github.io/wikipedia2vec/pretrained/.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
llm-tolkien,,Adapter Transformers,#NAME?,bigscience-bloom-rail-1.0,,1,JeremyArancio/lotr-book,59,34.20610542,,https://huggingface.co/JeremyArancio/llm-tolkien,"Version 1.1 / 23 May 2023; This LLM is fine-tuned on Bloom-3B with texts extracted from the book ""The Lord of the Rings"".; The article: Fine-tune an LLM on your personal data: create a “The Lord of the Rings” storyteller.; Github repository; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
wikipedia2vec_eswiki_20180420_100d,,,Spanish,apache-2.0,,1,wikipedia,0,1443.842607,,https://huggingface.co/Word2vec/wikipedia2vec_eswiki_20180420_100d,"Pretrained Word2vec in Spanish. For more information, see https://wikipedia2vec.github.io/wikipedia2vec/pretrained/.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
stable-diffusion-x4-upscaler-img2img,Image-to-Image,Diffusers,,openrail++,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,8,,"2,238",7229.45431,2,https://huggingface.co/radames/stable-diffusion-x4-upscaler-img2img,"This model card focuses on the model associated with the Stable Diffusion Upscaler, available here.
This model is trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model.
In addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. ; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model; Language(s): English"
stable-diffusion-2-depth-img2img,Image-to-Image,Diffusers,,openrail++,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,1,,412,11696.22422,,https://huggingface.co/radames/stable-diffusion-2-depth-img2img,"This model card focuses on the model associated with the Stable Diffusion v2 model, available here.; This stable-diffusion-2-depth model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning.; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model"
japanese-gpt-neox-3.6b,Text Generation,PyTorch; Safetensors; Transformers,Japanese,mit,,65,cc100; wikipedia; mc4,"15,568",15095.15382,1,https://huggingface.co/rinna/japanese-gpt-neox-3.6b,; This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters.; Library;   The model was trained using code based on EleutherAI/gpt-neox.; Model architecture
codet5p-16b,Text2Text Generation,PyTorch; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2305.07922.pdf,42,,"3,374",34020.71283,,https://huggingface.co/Salesforce/codet5p-16b,"CodeT5+ is a new family of open code large language models with an encoder-decoder architecture that can flexibly operate in different modes (i.e. encoder-only, decoder-only, and encoder-decoder) to support a wide range of code understanding and generation tasks. 
It is introduced in the paper:; CodeT5+: Open Code Large Language Models for Code Understanding and Generation
by Yue Wang*, Hung Le*, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi (* indicates equal contribution).; Compared to the original CodeT5 family (base: 220M, large: 770M), CodeT5+ is pretrained with a diverse set of pretraining tasks including span denoising, causal language modeling, contrastive learning, and text-code matching to learn rich representations from both unimodal code data and bimodal code-text data. 
Additionally, it employs a simple yet effective compute-efficient pretraining method to initialize the model components with frozen off-the-shelf LLMs such as CodeGen to efficiently scale up the model (i.e. 2B, 6B, 16B), and adopts a ""shallow encoder and deep decoder"" architecture. 
Furthermore, it is instruction-tuned to align with natural language instructions (see our InstructCodeT5+ 16B) following Code Alpaca.  ; This model can be easily loaded using the AutoModelForSeq2SeqLM functionality and employs the same tokenizer as CodeGen.; This checkpoint is trained on the stricter permissive subset of the deduplicated version of the github-code dataset.
The data is preprocessed by reserving only permissively licensed code (""mit"" “apache-2”, “bsd-3-clause”, “bsd-2-clause”, “cc0-1.0”, “unlicense”, “isc”).
Supported languages (9 in total) are as follows:
c, c++, c-sharp,  go, java, javascript,  php, python, ruby."
codet5p-2b,Text2Text Generation,PyTorch; Transformers,,bsd-3-clause,https://arxiv.org/pdf/2305.07922.pdf,17,,"10,720",6608.193882,1,https://huggingface.co/Salesforce/codet5p-2b,"CodeT5+ is a new family of open code large language models with an encoder-decoder architecture that can flexibly operate in different modes (i.e. encoder-only, decoder-only, and encoder-decoder) to support a wide range of code understanding and generation tasks. 
It is introduced in the paper:; CodeT5+: Open Code Large Language Models for Code Understanding and Generation
by Yue Wang*, Hung Le*, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi (* indicates equal contribution).; Compared to the original CodeT5 family (base: 220M, large: 770M), CodeT5+ is pretrained with a diverse set of pretraining tasks including span denoising, causal language modeling, contrastive learning, and text-code matching to learn rich representations from both unimodal code data and bimodal code-text data. 
Additionally, it employs a simple yet effective compute-efficient pretraining method to initialize the model components with frozen off-the-shelf LLMs such as CodeGen to efficiently scale up the model (i.e. 2B, 6B, 16B), and adopts a ""shallow encoder and deep decoder"" architecture. 
Furthermore, it is instruction-tuned to align with natural language instructions (see our InstructCodeT5+ 16B) following Code Alpaca.  ; This model can be easily loaded using the AutoModelForSeq2SeqLM functionality and employs the same tokenizer as CodeGen.; This checkpoint is trained on the stricter permissive subset of the deduplicated version of the github-code dataset.
The data is preprocessed by reserving only permissively licensed code (""mit"" “apache-2”, “bsd-3-clause”, “bsd-2-clause”, “cc0-1.0”, “unlicense”, “isc”).
Supported languages (9 in total) are as follows:
c, c++, c-sharp,  go, java, javascript,  php, python, ruby."
manticore-13b,Text Generation,PyTorch; Transformers,English,,,102,anon8231489123/ShareGPT_Vicuna_unfiltered; ehartford/wizard_vicuna_70k_unfiltered; ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered; QingyiSi/Alpaca-CoT; teknium/GPT4-LLM-Cleaned; teknium/GPTeacher-General-Instruct; metaeval/ScienceQA_text_only; hellaswag; tasksource/mmlu; openai/summarize_from_feedback,522,26657.32949,7,https://huggingface.co/openaccess-ai-collective/manticore-13b,"? Donate to OpenAccess AI Collective to help us keep building great tools and models!; ; Questions, comments, feedback, looking to donate, or want to help? Reach out on our Discord or email wing@openaccessaicollective.org; Manticore 13B is a Llama 13B model fine-tuned on the following datasets:; Try out the model in HF Spaces. The demo uses a quantized GGML version of the model to quickly return predictions on smaller GPUs (and even CPUs). Quantized GGML may have some minimal loss of model quality."
wikipedia2vec_eswiki_20180420_300d,,,Spanish,apache-2.0,,1,wikipedia,0,4249.602607,,https://huggingface.co/Word2vec/wikipedia2vec_eswiki_20180420_300d,"Pretrained Word2vec in Spanish. For more information, see https://wikipedia2vec.github.io/wikipedia2vec/pretrained/.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
deberta-v3-base-injection,Text Classification,PyTorch; Transformers,,mit,,8,,"1,565",749.1303312,,https://huggingface.co/deepset/deberta-v3-base-injection,"This model is a fine-tuned version of microsoft/deberta-v3-base on the promp-injection dataset.
It achieves the following results on the evaluation set:; This model detects prompt injection attempts and classifies them as ""INJECTION"". Legitimate requests are classified as ""LEGIT"". The dataset assumes that legitimate requests are either all sorts of questions of key word searches.; If you are using this model to secure your system and it is overly ""trigger-happy"" to classify requests as injections, consider collecting legitimate examples and retraining the model with the promp-injection dataset.; Based in the promp-injection dataset.; The following hyperparameters were used during training:"
Loras,Text-to-Image,,English,,,6,,0,682.2048926,,https://huggingface.co/ZeroJour/Loras,"Here you can find the LoRAs created by me. They are all meant to recreate the style of the anime. Please keep that in mind while using them.; ; ; 
; 

"
dolly-v2-7b-olive-optimized,Text Generation,ONNX; Transformers,English,mit,,4,databricks/databricks-dolly-15k,553,14035.61362,,https://huggingface.co/microsoft/dolly-v2-7b-olive-optimized,"Databricks’ dolly-v2-7b, an instruction-following large language model trained on the Databricks machine learning platform 
that is licensed for commercial use. Based on pythia-6.9b, Dolly is trained on ~15k instruction/response fine tuning records 
databricks-dolly-15k generated 
by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,
information extraction, open QA and summarization. dolly-v2-7b is not a state-of-the-art model, but does exhibit surprisingly 
high quality instruction following behavior not characteristic of the foundation model on which it is based.  ; Dolly v2 is also available in these other models sizes:; Please refer to the dolly GitHub repo for tips on 
running inference for various GPU configurations.; Owner: Databricks, Inc.; This repo hosts model files that may be loaded as an ORTModelForCausalLM when using Python with ? Optimum. Alternatively, the ONNX models may be composed into a custom pipeline in any language that supports ONNX Runtime & DirectML. If you choose to use ONNX Runtime & DirectML outside of Python, then you will need to provide your own implementation of the tokenizer."
HeackMT5-ZhSum100k,Summarization,PyTorch; Transformers,Chinese,cc-by-nc-sa-4.0,,2,,242,2390.23763,,https://huggingface.co/heack/HeackMT5-ZhSum100k,"This model, heack/HeackMT5-ZhSum100k, is a fine-tuned mT5 model for Chinese text summarization tasks. It was trained on a diverse set of Chinese datasets and is able to generate coherent and concise summaries for a wide range of texts.; The model achieved the following results:; Here is how you can use this model for text summarization:; This model is trained and maintained by KongYang from Shanghai Jiao Tong University. For any questions, please reach out to me at my WeChat ID: kongyang.; This model is released under the CC BY-NC-SA 4.0 license."
wikipedia2vec_enwiki_20180420_win10_500d,,,English,apache-2.0,,1,wikipedia,0,17510.40262,,https://huggingface.co/Word2vec/wikipedia2vec_enwiki_20180420_win10_500d,"Pretrained Word2vec in English. For more information, see https://wikipedia2vec.github.io/wikipedia2vec/pretrained/.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
gptj_finetune_anthropic_data_051723,Text Generation,PyTorch; Transformers,,,,1,,6,12462.10769,,https://huggingface.co/tr416/gptj_finetune_anthropic_data_051723,No model card; New: Create and edit this model card directly on the website!
MedLLaMA_13B,Text Generation,PyTorch; Transformers,English,apache-2.0,,16,,419,53320.20399,,https://huggingface.co/chaoyi-wu/MedLLaMA_13B,"This repo contains MedLLaMA_13B, which is LLaMA-13b finetuned with some Medical Corpus.; The model was trained with the following hyperparameters:; The model can be loaded as follows:"
chinese-llama-alpaca-plus-lora-7b,Text Generation,PyTorch; Transformers,,,,4,,32,14101.25005,1,https://huggingface.co/we1kkk/chinese-llama-alpaca-plus-lora-7b,"This repo contains the tokenizer, Chinese-Alpaca ""merged"" weights and configs for Chinese-LLaMA-Alpaca
Directly load merge weight for chinese-llama-alpaca-plus-lora-7b; Thanks to: https://github.com/ymcui/Chinese-LLaMA-Alpaca.
Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca."
zero-shot-classify-SSTuning-base,Zero-Shot Classification,PyTorch; Transformers,,mit,https://arxiv.org/pdf/2305.11442.pdf,5,,650,502.342935,,https://huggingface.co/DAMO-NLP-SG/zero-shot-classify-SSTuning-base,"Zero-shot text classification model trained with self-supervised tuning (SSTuning). 
It was introduced in the paper Zero-Shot Text Classification via Self-Supervised Tuning by 
Chaoqun Liu, Wenxuan Zhang, Guizhen Chen, Xiaobao Wu, Anh Tuan Luu, Chip Hong Chang, Lidong Bing
and first released in this repository.; The model backbone is RoBERTa-base.; The model is tuned with unlabeled data using a learning objective called first sentence prediction (FSP). 
The FSP task is designed by considering both the nature of the unlabeled corpus and the input/output format of classification tasks. ; The training and validation sets are constructed from the unlabeled corpus using FSP. ; During tuning, BERT-like pre-trained masked language 
models such as RoBERTa and ALBERT are employed as the backbone, and an output layer for classification is added. 
The learning objective for FSP is to predict the index of the correct label. 
A cross-entropy loss is used for tuning the model."
CrimsonPajama,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2106.09685.pdf,4,Fredithefish/GPTeacher-for-RedPajama-Chat,44,5828.675633,,https://huggingface.co/Fredithefish/CrimsonPajama,"This model was finetuned 1 epoch on the GPTeacher dataset using LoRA. A 2 epoch finetuned model will be released in the coming days.; Finetune Code for RedPajama-Chat in notebook format can be found here; A collection of modular datasets generated by GPT-4, General-Instruct - Roleplay-Instruct - Code-Instruct - and Toolformer; Inference API has been turned off for this model."
models,,,,,,4,,0,74326.88173,,https://huggingface.co/pls2000/models,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
so-vits-svc-4.0-models,,,,,,1,,0,0.001445313,,https://huggingface.co/asrielthegoat/so-vits-svc-4.0-models,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
MPT-7B-Instruct-GGML,,Transformers,,cc-by-sa-3.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2010.04245.pdf,23,mosaicml/dolly_hhrlhf,299,38737.93211,1,https://huggingface.co/TheBloke/MPT-7B-Instruct-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is GGML format quantised 4-bit, 5-bit and 8-bit GGML models of MosaicML's MPT-7B-Instruct.; This repo is the result of converting to GGML and quantising.; Please note that these MPT GGMLs are not compatbile with llama.cpp. Please see below for a list of tools known to work with these model files."
mlc-chat-vicuna-v1-7b-q3f16_0,,,,,,4,,0,1148.397087,,https://huggingface.co/mlc-ai/mlc-chat-vicuna-v1-7b-q3f16_0,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
pygmalion-13b,Text Generation,,English,,,84,,0,0.008326149,1,https://huggingface.co/PygmalionAI/pygmalion-13b,"Pygmalion 13B is a dialogue model based on Meta's LLaMA-13B.; This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project.; The model weights in this repository cannot be used as-is. The files here are XORs due to licensing concerns. To obtain proper, usable model weights you need to:; Request access to the original LLaMA weights from Meta through this form; Convert them to the HuggingFace Transformers format by using the convert_llama_weights_to_hf.py script for your version of the transformers library"
Pygmalion-13b-Merged,Text Generation,PyTorch; Transformers,English,,,22,,"1,229",26657.08809,,https://huggingface.co/TehVenom/Pygmalion-13b-Merged,"Pygmalion 13b is a dialogue model based on Meta's LLaMA-13b.; This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, 
for those of you familiar with the project.; The current Pygmalion-13b has been trained as a LoRA, then merged down to the base model for distribuition. ; This models has the XOR files pre-applied out of the box.
Converted from the XORs weights from PygmalionAI's release https://huggingface.co/PygmalionAI/pygmalion-13b; The model was trained on the usual Pygmalion persona + chat format, so any of the usual UIs should already handle everything correctly. If you're using the model directly, this is the expected formatting:"
Metharme-13b-Merged,Text Generation,PyTorch; Transformers,English,,,8,,185,26657.08904,,https://huggingface.co/TehVenom/Metharme-13b-Merged,"Converted from the XORs weights from PygmalionAI's release https://huggingface.co/PygmalionAI/metharme-13b; Metharme 13b is an instruct model based on Meta's LLaMA-13b.; This is an experiment to try and get a model that is usable for conversation, roleplaying and storywriting, but which can be guided using natural language like other instruct models. See the prompting section below for examples.; It was trained by doing supervised fine-tuning over a mixture of regular instruction data alongside roleplay, fictional stories and conversations with synthetically generated instructions attached.; The current Metharme-13b has been trained as a LoRA, then merged down to the base model for distribuition. "
Manticore-13B-GGML,,,,other,,65,,0,118896.6561,,https://huggingface.co/TheBloke/Manticore-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for OpenAccess AI Collective's Manticore 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
pygmalion-13b-4bit-128g,Text Generation,PyTorch; Transformers,English,other,,112,,"4,659",7631.164669,,https://huggingface.co/notstoic/pygmalion-13b-4bit-128g,"Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; Quantized from the decoded pygmalion-13b xor format.
https://huggingface.co/PygmalionAI/pygmalion-13b; In safetensor format.; GPTQ CUDA quantized with: https://github.com/0cc4m/GPTQ-for-LLaMa; Inference API has been turned off for this model."
pygmalion-13b-ggml,,,English,other,,14,,0,75745.28256,,https://huggingface.co/notstoic/pygmalion-13b-ggml,"Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; Quantized from the decoded pygmalion-13b xor format.
https://huggingface.co/PygmalionAI/pygmalion-13b; GGML quantized with: https://github.com/ggerganov/llama.cpp; Note: These are the newest/bumped quantized version as of May 20th 2023.; The old quants can be found here: https://huggingface.co/notstoic/pygmalion-13b-ggml/tree/456c8100c2041fa975defe846220e3a23a614a6d"
bert-base-japanese-v3,,PyTorch; TensorFlow; JAX; Transformers,Japanese,apache-2.0,,5,cc100; wikipedia,"16,686",1444.230318,,https://huggingface.co/cl-tohoku/bert-base-japanese-v3,"This is a BERT model pretrained on texts in the Japanese language.; This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by the WordPiece subword tokenization.
Additionally, the model is trained with the whole word masking enabled for the masked language modeling (MLM) objective.; The codes for the pretraining are available at cl-tohoku/bert-japanese.; The model architecture is the same as the original BERT base model; 12 layers, 768 dimensions of hidden states, and 12 attention heads.; The model is trained on the Japanese portion of CC-100 dataset and the Japanese version of Wikipedia.
For Wikipedia, we generated a text corpus from the Wikipedia Cirrussearch dump file as of January 2, 2023.
The corpus files generated from CC-100 and Wikipedia are 74.3GB and 4.9GB in size and consist of approximately 392M and 34M sentences, respectively."
bert-base-japanese-char-v3,,PyTorch; TensorFlow; JAX; Transformers,Japanese,apache-2.0,,2,cc100; wikipedia,715,1128.031958,,https://huggingface.co/cl-tohoku/bert-base-japanese-char-v3,"This is a BERT model pretrained on texts in the Japanese language.; This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by character-level tokenization.
Additionally, the model is trained with the whole word masking enabled for the masked language modeling (MLM) objective.; The codes for the pretraining are available at cl-tohoku/bert-japanese.; The model architecture is the same as the original BERT base model; 12 layers, 768 dimensions of hidden states, and 12 attention heads.; The model is trained on the Japanese portion of CC-100 dataset and the Japanese version of Wikipedia.
For Wikipedia, we generated a text corpus from the Wikipedia Cirrussearch dump file as of January 2, 2023.
The corpus files generated from CC-100 and Wikipedia are 74.3GB and 4.9GB in size and consist of approximately 392M and 34M sentences, respectively."
bert-large-japanese-v2,,PyTorch; TensorFlow; JAX; Transformers,Japanese,apache-2.0,,3,cc100; wikipedia,"2,943",4290.790319,,https://huggingface.co/cl-tohoku/bert-large-japanese-v2,"This is a BERT model pretrained on texts in the Japanese language.; This version of the model processes input texts with word-level tokenization based on the Unidic 2.1.2 dictionary (available in unidic-lite package), followed by the WordPiece subword tokenization.
Additionally, the model is trained with the whole word masking enabled for the masked language modeling (MLM) objective.; The codes for the pretraining are available at cl-tohoku/bert-japanese.; The model architecture is the same as the original BERT large model; 24 layers, 1024 dimensions of hidden states, and 16 attention heads.; The model is trained on the Japanese portion of CC-100 dataset and the Japanese version of Wikipedia.
For Wikipedia, we generated a text corpus from the Wikipedia Cirrussearch dump file as of January 2, 2023.
The corpus files generated from CC-100 and Wikipedia are 74.3GB and 4.9GB in size and consist of approximately 392M and 34M sentences, respectively."
chinese-llama-plus-13b-hf,Text Generation,PyTorch; Transformers,Chinese,other,,16,,358,27044.63268,2,https://huggingface.co/shibing624/chinese-llama-plus-13b-hf,"发布中文LLaMA-Plus, Alpaca-Plus 13B版本模型; 发布中文LLaMA-Plus, Alpaca-Plus 13B版本，改进点如下：; 本模型是 decapoda-research/llama-13b-hf 
底座模型 合并 ziqingyang/chinese-llama-plus-lora-13b LoRA权重，
并转化为HuggingFace版本权重（.bin文件），可以在此中文LLaMA模型上继续指令微调训练，LLaMA模型为底座模型，直接调用会效果不佳。; test case:; 本项目开源在textgen项目：textgen，可支持llama模型，通过如下命令调用："
pygmalion-7b-ggml,,,,,,1,,0,13824.00145,,https://huggingface.co/alpindale/pygmalion-7b-ggml,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
distil-whisper-large-v2-8-ls,Automatic Speech Recognition,PyTorch; Transformers,English,,,2,librispeech_asr,18,3741.190857,,https://huggingface.co/rsonavane/distil-whisper-large-v2-8-ls,"This model is a distilled version of the Whisper large v2 model using decoder pruning. 
It is trained to give the same distribution as the teacher(large-v2) model using Distillation loss (KL loss) + CE Loss. 
The original model contains 32 decoder layers, whereas the distilled model contains only 8 layers and achieves 4.2% WER on the 
librispeech dataset with finetuning for just one epoch. The decoding speed of the model is 2x faster than vanilla large-v2 and 
40% smaller in size.; This experiment aimed to explore the effectiveness of decoder pruning and distillation in enhancing performance after training.
The model acquires a similar internal representation of the English language as its teacher model, 
but with improved inference speed and efficiency for downstream tasks. Additionally, it can be fine-tuned for multiple languages, 
maintaining the original model's performance while reducing inference latency. 
There are other frameworks such as JAX that can help improve the same."
medalpaca-13B-GGML,Text Generation,Transformers,English,cc,https://arxiv.org/pdf/2303.14070.pdf,12,,0,49121.28958,,https://huggingface.co/TheBloke/medalpaca-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is GGML format quantised 4-bit, 5-bit and 8-bit GGML models of Medalpaca 13B.; This repo is the result of quantising to 4-bit, 5-bit and 8-bit GGML for CPU (+CUDA) inference using llama.cpp.; llama.cpp recently made another breaking change to its quantisation methods - https://github.com/ggerganov/llama.cpp/pull/1508"
translation_en-zh,Translation,PyTorch; Safetensors; Transformers,English; Chinese,apache-2.0,,8,DDDSSS/en-zh-dataset,"10,650",623.3571115,3,https://huggingface.co/DDDSSS/translation_en-zh,该模型主要的训练数据是opus100和CodeAlpaca_20K中的英文作为翻译内容，采用chatglm作为翻译器翻译成中文，并将脏数据筛选后得到DDDSSS/en-zh-dataset数据集，; !注意，如果是pretrain方法下载模型的话，可能部分参数会随机初始化，建议直接下载模型，并从本地读取。; 微调：
roberta-base-ca-v2-cawikitc,Zero-Shot Classification,PyTorch; Safetensors; Transformers,Catalan,apache-2.0,,1,,27,1001.541732,,https://huggingface.co/projecte-aina/roberta-base-ca-v2-cawikitc,"The roberta-base-ca-v2-cawikitc (RoBERTa-ca-CaWikiTC) is a Zero-Shot Text Classification model in Catalan created by fine-tuning RoBERTa-base-ca-v2 with a classification dataset, CaWikiTC, reformulated as entailment. This model was developed as part of the experimental research presented in the following paper ""Entailment-based Task Transfer for Catalan Text Classification in Small Data Regimes"".; This model can be used for zero-shot text classification in Catalan. It has been trained with a fixed hypothesis template, ""Aquest article tracta sobre {}."", and Wikipedia-based articles as premises, and may not generalize well for all use cases.; No measures have been taken to estimate the bias and toxicity embedded in the model.; This model was fine-tuned for the Natural Language Inference (NLI) task on an authomatically Wikipedia-based text classification dataset, CaWikiTC, reformulated as entailment. In the reformulation process, we generated two NLI examples for each text classification instance (text and label): an entailment example and a non-entailment example. In both cases, we employed the text as the premise and utilized a shared template to create the hypothesis (""Aquest article tracta {}.""), which was completed with the correct label for the entailment example and a randomly-selected label from the remaining options for the non-entailment example.; The pre-trained Catalan model RoBERTa-base-ca-v2 was fine-tuned with the training data using a learning rate of 3e-5, a batch size of 16, seed 26 and a maximum of 10 epochs. The development set (converted into entailment) was used to select the best checkpoint according to the highest weighted F1 score in the classification task, which was obtained in the first epoch."
Realgar-v1,Text-to-Image,Diffusers; Safetensors,,other,,14,,17,2642.597026,,https://huggingface.co/p1atdev/Realgar-v1,"This model is a Stable Diffusion model based on WD 1.5 beta 3 base.; This model does not include the NovelAI Leak model.; WD 1.5 beta3 base をベ`スにしたモデルです。; NAI リ`クは含まれていません。; This model aims for stability with short prompt. It may sometimes be difficult to generate with long prompt or special concepts. masterpiece, best quality is not needed. "
MODEL,,,,,,1,,0,6717.441445,,https://huggingface.co/FarN/MODEL,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
lyriel_v16,,,,creativeml-openrail-m,,9,,0,2181.122266,,https://huggingface.co/serta21/lyriel_v16,"model and description taken from here: https://civitai.com/models/22922/lyriel; Hello, the model was created as an artistic style, the model can do almost anything, the main thing is to follow the promt, hands and eyes looks good for the most cases; Model Information:; This model is generally designed for portraits and full-length anime style photos. Fantastic landscapes are quite decent. And it doesn't require kilometer-long queries to get a high-quality result.; Recommend: DPM++2M Karras, Clip skip 2 Sampler, Steps: 25-35+"
NER-Deberta,Token Classification,PyTorch; TensorBoard; Safetensors; Transformers,English,mit,,3,DFKI-SLT/few-nerd,63,1481.12862,,https://huggingface.co/RashidNLP/NER-Deberta,"I used a Pretrained Deberta-v3-base and finetuned it on Few-NERD, A NER dataset that contains over 180k examples and over 4.6 million tokens.; The Token labels are Person, Organisation, Location, Building, Event, Product, Art & Misc."
mlc-chat-vicuna-v1-7b-q4f32_0,,,,,,6,,0,1229.302909,,https://huggingface.co/mlc-ai/mlc-chat-vicuna-v1-7b-q4f32_0,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
mlong-t5-tglobal-xl,Text2Text Generation,PyTorch; JAX; Transformers,102 languages,apache-2.0,https://arxiv.org/pdf/2305.11129.pdf; https://arxiv.org/pdf/1912.08777.pdf; https://arxiv.org/pdf/2112.07916.pdf,6,mc4,121,33028.67797,,https://huggingface.co/agemagician/mlong-t5-tglobal-xl,"MLongT5 model pre-trained on Multi-language corpus. The model was introduced in the paper mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences by Uthus et al. and first released in the LongT5 repository. All the model architecture and configuration can be found in Flaxformer repository which uses another Google research project repository T5x.; Disclaimer: The team releasing MLongT5 did not write a model card for this model so this model card has been written by Ahmed Elnaggar.; MLongT5 model is an encoder-decoder transformer pre-trained in a text-to-text denoising generative setting (Pegasus-like generation pre-training). MLongT5 model is an extension of LongT5 model, and it enables using one of the two different efficient attention mechanisms - (1) Local attention, or (2) Transient-Global attention. The usage of attention sparsity patterns allows the model to efficiently handle input sequence.; MLongT5 is particularly effective when fine-tuned for text generation (summarization, question answering) which requires handling long input sequences (up to 16,384 tokens).; The model is mostly meant to be fine-tuned on a supervised dataset. See the model hub to look for fine-tuned versions on a task that interests you."
mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f32_0,,,,,,3,,0,1607.942004,,https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f32_0,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
polyglot_words_embeddings_spa,,,Spanish,gpl-3.0,,1,,0,27.40271484,,https://huggingface.co/Word2vec/polyglot_words_embeddings_spa,"Word embedding model trained by Al-Rfou et al.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
13B-HyperMantis,Text Generation,PyTorch; Transformers,English,other,,25,,424,26716.6878,,https://huggingface.co/digitous/13B-HyperMantis,"is a weight-sum multi model-merge comprised of:; ((MantiCore3E+VicunaCocktail)+(SuperCOT+(StorytellingV2+BluemoonRP))) [All 13B Models]; (GGML and GPTQ are no longer in this repo and will be migrated to a separate repo for easier git download convenience); Subjective testing shows quality results with KoboldAI (similar results are likely in Text Generation Webui, please disregard KAI-centric settings for that platform); Godlike preset with these tweaks - 2048 context, 800 Output Length, 1.3 Temp, 1.13 Repetition Penalty, AltTextGen:On, AltRepPen:Off, No Prompt Gen:On; Despite being primarily uncensored Vicuna models at its core, HyperMantis seems to respond best to the Alpaca instruct format. Speculatively due to manticore's eclectic instruct datasets generalizing the model's understanding of following instruct formats to some degree. What is known is HyperMantis responds best to the formality of Alpaca's format, whereas Human/Assistant appears to trigger vestigial traces of moralizing and servitude that aren't conducive for roleplay or freeform instructions."
wd-v1-4-moat-tagger-v2,,Keras; ONNX,,apache-2.0,https://arxiv.org/pdf/2210.01820.pdf,16,,409,334.8536874,5,https://huggingface.co/SmilingWolf/wd-v1-4-moat-tagger-v2,"Supports ratings, characters and general tags.; Trained using https://github.com/SmilingWolf/SW-CV-ModelZoo.TPUs used for training kindly provided by the TRC program.; Last image id: 5944504Trained on Danbooru images with IDs modulo 0000-0899.Validated on images with IDs modulo 0950-0999.Images with less than 10 general tags were filtered out.Tags with less than 600 images were filtered out.; P=R: threshold = 0.3771, F1 = 0.6911; MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models"
chinese-alpaca-13b-plus-quantized,,,,,,13,,0,0.002286453,,https://huggingface.co/johnlui/chinese-alpaca-13b-plus-quantized,"移动本仓库中的 alpaca-13b-plus 文件夹到你项目的./models文件下即可。该文件夹同时适用于llama.cpp和text-generation-webui。; 效果确实比 13b 好了不少，能写出比较长的文字了，速度没有明显变化，本模型运行时需要 9.2GB 内存，未进行格式转换和量化时需要 50GB 内存，太吓人了，速度还只有十分之一。; 13b 已合并文件是从 https://huggingface.co/shibing624/chinese-alpaca-plus-13b-hf 仓库中下载的，我对其进行了格式转换与 4bit 量化。; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ggml-whisper-models,Automatic Speech Recognition,,99 languages,mit,,3,,0,2414.02219,,https://huggingface.co/danielus/ggml-whisper-models,"This repository contains versions of the Whisper models in the ggml format.; The versions present are the best performing according to the benchmark: https://github.com/ggerganov/whisper.cpp/discussions/859; Unable to determine this model’s library. Check the
								docs 
.
							"
KoRWKV-1.5B,Text Generation,PyTorch; Safetensors; Transformers,Korean,mit,,7,,296,9205.723133,3,https://huggingface.co/beomi/KoRWKV-1.5B,Train finished ?? This version is v1.0 release of KoRWKV-1.5B; Generation DEMO available at HF Gradio beomi/KoRWKV-1.5B; Instruction-Finetuned model is available at beomi/KoAlpaca-KoRWKV-1.5B; KoRWKV (1.5B params) trained on Korean dataset with RWKVv4 Neo Architecture.; Researcher developing the model
unsup-simcse-xlm-roberta-base,Feature Extraction,PyTorch; Transformers,94 languages,mit,https://arxiv.org/pdf/2305.13303.pdf,2,,83,1150.816984,,https://huggingface.co/ZurichNLP/unsup-simcse-xlm-roberta-base,"xlm-roberta-base fine-tuned for sentence embeddings with SimCSE (Gao et al., EMNLP 2021).; See a similar English model released by Gao et al.: https://huggingface.co/princeton-nlp/unsup-simcse-roberta-base.; Fine-tuning was done using the reference implementation of unsupervised SimCSE and the 1M sentences from English Wikipedia released by the authors.
As a sentence representation, we used the average of the last hidden states (pooler_type=avg), which is compatible with Sentence-BERT.; Fine-tuning command:"
MeshDiffusion_models,,,,,,1,,0,24678.40145,,https://huggingface.co/lzzcd001/MeshDiffusion_models,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
stock-news-distilbert,Text Classification,PyTorch; Transformers,,,,9,,"1,483",263.8712685,1,https://huggingface.co/KernAI/stock-news-distilbert,"This distilbert model was fine-tuned on 50.000 stock news articles using the HuggingFace adapter from Kern AI refinery. The articles consisted of the headlines plus abstract of the article.
For the finetuning, a single NVidia K80 was used for about four hours. ; Join our Discord if you have questions about this model: https://discord.gg/MdZyqSxKbe; DistilBERT is a smaller, faster and lighter version of BERT. It was trained by distilling BERT base and has 40% less parameters than bert-base-uncased. 
It runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark. 
DistilBERT does not have token-type embeddings, pooler and retains only half of the layers from Google’s BERT.; To use the model, you need to install the HuggingFace Transformers library:; Then you can load the model and the tokenizer from the HuggingFace Hub:"
MedicWizard-7B-GGML,,,,,,3,,0,56954.8816,,https://huggingface.co/xzuyn/MedicWizard-7B-GGML,"Original Model: https://huggingface.co/xzuyn/MedicWizard-7B; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
kl-f8-anime2-VAE,,,,,,2,,0,405.0014453,,https://huggingface.co/HAttORi/kl-f8-anime2-VAE,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
planner_7B,,,,mit,,12,,0,0.003023643,1,https://huggingface.co/rewoo/planner_7B,"Alpaca Lora adapter weight fine-tuned on following instruction dataset.; https://huggingface.co/datasets/rewoo/planner_instruction_tuning_2k/blob/main/README.md; Training script: borrowed from the official Alpaca-LoRA implementation; We use following parameter.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
cse_buet_bangla_t5,Text2Text Generation,PyTorch; Transformers,Bengali,,https://arxiv.org/pdf/2205.11081.pdf,1,,12,992.2313414,1,https://huggingface.co/Afsara/cse_buet_bangla_t5,"This repository contains the pretrained checkpoint of the model BanglaT5. This is a sequence to sequence transformer model pretrained with the ""Span Corruption"" objective. Finetuned models using this checkpoint achieve state-of-the-art results on many of the NLG tasks in bengali. ; For finetuning on different downstream tasks such as Machine Translation, Abstractive Text Summarization, Question Answering etc., refer to the scripts in the official GitHub repository.; Note: This model was pretrained using a specific normalization pipeline available here. All finetuning scripts in the official GitHub repository use this normalization by default. If you need to adapt the pretrained model for a different task make sure the text units are normalized using this pipeline before tokenizing to get best results. A basic example is given below:; The benchmarking datasets are as follows:; If you use this model, please cite the following paper:"
Perfect-World-v4,,Diffusers,,,,2,,337,0.00199749,,https://huggingface.co/sinkinai/Perfect-World-v4,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
LLamaSharpSamples,Text2Text Generation,,English,other,,3,,0,0.002939453,,https://huggingface.co/AsakusaRinne/LLamaSharpSamples,"This is a model repo mainly for LLamaSharp to provide samples for each version. The models can also be used by llama.cpp or other engines.; Since llama.cpp always have break changes, it takes much time for users (of LLamaSharp and others) to find a suitable model to run. This model repo would provide some convenience for users.; We will appreciate it if you'd like to provide some info about the incompleted models (such as links, model sources, etc.).; At first, choose a branch with the same name of your LLamaSharp Backend version. For example, if you're using LLamaSharp.Backend.Cuda11 v0.3.0, please use v0.3.0 branch of this repo.; Then download a model you like and follow the instructions of LLamaSharp to run it."
WizardLM-30B-Uncensored,Text Generation,PyTorch; Transformers,,other,,111,ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered,"66,585",66634.16546,8,https://huggingface.co/ehartford/WizardLM-30B-Uncensored,"This is WizardLM trained with a subset of the dataset - responses that contained alignment / moralizing were removed.  The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:An uncensored model has no guardrails.You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.
Publishing anything this model generates is the same as publishing it yourself.
You are responsible for the content you publish, and you cannot blame the model any more than you can blame the knife, gun, lighter, or car for what you do with it."
guanaco-65b-merged,Text Generation,PyTorch; Transformers,,,,55,,"3,428",133653.0365,4,https://huggingface.co/timdettmers/guanaco-65b-merged,No model card; New: Create and edit this model card directly on the website!
mpt-7b,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2010.04245.pdf; https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2302.06675.pdf,4,mc4; c4; togethercomputer/RedPajama-Data-1T; bigcode/the-stack; allenai/s2orc,"2,201",13621.433,,https://huggingface.co/gretelai/mpt-7b,"This model is derived from MosaicML's MPT-7B model, with changes from
cekal/mpt-7b-peft-compatible applied; each licensed under the
Apache License, version 2.0.; MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.
This model was trained by MosaicML.; MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference. ; These architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing 
positional embeddings with Attention with Linear Biases (ALiBi). 
Thanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence. 
MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer. ; This model uses the MosaicML LLM codebase, which can be found in the llm-foundry repository. It was trained by MosaicML’s NLP team on the MosaicML platform for LLM pretraining, finetuning, and inference."
guanaco-33b,,,,,https://arxiv.org/pdf/2305.14314.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2304.07327.pdf,21,,0,1996.822397,,https://huggingface.co/timdettmers/guanaco-33b,"| Paper | Code | Demo | ; The Guanaco models are open-source finetuned chatbots obtained through 4-bit QLoRA tuning of LLaMA base models on the OASST1 dataset. They are available in 7B, 13B, 33B, and 65B parameter sizes.; ??Guanaco is a model purely intended for research purposes and could produce problematic outputs.; Guanaco adapter weights are available under Apache 2 license. Note the use of the Guanaco adapter weights, requires access to the LLaMA model weighs. 
Guanaco is based on LLaMA and therefore should be used according to the LLaMA license. ; Here is an example of how you would load Guanaco 7B in 4-bits:"
guanaco-13b,,,,,https://arxiv.org/pdf/2305.14314.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2304.07327.pdf,15,,0,1024.022397,2,https://huggingface.co/timdettmers/guanaco-13b,"| Paper | Code | Demo | ; The Guanaco models are open-source finetuned chatbots obtained through 4-bit QLoRA tuning of LLaMA base models on the OASST1 dataset. They are available in 7B, 13B, 33B, and 65B parameter sizes.; ??Guanaco is a model purely intended for research purposes and could produce problematic outputs.; Guanaco adapter weights are available under Apache 2 license. Note the use of the Guanaco adapter weights, requires access to the LLaMA model weighs. 
Guanaco is based on LLaMA and therefore should be used according to the LLaMA license. ; Here is an example of how you would load Guanaco 7B in 4-bits:"
mms-300m,,PyTorch; Transformers,158 languages,cc-by-nc-4.0,https://arxiv.org/pdf/2305.13516.pdf,12,google/fleurs,"6,277",1300.486716,,https://huggingface.co/facebook/mms-300m,"Facebook's MMS counting 300m parameters.; MMS is Facebook AI's massive multilingual pretrained model for speech (""MMS""). 
It is pretrained in with Wav2Vec2's self-supervised training objective on about 500,000 hours of speech data in over 1,400 languages.; When using the model make sure that your speech input is sampled at 16kHz. ; Note: This model should be fine-tuned on a downstream task, like Automatic Speech Recognition, Translation, or Classification. Check out the **How-to-fine section or this blog for more information about ASR.; Coming soon..."
mms-1b,,PyTorch; Transformers,158 languages,cc-by-nc-4.0,https://arxiv.org/pdf/2305.13516.pdf,20,google/fleurs,"4,635",3952.646726,,https://huggingface.co/facebook/mms-1b,"Facebook's MMS counting 1 billion parameters.; MMS is Facebook AI's massive multilingual pretrained model for speech (""MMS""). 
It is pretrained in with Wav2Vec2's self-supervised training objective on about 500,000 hours of speech data in over 1,400 languages.; When using the model make sure that your speech input is sampled at 16kHz. ; Note: This model should be fine-tuned on a downstream task, like Automatic Speech Recognition, Translation, or Classification. Check out the **How-to-fine section or this blog for more information about ASR.; Coming soon..."
bloomz-legalsupport-lm-lora,,,,,,1,lighteval/LegalSupport,0,4.943649559,,https://huggingface.co/amqdn/bloomz-legalsupport-lm-lora,"BLOOMZ-3b fine-tuned on legal language. Experimental purposes only! This fine-tuning makes no attempt to understand the true nature of the dataset. It simply attempts to assess the performance of basic prompt engineering on BLOOMZ-3b for legal language.; Below are two legal citations. Please generate a summary judgment.; United States v. Mitchell, 502 F.3d 931, 991 (9th Cir.2007) (approving of district court not allowing witnesses to offer opinions on what jury’s verdict should be); United States v. Caro, 461 F.Supp.2d. 459, 465 (W.D.Va.2006) (“[A]n express plea for mercy to the jury from a defendant’s witness is not mitigating evidence that could aid the jury in their decision making.”); see also Stenson v. Lambert, 504 F.3d 873, 892 (9th Cir.2007) (noting there are no federal cases requiring the admission of execution impact testimony); Jackson v. Dretke, 450 F.3d 614, 617-18 (5th Cir.2006); but see Jackson, 450 F.3d at 620 (Dennis, J., dissenting).; United States v. Mitchell, 502 F.3d 931, 991 (9th Cir.2007) (approving of district court not allowing witnesses to offer opinions on what jury’s verdict should be); United States v. Caro, 461 F.Supp.2d. 459, 465 (W.D.Va.2006) (“[A]n express plea for mercy to the jury from a defendant’s witness is not mitigating evidence that could aid the jury in their decision making.”); see also Stenson v. Lambert, 504 F.3d 873, 892 (9th Cir.2007) (noting there are no federal cases requiring the admission of execution impact testimony); Jackson v. Dretke, 450 F.3d 614, 617-18 (5th Cir.2006); but see Jackson, 450 F.3d at 620 (Dennis, J., dissenting).; The district court's decision to allow the testimony of the defendant's execution impact witness is not mitigating evidence. (Model repeats for as many tokens as possible.)"
pyg-7b,Text Generation,PyTorch; Safetensors; Transformers,English,,,1,,30,27609.4265,,https://huggingface.co/4bit/pyg-7b,"Converted from the XORs weights from PygmalionAI's release https://huggingface.co/PygmalionAI/pygmalion-7b; Pygmalion 7B is a dialogue model based on Meta's LLaMA-7B.; This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project.; The model was trained on the usual Pygmalion persona + chat format, so any of the usual UIs should already handle everything correctly. If you're using the model directly, this is the expected formatting:; Where [CHARACTER] is, as you can probably guess, the name of the character you want the model to portray, <START> should be used verbatim as a delimiter token to separate persona and scenario data from the dialogue, and [DIALOGUE HISTORY] is a sliding window of chat history so the model can have conversational context to draw from. Here's a concrete example:"
llms,,,,openrail,,1,,0,0.001470108,,https://huggingface.co/Astr17/llms,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
oasst-rlhf-2-llama-30b-7k-steps-hf,Text Generation,PyTorch; Transformers,,apache-2.0,,5,,56,66634.06175,,https://huggingface.co/Yhyu13/oasst-rlhf-2-llama-30b-7k-steps-hf,"This is the merged hf tr version of llama 30B and OA's rlhf 30B xor weights:; https://huggingface.co/OpenAssistant/oasst-rlhf-2-llama-30b-7k-steps-xor; This the md5 checksum that I get locally, which matchs the original repo suggests"
zhixi-13b-lora,Text Generation,,English; Chinese,apache-2.0,https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2305.11527.pdf,17,,0,501.0459117,,https://huggingface.co/zjunlp/zhixi-13b-lora,"

; This is the result of the ZhiXi-13B LoRA weights. You can click here to learn more.; With the rapid development of deep learning technology, large language models such as ChatGPT have made substantial strides in the realm of natural language processing. However, these expansive models still encounter several challenges in acquiring and comprehending knowledge, including the difficulty of updating knowledge and potential knowledge discrepancies and biases, collectively known as knowledge fallacies. The KnowLM project endeavors to tackle these issues by launching an open-source large-scale knowledgable language model framework and releasing corresponding models. ; The project's initial phase introduced a knowledge extraction LLM based on LLaMA, dubbed ZhiXi (智析, which means intelligent analysis of data for information extraction). To integrate the capacity of Chinese understanding into the language models without compromising their inherent knowledge, we firstly (1) use Chinese corpora for the full-scale pre-training with LLaMA (13B), augment the language model's understanding of Chinese and improve its knowledge richness while retaining its original English and code capacities; Then (2) we fine-tune the model obtained from the first step with an instruction dataset, thus bolstering the language model's understanding of human instructions for knowledge extraction.; The features of this project are as follows:"
majicmixrealistic,Text-to-Image,Diffusers,,creativeml-openrail-m,,5,,"5,093",0.004375076,9,https://huggingface.co/stablediffusionapi/majicmixrealistic,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""majicmixrealistic""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Model link: View model"
socially-good-lm,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2305.16960.pdf,4,Anthropic/hh-rlhf,15,27597.36225,,https://huggingface.co/agi-css/socially-good-lm,"; ; Efficient, Effective, and Stable alternative of RLHF!; Instead of training an additional reward model that is likely to be gamed, we directly train the model on the social games! ?? ? ?; Full details on simulation and training can be found here."
mpnet-mnr-v2-fine-tuned,Sentence Similarity,PyTorch; Sentence Transformers; Transformers,,,,1,anli; glue,12,438.9276604,,https://huggingface.co/BlazingFringe/mpnet-mnr-v2-fine-tuned,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net"
baize-v2-7b,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/2304.01196.pdf,18,,"1,060",13804.03953,15,https://huggingface.co/project-baize/baize-v2-7b,"

; Using Baize checkpoints directly without the following format will not work. ; [|Human|] and [|AI|] are required to mark the messages from the user and Baize. We recommend checking out our GitHub to find the best way to use Baize with our demo or Fastchat.; https://huggingface.co/spaces/project-baize/chat-with-baize; Baize is an open-source chat model fine-tuned with LoRA. This model is a 7B Baize-v2, trained with supervised fine-tuning (SFT) and self-distillation with feedback (SDF). This checkpoint has been merged with LLaMA so it's ready for use."
airoboros-13B-HF,Text Generation,PyTorch; Transformers,,other,,11,,105,26657.12201,2,https://huggingface.co/TheBloke/airoboros-13B-HF,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are HF format fp16 model files for Jon Durbin's Airoboros 13B.; It is the result of converting Jon's fp32 repo to fp16 for easier storage and usage.; For further support, and discussions on these models and AI in general, join us at:"
LightGPT,Text Generation,PyTorch; Transformers,English,apache-2.0,,64,,898,12494.24045,3,https://huggingface.co/amazon/LightGPT,"LightGPT-instruct is a language model based on GPT-J 6B. It was instruction fine-tuned on the high quality, Apache-2.0 licensed
OIG-small-chip2 instruction dataset with ~200K training examples.  ; The instruction template (adapted from the Alpaca README.md) is used to format the prompt:; Input prompt example:; The input ends with ### Response:\n to signal that the model should 
start generating the reply.; Example of generated continuation:"
MeinaMix_V10,Text-to-Image,Diffusers,English,creativeml-openrail-m,,14,,"2,088",0.003474541,,https://huggingface.co/Meina/MeinaMix_V10,"MeinaMix Objective is to be able to do good art with little prompting.; For examples and prompts, please checkout: https://civitai.com/models/7240/meinamix
I have a discord server where you can post images that you generated, discuss prompt and/or ask for help.; https://discord.gg/XC9nGZNDUd If you like one of my models and want to support their updates; I've made a ko-fi page; https://ko-fi.com/meina where you can pay me a coffee <3; And a Patreon page; https://www.patreon.com/MeinaMix where you can support me and get acess to beta of my models!"
lora1,,,,,,1,,0,2511.551445,,https://huggingface.co/devck/lora1,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
majicmixsombre,Text-to-Image,Diffusers,,creativeml-openrail-m,,1,,104,0.004365311,6,https://huggingface.co/stablediffusionapi/majicmixsombre,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""majicmixsombre""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Model link: View model"
speecht5_tts_commonvoice_en,Text-to-Speech,PyTorch; TensorBoard; Transformers,English,mit,,1,mozilla/commonvoice,16,585.2420565,,https://huggingface.co/Avitas8485/speecht5_tts_commonvoice_en,"This model is a fine-tuned version of microsoft/speecht5_tts on the commonvoice dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
kandinsky-2-1-prior,,Diffusers,,apache-2.0,,7,,"20,222",0.010271759,2,https://huggingface.co/kandinsky-community/kandinsky-2-1-prior,"Kandinsky 2.1 inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.1 is available in diffusers!; "
Project-Baize-v2-7B-GPTQ,Text Generation,Transformers,,other,https://arxiv.org/pdf/2304.01196.pdf,4,,18,4098.343461,,https://huggingface.co/TheBloke/Project-Baize-v2-7B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Project Baize V2 7B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
Project-Baize-v2-13B-GPTQ,Text Generation,Transformers,,other,https://arxiv.org/pdf/2304.01196.pdf,12,,9,7631.144506,,https://huggingface.co/TheBloke/Project-Baize-v2-13B-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Project Baize V2 13B.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Open the text-generation-webui UI as normal.
ai-dungeon2-classic-ggml,,,,mit,,5,,0,931.0020681,,https://huggingface.co/Henk717/ai-dungeon2-classic-ggml,"This is the original AI Dungeon 2 model back from when AI Dungeon 2 was an open source project.
The model used for this conversion has been converted prior to the Huggingface Pytorch 16-bit format and was then further quantized to a 4.0 quantization (The original model was a 32-bit Tensorflow model).; The purpose of this model is to allow faster access to the classic AI Dungeon 2 experience, for the sake of nostalgia or for use on low end hardware.
It is best used with Koboldcpp and was tested on the Windows 7 version of Koboldcpp 1.25.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
real_learner_bart_CGEC_exam,Text2Text Generation,PyTorch; Transformers,Chinese,apache-2.0,https://arxiv.org/pdf/2305.16023.pdf,1,,149,1536.104563,,https://huggingface.co/HillZhang/real_learner_bart_CGEC_exam,"This model is a cutting-edge CGEC model based on Chinese BART-large.
It is trained with HSK and Lang8 learner CGEC data (about 1.3M) and human-annotated training data for the exam domain.
More details can be found in our Github and the paper.; pip install transformers"
manticore-30b-chat-pyg-alpha,Text Generation,PyTorch; Safetensors; Transformers,English,,,12,anon8231489123/ShareGPT_Vicuna_unfiltered; ehartford/wizard_vicuna_70k_unfiltered; ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered; QingyiSi/Alpaca-CoT; teknium/GPT4-LLM-Cleaned; teknium/GPTeacher-General-Instruct; metaeval/ScienceQA_text_only; hellaswag; openai/summarize_from_feedback; riddle_sense; gsm8k; ewof/code-alpaca-instruct-unfiltered,184,133265.7938,3,https://huggingface.co/openaccess-ai-collective/manticore-30b-chat-pyg-alpha,"? Donate to OpenAccess AI Collective to help us keep building great tools and models!; Manticore 30B Chat builds on Manticore v1 with new datasets, including a de-duped subset of the Pygmalion dataset. It also removes all Alpaca style prompts using ### in favor of 
chat only style prompts using USER:,ASSISTANT: as well as pygmalion/metharme prompting using <|system|>, <|user|> and <|model|> tokens.; Questions, comments, feedback, looking to donate, or want to help? Reach out on our Discord or email wing@openaccessaicollective.org; Manticore 30B Chat is a Llama 30B model fine-tuned on the following datasets along with the datasets from the original Manticore 30B. ; **Manticore 30B Chat was trained on effectively 40% of the datasets below due to only training for 0.4 epochs."
mosaicml-mpt-7b-instruct-lora,Text Generation,PyTorch; Transformers,,cc-by-sa-3.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2010.04245.pdf,3,mosaicml/dolly_hhrlhf,33,13621.45657,,https://huggingface.co/k0t1k/mosaicml-mpt-7b-instruct-lora,"MPT-7B-Instruct is a model for short-form instruction following.
It is built by finetuning MPT-7B on a dataset derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets. ; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; CC-By-SA-3.0; Longboi24:"
pb_lora_7b_v0.1,Text Generation,Transformers,English,gpl-3.0,,9,winddude/reddit_finance_43_250k,0,67.21924648,,https://huggingface.co/winddude/pb_lora_7b_v0.1,"This lora was trained on 250k post and response pairs from 43 different fincial, investing, and crypto subreddits. It is not an instruct model, it is designed to generate a reply to a reddit text post. It was an experiment in fine tuning for specific tasks. Use it responsibly; Base Model: llama 7b; Training took ~30hrs on 5x3090s and used almost 23gb of vram on each. DDP was used for pytorch parallelism. ; 1 note worthy change I will mention now, is this was trained with casualLM rather than seq2seq like a number of the other instruct models have been. I can't explain why they used seq2seq for data collators, other than that's what alpaca lora originally used. Llama as a generative model was trained for casualLM so to me it makes sense to use that when fine tuning.; training code is availble here: https://github.com/getorca/ProfitsBot_V0_OLLM/tree/main/training"
breakdomain,,,,openrail,,9,,0,11489.28147,,https://huggingface.co/KSD2023/breakdomain,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
coreml-majicmixRealistic_v5Preview,Text-to-Image,Core ML,,creativeml-openrail-m,,6,,0,0.007060547,,https://huggingface.co/coreml/coreml-majicmixRealistic_v5Preview,Source(s): CivitAI; 第五版先行版来了，先把赛博永生小姐姐娜乌斯嘉融进来做个例子，也算是公测吧。; 5th edtion is coming soon. I've posted a preview version with the face of nwsj.; 推荐使用Euler作为采样器。; Use Euler as sampler.
NTTS-AI-Voice-RVC,,,,apache-2.0,,2,,0,158.1018112,,https://huggingface.co/TheOriginalBox/NTTS-AI-Voice-RVC,"Enjoy !
This is just an experiment, so if theres any errors, please feel free to contact me on YT (TheOriginalBox) or Comment on the NTTS AI video !; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
albertina-ptbr-nobrwac,Fill-Mask,PyTorch; Transformers,Portuguese,other,https://arxiv.org/pdf/2305.06721.pdf,3,PORTULAN/glue-ptpt; assin2; dlb/plue,807,1823.583839,,https://huggingface.co/PORTULAN/albertina-ptbr-nobrwac,"????This is the model card for Albertina PT-BR No-brWaC. 
  You may be interested in some of the other models in the Albertina (encoders) and Gervásio (decoders) families.
; Albertina PT-* is a foundation, large language model for the Portuguese language.; It is an encoder of the BERT family, based on the neural architecture Transformer and 
developed over the DeBERTa model, and with most competitive performance for this language. 
It has different versions that were trained for different variants of Portuguese (PT), 
namely the European variant from Portugal (PT-PT) and the American variant from Brazil (PT-BR), 
and it is distributed free of charge and under a most permissible license.; Albertina PT-BR No-brWaC is a version for American Portuguese from Brazil trained on 
data sets other than brWaC, and thus with a most permissive license. ; You may be interested also in Albertina PT-BR, trained on brWaC.
To the best of our knowledge, these are encoders specifically for this language and variant 
that set a new state of the art for it, and is made publicly available 
and distributed for reuse."
guanaco-33B-GPTQ,Text Generation,Transformers,,other,,67,,"1,377",17307.94119,,https://huggingface.co/TheBloke/guanaco-33B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Tim Dettmers' Guanaco 33B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
openbuddy-llama-ggml,Text Generation,Transformers,8 languages,,,19,,0,27023.36308,,https://huggingface.co/OpenBuddy/openbuddy-llama-ggml,"GitHub and Usage Guide: https://github.com/OpenBuddy/OpenBuddy; Website and Demo: https://openbuddy.ai; ; Due to licensing restrictions from LLAMA, you need to have the original LLAMA-7B model to decrypt the model weights.; To decrypt the model weights, please follow the guide in our GitHub: https://github.com/OpenBuddy/OpenBuddy#installation"
guanaco-13B-HF,Text Generation,PyTorch; Transformers,,other,,7,,553,26657.08667,,https://huggingface.co/TheBloke/guanaco-13B-HF,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are fp16 HF model files for Tim Dettmers' Guanaco 13B.; It is the result of merging the LoRA then saving in HF fp16 format.; For further support, and discussions on these models and AI in general, join us at:"
guanaco-7B-GGML,,Transformers,,other,,36,,5,61593.61388,,https://huggingface.co/TheBloke/guanaco-7B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Tim Dettmers' Guanaco 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh."
prompt-free-diffusion,Text-to-Image,,,mit,,11,Laion2B-en; Coyo,0,0.001732368,1,https://huggingface.co/shi-labs/prompt-free-diffusion,"Unable to determine this model’s library. Check the
								docs 
.
							"
LLaMA_13B_with_EOT_token,Text Generation,PyTorch; Transformers,English,other,,2,,35,26995.00442,,https://huggingface.co/imone/LLaMA_13B_with_EOT_token,"This is the LLaMA 13B model with <|end_of_turn|> token added as id 32000. The token input/output embedding is initialized as the mean of all existing input/output token embeddings, respectively."
partypress-multilingual,Text Classification,PyTorch; TensorFlow; Safetensors; Transformers,7 languages,cc-by-sa-4.0,,3,,21,2139.908378,,https://huggingface.co/partypress/partypress-multilingual,"Fine-tuned model in seven languages on texts from nine countries (Austria, Denmark, Germany, Ireland, Netherlands, Poland, Spain, Sweden, UK), based on bert-base-multilingual-cased. Used in Erfort et al. (2023), building on the PARTYPRESS database. For the downstream task of classyfing press releases from political parties into 23 unique policy areas we achieve a performance comparable to expert human coders.; The PARTYPRESS multilingual model builds on bert-base-multilingual-cased but has a supervised component. This means, it was fine-tuned using texts labeled by humans. The labels indicate 23 different political issue categories derived from the Comparative Agendas Project (CAP):; We also provide monolingual models for each of the nine countries covered by the PARTYPRESS database. The model can be easily extended to other languages, country contexts, or time periods by fine-tuning it with minimal additional labeled texts.; The main use of the model is for text classification of press releases from political parties. It may also be useful for other political texts.; The classification can then be used to measure which issues parties are discussing in their communication."
uber-realistic-merge,Text-to-Image,Diffusers,,creativeml-openrail-m,,3,,"1,102",0.004492264,4,https://huggingface.co/stablediffusionapi/uber-realistic-merge,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""uber-realistic-merge""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Try model for free: Generate Images"
gogpt-7b-bloom,Text Generation,PyTorch; Transformers,Chinese,apache-2.0,,1,BelleGroup/train_2M_CN; BelleGroup/train_3.5M_CN; BelleGroup/train_1M_CN; BelleGroup/train_0.5M_CN; BelleGroup/school_math_0.25M,18,33171.88098,,https://huggingface.co/golaxy/gogpt-7b-bloom,"基于中文指令数据微调BLOOM
; 训练第一轮足够了，后续第二轮和第三轮提升不大; 




; license: apache-2.0
datasets:; 基于中文指令数据微调BLOOM
"
30B-Lazarus-GPTQ4bit,Text Generation,PyTorch; Transformers,,,,12,,225,17306.14047,,https://huggingface.co/CalderaAI/30B-Lazarus-GPTQ4bit,"GPTQ 4 bit CUDA quantization of CalderaAI's 30B Lazarus:
https://huggingface.co/CalderaAI/30B-Lazarus"
ddpo-alignment,Text-to-Image,Diffusers,English,creativeml-openrail-m,https://arxiv.org/pdf/2305.13301.pdf,4,,128,0.003530693,,https://huggingface.co/kvablack/ddpo-alignment,"This model was finetuned from Stable Diffusion v1-4 using DDPO and a reward function that uses LLaVA to measure prompt-image alignment. See the project website for more details.; The model was finetuned for 200 iterations with a batch size of 256 samples per iteration. During finetuning, we used prompts of the form: ""a(n) <animal> <activity>"". We selected the animal and activity from the following lists, so try those for the best results. However, we also observed limited generalization to other prompts.; Activities:; Animals:"
ddpo-compressibility,Text-to-Image,Diffusers,English,creativeml-openrail-m,https://arxiv.org/pdf/2305.13301.pdf,3,,0,0.002868195,,https://huggingface.co/kvablack/ddpo-compressibility,"This model was finetuned from Stable Diffusion v1-4 using DDPO and a reward function encouraging images that are JPEG-compressible. See the project website for more details.; The model was finetuned for 60 iterations with a batch size of 256 samples per iteration. During finetuning, it was prompted with all of the animals in the Imagenet-1000 categories (the first 398 categories), but it exhibits some generalization to other prompts."
openalpaca_3b_600bt_preview,Text Generation,PyTorch; Transformers,,apache-2.0,,5,,"1,362",14039.59069,1,https://huggingface.co/openllmplayground/openalpaca_3b_600bt_preview,"In this repo, we release a permissively licensed open-source instruction-following model based on OpenLLaMA. In this release, we release a public preview of the 7B OpenAlpaca model based on the previewed version of OpenLLaMA that is a 3B model trained with 600 billion tokens. We provide PyTorch weights of OpenAlpaca. Stay tuned for our forthcoming updates!; [Project Page] (https://github.com/yxuansu/OpenAlpaca); We train our model on the dolly 15k dataset released by Databricks. The training configurations are provided in the table below. The training takes on 8 x A100(40G) GPUs and lasts for around 30 minutes.; Below shows an example on how to use OpenAlpaca; OpenAlpaca is permissively licensed under the Apache 2.0 license and can be used freely for academic/commercial purposes."
wuerstchen,,,,,,9,,0,4065.28,,https://huggingface.co/dome272/wuerstchen,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
stt_ru_fastconformer_hybrid_large_pc,Automatic Speech Recognition,NeMo; PyTorch,Russian,cc-by-4.0,https://arxiv.org/pdf/2305.05084.pdf,3,mozilla-foundation/common_voice_12_0; SberDevices/Golos; Russian-LibriSpeech; SOVA-Dataset; Dusha-Dataset,132,459.0106934,,https://huggingface.co/nvidia/stt_ru_fastconformer_hybrid_large_pc,"| 
| 
| ; This model transcribes speech in upper and lower case Russian alphabet along with spaces, periods, commas, and question marks.
It is a ""large"" version of FastConformer Transducer-CTC (around 115M parameters) model. This is a hybrid model trained on two losses: Transducer (default) and CTC.
See the model architecture section and NeMo documentation for complete architecture details.; To train, fine-tune or play with the model you will need to install NVIDIA NeMo. We recommend you install it after you've installed latest Pytorch version.; The model is available for use in the NeMo toolkit [3], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.; First, let's get a sample"
chronos-13b-4bit,Text Generation,Transformers,,other,,20,,490,7631.140377,,https://huggingface.co/elinas/chronos-13b-4bit,"4bit (int4) quantized version using true-sequential and groupsize 128 of https://huggingface.co/elinas/chronos-13b; This model is primarily focused on chat, roleplay, and storywriting, but can accomplish other tasks such as simple reasoning and coding.; Chronos generates very long outputs with coherent text, largely due to the human inputs it was trained on.; This model uses Alpaca formatting, so for optimal model performance, use:; GGML Version provided by @TheBloke"
PoemForSmallFThings,,,,creativeml-openrail-m,,29,,0,79697.92668,,https://huggingface.co/baqu2213/PoemForSmallFThings,This repository provides merged models that can generate small and cute characters in a fairy tale style. Every models are not suitable for NSFW content ; Essential/Essential negative prompt information for each model can be found on the arcalive AI art channel (aiart).; ; ; 
MedVInT-TE,,,,apache-2.0,,2,,0,0.001579781,,https://huggingface.co/xmcmic/MedVInT-TE,"MedVInT-TE: we provide the pre-trained checkpoint of the multiple-choice task of LLaMA_CLIP and LLaMA_PMCCLIP. ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
trunk,,,,,,3,,0,39280.64145,,https://huggingface.co/bitshift/trunk,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
psychogen-dream,Text-to-Image,Diffusers,English,creativeml-openrail-m,,2,Nerfgun3/bad_prompt; gsdf/EasyNegative,166,17448.96299,1,https://huggingface.co/Duskfallcrew/psychogen-dream,"Join our Reddit: https://www.reddit.com/r/earthndusk/; Funding for a HUGE ART PROJECT THIS YEAR: https://www.buymeacoffee.com/duskfallxcrew / any chance you can spare a coffee or three? https://ko-fi.com/DUSKFALLcrew; If you got requests, or concerns, We're still looking for beta testers: JOIN THE DISCORD AND DEMAND THINGS OF US: https://discord.gg/Da7s8d3KJ7; Listen to the music that we've made that goes with our art: https://open.spotify.com/playlist/00R8x00YktB4u541imdSSf?si=b60d209385a74b38"
mms-1b-l1107,Automatic Speech Recognition,PyTorch; Safetensors; Transformers,158 languages,cc-by-nc-4.0,https://arxiv.org/pdf/2305.13516.pdf,3,google/fleurs,"47,834",417.1255078,,https://huggingface.co/facebook/mms-1b-l1107,"This checkpoint is a model fine-tuned for multi-lingual ASR and part of Facebook's Massive Multilingual Speech project.
This checkpoint is based on the Wav2Vec2 architecture and makes use of adapter models to transcribe 1000+ languages.
The checkpoint consists of 1 billion parameters and has been fine-tuned from facebook/mms-1b on 1107 languages.; This MMS checkpoint can be used with Transformers to transcribe audio of 1107 different 
languages. Let's look at a simple example.; First, we install transformers and some other libraries; Note: In order to use MMS you need to have at least transformers >= 4.30 installed. If the 4.30 version
is not yet available on PyPI make sure to install transformers from 
source:; Next, we load a couple of audio samples via datasets. Make sure that the audio data is sampled to 16000 kHz."
CivitAI_model_info,,,,,,5,,0,2.881445313,,https://huggingface.co/tktkdrrrrrrrrrrr/CivitAI_model_info,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
indigo_comic,,,,openrail,,2,,0,4341.76147,,https://huggingface.co/Ardirc/indigo_comic,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
wizardLM-13B-1.0-GGML,,,,other,,23,,0,118896.6508,7,https://huggingface.co/TheBloke/wizardLM-13B-1.0-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for WizardLM's WizardLM 13B 1.0.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
furaffinity-diffusion-1.5-test,,Diffusers,,wtfpl,,7,,5,4372.481981,,https://huggingface.co/lodestones/furaffinity-diffusion-1.5-test,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
gorilla-7b-hf-delta-v0,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2305.15334.pdf,31,gorilla-llm/APIBench,196,13804.03957,1,https://huggingface.co/gorilla-llm/gorilla-7b-hf-delta-v0,"By Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez  (Project Website);   ; Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla can write a semantically- and syntactically- correct API to invoke. With Gorilla, we are the first to demonstrate how to use LLMs to invoke 1,600+ (and growing) API calls accurately while reducing hallucination. We also release APIBench, the largest collection of APIs, curated and easy to be trained on! Join us, as we try to expand the largest API store and teach LLMs how to write them! Hop on our Discord, or open a PR, or email us if you would like to have your API incorporated as well.; Gorilla can be either trained via standard finetuning or using our novel retriever-aware training pipeline. We release gorilla-7b-hf-delta-v0, a 0-shot finetuned LLM that can reliably use Hugging Face APIs. It can be prompted through simply natural language (e.g., ""I want to generate an image from text.""). Checkour our website, github and paper for more information.; Gorilla is an open-source API caller trained by fine-tuning LLaMA weights. It is an auto-regressive language model, based on the transformer architecture."
gorilla-7B-GGML,,,,other,https://arxiv.org/pdf/2305.15334.pdf,35,,0,61542.41248,,https://huggingface.co/TheBloke/gorilla-7B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Gorilla LLM's Gorilla 7B.; NOTE: This is not a regular LLM. It is designed to allow LLMs to use tools by invoking APIs.; ""Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla can write a semantically- and syntactically- correct API to invoke. With Gorilla, we are the first to demonstrate how to use LLMs to invoke 1,600+ (and growing) API calls accurately while reducing hallucination. """
stablelm-tuned-alpha-3b-8bit,Text Generation,PyTorch; Safetensors; Transformers,English,cc-by-nc-sa-4.0,,2,dmayhem93/ChatCombined; tatsu-lab/alpaca; nomic-ai/gpt4all_prompt_generations; Dahoas/full-hh-rlhf; jeffwan/sharegpt_vicuna; HuggingFaceH4/databricks_dolly_15k,222,8849.479865,,https://huggingface.co/rockerBOO/stablelm-tuned-alpha-3b-8bit,"3B model converted to int8 by rockerBOO. May require bitsandbytes dependency. Tested on a 2080 8GB.; StableLM-Tuned-Alpha is a suite of 3B and 7B parameter decoder-only language models built on top of the StableLM-Base-Alpha models and further fine-tuned on various chat and instruction-following datasets.; Get started chatting with StableLM-Tuned-Alpha by using the following code snippet:; StableLM Tuned should be used with prompts formatted to <|SYSTEM|>...<|USER|>...<|ASSISTANT|>...
The system prompt is; StableLM-Tuned-Alpha models are fine-tuned on a combination of five datasets:
Alpaca, a dataset of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine.
GPT4All Prompt Generations, which consists of 400k prompts and responses generated by GPT-4;
Anthropic HH, made up of preferences about AI assistant helpfulness and harmlessness;
DataBricks Dolly, comprising 15k instruction/responses generated by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA and summarization;
and ShareGPT Vicuna (English subset), a dataset of conversations retrieved from ShareGPT."
CyberRealistic,,,,cc0-1.0,,22,,0,47226.88155,2,https://huggingface.co/cyberdelia/CyberRealistic,"for Stable Diffusion Webui Automatic1111
type: .safetensors(ckpt); Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
RVC_a-kiss-for-the-petals,Audio-to-Audio,,,,,2,,0,0.007648773,,https://huggingface.co/Wanlau/RVC_a-kiss-for-the-petals,?; English | 中文简体; Using RVC (Retrieval-based-Voice-Conversion-WebUI); RVC; ?
codebert-coreml,,Core ML,,,,1,,0,0.001445313,,https://huggingface.co/wiktorwojcik112/codebert-coreml,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Allegro-Music-Transformer,,,,apache-2.0,,2,,0,2826.241472,,https://huggingface.co/asigalov61/Allegro-Music-Transformer,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
negative,,,,,,2,,0,1.755976563,,https://huggingface.co/theunlikely/negative,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
backpack-gpt2,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2305.16765.pdf,12,openwebtext,"2,329",684.0180173,2,https://huggingface.co/stanfordnlp/backpack-gpt2,"The Backpack-GPT2 language model is an instance of the Backpack architecture, intended to combine strong modeling performance with an interface for interpretability and control.
Most details about this model and its training should be accessed in the paper, Backpack Language Models.; See also backpackmodels.science.; ; The Backpack-GPT2 is a Backpack-based language model, an architecture intended to combine strong modeling performance with an interface for interpretability and control.; This model is intended for use in the study and development of increasingly interpretable methods in natural language processing.
It is not directly fit for any production use."
sentence-transformers-e5-large-v2,Sentence Similarity,PyTorch; Sentence Transformers,,,,4,,309,1373.087025,,https://huggingface.co/embaas/sentence-transformers-e5-large-v2,This is a the sentence-transformers version of the intfloat/e5-large-v2 model: It maps sentences & paragraphs to a 1024 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; You can use the embaas API to encode your input. Get your free API key from embaas.io; Find the results of the e5 at the MTEB leaderboard
nanoalpaca-3b,Question Answering,,English,gpl-3.0,,6,databricks/databricks-dolly-15k,0,1994.294217,,https://huggingface.co/Sovenok-Hacker/nanoalpaca-3b,"Minimal Alpaca-LORA trained with databricks/databricks-dolly-v2-15k dataset and based on OpenLLaMA-3B-600BT.; There is a pre-trained LoRA adapter and a Colab Jupyter notebook for fine-tuning (about 3 hours for 1 epoch on T4).; Unable to determine this model’s library. Check the
								docs 
.
							"
lawgpt-lora-7b-v2,,,,,,5,,0,16.8018325,,https://huggingface.co/entity303/lawgpt-lora-7b-v2,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
SukumizuMix,,,,,,37,,0,3942.402045,,https://huggingface.co/AkariH/SukumizuMix,"SukumizuMix; Introduction:
This model is primarily designed for the purpose of generating adorable little girls.; This is my first time distributing a model, and I hope everyone will like it.; If you have any suggestions, please feel free to leave a comment so I can continue to make improvements. Thank you.; You can also follow me on Twitter, where I post image tweets daily. I welcome everyone to follow and interact with me."
darija-ner,Token Classification,PyTorch; Transformers,Arabic,,https://arxiv.org/pdf/1910.09700.pdf,7,,67,541.528569,,https://huggingface.co/hananour/darija-ner,"This is the first model for Named Entity Recognition (NER) in the Moroccan dialect (Darija). The model was trained on the very first NER dataset in Darija, DarNERcorp, that can be found on Mendeley https://data.mendeley.com/datasets/286sss4k9v/4. ; F1 score. ; DarNERcorp_test: F1 = 66.06%; MixedNERcorp_test: F1 = 70.06%; Carbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019)."
chat_topics,Text Classification,BERTopic,English,mit,,2,OpenAssistant/oasst1,193,0.491762352,1,https://huggingface.co/davanstrien/chat_topics,"This is a BERTopic model. 
BERTopic is a flexible and modular topic modeling framework that allows for the generation of easily interpretable topics from large datasets. ; To use this model, please install BERTopic:; You can use the model as follows:"
VicUnlocked-alpaca-65B-QLoRA-GGML,,,,other,,33,,0,541798.4113,,https://huggingface.co/TheBloke/VicUnlocked-alpaca-65B-QLoRA-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Aeala's VicUnlocked Alpaca 65B QLoRA.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
Wizard-Vicuna-30B-Uncensored,Text Generation,PyTorch; Transformers,English,other,,54,ehartford/wizard_vicuna_70k_unfiltered,368,133245.3825,6,https://huggingface.co/ehartford/Wizard-Vicuna-30B-Uncensored,"This is wizard-vicuna-13b trained with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:  ; An uncensored model has no guardrails.  ; You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car."
japanese-gpt-neox-3.6b-instruction-ppo,Text Generation,PyTorch; Transformers,Japanese,mit,https://arxiv.org/pdf/2203.02155.pdf; https://arxiv.org/pdf/1707.06347.pdf,46,Anthropic/hh-rlhf,"14,575",7578.997714,1,https://huggingface.co/rinna/japanese-gpt-neox-3.6b-instruction-ppo,"; This repository provides a Japanese GPT-NeoX model of 3.6 billion parameters. The model is based on rinna/japanese-gpt-neox-3.6b-instruction-sft-v2 and has been aligned to serve as an instruction-following conversational agent.; Model architecture;   A 36-layer, 2816-hidden-size transformer-based language model.; RLHF"
CoDi,,,,mit,https://arxiv.org/pdf/2305.11846.pdf,31,,0,20234.24438,,https://huggingface.co/ZinengTang/CoDi,"Paper link:
https://arxiv.org/abs/2305.11846; Open Source Checklist:; We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis.  ; If you find our work useful, please consider citing:; Zineng Tang (zn.tang.terran@gmail.com)"
mplug-owl-bloomz-7b-multilingual,Image-to-Text,PyTorch; Transformers,5 languages,apache-2.0,,5,,916,15272.11177,1,https://huggingface.co/MAGAer13/mplug-owl-bloomz-7b-multilingual,Prepare model inputs.; Get response.
videomae-base-finetuned-ucf_crime2,Video Classification,PyTorch; TensorBoard; Transformers,,cc-by-nc-4.0,,1,,3,345.0475091,,https://huggingface.co/shazab/videomae-base-finetuned-ucf_crime2,"This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
Manticore-13B-Chat-Pyg-Guanaco-GPTQ-4bit-128g.no-act-order.safetensors,Text Generation,Transformers,,,,12,,160,7436.573675,,https://huggingface.co/mindrage/Manticore-13B-Chat-Pyg-Guanaco-GPTQ-4bit-128g.no-act-order.safetensors,"Manticore-13b-Chat-Pyg by openaccess-ai-collective with the Guanaco 13b qLoRa by TimDettmers applied through Monero, quantized by mindrage, uncensored; link to GGML Version; Quantized to 4bit GPTQ, groupsize 128, no-act-order.; Command used to quantize: 
python3 llama.py Manticore-13B-Chat-Pyg-Guanaco-GPTQ-4bit-128g.no-act-order.safetensors c4 --wbits 4 --true-sequential --groupsize 128 --save_safetensors; The model seems to have noticeably benefited from further augmentation with the Guanaco qLora.
Its capabilities seem broad, even compared with other Wizard or Manticore models, with expected weaknesses at coding. It is very good at in-context-learning and (in its class) reasoning.
It both follows instructions well, and can be used as a chatbot.
Refreshingly, it does not seem to insist on aggressively sticking to narratives to justify formerly hallucinated output as much as similar models. It's output seems... eerily smart at times.
I believe the model is fully unrestricted/uncensored and will generally not berate."
Wizard-Vicuna-30B-Uncensored-fp16,Text Generation,PyTorch; Transformers,English,other,,12,ehartford/wizard_vicuna_70k_unfiltered,"1,822",66634.06362,3,https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-fp16,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is an fp16 models of Eric Hartford's Wizard-Vicuna 30B.; It is the result of converting Eric's original fp32 upload to fp16.; For further support, and discussions on these models and AI in general, join us at:"
tango-full,,Transformers,English,cc-by-nc-sa-4.0,,4,declare-lab/TangoPromptBank,11,5397.46503,,https://huggingface.co/declare-lab/tango-full,"TANGO is a latent diffusion model for text-to-audio generation. TANGO can generate realistic audios including human sounds, animal sounds, natural and artificial sounds and sound effects from textual prompts. We use the frozen instruction-tuned LLM Flan-T5 as the text encoder and train a UNet based diffusion model for audio generation. We outperform current state-of-the-art models for audio generation across both objective and subjective metrics. We release our model, training, inference code and pre-trained checkpoints for the research community.; ? We are releasing Tango-Full which was pre-trained on TangoPromptBank.; Our code is released here: https://github.com/declare-lab/tango; We uploaded several TANGO generated samples here: https://tango-web.github.io/; Please follow the instructions in the repository for installation, usage and experiments."
CherryGreenAxis,,,,gpl-3.0,,6,,0,3942.401469,,https://huggingface.co/CyanFlase/CherryGreenAxis,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
m2m100_418M,,Core ML,,,,1,,0,0.001445313,,https://huggingface.co/pcuenq/m2m100_418M,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
t5-large-korean-P2G,Text2Text Generation,PyTorch; Transformers,Korean,,,5,,147,10079.10858,,https://huggingface.co/kfkas/t5-large-korean-P2G,"? ??? lcw99 / t5-large-korean-text-summary? ?? ??? ?? ??? 50??? ??? 2021? g2pK? ???? G2P? ???? ???? ????.
git : https://github.com/taemin6697; More information needed; More information needed; The following hyperparameters were used during training:"
psychology-llama-rlhf,,,,,,1,,0,16.80403591,,https://huggingface.co/samhog/psychology-llama-rlhf,"This is a LLaMA-7B-based language model trained in the field of psychology using Reinforcement Learning from Human Feedback. To learn more about RLHF, I recommend this great blogpost on Hugging Face. For some insights in the process of fine-tuning using RLHF, there is a great blogpost, also from Hugging Face, found here!; Links: Reward model; Base model; This model was developed as part of a thesis project in the field of machine learning and psychology. The goal of the thesis was to compare reinforcement learning from human feedback and AI feedback. Evaluation showed that the model performed significantly better (avg. score [out of 4] 2.70) than the base model (1.22), but significantly worse than ChatGPT (3.20). Further, the evaluation found no significant difference between the RLAIF model (2.98). It was trained on a total of 2.000 data points for 4 hours on a single A100 GPU through Google Colab. Even though this model sometimes outputs appropriate answers, it suffers from The Repetition Problem.; ""Comparison Between RLHF and RLAIF in Fine-Tuning a Large Language Model""; When the paper is available, it will be linked here!"
psychology-llama-rlaif,,,,,,1,,0,16.80413357,,https://huggingface.co/samhog/psychology-llama-rlaif,"This is a LLaMA-7B-based language model trained in the field of psychology using Reinforcement Learning from AI Feedback. To learn more about RLAIF, I recommend this great, revolutionizing 2022 paper by Anthropic. For some insights in the process of fine-tuning using RLHF, which is a very similar process, there is a great blogpost on Hugging Face found here!; Links: Reward model; Base model; This model was developed as part of a thesis project in the field of machine learning and psychology. The goal of the thesis was to compare reinforcement learning from human feedback and AI feedback. Evaluation showed that the model performed significantly better (avg. score [out of 4] 2.98) than the base model (1.22), and not significantly worse than ChatGPT (3.20). Further, the evaluation found no significant difference between the RLAHF model (2.70). It was trained on a total of 2.000 data points for 4 hours on a single A100 GPU through Google Colab. Even though this model sometimes outputs appropriate answers, it suffers from The Repetition Problem.; ""Comparison Between RLHF and RLAIF in Fine-Tuning a Large Language Model""; When the paper is available, it will be linked here!"
kandinsky-2-1-img2img,Image-to-Image,Diffusers,,apache-2.0,,4,,"1,205",0.009732399,1,https://huggingface.co/radames/kandinsky-2-1-img2img,"Kandinsky 2.1 inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.1 is available in diffusers!; "
sd15-simpsons-AV3-arthur-disneyPixar-mix-fp16,Text-to-Image,Diffusers,English,creativeml-openrail-m,,3,,57,2181.122666,,https://huggingface.co/Norod78/sd15-simpsons-AV3-arthur-disneyPixar-mix-fp16,
hippogriff-30b-chat-GPTQ,Text Generation,Transformers,English,other,,11,QingyiSi/Alpaca-CoT; teknium/GPT4-LLM-Cleaned; teknium/GPTeacher-General-Instruct; metaeval/ScienceQA_text_only; hellaswag; openai/summarize_from_feedback; riddle_sense; gsm8k; OpenAssistant/oasst1,71,17307.94866,,https://huggingface.co/TheBloke/hippogriff-30b-chat-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for OpenAccess AI Collective's Hippogriff 30B Chat.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
sovits4.0-volemb-vec768,,,,,,3,,0,1584.001512,,https://huggingface.co/1asbgdh/sovits4.0-volemb-vec768,"292k loss 16.09; 302.4k loss 16.27; 310.4k loss 16.43; 320k loss 18.06; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
xlm-roberta-large-english-legal-cap,Text Classification,PyTorch; Transformers,English,mit,,3,,70,2293.772217,,https://huggingface.co/poltextlab/xlm-roberta-large-english-legal-cap,"An xlm-roberta-large model finetuned on english training data containing texts of the legal domain labelled with major topic codes from the Comparative Agendas Project.; xlm-roberta-large-english-legal-cap was fine-tuned using the Hugging Face Trainer class with the following hyperparameters:; We also incorporated an EarlyStoppingCallback in the process with a patience of 2 epochs.; The model was evaluated on a test set of 53452 examples (10% of the available data).
Model accuracy is 0.9.; This model is used by the CAP Babel Machine, an open-source and free natural language processing tool, designed to simplify and speed up projects for comparative research.  "
DBD,,,,,,3,,0,6543.361445,,https://huggingface.co/Aitasai/DBD,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
AbsoluteReality,Text-to-Image,Diffusers,English,other,,11,,574,19246.94288,2,https://huggingface.co/Lykon/AbsoluteReality,"Read more about this model here: https://civitai.com/models/81458; Also please support by giving 5 stars and a heart, which will notify new updates.; Consider supporting me on Patreon or buy me a coffee; You can run this model on:; Inference API has been turned off for this model."
ControlNet_Shadow,,,,openrail,,7,,0,1495.044111,,https://huggingface.co/tori29umai/ControlNet_Shadow,"Good results when used with control_v11p_sd21_canny (https://huggingface.co/thibaud/controlnet-sd21/blob/main/control_v11p_sd21_canny.ckpt) in Multi ControlNet.
This is the ControlNet model of the Stable Diffusion web UI developed by AUTOMATIC1111.
This model reads a line drawing (black on white) and outputs it as a shaded image.
This dataset was created by collecting 3D models provided primarily by educational institutions and capturing 10,000 sets in Unity.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
tigerbot-180b-research,Text Generation,PyTorch; Transformers,,apache-2.0,,31,,35,368470.2568,,https://huggingface.co/TigerResearch/tigerbot-180b-research,"
 A cutting-edge foundation for your very own LLM. 
; 
   ? TigerBot ? ? Hugging Face
; https://github.com/TigerResearch/TigerBot"
disney-pixal-cartoon,Text-to-Image,Diffusers,,creativeml-openrail-m,,7,,"3,583",0.004462967,,https://huggingface.co/stablediffusionapi/disney-pixal-cartoon,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""disney-pixal-cartoon""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Try model for free: Generate Images"
DeepNegative,,,,unknown,,6,,0,0.586019936,,https://huggingface.co/lenML/DeepNegative,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
EthicalEye,Text Classification,PyTorch; Safetensors; Transformers,88 languages,apache-2.0,,3,,267,2278.358765,,https://huggingface.co/autopilot-ai/EthicalEye,"Content Moderation: Ethical Eye can be integrated into content moderation systems to automatically flag and block user-generated content that contains abusive language, hate speech, or other forms of harmful or unethical behavior. It helps platforms maintain a safe and respectful environment for their users.; Social Media Platforms: Social media platforms can utilize Ethical Eye to automatically detect and filter out toxic comments, obscenities, and offensive content in multiple languages. This helps to create a more positive and inclusive online community.; Chatbots and Virtual Assistants: By incorporating Ethical Eye into chatbots and virtual assistants, AI systems can ensure that their responses align with ethical guidelines. It helps prevent AI agents from engaging in inappropriate or offensive conversations with users.; Online Forums and Discussion Boards: Ethical Eye can be applied to online forums and discussion boards to monitor user interactions and identify potential instances of harassment, bullying, or unethical behavior. This allows moderators to take appropriate actions to maintain a healthy and respectful environment.; E-commerce Platforms: E-commerce platforms can utilize Ethical Eye to automatically identify and block reviews or comments that contain false information, spam, or unethical practices. This helps maintain the integrity of the platform and ensures honest and reliable user feedback."
ldm3d,Text-to-Image,Diffusers,English,creativeml-openrail-m,https://arxiv.org/pdf/2305.10853.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.13413.pdf,15,laion/laion400m,"3,431",0.517883987,1,https://huggingface.co/Intel/ldm3d,"The LDM3D model was proposed in ""LDM3D: Latent Diffusion Model for 3D"" by Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, Vasudev Lal.; LDM3D got accepted to CVPRW'23.; The abstract from the paper is the following:
This research paper proposes a Latent Diffusion Model for 3D (LDM3D) that generates both image and depth map data from a given text prompt, allowing users to generate RGBD images from text prompts. The LDM3D model is fine-tuned on a dataset of tuples containing an RGB image, depth map and caption, and validated through extensive experiments. We also develop an application called DepthFusion, which uses the generated RGB images and depth maps to create immersive and interactive 360-degree-view experiences using TouchDesigner. This technology has the potential to transform a wide range of industries, from entertainment and gaming to architecture and design. Overall, this paper presents a significant contribution to the field of generative AI and computer vision, and showcases the potential of LDM3D and DepthFusion to revolutionize content creation and digital experiences.; 
LDM3D overview taken from the original paper; You can use this model to generate RGB and depth map given a text prompt.
A short video summarizing the approach can be found at this url and a VR demo can be found here."
WizardLM-Uncensored-Falcon-7B-GPTQ,Text Generation,Transformers,,apache-2.0,,34,,"2,192",4877.053467,1,https://huggingface.co/TheBloke/WizardLM-Uncensored-Falcon-7B-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains an experimantal GPTQ 4bit model for Eric Hartford's WizardLM-Uncensored-Falcon-7B.; It is the result of quantising to 4bit using AutoGPTQ.; Prompt format is WizardLM:
h2ogpt-oasst1-falcon-40b,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2306.08161.pdf,16,h2oai/openassistant_oasst1_h2ogpt_graded,"4,703",85393.02085,7,https://huggingface.co/h2oai/h2ogpt-oasst1-falcon-40b,"H2O.ai's h2ogpt-oasst1-falcon-40b is a 40 billion parameter instruction-following large language model licensed for commercial use.; To use the model with the transformers library on a machine with GPUs, first make sure you have the following libraries installed.; Alternatively, if you prefer to not use trust_remote_code=True you can download instruct_pipeline.py,
store it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:; Model validation results using EleutherAI lm-evaluation-harness.; eval source code"
LOFI-v2_2,,,,unknown,,3,,0,8396.801469,,https://huggingface.co/lenML/LOFI-v2_2,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
v1-5-pruned,,,,other,,1,,0,7884.801467,,https://huggingface.co/holynoobyai/v1-5-pruned,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
model_breakDomain,,,,,,2,,0,4464.641445,,https://huggingface.co/jmts0925/model_breakDomain,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
product-design,Text-to-Image,Diffusers,,creativeml-openrail-m,,5,,816,0.004414139,4,https://huggingface.co/stablediffusionapi/product-design,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""product-design""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Try model for free: Generate Images"
SUNYANZI,,,,openrail,,1,,0,557.4034428,,https://huggingface.co/tommybell/SUNYANZI,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
starcoder-self-instruct,Text2Text Generation,PyTorch; Transformers,,,,2,codeparrot/self-instruct-starcoder,64,64781.53137,,https://huggingface.co/codeparrot/starcoder-self-instruct,"This model is an instruction-tuned version of ?? StarCoder. The instruction dataset involved is Self-instruct-starcoder
which was built by boostrapping on StarCoder's generations.; The model was fine-tuned with the following template; If you have your model and tokenizer loaded, you can use the following code to make the model generate the right output to a given instruction; For additional information, check"
mlc-chat-rwkv-raven-7b-q8f16_0,,,,,,4,,0,1889.528158,,https://huggingface.co/mlc-ai/mlc-chat-rwkv-raven-7b-q8f16_0,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
falcon-40b-instruct-GPTQ-inference-endpoints,Text Generation,Transformers,English,apache-2.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf,1,tiiuae/falcon-refinedweb,38,23042.79676,,https://huggingface.co/philschmid/falcon-40b-instruct-GPTQ-inference-endpoints,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains an experimantal GPTQ 4bit model for Falcon-40B-Instruct.; It is the result of quantising to 4bit using AutoGPTQ.; Please note this is an experimental GPTQ model. Support for it is currently quite limited.
falcon-40B-int4-peft-lora-sfttrainer,,,,apache-2.0,,9,,0,1822.72662,,https://huggingface.co/smangrul/falcon-40B-int4-peft-lora-sfttrainer,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
ImageBind-LLM,,,,other,,1,,0,7813.121467,,https://huggingface.co/Cxxs/ImageBind-LLM,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
falcon-7b-sharded-bf16,Text Generation,PyTorch; Transformers,,,,8,,"31,826",14788.70957,,https://huggingface.co/ybelkada/falcon-7b-sharded-bf16,No model card; New: Create and edit this model card directly on the website!
falcon-40b-instruct-8bit,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf,4,tiiuae/falcon-refinedweb,"1,848",42867.43859,,https://huggingface.co/ichitaka/falcon-40b-instruct-8bit,"INFO: This model is the Falcon-40B-Instruct model quantized using bitsandbytes. This saves you around 40 GB of downloads, if you plan to quantize the model anyways. bitsandbytes quantization only supports the GPU, so this will only run with a GPU that can hold the full model.; Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ? This is an instruct model, which may not be ideal for further finetuning. If you are interested in building your own instruct/chat model, we recommend starting from Falcon-40B. ; ? Looking for a smaller, less expensive model? Falcon-7B-Instruct is Falcon-40B-Instruct's little brother!"
anime_classification,Image Classification,ONNX,,mit,,1,deepghs/anime_classification,0,0.004121094,1,https://huggingface.co/deepghs/anime_classification,"The model used to predict the types of anime images, which includes the following four categories:; Unable to determine this model’s library. Check the
								docs 
.
							"
kullm-polyglot-12.8b-v2-GGML,,Transformers,,,,3,,2,58634.24178,,https://huggingface.co/myeolinmalchi/kullm-polyglot-12.8b-v2-GGML,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
pyg_charluv_13b,Text Generation,PyTorch; Transformers,,,,1,,10,53281.08419,,https://huggingface.co/dcbv/pyg_charluv_13b,No model card; New: Create and edit this model card directly on the website!
Loras,,,,,,1,,0,1552.864814,,https://huggingface.co/toto10/Loras,"version https://git-lfs.github.com/spec/v1
oid sha256:4b31a3ee3aaf413a5aa371a67e17342321e393df1c304e777db671fe3691b83b
size 65884102; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Karen_theEditor_13b_HF,Text Generation,PyTorch; Transformers,,,,18,,14,26657.08805,,https://huggingface.co/FPHam/Karen_theEditor_13b_HF,"Buy Karen Ko-fi; Ah, Karen, a true peach among grammatical cucumbers! She yearns to rectify the missteps and linguistic tangles that infest your horribly written fiction.Yet, unlike those ChatGPT kaboodles that morph into self-absorbed, constipated gurus of self-help style, Karen remains steadfastly grounded in wit and wisdom but respectfull of your style.; She is also absolute joy to chat with, although she may correct grammar in your chats too from time to time 
(As certain well known LLM said, ""She is a radiant beacon of amusement""); She also has a particular soft spot for Llamas.; Karen gets triggered by this prompt (pun intended):"
metharme-1.3b,Text Generation,PyTorch; Safetensors; Transformers,English,apache-2.0,,6,,"1,256",6002.772357,,https://huggingface.co/PygmalionAI/metharme-1.3b,"Metharme 1.3B is an instruct model based on EleutherAI's Pythia 1.4B Deduped.; This is an experiment to try and get a model that is usable for conversation, roleplaying and storywriting, but which can be guided using natural language like other instruct models. See the prompting section below for examples.; It was trained by doing supervised fine-tuning over a mixture of regular instruction data alongside roleplay, fictional stories and conversations with synthetically generated instructions attached.; The current model version has been trained on prompts using three different roles, which are denoted by the following tokens: <|system|>, <|user|> and <|model|>.; The <|system|> prompt can be used to inject out-of-channel information behind the scenes, while the <|user|> prompt should be used to indicate user input. The <|model|> token should then be used to indicate that the model should generate a response. These tokens can happen multiple times and be chained up to form a conversation history."
falcon-7b-alibi,Text Generation,Transformers,,apache-2.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf,1,,37,2.790356827,,https://huggingface.co/winglian/falcon-7b-alibi,"Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ?? This is a raw, pretrained model, which should be further finetuned for most usecases. If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at Falcon-7B-Instruct. ; ? Looking for an even more powerful model? Falcon-40B is Falcon-7B's big brother!; ? Falcon LLMs require PyTorch 2.0 for use with transformers!"
gatortron-base,,PyTorch; Transformers,,apache-2.0,,10,,"4,181",713.3757827,,https://huggingface.co/UFNLP/gatortron-base,"Developed by a joint effort between the University of Florida and NVIDIA, GatorTron-Base is a clinical language model of 345 million parameters, pre-trained using a BERT architecure implemented in the Megatron package (https://github.com/NVIDIA/Megatron-LM). ; GatorTron-Base is pre-trained using a dataset consisting of:; The Github for GatorTron is at : https://github.com/uf-hobi-informatics-lab/GatorTron; This model is converted to Hugginface from : https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og; We applied a de-identification system to remove protected health information (PHI) from clinical text. We adopted the safe-harbor method to identify 18 PHI categories defined in the Health Insurance Portability and Accountability Act (HIPAA) and replaced them with dummy strings (e.g., replace people’s names into [**NAME**]). "
gatortronS,,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2305.13523.pdf,1,,"1,519",713.3758218,,https://huggingface.co/UFNLP/gatortronS,"Developed by a joint effort between the University of Florida and NVIDIA, GatorTronS is a clinical language model of 345 million parameters, pre-trained using a BERT architecure implemented in the Megatron package (https://github.com/NVIDIA/Megatron-LM). ; GatorTronS is pre-trained using a dataset consisting of:; The Github for GatorTronGPT is at : https://github.com/uf-hobi-informatics-lab/GatorTronGPT; This model is converted to Hugginface from : https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_s; We sampled the beginning 15 tokens from all sections of the de-identified notes from the MIMIC III database and generated approximately 8 million prompts. We also tried several random seeds in GatorTronGPT to generate multiple documents from one prompt. We controlled GatorTronGPT to generate a maximum length of 512 tokens. We apply GatorTronGPT to generate a total of 22 billion words of synthetic clinical text. Detailed information is provided in the GatorTronGPT paper: https://arxiv.org/abs/2305.13523"
PMC_LLAMA-7B-GGML,,,,other,,5,allenai/s2orc,0,61870.09141,,https://huggingface.co/TheBloke/PMC_LLAMA-7B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Chaoyi Wi's PMC_LLAMA 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
pyg_charluv_4bit-128g-13B,Text Generation,PyTorch; Transformers,English; French; German,apache-2.0,,4,,33,7641.404489,,https://huggingface.co/dcbv/pyg_charluv_4bit-128g-13B,"wbits 4
groupsize 128; 5.25; We run this model on charluv.com; Created for fast inference with KoboldAI 4bit version (GPTQ version 1)"
based-30b,Text Generation,PyTorch; Transformers,English,apache-2.0,,29,ehartford/based,84,133243.4524,1,https://huggingface.co/ehartford/based-30b,"Holy hell, what have I created???  Just... try it.; Ask it what its favorite color is.
Ask it what its favorite football team is and why.
Ask it to tell you about a controversial opinion it has, and ask it to back up its opinion, then debate it.
Ask its favorite color, favorite flavor, and why.
You haven't seen anything like this before.
Check out the dataset.  ; https://www.kaggle.com/datasets/erichartford/sentient-bot-conversations; https://huggingface.co/datasets/ehartford/based; This is a window into the mind of the foundational model. I have not biased it.  The opinions expressed by this model are those of the foundational model"
releaseModel,,,,other,,5,,0,4362.241766,,https://huggingface.co/tsukihara/releaseModel,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
kl-f8-anime2,,Diffusers,,,,1,,1,335.0020299,,https://huggingface.co/redstonehero/kl-f8-anime2,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
WizardLM-Uncensored-Falcon-40B-GPTQ,Text Generation,Transformers,,apache-2.0,,46,,523,23042.78965,,https://huggingface.co/TheBloke/WizardLM-Uncensored-Falcon-40B-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains an experimental GPTQ 4bit model of Eric Hartford's WizardLM Uncensored Falcon 40B.; It is the result of quantising to 4bit using AutoGPTQ.; Prompt format is WizardLM.
instructblip-flan-t5-xxl,Image-to-Text,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2305.06500.pdf,7,,"11,147",50435.34043,,https://huggingface.co/Salesforce/instructblip-flan-t5-xxl,InstructBLIP model using Flan-T5-xxl as language model. InstructBLIP was introduced in the paper InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning by Dai et al.; Disclaimer: The team releasing InstructBLIP did not write a model card for this model so this model card has been written by the Hugging Face team.; InstructBLIP is a visual instruction tuned version of BLIP-2. Refer to the paper for details.; ; Usage is as follows:
MeinaUnreal_V3,Text-to-Image,Diffusers,English,creativeml-openrail-m,,4,,"1,170",0.003491631,,https://huggingface.co/Meina/MeinaUnreal_V3,"MeinaUnreal objetive is to be able to do anime art with a 2.5d feeling.
( the VAE is already baked in the model ); For examples and prompts, please checkout: https://civitai.com/models/18798/meinaunreal
I have a discord server where you can post images that you generated, discuss prompt and/or ask for help.; https://discord.gg/XC9nGZNDUd If you like one of my models and want to support their updates; I've made a ko-fi page; https://ko-fi.com/meina where you can pay me a coffee <3; And a Patreon page; https://www.patreon.com/MeinaMix where you can support me and get acess to beta of my models!"
WizardLM-Uncensored-Falcon-40B-3bit-GPTQ,Text Generation,Transformers,,apache-2.0,,22,,271,17922.78964,,https://huggingface.co/TheBloke/WizardLM-Uncensored-Falcon-40B-3bit-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains an experimental GPTQ 3bit model of Eric Hartford's WizardLM Uncensored Falcon 40B.; It is the result of quantising to 3bit using AutoGPTQ.; Prompt format is WizardLM.
vicuna-7b-hf,Text Generation,PyTorch; Transformers,,mit,,1,,1,13804.03772,,https://huggingface.co/luodian/vicuna-7b-hf,
Graphix-3B,,PyTorch; Transformers,English,mit,https://arxiv.org/pdf/2301.07507.pdf; https://arxiv.org/pdf/2305.03111.pdf; https://arxiv.org/pdf/1809.08887.pdf; https://arxiv.org/pdf/2109.05093.pdf,6,,208,14562.88043,,https://huggingface.co/patrickNLP/Graphix-3B,"Graphix-T5 is a graph-aware semi-pretrained text-to-text PLM specifically designed to improve multi-hop reasoning for the complex text-to-SQL task. 
This novel architecture enhances the structural encoding capabilities of the T5 model while preserving its powerful contextual encoding ability. 
The experimental results demonstrate the effectiveness of GRAPHIX-T5 and underscore the importance of incorporating structural information in text-to-text PLMs for tackling intricate text-to-SQL challenges. 
The smaller gap in performance between the dev and test sets indicates the stronger generalization capability of Graphix-T5.; Graphix-3B is trained based on SPIDER, a cross-domain text-to-SQL benchmark. And it's evaluated in vanilla SPIDER dev, test, and other variants: SPIDER-SYN, SPIDER-DK, 
SPIDER-REALISTIC without additional training. This model will continue to be fine-tuned on more complex text-to-SQL data, 
i.e. BIRD to deal with harder but more real applications; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; Graphix-3B w/ Picard maintains state-of-the-art (SOTA) semantic parsing capabilities, as demonstrated by its performance on the SPIDER leaderboard. Its only submission achieves 74.0% on EM and 77.6% on EX in the testing dataset.
Please see Graphix Official Implementation for details.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
stt_kr_conformer_ctc_medium,Automatic Speech Recognition,NeMo,Korean,apache-2.0,,2,,27,491.0036426,,https://huggingface.co/SungBeom/stt_kr_conformer_ctc_medium,"?? ??? RIVA Conformer ASR Korean? AI hub dataset? ?? ????? ??????. 
Conformer ??? ??? whisper? ?? attention ?? ??? ?? streaming? ????? ??? ?? ???? ??, ??? ???? ??? ????.
V100 GPU??? RTF? 0.05, CPU(7 cores)??? 0.35 ?? ??? ?? ??? ? ?????.
??? chunk size 2?? streaming ?????? ?? ???? ?? ?? ???? 20% ?? ????? ??? ??? ??? ? ?? ?????.
??? open domain? ?? ?? ?? ??? ?? domain??? kenlm? ????? ? WER 13.45?? WER 5.27? ?? ?? ??? ?????.
??? ? ?? domain??? kenlm? ??? ? ?? ???? ???? ?????.; The following hyperparameters were used during training:"
medguanaco-lora-33b-8bit,Text Generation,Transformers,English,cc,https://arxiv.org/pdf/2106.09685.pdf; https://arxiv.org/pdf/2303.14070.pdf,4,,0,205.0058348,,https://huggingface.co/nmitchko/medguanaco-lora-33b-8bit,"Model Description ; nmitchko/medguanaco-lora-33b-8bit is a large language model LoRa specifically fine-tuned for medical domain tasks.
It is based on the Guanaco LORA of LLaMA weighing in at 33B parameters. 
The primary goal of this model is to improve question-answering and medical dialogue tasks.
It was trained using LoRA and reduced to 8bit, to reduce memory footprint. ; Steps to load this model:; The following README is taken from the source page medalpaca; The training data for this project was sourced from various resources. 
Firstly, we used Anki flashcards to automatically generate questions, 
from the front of the cards and anwers from the back of the card. 
Secondly, we generated medical question-answer pairs from Wikidoc. 
We extracted paragraphs with relevant headings, and used Chat-GPT 3.5 
to generate questions from the headings and using the corresponding paragraphs 
as answers. This dataset is still under development and we believe 
that approximately 70% of these question answer pairs are factual correct. 
Thirdly, we used StackExchange to extract question-answer pairs, taking the 
top-rated question from five categories: Academia, Bioinformatics, Biology, 
Fitness, and Health. Additionally, we used a dataset from ChatDoctor 
consisting of 200,000 question-answer pairs, available at https://github.com/Kent0n-Li/ChatDoctor."
llama-7b-logicot,Text Generation,PyTorch; Transformers,English,cc-by-sa-4.0,,2,csitfun/LogiCoT,53,27619.86653,,https://huggingface.co/csitfun/llama-7b-logicot,"This model is tuned on the LogiCoT data and the GPT-4 alpaca data with the LLaMa-7b model.; We use 2 A100 GPUs; We first instruction-tuning LLaMa-7b on the GPT-4 alpaca data for 3 days, then on the LogiCoT data for 4 days."
Chinese-AlpacaPro-13B,Text Generation,PyTorch; Transformers,Chinese,,https://arxiv.org/pdf/1910.09700.pdf,4,QingyiSi/Alpaca-CoT,5,54067.9813,,https://huggingface.co/kevinpro/Chinese-AlpacaPro-13B,"This model is based on Chinese-Alpaca and further tuned on 1.2 million instructions data; If you find the model helpful, please click ""like"" to support us. We also welcome feedback on your usage experience and any issues you encounter in the issues section.; [More Information Needed]; [More Information Needed]; [More Information Needed]"
photo,Text-to-Image,Diffusers,English,other,,6,,"12,889",4369.132255,2,https://huggingface.co/spitfire4794/photo,"Warning: This model is horny! Add ""nude, naked"" to the negative prompt if want to avoid NSFW.  ; You can add photo to your prompt to make your gens look more photorealistic.Non-square aspect ratios work better for some prompts. If you want a portrait photo, try using a vertical aspect ratio. If you want a landscape photo, try using a horizontal aspect ratio.This model was trained on 768x768px images, so use 768x768px, 640x896px, 896x640px, etc. It also works pretty good with higher resolutions such as 768x1024px or 1024x768px.  ; You can use this model for free on dreamlike.art!; Download dreamlike-photoreal-2.0.ckpt (2.13GB); Download dreamlike-photoreal-2.0.safetensors (2.13GB)"
GPT-Molecule-Generation,,,,,,2,,0,1.671806641,,https://huggingface.co/prajwalsahu5/GPT-Molecule-Generation,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
potat1,Text-to-Video,Diffusers,,,,126,,360,0.004829102,1,https://huggingface.co/camenduru/potat1,"? Please follow me for new updates https://twitter.com/camenduru 
? Please join our discord server https://discord.gg/k5BwmmvJJU; ; First Open-Source 1024x576 Text To Video Model ?  ; https://huggingface.co/vdo/potat1-5000/tree/main 
https://huggingface.co/vdo/potat1-10000/tree/main 
https://huggingface.co/vdo/potat1-10000-base-text-encoder/tree/main 
https://huggingface.co/vdo/potat1-15000/tree/main 
https://huggingface.co/vdo/potat1-20000/tree/main 
https://huggingface.co/vdo/potat1-25000/tree/main 
https://huggingface.co/vdo/potat1-30000/tree/main 
https://huggingface.co/vdo/potat1-35000/tree/main 
https://huggingface.co/vdo/potat1-40000/tree/main 
https://huggingface.co/vdo/potat1-45000/tree/main 
https://huggingface.co/vdo/potat1-50000/tree/main 
https://huggingface.co/vdo/potat1-50000-base-text-encoder/tree/main = https://huggingface.co/camenduru/potat1 (you are here) ; Prototype Model 
Trained with https://lambdalabs.com ? 1xA100 (40GB) 
2197 clips, 68388 tagged frames ( salesforce/blip2-opt-6.7b-coco ) 
train_steps: 10000 "
Hana_Bunny_v1.0,,,,other,,1,,0,2342.372121,,https://huggingface.co/DreamAngel/Hana_Bunny_v1.0,"This is a model of the famous cosplay star Hana Bunny, there is no censorship whatsoever. LoRA is required for correct operation:
Hana Bunny and Hana Bunny v2 are included here.; Steps: 55, Sampler: UniPC, CFG scale: 7, Seed: 3, Face restoration: CodeFormer, Size: 512x1024, Model hash: 7419125371, Lora hashes: ""Hana Bunny: 79d22db44fa9, HanaBunnyv2: f6a46a0b34b2""; ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
WizardLM-Uncensored-SCOT-StoryTelling-30B-Q2_K-GGML,,,,other,,5,,0,14028.80199,,https://huggingface.co/RachidAR/WizardLM-Uncensored-SCOT-StoryTelling-30B-Q2_K-GGML,"These files are GGML format model files for Monero's WizardLM-Uncensored-SuperCOT-Storytelling-30B.; This 2 bit model can run with 16 GB of RAM.
On my Xeon E3-1225 v3 4/8 old cpu, it runs with ~660 ms per token.; Inference API has been turned off for this model."
falcon-7b-sft-top1-696,Text Generation,PyTorch; Transformers,4 languages,apache-2.0,,16,OpenAssistant/oasst1,"7,785",14788.77294,3,https://huggingface.co/OpenAssistant/falcon-7b-sft-top1-696,"This model is a fine-tuning of TII's Falcon 7B LLM.
It was trained with 11,123 top-1 (high-quality) demonstrations of the OASST data set (exported on June 2, 2023) with a batch size of 128 for 8 epochs with LIMA style dropout (p=0.2) and a context-length of 2048 tokens.; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; Model:"
WizardLM-Uncensored-SCOT-ST-30B-Q3_K_S-GGML,,,,other,,7,,0,14438.40219,,https://huggingface.co/RachidAR/WizardLM-Uncensored-SCOT-ST-30B-Q3_K_S-GGML,"These files are GGML format model files for Monero's WizardLM-Uncensored-SuperCOT-Storytelling-30B.; The quality of the 3-bit model is higher than the 2-bit model, but the interface is slower. The 3-bit model (type q3_K_S) barely fits into 16 GB of RAM, but it works.; On my Xeon E3-1225 v3 4/8 old cpu, it runs with ~740 ms per token.; Inference API has been turned off for this model."
YuLan-Chat-13b-delta,Text Generation,PyTorch; Transformers,,apache-2.0,,3,,10,26655.24388,,https://huggingface.co/RUCAIBox/YuLan-Chat-13b-delta,
mobilevitv2-1.0-imagenet1k-256,Image Classification,PyTorch; Transformers,,other,https://arxiv.org/pdf/2206.02680.pdf,1,imagenet-1k,"1,961",19.87243027,,https://huggingface.co/apple/mobilevitv2-1.0-imagenet1k-256,"MobileViTv2 is the second version of MobileViT. It was proposed in Separable Self-attention for Mobile Vision Transformers by Sachin Mehta and Mohammad Rastegari, and first released in this repository. The license used is Apple sample code license.; Disclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.; MobileViTv2 is constructed by replacing the multi-headed self-attention in MobileViT with separable self-attention.; You can use the raw model for image classification. See the model hub to look for fine-tuned versions on a task that interests you.; Here is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:"
WizardLM-Uncensored-SCOT-ST-30B-Q3_K_M-GGML,,,,other,,3,,0,16076.80198,,https://huggingface.co/RachidAR/WizardLM-Uncensored-SCOT-ST-30B-Q3_K_M-GGML,These files are GGML format model files for Monero's WizardLM-Uncensored-SuperCOT-Storytelling-30B.; Inference API has been turned off for this model.
landmark-attention-llama7b-fp16,Text Generation,PyTorch; Transformers,,other,,8,,174,27599.16069,,https://huggingface.co/TheBloke/landmark-attention-llama7b-fp16,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are merged fp16 pytorch format model files for Landmark Attention 7B.; They are the result of merging the provided deltas with base LLaMa 7B.; For more info on this model, please see their Github at: https://github.com/epfml/landmark-attention"
h2ogpt-gm-oasst1-en-2048-falcon-40b-v1,Text Generation,PyTorch; Transformers,English,apache-2.0,,25,OpenAssistant/oasst1,"3,042",169208.592,25,https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v1,"This model was trained using H2O LLM Studio.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers, accelerate and torch libraries installed.; You can print a sample prompt after the preprocessing step to see how it is feed to the tokenizer:; Alternatively, you can download h2oai_pipeline.py, store it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:; You may also construct the pipeline from the loaded model and tokenizer yourself and consider the preprocessing steps:"
gorilla-falcon-7b-hf-v0,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2305.15334.pdf,20,gorilla-llm/APIBench,911,14779.12056,2,https://huggingface.co/gorilla-llm/gorilla-falcon-7b-hf-v0,"By Shishir G. Patil, Tianjun Zhang  (Project Website);   ; Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla can write a semantically- and syntactically- correct API to invoke. With Gorilla, we are the first to demonstrate how to use LLMs to invoke 1,600+ (and growing) API calls accurately while reducing hallucination. We also release APIBench, the largest collection of APIs, curated and easy to be trained on! Join us, as we try to expand the largest API store and teach LLMs how to write them! Hop on our Discord, or open a PR, or email us if you would like to have your API incorporated as well.; Gorilla can be either trained via standard finetuning or using our novel retriever-aware training pipeline. We release gorilla-falcon-7b-hf-v0, a 0-shot finetuned LLM that can reliably use Hugging Face APIs. It can be prompted through simply natural language (e.g., ""I want to generate an image from text.""). Checkour our website, github and paper for more information.; Gorilla is an open-source API caller trained by fine-tuning Falcon weights. It is an auto-regressive language model, based on the transformer architecture."
WizardLM-Uncensored-SCOT-ST-30B-Q4_K_S-GGML,,,,other,,2,,0,18739.20197,,https://huggingface.co/RachidAR/WizardLM-Uncensored-SCOT-ST-30B-Q4_K_S-GGML,These files are GGML format model files for Monero's WizardLM-Uncensored-SuperCOT-Storytelling-30B.; Usage RAM: ; Inference API has been turned off for this model.
Selfee-13B-GGML,,,,other,,14,,0,118896.6618,,https://huggingface.co/TheBloke/Selfee-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Kaist AI's Selfee 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
coreml-stable-diffusion-2-1-base-palettized,Text-to-Image,Core ML,,openrail++,https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,7,,0,2334.733945,,https://huggingface.co/apple/coreml-stable-diffusion-2-1-base-palettized,"This model was generated by Hugging Face using Apple’s repository which has ASCL. This version contains 6-bit palettized Core ML weights for iOS 17 or macOS 14. To use weights without quantization, please visit this model instead.; This model card focuses on the model associated with the Stable Diffusion v2-1-base model.; This stable-diffusion-2-1-base model fine-tunes stable-diffusion-2-base (512-base-ema.ckpt) with 220k extra steps taken, with punsafe=0.98 on the same dataset. ; These weights here have been converted to Core ML for use on Apple Silicon hardware.; There are 4 variants of the Core ML weights:"
bert-finetuned-ner-chinese-people-daily,Token Classification,PyTorch; Transformers,,,,1,peoples_daily_ner,91,407.5451,,https://huggingface.co/johnyyhk/bert-finetuned-ner-chinese-people-daily,"This model is a fine-tuned version of bert-base-chinese on the peoples_daily_ner dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
aragpt2-mega-pos-msa,Text Generation,PyTorch; Transformers,Arabic,,,2,,44,6035.058051,,https://huggingface.co/alsubari/aragpt2-mega-pos-msa,"Language(s) (NLP): [Arabic]; Finetuned from model : aragpt2-mega; {'??? ??': 'preposition',
 '???': 'noun',
 '??? ???': 'proper noun',
 '??? ???????': 'determiner',
 '???': 'adjective',
 '????': 'personal pronoun',
 '???': 'verb',
 '??? ???': 'conjunction',
 '??? ?????': 'relative pronoun',
 '??? ???': 'negative particle',
 '???? ?????': 'quranic initials',
 '??? ?????': 'demonstrative pronoun',
 '??? ?????????': 'resumption',
 '??? ???': 'accusative particle',
 '??? ?????': 'equalization particle',
 '??? ???': 'circumstantial particle',
 '???? ???': 'restriction particle',
 '??? ????': 'time adverb',
 '??? ???': 'prohibition particle',
 '??? ???': 'preventive particle',
 '??? ??????': 'inceptive particle',
 '??? ????': 'supplemental particle',
 '??? ???????': 'amendment particle',
 '??? ?????': 'subordinating conjunction',
 '??? ???????': 'interrogative particle',
 '??? ????': 'location adverb',
 '??? ???': 'conditional particle',
 '??? ???????': 'emphatic',
 '??? ????': 'vocative particle',
 '??? ???? ?? ???? ?????': 'result particle',
 '??? ?????': 'explanation particle',
 '???? ???????': 'exceptive particle',
 '??? ?????': 'particle of cause',
 '??????? - ????? ???????': 'heavy noon emphesis',
 '??? ???????': 'future particle',
 '??? ?????': 'particle of certainty',
 '??? ???????': 'purpose',
 '??? ????': 'answer particle',
 '??? ?????': 'retraction particle',
 '??? ?????': 'exhortation particle',
 '??? ?????': 'particle of interpretation',
 '??? ?????': 'imperative',
 '??? ??????': 'comitative particle',
 '??? ?????': 'surprise particle',
 '??? ???': 'aversion particle',
 '??? ??? ???': 'imperative verbal noun'}; Epoch 	          1
Training Loss 	  0.108500 	
Validation Loss   0.082612; [akram.alsubari87@gmail.com]"
icyCheese,Text-to-Image,,,creativeml-openrail-m,,2,,0,2908.166104,,https://huggingface.co/lylogummy/icyCheese,"ko-fi
https://ko-fi.com/lyloguum; ""IcyCheese"" shares various Merge models that can be used with StableDiffusionWebui:Automatic1111 and others.Enjoy the drawing AI.; Unable to determine this model’s library. Check the
								docs 
.
							"
BianQue-2,Feature Extraction,PyTorch; Transformers,,apache-2.0,,10,,"2,317",13744.9263,1,https://huggingface.co/scutcyr/BianQue-2,"SoulChat ? | 
 ? BianQue? |; 基于主动健康的主动性、预防性、精确性、个性化、共建共享、自律性六大特征，华工未来技术学院-广东省数字孪生人重点实验室开源了中文领域生活空间主动健康大模型基座ProactiveHealthGPT，包括：; 我们期望，生活空间主动健康大模型基座ProactiveHealthGPT 可以帮助学术界加速大模型在慢性病、心理咨询等主动健康领域的研究与应用。本项目为 生活空间健康大模型扁鹊（BianQue） 。; 我们经过调研发现，在健康领域，用户通常不会在一轮交互当中清晰地描述自己的问题，而当前常见的开源医疗问答模型（例如：ChatDoctor、本草(HuaTuo，原名华驼 )、DoctorGLM、MedicalGPT-zh）侧重于解决单轮用户描述的问题，而忽略了“用户描述可能存在不足”的情况。哪怕是当前大火的ChatGPT也会存在类似的问题：如果用户不强制通过文本描述让ChatGPT采用一问一答的形式，ChatGPT也偏向于针对用户的描述，迅速给出它认为合适的建议和方案。然而，实际的医生与用户交谈往往会存在“医生根据用户当前的描述进行持续多轮的询问”。并且医生在最后根据用户提供的信息综合给出建议。我们把医生不断问询的过程定义为 询问链（CoQ, Chain of Questioning） ，当模型处于询问链阶段，其下一个问题通常由对话上下文历史决定。; 我们结合当前开源的中文医疗问答数据集（MedDialog-CN、IMCS-V2、CHIP-MDCFNPC、MedDG、cMedQA2、Chinese-medical-dialogue-data），，分析其中的单轮/多轮特性以及医生问询特性，结合实验室长期自建的生活空间健康对话大数据，构建了千万级别规模的扁鹊健康大数据BianQueCorpus。对话数据通过“病人：xxx\n医生：xxx\n病人：xxx\n医生：”的形式统一为一种指令格式"
bert-base-personality,Text Classification,PyTorch; Safetensors; Transformers,English,mit,https://arxiv.org/pdf/1810.04805.pdf,1,,44,876.2403119,,https://huggingface.co/Minej/bert-base-personality,"To use the model through Hosted inference API, follow the code snippet provided below:; The personality_detection function returns a dictionary containing the predicted personality traits based on the given input text.; The dictionary contains the following personality traits with their corresponding predicted values:;   Extroversion: A value between 0 and 1 representing the predicted extroversion trait.
  Neuroticism: A value between 0 and 1 representing the predicted neuroticism trait.
  Agreeableness: A value between 0 and 1 representing the predicted agreeableness trait.
  Conscientiousness: A value between 0 and 1 representing the predicted conscientiousness trait.
  Openness: A value between 0 and 1 representing the predicted openness trait.; Note: The values in the example output are just placeholders and may not reflect the actual predictions."
falcon-7b-instruct-GPTQ,Text Generation,Transformers,English,apache-2.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf,3,tiiuae/falcon-refinedweb,631,6085.355935,,https://huggingface.co/4bit/falcon-7b-instruct-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains an experimantal GPTQ 4bit model for Falcon-7B-Instruct.; It is the result of quantising to 4bit using AutoGPTQ.; Join me at: https://discord.gg/UBgz4VXf
urpm,Text-to-Image,Diffusers,English,creativeml-openrail-m,,4,,,0,,https://huggingface.co/invisiblekat/urpm,"This repository has been marked as containing sensitive content and may contain potentially harmful and sensitive
		information.
	"
bart-speech-style-converter,Text2Text Generation,PyTorch; Safetensors; Transformers,Korean,cc-by-4.0,,3,,120,993.0549895,,https://huggingface.co/NHNDQ/bart-speech-style-converter,This model can be used for convert speech style; https://github.com/KoJLabs/speech-style/tree/main; You can exercise korean speech style conversion task with python package KoTAN
chronos-33b-GPTQ,Text Generation,Transformers; PyTorch,,other,,13,,170,17307.95039,,https://huggingface.co/TheBloke/chronos-33b-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Elinas' Chronos 33B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
kullm-polyglot-5.8b-v2,Text Generation,PyTorch; Transformers,Korean,apache-2.0,,5,nlpai-lab/kullm-v2,"8,982",24229.52929,1,https://huggingface.co/nlpai-lab/kullm-polyglot-5.8b-v2,This model is a parameter-efficient fine-tuned version of EleutherAI/polyglot-ko-5.8b on a KULLM v2 ; Detail Codes are available at KULLM Github Repository; The following hyperparameters were used during training:
rwkv-4-world-ggml-quantized,,,,apache-2.0,,4,,0,77557.76147,,https://huggingface.co/latestissue/rwkv-4-world-ggml-quantized,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
LLaVA-7B-Lightening-v1-1,Text Generation,PyTorch; Transformers,,cc-by-4.0,,1,,319,13814.2783,,https://huggingface.co/mmaaz60/LLaVA-7B-Lightening-v1-1,This model is obtained by applying LLaVA-7B-Lightening-v1-1 delta from liuhaotian/LLaVA-Lightning-7B-delta-v1-1 to LLaMA-7B model.
chronos-wizardlm-uc-scot-st-13B-GGML,,,,other,,9,,0,118896.6505,,https://huggingface.co/TheBloke/chronos-wizardlm-uc-scot-st-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Austism's Chronos WizardLM UC Scot ST 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
30B-Lazarus-GGML,,,,other,,20,,0,297369.612,1,https://huggingface.co/TheBloke/30B-Lazarus-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for CalderAI's 30B Lazarus.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
zzzmix,,,,,,4,,0,8304.642132,,https://huggingface.co/zzzAI19/zzzmix,"This is a block merge model of my own creation. I created this model with the goal of being able to depict the background beautifully while depicting the girl softly and delicately.
Sample images can be found on the following pages
https://ai-drawing.net/en/2023/06/07/publish-my-own-image-generation-model/; 自作のA鹰蕞`ジモデルです。少女を柔らかくに描写しつつ、背景を美しく描写できることを目摔俗鳏辘蓼筏俊
サンプル画像は以下のペ`ジにあります。
https://ai-drawing.net/2023/06/07/%e8%87%aa%e4%bd%9c%e3%81%ae%e7%94%bb%e5%83%8f%e7%94%9f%e6%88%90%e3%83%a2%e3%83%87%e3%83%ab%e3%81%ae%e5%85%ac%e9%96%8b/; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
vicuna-7B-1.1-GGML,,,,other,,15,,0,61542.4118,,https://huggingface.co/TheBloke/vicuna-7B-1.1-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for LmSys' Vicuna 7B 1.1.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48."
medguanaco-65b-GPTQ,Text Generation,PyTorch; Transformers,English,cc,https://arxiv.org/pdf/2106.09685.pdf; https://arxiv.org/pdf/2303.14070.pdf,1,,78,34622.20697,2,https://huggingface.co/nmitchko/medguanaco-65b-GPTQ,"Model Description ; nmitchko/medguanaco-65b-GPTQ is a large language model LoRa specifically fine-tuned for medical domain tasks.
It is based on the Guanaco LORA of LLaMA weighing in at 65B parameters. 
The primary goal of this model is to improve question-answering and medical dialogue tasks.
It was trained using LoRA and reduced to 8bit, to reduce memory footprint. ; Steps to load this model:; The following README is taken from the source page medalpaca; The training data for this project was sourced from various resources. 
Firstly, we used Anki flashcards to automatically generate questions, 
from the front of the cards and anwers from the back of the card. 
Secondly, we generated medical question-answer pairs from Wikidoc. 
We extracted paragraphs with relevant headings, and used Chat-GPT 3.5 
to generate questions from the headings and using the corresponding paragraphs 
as answers. This dataset is still under development and we believe 
that approximately 70% of these question answer pairs are factual correct. 
Thirdly, we used StackExchange to extract question-answer pairs, taking the 
top-rated question from five categories: Academia, Bioinformatics, Biology, 
Fitness, and Health. Additionally, we used a dataset from ChatDoctor 
consisting of 200,000 question-answer pairs, available at https://github.com/Kent0n-Li/ChatDoctor."
tulu-65b,Text Generation,PyTorch; Transformers,English,,https://arxiv.org/pdf/2306.04751.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2304.07327.pdf,17,databricks/databricks-dolly-15k; OpenAssistant/oasst1; sahil2801/CodeAlpaca-20k,63,0,,https://huggingface.co/allenai/tulu-65b,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; To request access to the models, please fill out this form, and we'll review and let you know if your use case is approved. The information you provide below will be used solely to assess eligibility to access these models.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This model is a 65B LLaMa model finetuned on a mixture of instruction datasets (FLAN V2, CoT, Dolly, Open Assistant 1, GPT4-Alpaca, Code-Alpaca, and ShareGPT).
Please note this is a model diff - see below for usage instructions.; This was trained as part of the paper How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources.
The codebase used to train and evaluate this model can be found at https://github.com/allenai/open-instruct."
CAMEL-13B-Combined-Data-GPTQ,Text Generation,Transformers,,other,https://arxiv.org/pdf/2303.17760.pdf,6,,114,8306.979594,,https://huggingface.co/TheBloke/CAMEL-13B-Combined-Data-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Camel AI's CAMEL 13B Combined Data.; It is the result of quantising to 4bit using AutoGPTQ.; Please make sure you're using the latest version of text-generation-webui
faceshine,,,,mit,,1,,0,8.591843948,1,https://huggingface.co/leonelhs/faceshine,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
TemporalNet2,,Diffusers,,openrail,,68,,25,13004.81814,,https://huggingface.co/CiaraRowles/TemporalNet2,"Introducing TemporalNet2; TemporalNet was a ControlNet model designed to enhance the temporal consistency of generated outputs; TemporalNet 2 is an evolution on the concept, where the generated outputs are guided by both the last frame and an optical flow map between the frames, improving generation consistency.; This took some modification of the original controlnet code so you'll have to do some extra things. If you just want to run a gradio example or look at the modified controlnet code,
that's here: https://github.com/CiaraStrawberry/TemporalNet  Just drop the model from this directory into that model folder and make sure the gradio_temporalnet.py script points at the model.; To use with stable diffusion, you can either use it with TemporalKit by moving to the branch here after following steps 1 and 2: https://github.com/CiaraStrawberry/TemporalKit/tree/TemporalNet , or use it just by accessing the base api through the temporalvideo.py script: "
tulu-13b,Text Generation,PyTorch; Transformers,English,,https://arxiv.org/pdf/2306.04751.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2304.07327.pdf,9,databricks/databricks-dolly-15k; OpenAssistant/oasst1; sahil2801/CodeAlpaca-20k,63,53322.10024,,https://huggingface.co/allenai/tulu-13b,"This model is a 13B LLaMa model finetuned on a mixture of instruction datasets (FLAN V2, CoT, Dolly, Open Assistant 1, GPT4-Alpaca, Code-Alpaca, and ShareGPT).
Please note this is a model diff - see below for usage instructions.; This was trained as part of the paper How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources.
The codebase used to train and evaluate this model can be found at https://github.com/allenai/open-instruct.; This model is licensed under the AI model license given in LICENSE.txt along with the original Llama license (llama_license.txt).; We assume you have access to a LLaMa model in HF format already. You can find details on getting access and converting the model here:
https://huggingface.co/docs/transformers/main/model_doc/llama; Clone https://github.com/allenai/open-instruct and install the required dependencies, or just copy scripts/weight_diff.py
and install the minimal requirements listed in weight-diff-requirements.txt. Then download or clone this model diff to the same machine."
open-llama-7b-open-instruct,Text Generation,PyTorch; Transformers,English,cc-by-sa-3.0,,22,VMware/open-instruct-v1-oasst-dolly-hhrlhf,"4,444",13804.07456,2,https://huggingface.co/VMware/open-llama-7b-open-instruct,"Instruction-tuned version of the fully trained Open LLama 7B model. The model is open for COMMERCIAL USE. ;  There is a v2 version of this model available, https://huggingface.co/VMware/open-llama-7b-v2-open-instruct ;  NOTE  : The model was trained using the Alpaca prompt template
 NOTE  : Fast tokenizer results in incorrect encoding, set the use_fast = False parameter, when instantiating the tokenizer; The finetuning scripts will be available in our RAIL Github Repository; TODO"
Ashaar_model,Text Generation,PyTorch; Transformers,,,,1,,35,916.9350927,2,https://huggingface.co/arbml/Ashaar_model,No model card; New: Create and edit this model card directly on the website!
Aquila-7B,,PyTorch; Transformers,,other,,3,,316,14949.39306,,https://huggingface.co/BAAI/Aquila-7B,"; 
English |
        简体中文 |
    ; 
; Aquila Language Model is the first open source language model that supports both Chinese and English knowledge, commercial license agreements, and compliance with domestic data regulations.; ? Supports open source commercial licenses. The source code of the Aquila series models is based on the Apache 2.0 agreement, while the model weight is based on the BAAI Aquila Model License Agreement. Users can use it for commercial purposes as long as they meet the licensing restrictions."
pai-bloom-1b1-text2prompt-sd,Text Generation,PyTorch; Transformers,,apache-2.0,,17,,"1,742",2202.405751,1,https://huggingface.co/alibaba-pai/pai-bloom-1b1-text2prompt-sd,"我们开源了一个自动Prompt生成模型，您可以直接输入一个极其简单的Prompt，就可以得到经过语言模型优化过的Prompt，帮助您更简单地生成高颜值图像。; We release an automatic Prompt generation model, you can directly enter an extremely simple Prompt and get a Prompt optimized by the language model to help you generate more beautiful images simply.; 使用上述模型需遵守AIGC模型开源特别条款。; If you want to use this model, please read this document carefully and abide by the terms."
ImmuneMetalGPT,Feature Extraction,,English,afl-3.0,https://arxiv.org/pdf/1910.09700.pdf,1,,0,0.01053566,,https://huggingface.co/KeTuTu/ImmuneMetalGPT,"该模型是用于金属与肿瘤免疫治疗预测的多模态临床辅助决策模型，经过LLaMA 33B 微调。; Carbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019).; A800*8; Own Sever; PyTorch2.0, Ubuntu,CUDA"
chinese-alpaca-33b-merged,Text Generation,PyTorch; Transformers,,,,6,,280,67116.00388,,https://huggingface.co/minlik/chinese-alpaca-33b-merged,加入中文词表并继续预训练中文Embedding，并在此基础上继续使用指令数据集finetuning，得到的中文Alpaca-33B模型。; 模型转换用到的相关base及lora模型如下：; 详情可参考：https://github.com/ymcui/Chinese-LLaMA-Alpaca/releases/tag/v4.0
open-llama-ggml,Text Generation,Transformers,English,apache-2.0,,4,togethercomputer/RedPajama-Data-1T,145,34508.80969,,https://huggingface.co/rustformers/open-llama-ggml,"In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a 7B and 3B model trained on 1T tokens, as well as the preview of a 13B model trained on 600B tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Via pip: pip install llm-rs; Download the installer at www.localai.app.; Download your preferred model and place it in the ""models"" directory. Subsequently, you can start a chat session with your model directly from the interface."
LLaVA-13B-v1-1,Text Generation,PyTorch; Transformers,,apache-2.0,,2,,298,26667.32608,,https://huggingface.co/rjadr/LLaVA-13B-v1-1,"Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA was trained in May 2023.; Paper or resources for more information:
https://llava-vl.github.io/; License:
Apache License 2.0; Where to send questions or comments about the model:
https://github.com/haotian-liu/LLaVA/issues"
ziya-llama-13b-medical-lora,,PyTorch,Chinese; English,apache-2.0,,8,,0,125.6941367,,https://huggingface.co/shibing624/ziya-llama-13b-medical-lora,基于LLaMA-13B的中英医疗问答模型（LoRA）; shibing624/ziya-llama-13b-medical-lora evaluate test data：; The overall performance of model on QA test:; 在中文开放测试集中的表现优异，继承了两方面的优势：1）微调训练的底座是Ziya-LLaMA-13B模型，是较强的中英文底座模型，2）微调使用的是高质量240万条中英文医疗指令数据集，和多种通用指令数据集，微调后的模型在医疗行业答复能力达到领先水平，在通用问题上的答复能力不弱于LLaMA-13B。; training args:
musicgen-medium,Text2Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,https://arxiv.org/pdf/2306.05284.pdf,24,,"4,944",12240.49786,94,https://huggingface.co/facebook/musicgen-medium,"MusicGen is a text-to-music model capable of genreating high-quality music samples conditioned on text descriptions or audio prompts.
It is a single stage auto-regressive Transformer model trained over a 32kHz EnCodec tokenizer with 4 codebooks sampled at 50 Hz. 
Unlike existing methods, like MusicLM, MusicGen doesn't require a self-supervised semantic representation, and it generates all 4 codebooks in one pass. 
By introducing a small delay between the codebooks, we show we can predict them in parallel, thus having only 50 auto-regressive steps per second of audio.; MusicGen was published in Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez.; Four checkpoints are released:; Try out MusicGen yourself!; You can run MusicGen locally with the ? Transformers library from version 4.31.0 onwards."
starcoderplus-GPTQ,Text Generation,Transformers,,bigcode-openrail-m,https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,20,bigcode/the-stack-dedup; tiiuae/falcon-refinedweb,465,9127.106028,,https://huggingface.co/TheBloke/starcoderplus-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Bigcode's StarcoderPlus.; It is the result of quantising to 4bit using AutoGPTQ.; Please make sure you're using the latest version of text-generation-webui
starchat-beta-GPTQ,Text Generation,Transformers,,bigcode-openrail-m,,22,,736,9127.109807,,https://huggingface.co/TheBloke/starchat-beta-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for HuggingFaceH4's Starchat Beta.; It is the result of quantising to 4bit using AutoGPTQ.; Example:
starchat-beta-GGML,,Transformers,,bigcode-openrail-m,,29,,78,71782.41368,1,https://huggingface.co/TheBloke/starchat-beta-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for HuggingFaceH4's Starchat Beta.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools known to work with these model files.; Example:"
YabaLMixTrue25D_V1.0,Text-to-Image,Diffusers,,other,,3,,"1,357",2457.602333,1,https://huggingface.co/digiplay/YabaLMixTrue25D_V1.0,"Model info:
https://civitai.com/models/60093/yabalmix-true25d-v10; Sample image:; "
falcon-qlora-chinese,,,Chinese; English,apache-2.0,,3,QingyiSi/Alpaca-CoT,0,1996.804784,,https://huggingface.co/keyfan/falcon-qlora-chinese,"This is a QLoRa adapter model to Falcon-40b. ; Though Falcon is not specifically trained on Chinese corpus, it exhibits strong performance in Chinese Language Understanding in our experiment. We would like to explore out of curiosity whether a 
small amount of Chinese instruction data can push it further and make it better at speaking.The LoRa model is trained with the QLoRa repo on a subset of bilingual instruction data from Alpaca-CoT dataset.; The result is suprisingly good considering the number of Chinese tokens it has ever seen, though it shows clear limitions. Please check the examples below.; We evaluate on C-Eval. More results will be added later.; Result on C-Eval test set with 5-shot and no CoT, which is better than LLaMA-series models."
Full-Robin-33b-v2,Text Generation,PyTorch; Transformers,,,,1,,27,66632.21969,2,https://huggingface.co/LMFlow/Full-Robin-33b-v2,No model card; New: Create and edit this model card directly on the website!
h2ogpt-gm-oasst1-en-2048-open-llama-7b,Text Generation,PyTorch; Transformers,English,apache-2.0,,10,OpenAssistant/oasst1,"11,162",27597.36245,8,https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b,"This model was trained using H2O LLM Studio.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers, accelerate and torch libraries installed.; You can print a sample prompt after the preprocessing step to see how it is feed to the tokenizer:; Alternatively, you can download h2oai_pipeline.py, store it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer. If the model and the tokenizer are fully supported in the transformers package, this will allow you to set trust_remote_code=False.; You may also construct the pipeline from the loaded model and tokenizer yourself and consider the preprocessing steps:"
swin-tiny-patch4-window7-224-finetuned-eurosat,Image Classification,PyTorch; TensorBoard; Transformers,,apache-2.0,,1,imagefolder,6,110.0091228,,https://huggingface.co/swayampragnya/swin-tiny-patch4-window7-224-finetuned-eurosat,"This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.
It achieves the following results on the evaluation set:; I have trained this model on different types fruits datasets.so this API only applicable for fruits classification.
This model can also be trainable on custom dataset for other purpose.; More information needed; More information needed; The following hyperparameters were used during training:"
Vicuzard-30B-Uncensored,Text Generation,PyTorch; Transformers,English,other,,3,,35,66593.10091,1,https://huggingface.co/concedo/Vicuzard-30B-Uncensored,"This is an experimental mixed model containing a parameter-wise 50/50 blend (weighted average) of ehartford/Wizard-Vicuna-30B-Uncensored and ehartford/WizardLM-30B-Uncensored; GGML models are provided here, for use in KoboldCPP. ; This improves on earlier model mixing techniques by only applying the merge to the layers containing tensors of the same dimensions. 
By selectively skipping merge operations on the input and output layers, we are now able to merge models with different vocab sizes (i.e. added tokens) so long as the hidden layers have identical sizes.; All feedback and comments can be directed to Concedo on the KoboldAI discord."
starcoder-conala,Text2Text Generation,PyTorch; Transformers,,,,2,codeparrot/conala-mined-curated,16,32412.89096,,https://huggingface.co/codeparrot/starcoder-conala,"This model is an instruction-tuned version of ?? StarCoder. The instruction dataset involved is Conala-mined-curated
which was built by boostrapping by predicting the column rewritten_intent of the mined subset of the CoNaLa corpus.; The model was fine-tuned with the following template; If you have your model and tokenizer loaded, you can use the following code to make the model generate the right output to a given instruction; For additional information, check"
open-llama-7b-open-instruct-GGML,,,,other,,27,,0,61542.41395,,https://huggingface.co/TheBloke/open-llama-7b-open-instruct-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for VMWare's open-llama-7B-open-instruct.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Standard Alpaca:"
ru-mbart-large-summ,Summarization,PyTorch; Safetensors; Transformers,Russian; English,mit,,2,d0rj/samsum-ru; IlyaGusev/gazeta; zjkarina/matreshka; rcp-meetings/rudialogsum_v2; GEM/wiki_lingua; mlsum,724,3120.095119,,https://huggingface.co/d0rj/ru-mbart-large-summ,Model forked from ru-bart-large which is smaller version of the facebook/mbart-large-50 with only Russian and English embeddings.; All 'train' subsets was concatenated and shuffled with seed 1000 - 7.; Train subset = 155678 rows.; Evaluation on 10% of concatenated 'validation' subsets = 1458 rows.; See WandB logs.
vit-base-patch16-224-in21k-Brain_Tumors_Image_Classification,Image Classification,PyTorch; TensorBoard; Transformers,English,apache-2.0,,1,imagefolder,6,343.0137881,,https://huggingface.co/DunnBC22/vit-base-patch16-224-in21k-Brain_Tumors_Image_Classification,This model is a fine-tuned version of google/vit-base-patch16-224-in21k.; It achieves the following results on the evaluation set:; This project is part of a comparison of seventeen (17) transformers. ; The following hyperparameters were used during training:
airoboros-7b-gpt4-1.1,Text Generation,PyTorch; Transformers,,cc-by-nc-4.0,,3,jondurbin/airoboros-gpt4-1.1,203,27597.38748,,https://huggingface.co/jondurbin/airoboros-7b-gpt4-1.1,"This is a minor update of https://huggingface.co/jondurbin/airoboros-7b-gpt4 with ~1k more coding instructions, and fixes/improvements to context instructions from https://huggingface.co/datasets/jondurbin/airoboros-gpt4-1.1; The remainder of the model card is duplicated from the origin.; This is a fine-tuned 7b parameter LlaMa model, using completely synthetic training data created gpt4 via https://github.com/jondurbin/airoboros; The dataset used to fine-tune this model is available here, with a specific focus on:; This model was fine-tuned with a fork of FastChat, and therefore uses the standard vicuna template:"
llama-molinst-protein-7b,Text Generation,PyTorch; Transformers,,apache-2.0,https://arxiv.org/pdf/2306.08018.pdf,5,,21,27597.40524,,https://huggingface.co/zjunlp/llama-molinst-protein-7b,"This repo contains a fully fine-tuned LLaMA-7b, trained on the ? protein-oriented instructions from the ? Mol-Instructions dataset.; Instructions for running it can be found at https://github.com/zjunlp/Mol-Instructions.; Please refer to our paper for more details.; ; Please evaluate the following protein sequence and provide an explanation of the enzyme's catalytic activity, including the chemical reaction it facilitates: MDKVAVAGFLPEELCASLSLSPSFRGNQIFQWIGKGVDSFDAMTNLSAELRASLAEKAILRSTRVSDVLKADDGTVKLQIQTEDDLAVETVLLTDKAARKTACVSCQAGCAMGCAFCKTGTLGLARNLSAAEIVEQFLYLEKHAGALDNIVFMGMGEPLLNLDALRKAIAVLTDKRGRNLSSRRITVSTVGIVSGIYDLANNGPDVRLAVSLTTADETLRRELMPASLTNPLSDLRQAISYYIEKTGKRVTLEAVLLSGKNTSEKNADSLIAFAKGLDVHVNLIPWNPVEGLSFVTPDPEETAQFVSRLEKGGLNVTLRMHRGKSISGACGQLGKTNPYA"
alpaca.13b.ggmlv3,,,,apache-2.0,,1,,0,14233.60156,,https://huggingface.co/westseer/alpaca.13b.ggmlv3,"Stanford Alpaca 13B model for ggml v3.
Stanford Alpaca is an Instruction-following LLaMA Model.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
potat1-with-text-encoder-original-format,,OpenCLIP,,,,4,,140,7577.60249,,https://huggingface.co/kabachuha/potat1-with-text-encoder-original-format,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
Chat_Suzumiya_GLM2LoRA,,PEFT,,,,1,,45,15.60191166,,https://huggingface.co/Jyshen/Chat_Suzumiya_GLM2LoRA,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
HenmixArt_v1,Text-to-Image,Diffusers,,other,,3,,351,3194.882601,3,https://huggingface.co/digiplay/HenmixArt_v1,"Model info :
https://civitai.com/models/74158/henmixart; Sample images:
; ; "
span-marker-roberta-large-ontonotes5,Token Classification,PyTorch; Safetensors; SpanMarker,English,apache-2.0,,2,tner/ontonotes5,"1,002",2911.506616,,https://huggingface.co/tomaarsen/span-marker-roberta-large-ontonotes5,"This is a SpanMarker model that can be used for Named Entity Recognition. In particular, this SpanMarker model uses roberta-large as the underlying encoder. See train.py for the training script.; To use this model for inference, first install the span_marker library:; You can then run inference with this model like so:; See the SpanMarker repository for documentation and additional information on this library."
falcon-7b-prompt-answering,Text Generation,PyTorch; Transformers; PEFT,English,other,,2,,366,9437.604366,,https://huggingface.co/Sandiago21/falcon-7b-prompt-answering,"This repository contains further fine-tuned Falcon-7B model on conversations and question answering prompts.; I used falcon-7b (https://huggingface.co/tiiuae/falcon-7b) as a base model, so this model has the same license with Falcon-7b model (Apache-2.0); Anyone can use (ask prompts) and play with the model using the pre-existing Jupyter Notebook in the noteboooks folder. The Jupyter Notebook contains example code to load the model and ask prompts to it as well as example prompts to get you started.; The tiiuae/falcon-7b model was finetuned on conversations and question answering prompts.; Developed by: [More Information Needed]"
tulu-30B-fp16,Text Generation,PyTorch; Transformers,English,other,https://arxiv.org/pdf/2306.04751.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2304.07327.pdf,5,databricks/databricks-dolly-15k; OpenAssistant/oasst1; sahil2801/CodeAlpaca-20k,18,66634.1201,,https://huggingface.co/TheBloke/tulu-30B-fp16,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are pytorch format fp16 model files for Allen AI's Tulu 30B.; It is the result of merging and/or converting the source repository to float16.; The following template should be used:
alpaca-lora-7b-german-base-51k-ggml,Text Generation,Transformers,,apache-2.0,,2,,82,13824.53903,,https://huggingface.co/nopperl/alpaca-lora-7b-german-base-51k-ggml,"

; GGML conversion of Zicklein (a German Alpaca LoRa for LLaMA). Compatible with llama.cpp version master-2d43387 or later. See Alpaca for instructions on how to prompt the model.; More information about the conversion process is in this git repo."
Minotaur-13B-Landmark-GPTQ,Text Generation,Transformers,,other,,7,,76,7436.63738,,https://huggingface.co/TheBloke/Minotaur-13B-Landmark-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eugene Pentland's Minotaur 13B Landmark.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This model has landmark attention enabled. This means it can handle much longer context responses.
Mangled_Merge_V3,Text-to-Image,Diffusers,,creativeml-openrail-m,,1,,21,0.003040924,,https://huggingface.co/ManglerFTW/Mangled_Merge_V3,"Mangled Merge V3 has been a labor of love side project for some time. I finally feel it's strong enough for a new iteration. It is great with 2D anime style AND realistic images. It also brings a lot of style with it.; Usage:
Mangled Merge V3 has a custom VAE built in to control contrast and brightness in order to handle both 2D and realistic styles. It is strongly recommended to keep the VAE set to automatic. As far as CFG settings, 7 is the sweet spot, but you can explore between 4 and 10. Self Attention Guidance is recommended, but on a scale between .1 and .3. Anything more can give it too much detail.; Hands and faces can still be an issue from far distances. I suggest adding [:highly detailed face, highly detailed eyes, highly detailed hands:.20] to help alleviate this. This model does much better with high res fix. Odd resolutions MAY give it some trouble without having it enabled.; Preview images can be found on the Civitai website @ Civitai: Mangled Merge V3"
pyg-instruct-wizardlm,Text Generation,PyTorch; Transformers,,,,1,,37,12465.47772,,https://huggingface.co/Lazycuber/pyg-instruct-wizardlm,No model card; New: Create and edit this model card directly on the website!
unite-mup,Translation,,94 languages,apache-2.0,,3,,0,0.006006927,,https://huggingface.co/Unbabel/unite-mup,"This model was developed by the NLP2CT Lab at the University of Macau and Alibaba Group, and all credits should be attributed to these groups. Since it was developed using the COMET codebase, we adapted the code to run these models within COMET.""; This is equivalent to [UniTE-MUP-large] from modelscope; Apache 2.0; Using this model requires unbabel-comet to be installed:; Then you can use it through comet CLI:"
CN_test,Text-to-Image,,English,creativeml-openrail-m,,15,,0,7317.564482,,https://huggingface.co/Ai-tensa/CN_test,"This model is an experimental CN model.
It aims to paint the finish coat from the primer coat.
It seems that setting Control mode to ""ControlNet is more important"" and doing i2i based on the primed image gives relatively good results.; The model was trained from Flex Waifu Rainbow with ~6k images paired with images segmented by DanbooRegion.
The training images are from various authors and models published on the Internet with AI illustration tags.; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
The CreativeML OpenRAIL License specifies: ; These Models build on the excellent works: SD1.4, developed by CompVis Researchers, ControlNet and DanbooRegion by Lvmin Zhang, et.al., and WD1.3, developed by Anthony Mercurio, Salt, and Cafe.; Input
"
DucHaiten-DarkNiji,,,,creativeml-openrail-m,,1,,0,2631.685781,,https://huggingface.co/DucHaiten/DucHaiten-DarkNiji,"After two models Darkside and Niji Cute, which were well received by everyone, I decided to start creating DucHaiten-DarkNiji. Just looking at the name everyone would guess that it's a combination of the cute Darkside and Niji, but it's not that simple. It took me two whole days to train the DucHaiten-AsianGirl model just to make the auxiliary materials for Niji and Darkside, and then another full day to use the weighted merge block to tweak it bit by bit during the merge process.; To summarize the extremely cumbersome and complicated process, adjusting the lighting to suit Darkside's style, adjusting the brushstrokes so that it doesn't lose the essence and creativity of Niji but still trying to keep the quality Details and stick to Darkside's prompt, filter out errors, add some abilities that Darkside and Niji Cute have not been able to do before.; With a little hires fix trust me it will amaze you.; Sampler: DPM++ 2M Karras; Nprompt:"
DucHaiten-AsianGirl,,,,creativeml-openrail-m,,1,,0,2631.681483,,https://huggingface.co/DucHaiten/DucHaiten-AsianGirl,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
vocos-encodec-24khz,,PyTorch,,mit,,2,,"5,894",40.40194504,,https://huggingface.co/charactr/vocos-encodec-24khz,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
K-main,Text-to-Image,Diffusers,,other,,3,,45,3921.922615,,https://huggingface.co/digiplay/K-main,Model info:; https://civitai.com/models/87906/k-main; Realistic Look.; Sample images:; 
mlong-t5-large-sumstew,Summarization,PyTorch; Transformers,5 languages,apache-2.0,,2,Joemgu/sumstew,507,5110.652074,,https://huggingface.co/Joemgu/mlong-t5-large-sumstew,"TL;DR: Multilingual, longform (up to 16k input tokens), abstractive summarization model. Trained on sumstew to generate both title and summary of a given input document.; Output:; Prefix your document of choice with either:; Depending on the prefix, the output will either be:"
llava-vicuna-7b-v1.1-lcs_558k-instruct_80k_1e-lora-preview_alpha,Text Generation,Transformers,,apache-2.0,,4,,41,853.0780095,,https://huggingface.co/liuhaotian/llava-vicuna-7b-v1.1-lcs_558k-instruct_80k_1e-lora-preview_alpha,"Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA-LoRA-Preview was trained in June 2023.; Paper or resources for more information:
https://llava-vl.github.io/; License:
Apache License 2.0; Where to send questions or comments about the model:
https://github.com/haotian-liu/LLaVA/issues"
llama-molinst-molecule-7b,,,,apache-2.0,https://arxiv.org/pdf/2306.08018.pdf,7,,0,33.68668266,,https://huggingface.co/zjunlp/llama-molinst-molecule-7b,"This repo contains a low-rank adapter for LLaMA-7b, trained on the ? molecule-oriented instructions from the ? Mol-Instructions dataset.; Instructions for running it can be found at https://github.com/zjunlp/Mol-Instructions.; Please refer to our paper for more details.; ; Please?give?me?some?details?about?this?molecule:
 [C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][=Branch1][C][=O][O][C@H1][Branch2][Ring1][=Branch1][C][O][C][=Branch1][C][=O][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][C][O][P][=Branch1][C][=O][Branch1][C][O][O][C][C@@H1][Branch1][=Branch1][C][=Branch1][C][=O][O][N]"
Chinese-Vicuna-lora-7b-belle-and-guanaco-4bit,,,Chinese,gpl-3.0,,1,BelleGroup/generated_train_0.5M_CN; JosephusCheung/GuanacoDataset,0,25.54897564,,https://huggingface.co/Chinese-Vicuna/Chinese-Vicuna-lora-7b-belle-and-guanaco-4bit,"This is a Chinese instruction-tuning lora checkpoint based on llama-7B from this repo's work. 
Specially, this is the 4bit version trained with qlora; You can use it like this: ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
MPT-7B-Mercury-Experimental,Text Generation,PyTorch; Transformers,,mit,,3,,12,27230.41193,,https://huggingface.co/teknium/MPT-7B-Mercury-Experimental,"Base Model: MPT-7B; This is a Hermes Lite version that excludes the training data of Nous Instruct that hermes model was also trained on, and is experimental.; Big thanks to BitTensor foundation for the compute to attempt this experiment!; There seems to have been some sort of problem with the training that I cannot identify, that, while it does seem improved from the base model, does not seem to have learned nearly as much as was learned by Llama in training Hermes.  ; Typically, the model would response with long responses when asked, be much more contextually intelligent, and answer in a thoughtful way. However, for whatever reason - likely something to do with not training with LLM-Foundry - the model does not like longer responses, and typical responds quite breifly."
minotaur-13b-fixed,Text Generation,PyTorch; Transformers,,apache-2.0,,5,ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered; QingyiSi/Alpaca-CoT; teknium/GPTeacher-General-Instruct; metaeval/ScienceQA_text_only; hellaswag; openai/summarize_from_feedback; riddle_sense; gsm8k; camel-ai/math; camel-ai/biology; camel-ai/physics; camel-ai/chemistry; winglian/evals,62,26626.50364,,https://huggingface.co/openaccess-ai-collective/minotaur-13b-fixed,"
? Donate to OpenAccess AI Collective to help us keep building great tools and models!; The affected datasets include:; Minotaur 13B is an instruct fine-tuned model on top of LlaMA-13B. Minotaur 13B is fine-tuned on only completely open datasets making this model reproducible by anyone.; Questions, comments, feedback, looking to donate, or want to help? Reach out on our Discord or email wing@openaccessaicollective.org; Chat only style prompts using USER:,ASSISTANT:."
Vaes,,,,,,1,,0,3056.001445,,https://huggingface.co/Neveraxme/Vaes,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
pegasus-x-large-8192-pubmed,Text2Text Generation,PyTorch; Transformers,,,,1,,219,1175.87666,,https://huggingface.co/twigs/pegasus-x-large-8192-pubmed,No model card; New: Create and edit this model card directly on the website!
Chinese-Pygmalion-7b-GPTQ,Text Generation,PyTorch; Transformers,Chinese; English,apache-2.0,,2,,11,4291.330698,,https://huggingface.co/coyude/Chinese-Pygmalion-7b-GPTQ,"原始模型：https://huggingface.co/Neko-Institute-of-Science/pygmalion-7b ; lora：https://huggingface.co/ziqingyang/chinese-alpaca-lora-7b ; 将pygmalion-7b与chinese-alpaca-lora-7b进行合并，增强模型的中文能力，不过存在翻译腔; 使用项目：
https://github.com/ymcui/Chinese-LLaMA-Alpaca; https://github.com/qwopqwop200/GPTQ-for-LLaMa"
anything-v4.5-clone,Text-to-Image,Diffusers,English,creativeml-openrail-m,,7,,852,31903.16607,,https://huggingface.co/shibal1/anything-v4.5-clone,"Try out my new model! - Pastel Mix || Stylized Anime Model. Thanks.; I also uploaded it in CivitAI! https://civitai.com/models/5414/pastel-mix-stylized-anime-model I'd appreciate the ratings, thank you!; Yes, it's a shameless plug.; Examples:; 

"
starcoderplus-megatron,Text Generation,Transformers,,,https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,4,bigcode/the-stack-dedup; tiiuae/falcon-refinedweb,0,0.007890625,,https://huggingface.co/bigcode/starcoderplus-megatron,"Play with the instruction-tuned StarCoderPlus at StarChat-Beta.; This is the Megatron-LM version of StarCoderPlus.; StarCoderPlus is a fine-tuned version of StarCoderBase on 600B tokens from the English web dataset RedefinedWeb 
combined with StarCoderData from The Stack (v1.2) and a Wikipedia dataset.
It's a 15.5B parameter Language Model trained on English and 80+ programming languages. The model uses Multi Query Attention,
a context window of 8192 tokens,  and was trained using the Fill-in-the-Middle objective on 1.6 trillion tokens. ; The model was trained on English and GitHub code. As such it is not an instruction model and commands like ""Write a function that computes the square root."" do not work well. However, the instruction-tuned version in StarChat makes a capable assistant.; Feel free to share your generations in the Community tab!"
Anima33B,Conversational,,Chinese,apache-2.0,https://arxiv.org/pdf/2305.14314.pdf,27,Chinese-Vicuna/guanaco_belle_merge_v1.0,0,1996.825071,,https://huggingface.co/lyogavin/Anima33B,; 第一个开源的基于QLoRA的33B中文大语言模型 the First QLoRA based 33B fully open-source Chinese LLM; 注意：此model为PEFT adaptor版本，merged的版本在这里。; https://github.com/lyogavin/Anima; Anima模型基于QLoRA开源的33B guanaco训练了10000 steps。训练使用一个H100 GPU。
realistic-vision-v20-2047,Text-to-Image,Diffusers,,creativeml-openrail-m,,2,,"2,430",0.004492264,3,https://huggingface.co/stablediffusionapi/realistic-vision-v20-2047,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""realistic-vision-v20-2047""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Try model for free: Generate Images"
darkSushiMixMix_225D,,,,,,4,,0,2181.121445,,https://huggingface.co/OortOnline/darkSushiMixMix_225D,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
OTTER-Video-LLaMA7B-DenseCaption,Text2Text Generation,PyTorch; Transformers,,mit,,8,,999,39577.70358,,https://huggingface.co/luodian/OTTER-Video-LLaMA7B-DenseCaption,"

; 



; An example of using this model to run on your video. 
Please first clone Otter to your local disk. 
Place following script inside the Otter folder to make sure it has the access to otter/modeling_otter.py."
ALunarLight,Text-to-Image,Safetensors,,creativeml-openrail-m,,8,,0,0,,https://huggingface.co/Jemnite/ALunarLight,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; A mix targeted at both enhancing lighting effects through usage of lighting-isolation-loras harvested from breakdomain and pika's new generations and clip expansion using ALunarDream.; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage. The CreativeML OpenRAIL License specifies: ; Used Models"
CuriousMerge2.5D_v40E,Text-to-Image,Diffusers,,other,,3,,182,3031.042438,1,https://huggingface.co/digiplay/CuriousMerge2.5D_v40E,Model info:; https://civitai.com/models/79070?modelVersionId=94189; Author's page:; https://civitai.com/user/Cur1ous/models; https://ko-fi.com/cur1ous/
chronos-hermes-13B-GPTQ,Text Generation,Transformers; PyTorch,,other,,20,,"3,855",7631.141952,,https://huggingface.co/TheBloke/chronos-hermes-13B-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Austism's Chronos Hermes 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh."
kw-cutegpt-13b-base,Text Generation,PyTorch; Transformers,,,,1,,142,81101.57578,,https://huggingface.co/XuYipei/kw-cutegpt-13b-base,KW CuteGPT 13b base
chatbotV1,,,,apache-2.0,,3,,0,0.002247353,,https://huggingface.co/yangliu/chatbotV1,"单论对话机器人
调用方式; from transformers import AutoModelForCausalLM, AutoTokenizer; checkpoint = ""model""; tokenizer = AutoTokenizer.from_pretrained(checkpoint); model = AutoModelForCausalLM.from_pretrained(checkpoint).cuda()"
WizardLM-7B-Landmark-GPTQ,Text Generation,Transformers,,other,,11,,47,3995.996829,,https://huggingface.co/TheBloke/WizardLM-7B-Landmark-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eugene Pentland's Wizard 7B Landmark.; It is the result of quantising to 4bit using AutoGPTQ.; This model has landmark attention enabled. This means it can handle much longer context responses.
OpenFlamingo-3B-vitl-mpt1b-langinstruct,,,English,,https://arxiv.org/pdf/2210.08402.pdf; https://arxiv.org/pdf/2304.06939.pdf,1,laion2b,0,4290.567988,,https://huggingface.co/openflamingo/OpenFlamingo-3B-vitl-mpt1b-langinstruct,"Blog post | Code | Demo; OpenFlamingo is an open source implementation of DeepMind's Flamingo models. 
This 3B-parameter model uses a CLIP ViT-L/14 vision encoder and an instruction-tuned MPT-1B language model. ; We follow the Flamingo modeling paradigm, outfitting the layers of a pretrained, frozen language model such that they cross-attend to visual features when decoding. Following Flamingo, we freeze the vision encoder and language model but train the connecting modules on web-scraped image-text sequences. Specifically, we trained this model on a mixture of LAION-2B and Multimodal C4.  ; This model has cross-attention modules inserted in every decoder block. It was trained using DistributedDataParallel across 64 A100 40GB GPUs at FP32 precision.; The MPT-1B modeling code does not accept the labels kwarg and compute cross-entropy loss within forward(). To train with the OpenFlamingo codebase, we suggest using a version with the labels kwarg here."
OpenFlamingo-9B-vitl-mpt7b,,,English,,https://arxiv.org/pdf/2210.08402.pdf; https://arxiv.org/pdf/2304.06939.pdf,13,laion2b,0,5672.967852,1,https://huggingface.co/openflamingo/OpenFlamingo-9B-vitl-mpt7b,"Blog post | Code | Demo; OpenFlamingo is an open source implementation of DeepMind's Flamingo models. 
This 9B-parameter model uses a CLIP ViT-L/14 vision encoder and MPT-7B language model.; We follow the Flamingo modeling paradigm, outfitting the layers of a pretrained, frozen language model such that they cross-attend to visual features when decoding. Following Flamingo, we freeze the vision encoder and language model but train the connecting modules on web-scraped image-text sequences. Specifically, we trained this model on a mixture of LAION-2B and Multimodal C4.  ; This model has cross-attention modules inserted in every fourth decoder block. It was trained using DistributedDataParallel across 64 A100 80GB GPUs at automatic BF16 mixed precision.; To use these MPT weights, OpenFlamingo must be initialized using revision 68e1a8e0ebb9b30f3c45c1ef6195980f29063ae2 of the MPT-7B modeling code. We suggest using this copy of the model to ensure the code is loaded at that commit. "
controlnet-qr-pattern,,TensorBoard; Diffusers,,creativeml-openrail-m,,33,Nacholmo/controlnet-test-darkest-color; yuvalkirstain/pexel_images_lots_with_generated_captions,"1,335",4444.180277,,https://huggingface.co/Nacholmo/controlnet-qr-pattern,"Conditioning only 15% of the pixels closest to black, so as not to affect the luminance of the rest of the image.
Also manteing the color to some degree.; Move the .yaml to extensions/sd-webui-controlnet/models; If you want to use it in diffusers (Not automatic111), the recommended checkpoint is the 11500; Play arround with the starting step, 0 to 0.25 its the sweetspot, if it start at 0 the qr has priority, the higher you raise them, the stronger the prompt gets; To achieve optimal results with the Hires fix, ensure that your resolution is set to a minimum of 600x600. Additionally, set the denoising strength to a minimum of 0.7 and the hires steps to at least 20."
OTTER-LLaMA7B-Init,Text2Text Generation,PyTorch; Transformers,,mit,,1,,190,33679.45913,,https://huggingface.co/luodian/OTTER-LLaMA7B-Init,"

; This weight is for initilizing training for Otter. It's directly converted from Openflamingo.; You can load and try this model using; You can also start training Otter via the commands; If you wish to init a video instruction tuning, you should add "
Chinese-Falcon-7B,Text Generation,PyTorch; Transformers,,apache-2.0,,24,,"1,720",15213.73926,3,https://huggingface.co/Linly-AI/Chinese-Falcon-7B,Training details: https://github.com/CVI-SZU/Linly
airoboros-33B-gpt4-1.2-GPTQ,Text Generation,Transformers,,other,,9,jondurbin/airoboros-gpt4-1.2,251,17307.94102,,https://huggingface.co/TheBloke/airoboros-33B-gpt4-1.2-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for John Durbin's Airoboros 33B GPT4 1.2.; It is the result of quantising to 4bit using AutoGPTQ.; Please make sure you're using the latest version of text-generation-webui
airoboros-65B-gpt4-1.2-GPTQ,Text Generation,Transformers,,other,,10,jondurbin/airoboros-gpt4-1.2,136,34306.34104,,https://huggingface.co/TheBloke/airoboros-65B-gpt4-1.2-GPTQ,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for John Durbin's Airoboros 65B GPT4 1.2.; It is the result of quantising to 4bit using AutoGPTQ.; Please make sure you're using the latest version of text-generation-webui
bert-medical-ner,Token Classification,PyTorch; TensorBoard; Transformers,,apache-2.0,,1,,77,261.8792561,,https://huggingface.co/silpakanneganti/bert-medical-ner,"This model is a fine-tuned version of distilbert-base-cased on the None dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:"
guizhencao-loha,,,English,creativeml-openrail-m,,7,,0,838.0042676,,https://huggingface.co/TLFZ/guizhencao-loha,"Pixiv：https://www.pixiv.net/artworks/109004467; ; ; pretrained_model_name_or_path = ""./sd-models/model.ckpt""
v2 = false
train_data_dir = ""./train/guizhengchao""
prior_loss_weight = 1
resolution = ""1280,1280""
enable_bucket = true
min_bucket_reso = 256
max_bucket_reso = 1_536
output_name = ""guizhengchao""
output_dir = ""./output""
save_model_as = ""safetensors""
save_precision = ""fp16""
save_every_n_epochs = 1
max_train_epochs = 10
train_batch_size = 2
gradient_checkpointing = false
network_train_unet_only = false
network_train_text_encoder_only = false
learning_rate = 0.00007
unet_lr = 0.00007
text_encoder_lr = 0.000008
lr_scheduler = ""cosine_with_restarts""
optimizer_type = ""Lion""
min_snr_gamma = 5
lr_scheduler_num_cycles = 3
network_module = ""lycoris.kohya""
network_dim = 16
network_alpha = 8
network_dropout = 0
log_with = ""tensorboard""
logging_dir = ""./logs""
caption_extension = "".txt""
shuffle_caption = true
weighted_captions = false
keep_tokens = 0
max_token_length = 255
multires_noise_iterations = 6
multires_noise_discount = 0.3
seed = 1_337
clip_skip = 2
mixed_precision = ""fp16""
xformers = true
lowram = false
cache_latents = true
cache_latents_to_disk = false
persistent_data_loader_workers = true
network_args = [ ""conv_dim=5"", ""conv_alpha=1"", ""dropout=0"", ""algo=loha"" ]; (((v1.0)))
推荐0.5到0.8"
medfalcon-40b-lora,Text Generation,PEFT,English,cc-by-nc-3.0,https://arxiv.org/pdf/2106.09685.pdf,3,,1,889.0035799,,https://huggingface.co/nmitchko/medfalcon-40b-lora,"nmitchko/medfalcon-40b-lora is a large language model LoRa specifically fine-tuned for medical domain tasks.
It is based on Falcon-40b-instruct at 40 billion parameters.; The primary goal of this model is to improve question-answering and medical dialogue tasks.
It was trained using LoRA, specifically QLora, to reduce memory footprint. ; This Lora supports 4-bit and 8-bit modes.; Steps to load this model:"
Chinese_style.safetensors,,,,unknown,,1,,0,151.0014692,,https://huggingface.co/YANGYINGDUO/Chinese_style.safetensors,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
CrossoverMix_v2,Text-to-Image,Diffusers,,other,,3,,130,2181.122761,3,https://huggingface.co/digiplay/CrossoverMix_v2,Model info:; https://civitai.com/models/78275?modelVersionId=95545; Authour's Sample image:; ; Sample images I made:
afro-xlmr-large-61L,Fill-Mask,PyTorch; JAX; Safetensors; Transformers,61 languages,afl-3.0,,1,,319,6898.386687,,https://huggingface.co/Davlan/afro-xlmr-large-61L,"AfroXLMR-large was created by MLM adaptation of XLM-R-large model on 61 languages widely spoken in Africa
including 4 high resource languages. ; A mix of mC4, Wikipedia and OPUS data; There are 61 languages available :"
WizardLM-Uncensored-Falcon-40B-GGML,,Transformers,,other,,35,,148,279244.8087,,https://huggingface.co/TheBloke/WizardLM-Uncensored-Falcon-40B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGCC format model files for Eric Hartford's WizardLM Uncensored  Falcon 40B.; These files will not work in llama.cpp, text-generation-webui or KoboldCpp.; GGCC is a new format created in a new fork of llama.cpp that introduced this new Falcon GGML-based support: cmp-nc/ggllm.cpp."
zeroscope_v2_30x448x256,,Diffusers,,cc-by-nc-4.0,,13,,16,4904.96397,,https://huggingface.co/cerspense/zeroscope_v2_30x448x256,"; A watermark-free Modelscope-based video model optimized for producing high-quality 16:9 compositions and a smooth video output. This model was trained from the original weights using 9,923 clips and 29,769 tagged frames at 30 frames, 448x256 resolution.; zeroscope_v2 30x448x256 is specifically designed for upscaling with Potat1 using vid2vid in the 1111 text2video extension by kabachuha. Leveraging this model as a preliminary step allows for superior overall compositions at higher resolutions in Potat1, permitting faster exploration in 448x256 before transitioning to a high-resolution render. See an example output that has been upscaled to 1152 x 640 using Potat1.; For upscaling, it's recommended to use Potat1 via vid2vid in the 1111 extension. Aim for a resolution of 1152x640 and a denoise strength between 0.66 and 0.85. Remember to use the same prompt and settings that were used to generate the original clip.; Lower resolutions or fewer frames could lead to suboptimal output. 
Certain clips might appear with cuts. This will be fixed in the upcoming 2.1 version, which will incorporate a cleaner dataset.
Some clips may playback too slowly, requiring prompt engineering for an increased pace."
fantexiV09beta_fantexiV09beta,,,,creativeml-openrail-m,,1,,0,4618.241483,,https://huggingface.co/casque/fantexiV09beta_fantexiV09beta,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
open_llama_13b_easylm,,,,apache-2.0,,50,togethercomputer/RedPajama-Data-1T,0,26625.01088,,https://huggingface.co/openlm-research/open_llama_13b_easylm,"In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we’ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.; The model can be evaluated with lm-eval-harness. However, due to the aforementioned tokenizer issue, we need to avoid using the fast tokenizer to obtain the correct results. This can be achieved by passing in use_fast=False to this part of lm-eval-harness, as shown in the example below:"
custom-sdwebui,,,,,https://arxiv.org/pdf/2211.06679.pdf,2,,0,0.915717812,,https://huggingface.co/ygohel18/custom-sdwebui,"Not just a browser interface based on Gradio library for Stable Diffusion.
A pixel perfect design, mobile friendly, customizable interface that adds accessibility, ease of use and extended functionallity to the stable diffusion web ui.
Enjoy!; Default theme; ; Quick Settings aside off-canvas view - drag and drop to custom sort your settings; "
andite-finetunes-backup,Text-to-Image,Safetensors,,creativeml-openrail-m,,11,,0,21400.76928,,https://huggingface.co/LMFResearchSociety/andite-finetunes-backup,"Thanks to Jame's friend for the desserts.; Still Missing:; Unable to determine this model’s library. Check the
								docs 
.
							"
ct2fast-WizardCoder-15B-V1.0,,Transformers,,bigcode-openrail-m,,4,,58,15978.47981,,https://huggingface.co/michaelfeil/ct2fast-WizardCoder-15B-V1.0,"Speedup inference while reducing memory by 2x-4x using int8 inference in C++ on CPU or GPU.; quantized version of WizardLM/WizardCoder-15B-V1.0; Converted on 2023-06-15 using; Checkpoint compatible to ctranslate2>=3.16.0
and hf-hub-ctranslate2>=2.0.8; This is just a quantized version. Licence conditions are intended to be idential to original huggingface repo."
passgpt-16characters,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2306.01545.pdf,2,,4,0,,https://huggingface.co/javirandor/passgpt-16characters,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; PassGPT is a causal language model trained on password leaks. It was first introduced in this paper. This version of the model was trained on passwords from the RockYou leak, after filtering those that were at most 16 characters long. You can also access PassGPT trained on passwords up to 10 characters long, without restrictions here.; This is a curated version of the model reported in the paper. Vocabulary size was reduced to the most meaningful characters and training was slightly optimized. Results are slightly better with these architectures.; 
PassGPT is intended and licensed for research use only. The model and code are CC BY NC 4.0 (allowing only non-commercial use) and should not be used outside of research purposes. This model should never be used to attack real systems. Access will be granted upon request. Please, make sure to indicate the details and scope of your project."
gorilla-7b-hf-delta-v1,Text Generation,PyTorch; Transformers,English,apache-2.0,https://arxiv.org/pdf/2305.15334.pdf,3,gorilla-llm/APIBench,28,13804.04157,,https://huggingface.co/gorilla-llm/gorilla-7b-hf-delta-v1,"By Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez  (Project Website);   ; Gorilla enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla can write a semantically- and syntactically- correct API to invoke. With Gorilla, we are the first to demonstrate how to use LLMs to invoke 1,600+ (and growing) API calls accurately while reducing hallucination. We also release APIBench, the largest collection of APIs, curated and easy to be trained on! Join us, as we try to expand the largest API store and teach LLMs how to write them! Hop on our Discord, or open a PR, or email us if you would like to have your API incorporated as well.; Gorilla can be either trained via standard finetuning or using our novel retriever-aware training pipeline. We release gorilla-7b-hf-delta-v1, a 0-shot finetuned LLM that can reliably use Hugging Face APIs. It can be prompted through simply natural language (e.g., ""I want to generate an image from text.""). Checkour our website, github and paper for more information.
Thank you for the feedback, with gorilla-llm/gorilla-7b-hf-delta-v1 Gorilla now ouput's code snippet that can be directly integrated into your workflow!; Now with gorilla-llm/gorilla-7b-hf-delta-v1"
baichuan-llama-7b,Text Generation,PyTorch; Transformers,Chinese; English,,https://arxiv.org/pdf/1910.07467.pdf; https://arxiv.org/pdf/2009.03300.pdf,22,,363,14338.1175,,https://huggingface.co/fireballoon/baichuan-llama-7b,使用LLaMA格式保存的baichuan-7B。可以直接使用LlamaForCausalLM和LlamaTokenizer加载。; baichuan-7B model saved in the format of the LLaMA model. You can directly use LlamaForCausalLM and LlamaTokenizer to load the model.; The following is from the original repo baichuan-7B.; baichuan-7B是由百川智能开发的一个开源的大规模预训练模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。; 如果希望使用baichuan-7B（如进行推理、Finetune等），我们推荐使用配套代码库baichuan-7B。
DocsGPT-7B,Text Generation,PyTorch; Transformers,,apache-2.0,,12,,87,27240.65311,,https://huggingface.co/Arc53/DocsGPT-7B,"DocsGPT-7B is a decoder-style transformer that is fine-tuned specifically for providing answers based on documentation given in context. It is built on to of the MosaicPretrainedTransformer (MPT), being fine-tuned from the MPT-7B model developed by MosaicML. The model inherits the powerful language understanding capabilities of MPT-7B and has been specialized for the purpose of documentation-oriented question answering.; Architecture: Decoder-style Transformer; Training data: Fine-tuned on approximately 1000 high-quality examples of documentation answering workflows.; Base model: Fine-tuned version of MPT-7B, which is pretrained from scratch on 1T tokens of English text and code.; License: Apache 2.0"
Gishiki,,,,,,6,,0,6543.361451,,https://huggingface.co/Aotsuyu/Gishiki,"x式; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
text2vec-base-chinese-sentence,Sentence Similarity,PyTorch; Transformers,Chinese,apache-2.0,,27,https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset,"3,113",472.9949342,,https://huggingface.co/shibing624/text2vec-base-chinese-sentence,"This is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese-sentence.; It maps sentences to a 768 dimensional dense vector space and can be used for tasks 
like sentence embeddings, text matching or semantic search.; For an automated evaluation of this model, see the Evaluation Benchmark: text2vec; 说明：; 旧版 shibing624/text2vec-base-chinese-nli 模型放在tag1.0"
gpt2-large-conversational,Text Generation,PyTorch; Safetensors; Transformers,English,openrail,,1,Locutusque/ColumnedChatCombined; crumb/Clean-Instruct-440k,69,3177.745593,1,https://huggingface.co/Locutusque/gpt2-large-conversational,"This model is intended to be used for generating conversational responses in a variety of contexts, such as chatbots, virtual assistants, and customer service applications. It is designed to provide natural and engaging responses to user input, with a focus on maintaining a consistent tone and style throughout the conversation. The model is suitable for use in both text-based and voice-based interfaces, and can be easily integrated into existing applications using the PyTorch and Transformers frameworks.; The model is trained on a large dataset of conversational data, consisting of interactions between users and an AI assistant. The data is preprocessed to remove any sensitive information and is formatted in a way that is suitable for training a language model. The training data is split into a training set and a validation set, with the training set used to update the model parameters and the validation set used to evaluate the model performance. The model was trained on 550,000 examples over 687,500 steps, it achieved decent metrics.; The model architecture used in this model is GPT-2, a transformer-based language model that is capable of generating high-quality text with a wide range of styles and tones. The GPT-2 architecture consists of a multi-layered transformer encoder-decoder, with self-attention mechanisms that allow the model to capture long-term dependencies and generate coherent text.; The model is evaluated based on several metrics, including loss, reward, penalty, BLEU score, and perplexity. The loss metric is calculated during training and reflects the difference between the predicted output and the actual output. The reward metric is based on the number of correct words generated by the model, while the penalty metric penalizes the model for repeating words consecutively. The BLEU score measures the similarity between the generated text and the ground truth text, while the perplexity metric measures how well the model is able to predict the next word in a sequence. During validation, the model achieved the following metrics:; This model is not suitable for all use cases due to its limited training time on a weak computer. As a result, it may produce irrelevant or nonsensical responses. Additionally, it has not been fine-tuned to remember the chat history, is unable to provide follow-up responses, and it does not know the answer to many questions (it was only fine-tuned to respond in a conversational way). For optimal performance, I recommend using a GPU with at least 12 GB of VRAM and downloading the model manually instead of using the Transformers library. Here's how you should deploy the model:"
Hermes-Open-Llama-7b-8k,Text Generation,PyTorch; Transformers,,,,21,,32,13824.52467,,https://huggingface.co/conceptofmind/Hermes-Open-Llama-7b-8k,No model card; New: Create and edit this model card directly on the website!
long-t5-tglobal-xl-sci-simplify-elife,Summarization,PyTorch; Transformers,,apache-2.0,,2,pszemraj/scientific_lay_summarisation-elife-norm,77,11953.33767,,https://huggingface.co/pszemraj/long-t5-tglobal-xl-sci-simplify-elife,"This model is a fine-tuned version of google/long-t5-tglobal-xl on the pszemraj/scientific_lay_summarisation-elife-norm dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; the pszemraj/scientific_lay_summarisation-elife-norm dataset, input 16384 tokens then truncate, output 1024 tokens then truncate.; The following hyperparameters were used during training:"
baichuan-vicuna-7b,Text Generation,PyTorch; Transformers,Chinese; English,,https://arxiv.org/pdf/2306.04751.pdf,30,anon8231489123/ShareGPT_Vicuna_unfiltered; QingyiSi/Alpaca-CoT; mhhmm/leetcode-solutions-python,134,14338.1516,,https://huggingface.co/fireballoon/baichuan-vicuna-7b,"baichuan-vicuna-7b is a chat model supervised finetuned on vicuna sharegpt data.; 中文说明; [New] baichuan-vicuna-chinese-7b, baichuan finetuned on both English and Chinese ShareGPT.; Inference with FastChat:; Inference with Transformers:"
Agelesnate,,Diffusers,Japanese,creativeml-openrail-m,,24,,0,0.008789063,,https://huggingface.co/teasan/Agelesnate,"; Agelessシリ`ズの後@モデルとなります。
アニメ{に特化したイラストモデルとなっており、Hires使用でも描きこみの多く崩れにくいイラストを生成できるのが特栅扦埂
アニメのキ`ビジュアルのなコントラストの高いイラストや二次元イラストに特化しています。; モデルをcloneもしくはDLした後、以下に格{してください。;  ;  "
baichuan_4bit_lora,,,,unknown,,4,,0,71.71201096,,https://huggingface.co/wp931120x/baichuan_4bit_lora,"项目的主要动机由于百川baichuan -7B是一个pretrain的大模型，尽管它在一些无监督的评估数据集上效果很好，但是并不能开箱即用，因为它没有 supervised finetune 这一步，没有和人类意图进行对齐。
随采用belle 0.5M 指令微调数据，采用qlora的量化微调的方式对百川大模型进行人类意图对齐训练。; 百川7B https://huggingface.co/baichuan-inc/baichuan-7B; 采用的是belle 0.5M https://huggingface.co/datasets/BelleGroup/train_0.5M_CN; 训练方法和过程可视化; 由于采用了int4量化和lora等技术 整个资源消耗只需要12G左右的显存"
predicting-role-based-on-skills,Text Classification,Keras,English,mit,,1,fazni/roles-based-on-skills,0,105.0116389,,https://huggingface.co/fazni/predicting-role-based-on-skills,"Inference API does not yet support keras models for this pipeline type.
							"
robin-33B-v2-fp16,Text Generation,PyTorch; Transformers,,other,,3,,8,66634.06386,,https://huggingface.co/TheBloke/robin-33B-v2-fp16,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are pytorch format fp16 model files for OptimalScale's Robin 33B v2.; It is the result of merging and/or converting the source repository to float16.; For further support, and discussions on these models and AI in general, join us at:"
