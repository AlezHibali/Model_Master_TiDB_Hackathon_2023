Blog Post - Title,Blog Post - Link,Collection ID,Item ID,Created On,Updated On,Published On,Blog Post - Featured Image (Page),Blog Post - Thumbnail Image (Card Grid),abstract,category,task,huggingface_link,paper,license,datasets,language,library,likes icon,likes,downloads icon,downloads_last_month,model size icon,model_size,usage icon,model_usage
Llama-2-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",natural-language-processing,text-generation,https://huggingface.co/meta-llama/Llama-2-7b,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 896,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
Llama-2-70b-chat-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",natural-language-processing,text-generation,https://huggingface.co/meta-llama/Llama-2-70b-chat-hf,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 550,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 41430,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 94
Llama-2-7b-chat-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",natural-language-processing,text-generation,https://huggingface.co/meta-llama/Llama-2-7b-chat-hf,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 328,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 77918,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 57
Llama-2-70b-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",natural-language-processing,text-generation,https://huggingface.co/meta-llama/Llama-2-70b-hf,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 307,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 155399,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 45
Llama-2-13B-chat-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 13B-chat.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML,,License: other,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 264,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 796,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
stable-diffusion-xl-base-0.9,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"SDXL 0.9 Research License Agreement; Copyright (c) Stability AI Ltd.; This License Agreement (as may be amended in accordance with this License Agreement, “License”), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (“Licensee” or “you”) and Stability AI Ltd. (“Stability AI” or “we”) applies to your use of any computer program, algorithm, source code, object code, software, models, or model weights that is made available by Stability AI under this License (“Software”) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (“Documentation”). By using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; LICENSE GRANTa. Subject to your compliance with the Documentation and Sections 2, 3, and 5, Stability AI grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty free and limited license under Stability AI’s copyright interests to use, reproduce, and create derivative works of the Software solely for your non-commercial research purposes. The foregoing license is personal to you, and you may not assign, sublicense, distribute, publish, host, or otherwise make available this Software, derivative works of the Software, models or model weights associated with the Software, this License, or any other rights or obligations under this License without Stability AI’s prior written consent; any such assignment or sublicense without Stability AI’s prior written consent will be void and will automatically and immediately terminate this License.  For sake of clarity, this License does not grant to you the right or ability to extend any license to the Software, derivative works of the Software, or associated models or model weights to a non-Licensee, nor does this License permit you to create a new Licensee, such as by making available a copy of this License.  If you would like rights not granted by this License, you may seek permission by sending an email to legal@stability.ai.b. You may make a reasonable number of copies of the Documentation solely for your use in connection with the license to the Software granted above.c. The grant of rights expressly set forth in this Section 1 (License Grant) are the complete grant of rights to you in the Software Products, and no other licenses are granted, whether by waiver, estoppel, implication, equity or otherwise. Stability AI and its licensors reserve all rights not expressly granted by this License.; RESTRICTIONSYou will not, and will not permit, assist or cause any third party to:a. use, modify, copy, reproduce, create derivative works of, or distribute the Software Products (or any derivative works thereof, works incorporating the Software Products, or any data produced by the Software), in whole or in part, for (i) any commercial or production purposes, (ii) military purposes or in the service of nuclear technology, (iii) purposes of surveillance, including any research or development relating to surveillance, (iv) biometric processing, (v) in any manner that infringes, misappropriates, or otherwise violates any third-party rights, or (vi) in any manner that violates any applicable law and violating any privacy or security laws, rules, regulations, directives, or governmental requirements (including the General Data Privacy Regulation (Regulation (EU) 2016/679), the California Consumer Privacy Act, and any and all laws governing the processing of biometric information), as well as all amendments and successor laws to any of the foregoing;b. alter or remove copyright and other proprietary notices which appear on or in the Software Products;c. utilize any equipment, device, software, or other means to circumvent or remove any security or protection used by Stability AI in connection with the Software, or to circumvent or remove any usage restrictions, or to enable functionality disabled by Stability AI; ord. offer or impose any terms on the Software Products that alter, restrict, or are inconsistent with the terms of this License.e. 1) violate any applicable U.S. and non-U.S. export control and trade sanctions laws (“Export Laws”); 2) directly or indirectly export, re-export, provide, or otherwise transfer Software Products: (a) to any individual, entity, or country prohibited by Export Laws; (b) to anyone on U.S. or non-U.S. government restricted parties lists; or (c) for any purpose prohibited by Export Laws, including nuclear, chemical or biological weapons, or missile technology applications; 3) use or download Software Products if you or they are: (a) located in a comprehensively sanctioned jurisdiction, (b) currently listed on any U.S. or non-U.S. restricted parties list, or (c) for any purpose prohibited by Export Laws; and (4) will not disguise your location through IP proxying or other methods.",multimodel,text-to-image,https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9,Paper: https://arxiv.org/pdf/2108.01073.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2307.01952.pdf,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.2k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 245508,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 49
FreeWilly2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"FreeWilly2 is a Llama2 70B model finetuned on an Orca style Dataset; Start chatting with FreeWilly2 using the following code snippet:; FreeWilly should be used with this prompt format:; FreeWilly2 is trained on our internal Orca-style dataset; Models are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (BF16), and optimized with AdamW. We outline the following hyperparameters:",natural-language-processing,text-generation,https://huggingface.co/stabilityai/FreeWilly2,Paper: https://arxiv.org/pdf/2307.09288.pdf; https://arxiv.org/pdf/2306.02707.pdf,License: cc-by-nc-4.0,Datasets: conceptofmind/cot_submix_original; conceptofmind/flan2021_submix_original; conceptofmind/t0_submix_original; conceptofmind/niv2_submix_original,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 220,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 276.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
Llama-2-13b-chat-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",natural-language-processing,text-generation,https://huggingface.co/meta-llama/Llama-2-13b-chat-hf,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 213,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 14790,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 30
Llama-2-7b-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",natural-language-processing,text-generation,https://huggingface.co/meta-llama/Llama-2-7b-hf,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 187,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 19069,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 10
Llama-2-13b-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",natural-language-processing,text-generation,https://huggingface.co/meta-llama/Llama-2-13b-hf,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 155,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 39218,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 11
Llama-2-70b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",natural-language-processing,text-generation,https://huggingface.co/meta-llama/Llama-2-70b,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 151,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
Llama-2-70b-chat,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",natural-language-processing,text-generation,https://huggingface.co/meta-llama/Llama-2-70b-chat,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 151,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
chatglm2-6b,,,,,,,,,"
  ? Github Repo ? ? Twitter ? ? [GLM@ACL 22] [GitHub] ? ? [GLM-130B@ICLR 23] [GitHub] 
; 
    ? Join our Slack and WeChat
; ChatGLM2-6B 是开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM2-6B 引入了如下新特性：; ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the following new features:; 可以通过如下代码调用 ChatGLM-6B 模型来生成对话：",,,https://huggingface.co/THUDM/chatglm2-6b,Paper: https://arxiv.org/pdf/2103.10360.pdf; https://arxiv.org/pdf/2210.02414.pdf; https://arxiv.org/pdf/1911.02150.pdf,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.24k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1402185,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 65
Llama-2-7b-chat,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",natural-language-processing,text-generation,https://huggingface.co/meta-llama/Llama-2-7b-chat,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 142,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
Llama-2-13B-chat-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 13B-chat.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; To continue a conversation:",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ,,License: other,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 122,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7726,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-13B-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-13B-GGML,,License: other,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 110,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 139,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
Baichuan-13B-Chat,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Baichuan-13B-Chat为Baichuan-13B系列模型中对齐后的版本，预训练模型可见Baichuan-13B-Base。; Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。本次发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。Baichuan-13B 有如下几个特点：; Baichuan-13B-Chat is the aligned version in the Baichuan-13B series of models, and the pre-trained model can be found at Baichuan-13B-Base.; Baichuan-13B is an open-source, commercially usable large-scale language model developed by Baichuan Intelligence, following Baichuan-7B. With 13 billion parameters, it achieves the best performance in standard Chinese and English benchmarks among models of its size. This release includes two versions: pre-training (Baichuan-13B-Base) and alignment (Baichuan-13B-Chat). Baichuan-13B has the following features:; 如下是一个使用Baichuan-13B-Chat进行对话的示例，正确输出为""乔戈里峰。世界第二高峰―――乔戈里峰西方登山者称其为k2峰，海拔高度是8611米，位于喀喇昆仑山脉的中巴边境上""",natural-language-processing,text-generation,https://huggingface.co/baichuan-inc/Baichuan-13B-Chat,Paper: https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2009.03300.pdf,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 386,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 983805,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
Llama-2-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",natural-language-processing,text-generation,https://huggingface.co/meta-llama/Llama-2-13b,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 98,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-7B-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-7B-GGML,,License: other,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 95,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 351,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-7B-Chat-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 7b Chat.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML,,License: other,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 95,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 574,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
stable-diffusion-v1-5,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
For more information about how Stable Diffusion functions, please have a look at ?'s Stable Diffusion blog.; The Stable-Diffusion-v1-5 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 
checkpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on ""laion-aesthetics v2 5+"" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.; You can use this both with the ?Diffusers library and the RunwayML GitHub repository.; For more detailed instructions, use-cases and examples in JAX follow the instructions here; Download the weights ",multimodel,text-to-image,https://huggingface.co/runwayml/stable-diffusion-v1-5,Paper: https://arxiv.org/pdf/2207.12598.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8.74k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6901572,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 24.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1546
chatglm-fitness-RLHF,,,,,,,,,模型体验地址：https://huggingface.co/spaces/fb700/chatglm-fitness-RLHF; ChatGLM-6B 是开源中英双语对话模型，本次训练基于ChatGLM-6B 的第一代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上开展训练。通过训练我们对模型有了更深刻的认知，LLM在一直在进化，好的方法和数据可以挖掘出模型的更大潜能。; 首先，用40万条高质量数据进行强化训练，以提高模型的基础能力；; 第二，使用30万条人类反馈数据，构建一个表达方式规范优雅的语言模式（RM模型）；; 第三，在保留SFT阶段三分之一训练数据的同时，增加了30万条fitness数据，叠加RM模型，对ChatGLM-6B进行强化训练。,,,https://huggingface.co/fb700/chatglm-fitness-RLHF,,License: apache-2.0,,Chinese; English,PyTorch; PEFT,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 131,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 12.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
Llama-2-13b-chat,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",natural-language-processing,text-generation,https://huggingface.co/meta-llama/Llama-2-13b-chat,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 83,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
ruGPT-3.5-13B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Language model for Russian. Model has 13B parameters as you can guess from it's name. This is our biggest model so far and it was used for trainig GigaChat (read more about it in the article).; Model was pretrained on a 300Gb of various domains, than additionaly trained on the 100 Gb of code and legal documets. Here is the dataset structure:; ; Training data was deduplicated, the text deduplication includes 64-bit hashing of each text in the corpus for keeping texts with a unique hash. We also filter the documents based on their text compression rate using zlib4. The most strongly and weakly compressing deduplicated texts are discarded.; Model was trained using Deepspeed and Megatron libraries, on 300B tokens dataset for 3 epochs, around 45 days on 512 V100. After that model was finetuned 1 epoch with sequence length 2048 around 20 days on 200 GPU A100 on additional data (see above).",natural-language-processing,text-generation,https://huggingface.co/ai-forever/ruGPT-3.5-13B,,License: mit,,English; Russian,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 79,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 970,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 53.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
ControlNet-v1-1,,,,,,,,,"This is the model files for ControlNet 1.1.
This model card will be filled in a more detailed way after 1.1 is officially merged into ControlNet.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/lllyasviel/ControlNet-v1-1,,License: openrail,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2.06k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 20.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-40b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Falcon-40B is a 40B parameters causal decoder-only model built by TII and trained on 1,000B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.; Paper coming soon ?.; We get it. AI is everywhere! Is it taking over? ; Before we debate the scant likelihood of a cyborg assassin from the future terminating humanity, let’s get to know the newbie that has soared to top-spot on the leaderboard C Falcon 40B.; Falcon 40B is the UAE’s and the Middle East’s first home-grown, open-source large language model (LLM) with 40 billion parameters trained on one trillion tokens. The brainchild of the Technology Innovation Institute (TII), Falcon 40B has generated a tremendous amount of global interest and intrigue, but what really sweetens the deal is its transparent, open-source feature. ",natural-language-processing,text-generation,https://huggingface.co/tiiuae/falcon-40b,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2306.01116.pdf,License: apache-2.0,Datasets: tiiuae/falcon-refinedweb,4 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2.14k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 151418,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 84.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 40
Llama-2-70B-chat-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 70B Chat.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware for these quantisations!",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ,,License: other,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 71,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8020,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 35.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LLongMA-2-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"LLongMA-2, a suite of Llama-2 models, trained at 8k context length using linear positional interpolation scaling. The model was trained in collaboration with Emozilla of NousResearch and Kaiokendev.; We worked directly with Kaiokendev, to extend the context length of the Llama-2 7b model through fine-tuning. The models pass all our evaluations and maintain the same perplexity at 8k extrapolation surpassing the performance of other recent methodologies.; The model has identical performance to LLaMA 2 under 4k context length, performance scales directly to 8k, and works out-of-the-box with the new version of transformers (4.31) or with trust_remote_code for <= 4.30.; A Llama-2 13b model trained at 8k will release soon on huggingface here: https://huggingface.co/conceptofmind/LLongMA-2-13b; Applying the method to the rotary position embedding requires only slight changes to the model's code by dividing the positional index, t, by a scaling factor.",natural-language-processing,text-generation,https://huggingface.co/conceptofmind/LLongMA-2-7b,Paper: https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2212.10554.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 69,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 272,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
llama-30b-instruct-2048,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No other data was used except for the dataset mentioned above,natural-language-processing,text-generation,https://huggingface.co/upstage/llama-30b-instruct-2048,,,Datasets: sciq; metaeval/ScienceQA_text_only; GAIR/lima; Open-Orca/OpenOrca; openbookqa,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 67,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 725,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
Nous-Hermes-Llama2-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Compute provided by our project sponsor Redmond AI, thank you! Follow RedmondAI on Twitter @RedmondAI.; Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Emozilla leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors.; This Hermes model uses the exact same dataset as Hermes on Llama-1. This is to ensure consistency between the old Hermes and new, for anyone who wanted to keep Hermes as similar to the old one, just more capable.; This model stands out for its long responses, lower hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 4096 sequence length on an 8x a100 80GB DGX machine.; 


",natural-language-processing,text-generation,https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 63,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 362,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
control_v1p_sd15_qrcode_monster,,,,,,,,,"; This model is made to generate creative QR codes that still scan.
Keep in mind that not all generated codes might be readable, but you can try different parameters and prompts to get the desired results.; NEW VERSION; Introducing the upgraded version of our model - Controlnet QR code Monster v2.
V2 is a huge upgrade over v1, for scannability AND creativity.; QR codes can now seamlessly blend the image by using a gray-colored background (#808080).",,,https://huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster,,License: openrail++,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 218,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2805,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
Luna-AI-Llama2-Uncensored,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"“Luna AI Llama2 Uncensored” is a Llama2 based Chat model fine-tuned on over 40,000 long form chat discussions 
  This model was fine-tuned by Tap, the creator of Luna AI.  
  The result is an enhanced Llama2 7b model that rivals ChatGPT in performance across a variety of tasks.; This model stands out for its long responses,  low hallucination rate, and absence of censorship mechanisms. ; The fine-tuning process was performed on an 8x a100 80GB machine.
  The model was trained almost entirely on synthetic outputs.
  This includes data from diverse sources which we included to create our custom dataset, it includes multiple rounds of chats between Human & AI.
; 4bit GPTQ Version provided by @TheBloke - for GPU inference
GGML Version provided by @TheBloke - For CPU inference; The model follows the Vicuna 1.1/ OpenChat format:",natural-language-processing,text-generation,https://huggingface.co/Tap-M/Luna-AI-Llama2-Uncensored,,License: cc-by-sa-4.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 61,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 498,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
Llama-2-13B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-13B-GPTQ,,License: other,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 60,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2021,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
waifu-diffusion-xl,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"waifu-diffusion-xl is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning StabilityAI's SDXL 0.9 model provided as a research preview.; ; masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck; This model has been released under the SDXL 0.9 RESEARCH LICENSE AGREEMENT due to the repository containing the SDXL 0.9 weights before an official release. We have been given permission to release this model.; This model can be used for entertainment purposes and as a generative art assistant.",multimodel,text-to-image,https://huggingface.co/hakurei/waifu-diffusion-xl,,License: unknown,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 102,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-7b-chat-coreml,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a Core ML version of meta-llama/Llama-2-7b-chat-hf. For license information, model details and acceptable use policy, please refer to the original model card.; This conversion was performed in float16 mode with a fixed sequence length of 64, and is intended for evaluation and test purposes. Please, open a conversation in the Community tab if you have questions or want to report an issue.",natural-language-processing,text-generation,https://huggingface.co/pcuenq/Llama-2-7b-chat-coreml,,License: other,,,Core ML; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 56,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 239,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.86MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
stable-diffusion-2-1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This model card focuses on the model associated with the Stable Diffusion v2-1 model, codebase available here.; This stable-diffusion-2-1 model is fine-tuned from stable-diffusion-2 (768-v-ema.ckpt) with an additional 55k steps on the same dataset (with punsafe=0.1), and then fine-tuned for another 155k extra steps with punsafe=0.98.; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model; Language(s): English",multimodel,text-to-image,https://huggingface.co/stabilityai/stable-diffusion-2-1,Paper: https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: openrail++,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2.93k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 842185,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 21.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 796
stable-diffusion-xl-refiner-0.9,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"SDXL 0.9 Research License Agreement; Copyright (c) Stability AI Ltd.; This License Agreement (as may be amended in accordance with this License Agreement, “License”), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (“Licensee” or “you”) and Stability AI Ltd. (“Stability AI” or “we”) applies to your use of any computer program, algorithm, source code, object code, software, models, or model weights that is made available by Stability AI under this License (“Software”) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (“Documentation”). By using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; LICENSE GRANTa. Subject to your compliance with the Documentation and Sections 2, 3, and 5, Stability AI grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty free and limited license under Stability AI’s copyright interests to use, reproduce, and create derivative works of the Software solely for your non-commercial research purposes. The foregoing license is personal to you, and you may not assign, sublicense, distribute, publish, host, or otherwise make available this Software, derivative works of the Software, models or model weights associated with the Software, this License, or any other rights or obligations under this License without Stability AI’s prior written consent; any such assignment or sublicense without Stability AI’s prior written consent will be void and will automatically and immediately terminate this License.  For sake of clarity, this License does not grant to you the right or ability to extend any license to the Software, derivative works of the Software, or associated models or model weights to a non-Licensee, nor does this License permit you to create a new Licensee, such as by making available a copy of this License.  If you would like rights not granted by this License, you may seek permission by sending an email to legal@stability.ai.b. You may make a reasonable number of copies of the Documentation solely for your use in connection with the license to the Software granted above.c. The grant of rights expressly set forth in this Section 1 (License Grant) are the complete grant of rights to you in the Software Products, and no other licenses are granted, whether by waiver, estoppel, implication, equity or otherwise. Stability AI and its licensors reserve all rights not expressly granted by this License.; RESTRICTIONSYou will not, and will not permit, assist or cause any third party to:a. use, modify, copy, reproduce, create derivative works of, or distribute the Software Products (or any derivative works thereof, works incorporating the Software Products, or any data produced by the Software), in whole or in part, for (i) any commercial or production purposes, (ii) military purposes or in the service of nuclear technology, (iii) purposes of surveillance, including any research or development relating to surveillance, (iv) biometric processing, (v) in any manner that infringes, misappropriates, or otherwise violates any third-party rights, or (vi) in any manner that violates any applicable law and violating any privacy or security laws, rules, regulations, directives, or governmental requirements (including the General Data Privacy Regulation (Regulation (EU) 2016/679), the California Consumer Privacy Act, and any and all laws governing the processing of biometric information), as well as all amendments and successor laws to any of the foregoing;b. alter or remove copyright and other proprietary notices which appear on or in the Software Products;c. utilize any equipment, device, software, or other means to circumvent or remove any security or protection used by Stability AI in connection with the Software, or to circumvent or remove any usage restrictions, or to enable functionality disabled by Stability AI; ord. offer or impose any terms on the Software Products that alter, restrict, or are inconsistent with the terms of this License.e. 1) violate any applicable U.S. and non-U.S. export control and trade sanctions laws (“Export Laws”); 2) directly or indirectly export, re-export, provide, or otherwise transfer Software Products: (a) to any individual, entity, or country prohibited by Export Laws; (b) to anyone on U.S. or non-U.S. government restricted parties lists; or (c) for any purpose prohibited by Export Laws, including nuclear, chemical or biological weapons, or missile technology applications; 3) use or download Software Products if you or they are: (a) located in a comprehensively sanctioned jurisdiction, (b) currently listed on any U.S. or non-U.S. restricted parties list, or (c) for any purpose prohibited by Export Laws; and (4) will not disguise your location through IP proxying or other methods.",computer-vision,image-to-image,https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9,Paper: https://arxiv.org/pdf/2108.01073.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2307.01952.pdf,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 273,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 91225,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 13
starcoder,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Please read the BigCode OpenRAIL-M license agreement before accepting it.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; ; Play with the model on the StarCoder Playground.",natural-language-processing,text-generation,https://huggingface.co/bigcode/starcoder,Paper: https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,License: bigcode-openrail-m,Datasets: bigcode/the-stack-dedup,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2.11k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 70238,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 68
m3e-base,,,,,,,,,m3e-small | m3e-base; M3E 是 Moka Massive Mixed Embedding 的缩写; 说明：; Tips:; 您需要先安装 sentence-transformers,,,https://huggingface.co/moka-ai/m3e-base,,,,Chinese; English,PyTorch; Safetensors; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 350,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 85440,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 819.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
Luna-AI-Llama2-Uncensored-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Tap-M's Luna AI Llama2 Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,https://huggingface.co/TheBloke/Luna-AI-Llama2-Uncensored-GGML,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 46,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
llama2_7b_chat_uncensored,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Fine-tuned Llama-2 7B with an uncensored/unfiltered Wizard-Vicuna conversation dataset ehartford/wizard_vicuna_70k_unfiltered.
Used QLoRA for fine-tuning. Trained for one epoch on a 24GB GPU (NVIDIA A10G) instance, took ~19 hours to train.; The version here is the fp16 HuggingFace model.; Thanks to TheBloke, he has created the GGML and GPTQ versions:; The model was trained with the following prompt style:; Code used to train the model is available here.",natural-language-processing,text-generation,https://huggingface.co/georgesung/llama2_7b_chat_uncensored,,License: other,Datasets: ehartford/wizard_vicuna_70k_unfiltered,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 45,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 61,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
Chinese-Llama-2-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"全部开源，完全可商用的中文版 Llama2 模型及中英文 SFT 数据集，输入格式严格遵循 llama-2-chat 格式，兼容适配所有针对原版 llama-2-chat 模型的优化。; ; ; Talk is cheap, Show you the Demo.; 模型下载：Chinese Llama2 Chat Model",natural-language-processing,text-generation,https://huggingface.co/LinkSoul/Chinese-Llama-2-7b,,License: openrail,Datasets: LinkSoul/instruction_merge_set,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 43,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 147,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
whisper-large-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al. from OpenAI. The original code repository can be found here.; Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization 
for improved performance.; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. ",audio,automatic-speech-recognition,https://huggingface.co/openai/whisper-large-v2,Paper: https://arxiv.org/pdf/2212.04356.pdf,License: apache-2.0,,99 languages,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 762,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 199338,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 19.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 122
Redmond-Puffin-13B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; The first commercially available language model released by Nous Research!; Redmond-Puffin-13B is one of the worlds first llama-2 based, fine-tuned language models, leveraging a hand curated set of 3K high quality examples, many of which take full advantage of the 4096 context length of Llama 2. This model was fine-tuned by Nous Research, with LDJ leading the training and dataset curation, along with significant dataset formation contributions by J-Supha.; Special thank you to Redmond AI for sponsoring the compute.; Special thank you to Emozilla for assisting with training experimentations and many issues encountered during training.",natural-language-processing,text-generation,https://huggingface.co/NousResearch/Redmond-Puffin-13B,,License: mit,,eng,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 41,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 308,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
llama2-7b-chat-hf-codeCherryPop-qLoRA-merged,,,,,,,,,"  description:;   additional_info:;   next_steps: ""I've a few things in mind and after that this will be more valuable."";   tasks:;   commercial_use: |
    So far I think this can be used commercially but this is a adapter on Meta's llama2 with
    some gating issues so that is there.
  contact_info: ""If you find any issues or want to just holler at me, you can reach out to me - https://twitter.com/4evaBehindSOTA""",,,https://huggingface.co/TokenBender/llama2-7b-chat-hf-codeCherryPop-qLoRA-merged,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 37,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 40,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-7b-instruct,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ? To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF!; ? This is an instruct model, which may not be ideal for further finetuning. If you are interested in building your own instruct/chat model, we recommend starting from Falcon-7B. ; ? Looking for an even more powerful model? Falcon-40B-Instruct is Falcon-7B-Instruct's big brother!",natural-language-processing,text-generation,https://huggingface.co/tiiuae/falcon-7b-instruct,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2306.01116.pdf,License: apache-2.0,Datasets: tiiuae/falcon-refinedweb,English,PyTorch; Core ML; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 516,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 526338,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 77
Nous-Hermes-Llama2-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Nous Research's Nous Hermes Llama 2 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,https://huggingface.co/TheBloke/Nous-Hermes-Llama2-GGML,,License: other,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 33,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 118.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bloom,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"BigScience Large Open-science Open-access Multilingual Language ModelVersion 1.3 / 6 July 2022; Current Checkpoint: Training Iteration  95000; Link to paper: here; Total seen tokens: 366B; BLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks.",natural-language-processing,text-generation,https://huggingface.co/bigscience/bloom,Paper: https://arxiv.org/pdf/2211.05100.pdf; https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2110.02861.pdf; https://arxiv.org/pdf/2108.12409.pdf,License: bigscience-bloom-rail-1.0,,46 languages,PyTorch; TensorBoard; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3.85k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 15365,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 224.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 322
falcon-40b-instruct,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ? To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF!; ? This is an instruct model, which may not be ideal for further finetuning. If you are interested in building your own instruct/chat model, we recommend starting from Falcon-40B. ; ? Looking for a smaller, less expensive model? Falcon-7B-Instruct is Falcon-40B-Instruct's little brother!",natural-language-processing,text-generation,https://huggingface.co/tiiuae/falcon-40b-instruct,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2306.01116.pdf,License: apache-2.0,Datasets: tiiuae/falcon-refinedweb,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.01k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 327284,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 84.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 28
OpenOrca-Preview1-13B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; ; ; We have used our own OpenOrca dataset to fine-tune LLaMA-13B.
This dataset is our attempt to reproduce the dataset generated for Microsoft Research's Orca Paper.; We have trained on less than 6% of our data, just to give a preview of what is possible while we further refine our dataset!
We trained a refined selection of 200k GPT-4 entries from OpenOrca.
We have filtered our GPT-4 augmentations to remove statements like, ""As an AI language model..."" and other responses which have been shown to harm model reasoning capabilities. Further details on our dataset curation practices will be forthcoming with our full model releases.",natural-language-processing,text-generation,https://huggingface.co/Open-Orca/OpenOrca-Preview1-13B,Paper: https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2301.13688.pdf,License: mit,Datasets: Open-Orca/OpenOrca,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 128,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1659,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
Llama-2-7b-Chat-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 7b Chat.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; To continue a conversation:",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ,,License: other,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 32,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3136,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Luna-AI-Llama2-Uncensored-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Tap-M's Luna AI Llama2 Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Luna-AI-Llama2-Uncensored-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 32,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 631,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Redmond-Puffin-13B-Preview,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; The first commercially available language model released by Nous Research!; Redmond-Puffin-13B is one of the worlds first llama-2 based, fine-tuned language models, leveraging a hand curated set of 3K high quality examples, many of which take full advantage of the 4096 context length of Llama 2. This model was fine-tuned by Nous Research, with LDJ leading the training and dataset curation, along with significant dataset formation contributions by J-Supha.; Special thank you to Redmond AI for sponsoring the compute.; Special thank you to Emozilla for assisting with training experimentations and many issues encountered during training.",natural-language-processing,text-generation,https://huggingface.co/NousResearch/Redmond-Puffin-13B-Preview,,License: mit,,eng,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 31,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 26,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
vicuna-33b-v1.3,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning.
The training data is around 140K conversations collected from ShareGPT.com.
See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.; Vicuna is evaluated with standard benchmarks, human preference, and LLM-as-a-judge. See more details in this paper and leaderboard.; See vicuna_weights_version.md",natural-language-processing,text-generation,https://huggingface.co/lmsys/vicuna-33b-v1.3,Paper: https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 175,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 20883,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
Llama-2-13B-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are fp16 format model files for Meta's Llama 2 13B.; They were produced by downloading the PTH files from Meta, and then converting to HF format using the latest Transformers 4.32.0.dev0, from Git, with the Llama 2 PR included: https://github.com/huggingface/transformers/pull/24891.; Command to convert was:",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-13B-fp16,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9906,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-70B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 70B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware for these quantisations!",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-70B-GPTQ,,License: other,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1822,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 35.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
FreeWilly2-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GPTQ model files for Stability AI's FreeWilly 2.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; None",natural-language-processing,text-generation,https://huggingface.co/TheBloke/FreeWilly2-GPTQ,Paper: https://arxiv.org/pdf/2307.09288.pdf; https://arxiv.org/pdf/2306.02707.pdf,License: other,Datasets: conceptofmind/cot_submix_original; conceptofmind/flan2021_submix_original; conceptofmind/t0_submix_original; conceptofmind/niv2_submix_original,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 176,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 35.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
all-MiniLM-L6-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2,Paper: https://arxiv.org/pdf/1904.06472.pdf; https://arxiv.org/pdf/2102.07033.pdf; https://arxiv.org/pdf/2104.08727.pdf; https://arxiv.org/pdf/1704.05179.pdf; https://arxiv.org/pdf/1810.09305.pdf,License: apache-2.0,Datasets: s2orc; flax-sentence-embeddings/stackexchange_xml; ms_marco; gooaq; yahoo_answers_topics; code_search_net; search_qa; eli5; snli; multi_nli; wikihow; natural_questions; trivia_qa; embedding-data/sentence-compression; embedding-data/flickr30k-captions; embedding-data/altlex; embedding-data/simple-wiki; embedding-data/QQP; embedding-data/SPECTER; embedding-data/PAQ_pairs; embedding-data/WikiAnswers,English,PyTorch; TensorFlow; Rust; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 733,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1932147,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 274.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 135
ioc-controlnet,,,,,,,,," Download our ControlNet Models for AUTOMATIC1111 Stable Diffusion Web UI!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/ioclab/ioc-controlnet,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 126,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.31KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
FreeWilly1-Delta-SafeTensor,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"FreeWilly is a Llama65B model fine-tuned on an Orca style Dataset; FreeWilly1 cannot be used from the stabilityai/FreeWilly1-Delta-SafeTensor weights alone. To obtain the correct model, one must add back the difference between LLaMA 65B and stabilityai/FreeWilly1-Delta-SafeTensor weights. We provide the apply_delta.py script to automate the conversion, which you can run as:; Start chatting with FreeWilly using the following code snippet:; FreeWilly should be used with prompts formatted similarly to Alpaca as below:; FreeWilly is trained on our internal Orca-style dataset",natural-language-processing,text-generation,https://huggingface.co/stabilityai/FreeWilly1-Delta-SafeTensor,Paper: https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.02707.pdf,License: cc-by-nc-4.0,Datasets: conceptofmind/cot_submix_original; conceptofmind/flan2021_submix_original; conceptofmind/t0_submix_original; conceptofmind/niv2_submix_original,English,Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 29,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 131.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
chatglm-6b,,,,,,,,,"
   ? Blog ? ? Github Repo ? ? Twitter ? ? [GLM@ACL 22] [GitHub] ? ? [GLM-130B@ICLR 23] [GitHub] 
; 
    ? Join our Slack and WeChat
; 我们发布了 ChatGLM2-6B，ChatGLM-6B 的升级版本，在保留了了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，引入了更强大的性能、更长的上下文、更高效的推理等升级。; ChatGLM-6B 是一个开源的、支持中英双语问答的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 ChatGLM 相同的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。 ChatGLM-6B 权重对学术研究完全开放，在填写问卷进行登记后亦允许免费商业使用。; ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM-6B uses technology similar to ChatGPT, optimized for Chinese QA and dialogue. The model is trained for about 1T tokens of Chinese and English corpus, supplemented by supervised fine-tuning, feedback bootstrap, and reinforcement learning with human feedback. With only about 6.2 billion parameters, the model is able to generate answers that are in line with human preference. ChatGLM-6B weights are completely open for academic research, and free commercial use is also allowed after completing the questionnaire.",,,https://huggingface.co/THUDM/chatglm-6b,Paper: https://arxiv.org/pdf/2103.10360.pdf; https://arxiv.org/pdf/2210.02414.pdf,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2.53k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 586003,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 200
dolphin-llama-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Dolphin ?
https://erichartford.com/dolphin; This model is based on llama1, so it is for non-commercial use only.  Future versions will be trained on llama2 and other open models that are suitable for commercial use.; This model is uncensored.  I have filtered the dataset to remove alignment and bias.  This makes the model compliant to any requests.  You are advised to implement your own alignment layer before exposing the model as a service.  It will be highly compliant to any requests, even unethical ones.  Please read my blog post about uncensored models.  https://erichartford.com/uncensored-models
You are responsible for any content you create using this model.  Enjoy responsibly.; This dataset is an open source implementation of Microsoft's Orca; After uncensoring, deduping, and cleaning, our dataset consists of:",natural-language-processing,text-generation,https://huggingface.co/ehartford/dolphin-llama-13b,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 33,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Nous-Hermes-Llama2-13b-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Compute provided by our project sponsor Redmond AI, thank you! Follow RedmondAI on Twitter @RedmondAI.; Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Emozilla leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors.; This Hermes model uses the exact same dataset as Hermes on Llama-1. This is to ensure consistency between the old Hermes and new, for anyone who wanted to keep Hermes as similar to the old one, just more capable.; This model stands out for its long responses, lower hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 4096 sequence length on an 8x a100 80GB DGX machine.; 


",natural-language-processing,text-generation,https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b-GPTQ,,License: llama2,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 15.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
llama2_7b_chat_uncensored-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for George Sung's Llama2 7B Chat Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,https://huggingface.co/TheBloke/llama2_7b_chat_uncensored-GGML,,License: other,Datasets: ehartford/wizard_vicuna_70k_unfiltered,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 32,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
stable-diffusion-v1-4,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
For more information about how Stable Diffusion functions, please have a look at ?'s Stable Diffusion with ?Diffusers blog.; The Stable-Diffusion-v1-4 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 
checkpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on ""laion-aesthetics v2 5+"" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.; This weights here are intended to be used with the ? Diffusers library. If you are looking for the weights to be loaded into the CompVis Stable Diffusion codebase, come here; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model",multimodel,text-to-image,https://huggingface.co/CompVis/stable-diffusion-v1-4,Paper: https://arxiv.org/pdf/2207.12598.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5.79k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 585404,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 90.77KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 862
falcon-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ? To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF!; ?? This is a raw, pretrained model, which should be further finetuned for most usecases. If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at Falcon-7B-Instruct. ; ? Looking for an even more powerful model? Falcon-40B is Falcon-7B's big brother!",natural-language-processing,text-generation,https://huggingface.co/tiiuae/falcon-7b,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2306.01116.pdf,License: apache-2.0,Datasets: tiiuae/falcon-refinedweb,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 691,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1107310,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 27
control_v1p_sd15_brightness,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.; huggingface.co/spaces/ioclab/brightness-controlnet; [More Information Needed]; [More Information Needed]; [More Information Needed]",computer-vision,image-to-image,https://huggingface.co/ioclab/control_v1p_sd15_brightness,,License: creativeml-openrail-m,Datasets: ioclab/grayscale_image_aesthetic_3M,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 85,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 33556,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
detailed_eye-10,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/casque/detailed_eye-10,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 26,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 19.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardCoder-15B-V1.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is the Full-Weight of WizardCoder.; Repository: https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder; Twitter: https://twitter.com/WizardLM_AI/status/1669109414559911937; Paper: WizardCoder: Empowering Code Large Language Models with Evol-Instruct; To develop our WizardCoder model, we begin by adapting the Evol-Instruct method specifically for coding tasks. This involves tailoring the prompt to the domain of code-related instructions. Subsequently, we fine-tune the Code LLM, StarCoder, utilizing the newly created instruction-following training set.",natural-language-processing,text-generation,https://huggingface.co/WizardLM/WizardCoder-15B-V1.0,Paper: https://arxiv.org/pdf/2306.08568.pdf,License: cc-by-nc-4.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 404,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 13626,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 31.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 32
zeroscope_v2_576w,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c634e3959a06df701b41_Text-to-Video.svg,,"; A watermark-free Modelscope-based video model optimized for producing high-quality 16:9 compositions and a smooth video output. This model was trained from the original weights using 9,923 clips and 29,769 tagged frames at 24 frames, 576x320 resolution.
zeroscope_v2_567w is specifically designed for upscaling with zeroscope_v2_XL using vid2vid in the 1111 text2video extension by kabachuha. Leveraging this model as a preliminary step allows for superior overall compositions at higher resolutions in zeroscope_v2_XL, permitting faster exploration in 576x320 before transitioning to a high-resolution render. See some example outputs that have been upscaled to 1024x576 using zeroscope_v2_XL. (courtesy of dotsimulate); zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320; For upscaling, it's recommended to use zeroscope_v2_XL via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. ; Let's first install the libraries required:",multimodel,text-to-video,https://huggingface.co/cerspense/zeroscope_v2_576w,,License: cc-by-nc-4.0,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 220,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 21059,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.13KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 33
llama-2-13B-Guanaco-QLoRA-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Mikael110's Llama2 13B Guanaco QLoRA.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",natural-language-processing,text-classification,https://huggingface.co/TheBloke/llama-2-13B-Guanaco-QLoRA-GGML,,License: other,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 87,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
OrangeMixs,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"""OrangeMixs"" shares various Merge models that can be used with StableDiffusionWebui:Automatic1111 and others.
?
; Maintain a repository for the following purposes.; 
Hero image prompts(AOM3B2):https://majinai.art/ja/i/jhw20Z_; We support a Gradio Web UI to run OrangeMixs:
; +/hdg/ Stable Diffusion Models Cookbook - https://rentry.org/hdgrecipes#g-anons-unnamed-mix-e93c3bf7
Model names are named after Cookbook precedents?",multimodel,text-to-image,https://huggingface.co/WarriorMama777/OrangeMixs,,License: creativeml-openrail-m,Datasets: Nerfgun3/bad_prompt,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3.24k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11766,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 73.35KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 138
llama-7b-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"LLaMA-7B converted to work with Transformers/HuggingFace. This is under a special license, please see the LICENSE file for details.; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model.",natural-language-processing,text-generation,https://huggingface.co/decapoda-research/llama-7b-hf,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.15k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 245723,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 138
text2vec-large-chinese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"Based on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replace MacBERT with LERT, and keep other training conditions unchanged。; Talk to me: https://twitter.com/GanymedeNil",natural-language-processing,sentence-similarity,https://huggingface.co/GanymedeNil/text2vec-large-chinese,,License: apache-2.0,,Chinese,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 489,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 288225,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 56
Llama-2-7B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 7B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-7B-GPTQ,,License: other,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 23,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1626,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
dreamlike-photoreal-2.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Warning: This model is horny! Add ""nude, naked"" to the negative prompt if want to avoid NSFW.  ; You can add photo to your prompt to make your gens look more photorealistic.Non-square aspect ratios work better for some prompts. If you want a portrait photo, try using a vertical aspect ratio. If you want a landscape photo, try using a horizontal aspect ratio.This model was trained on 768x768px images, so use 768x768px, 640x896px, 896x640px, etc. It also works pretty good with higher resolutions such as 768x1024px or 1024x768px.  ; You can use this model for free on dreamlike.art!; Download dreamlike-photoreal-2.0.ckpt (2.13GB); Download dreamlike-photoreal-2.0.safetensors (2.13GB)",multimodel,text-to-image,https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0,,License: other,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.45k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 116056,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 129
Baichuan-7B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Baichuan-7B是由百川智能开发的一个开源的大规模预训练模型。基于Transformer结构，在大约1.2万亿tokens上训练的70亿参数模型，支持中英双语，上下文窗口长度为4096。在标准的中文和英文权威benchmark（C-EVAL/MMLU）上均取得同尺寸最好的效果。; 如果希望使用Baichuan-7B（如进行推理、Finetune等），我们推荐使用配套代码库Baichuan-7B。; Baichuan-7B is an open-source large-scale pre-trained model developed by Baichuan Intelligent Technology. Based on the Transformer architecture, it is a model with 7 billion parameters trained on approximately 1.2 trillion tokens. It supports both Chinese and English, with a context window length of 4096. It achieves the best performance of its size on standard Chinese and English authoritative benchmarks (C-EVAL/MMLU).; If you wish to use Baichuan-7B (for inference, finetuning, etc.), we recommend using the accompanying code library Baichuan-7B.; 在同尺寸模型中Baichuan-7B达到了目前SOTA的水平，参考下面MMLU指标",natural-language-processing,text-generation,https://huggingface.co/baichuan-inc/Baichuan-7B,Paper: https://arxiv.org/pdf/1910.07467.pdf; https://arxiv.org/pdf/2009.03300.pdf,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 704,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 44902,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
openchat_v2_openorca_preview,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This is a preview version of OpenChat V2 trained for 2 epochs (total 5 epochs) on full (4.5M) OpenOrca dataset.; Important Notice: Beta Release for Limited Testing Purposes Only; This release is intended solely for a small group of beta testers and is not an official release or preview. We caution against publicizing or sharing this version as it may contain bugs, errors, or incomplete features that could negatively impact performance. We are actively working on improving the model and preparing it for an official release.",natural-language-processing,text-generation,https://huggingface.co/openchat/openchat_v2_openorca_preview,,License: other,Datasets: Open-Orca/OpenOrca,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 35,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 79,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Redmond-Puffin-13B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for NousResearch's Redmond Puffin 13B V1.3.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Redmond-Puffin-13B-GPTQ,,License: other,,eng,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 268,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Nous-Hermes-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Nous-Hermes-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Karan4D leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors. The result is an enhanced Llama 13b model that rivals GPT-3.5-turbo in performance across a variety of tasks.; This model stands out for its long responses, low hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 2000 sequence length on an 8x a100 80GB DGX machine for over 50 hours. ; The model was trained almost entirely on synthetic GPT-4 outputs. This includes data from diverse sources such as GPTeacher, the general, roleplay v1&2, code instruct datasets, Nous Instruct & PDACTL (unpublished), CodeAlpaca, Evol_Instruct Uncensored, GPT4-LLM, and Unnatural Instructions. ; Additional data inputs came from Camel-AI's Biology/Physics/Chemistry and Math Datasets, Airoboros' GPT-4 Dataset, and more from CodeAlpaca. The total volume of data encompassed over 300,000 instructions.; The model fine-tuning and the datasets were a collaboration of efforts and resources between Teknium, Karan4D, Nous Research, Huemin Art, and Redmond AI. ",natural-language-processing,text-generation,https://huggingface.co/NousResearch/Nous-Hermes-13b,,License: gpl,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 350,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6780,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 20
vicuna-7b-v1.3,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning.
The training data is around 140K conversations collected from ShareGPT.com.
See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.; Vicuna is evaluated with standard benchmarks, human preference, and LLM-as-a-judge. See more details in this paper and leaderboard.; See vicuna_weights_version.md",natural-language-processing,text-generation,https://huggingface.co/lmsys/vicuna-7b-v1.3,Paper: https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 86,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 93274,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
zeroscope_v2_XL,,,,,,,,,"
example outputs (courtesy of dotsimulate); A watermark-free Modelscope-based video model capable of generating high quality video at 1024 x 576. This model was trained from the original weights with offset noise using 9,923 clips and 29,769 tagged frames at 24 frames, 1024x576 resolution.
zeroscope_v2_XL is specifically designed for upscaling content made with zeroscope_v2_576w using vid2vid in the 1111 text2video extension by kabachuha. Leveraging this model as an upscaler allows for superior overall compositions at higher resolutions, permitting faster exploration in 576x320 (or 448x256) before transitioning to a high-resolution render.; zeroscope_v2_XL uses 15.3gb of vram when rendering 30 frames at 1024x576; For upscaling, it's recommended to use the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip.; Let's first install the libraries required:",,,https://huggingface.co/cerspense/zeroscope_v2_XL,,License: cc-by-nc-4.0,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 395,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 12029,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.91KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 12
llama-2-7b-chat-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"These are the converted model weights for Llama-2-7B-chat in Huggingface format.; Courtesy of Mirage-Studio.io, home of MirageGPT: the private ChatGPT alternative.; license: other
LLAMA 2 COMMUNITY LICENSE AGREEMENT	
Llama 2 Version Release Date: July 18, 2023; ""Agreement"" means the terms and conditions for use, reproduction, distribution and 
modification of the Llama Materials set forth herein.; ""Documentation"" means the specifications, manuals and documentation 
accompanying Llama 2 distributed by Meta at ai.meta.com/resources/models-and-
libraries/llama-downloads/.",natural-language-processing,text-generation,https://huggingface.co/daryl149/llama-2-7b-chat-hf,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 91504,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
Llama-2-70B-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are fp16 format model files for Meta's Llama 2 70B.; They were produced by downloading the PTH files from Meta, and then converting to HF format using the latest Transformers 4.32.0.dev0, from Git, with the Llama 2 PR included: https://github.com/huggingface/transformers/pull/24891.; Command to convert was:",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-70B-fp16,,License: other,,English,Safetensors; Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1161,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 138.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Nous-Hermes-Llama2-13b-GGML,,,,,,,,,"Compute provided by our project sponsor Redmond AI, thank you! Follow RedmondAI on Twitter @RedmondAI.; Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Emozilla leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors.; This Hermes model uses the exact same dataset as Hermes on Llama-1. This is to ensure consistency between the old Hermes and new, for anyone who wanted to keep Hermes as similar to the old one, just more capable.; This model stands out for its long responses, lower hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 4096 sequence length on an 8x a100 80GB DGX machine.; 


",,,https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b-GGML,,License: llama2,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 34.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
waifu-diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.; ; masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck; Original Weights; We also support a Gradio Web UI and Colab with Diffusers to run Waifu Diffusion:

",multimodel,text-to-image,https://huggingface.co/hakurei/waifu-diffusion,,License: creativeml-openrail-m,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2.21k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 267780,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.17KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 227
VoiceConversionWebUI,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/lj1995/VoiceConversionWebUI,,License: mit,,,ONNX,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 411,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 45.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 12
rwkv-4-world,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"RWKV-4 trained on 100+ world languages (70% English, 15% multilang, 15% code).; World = Some_Pile + Some_RedPajama + Some_OSCAR + All_Wikipedia + All_ChatGPT_Data_I_can_find; XXXtuned = finetune of World on MC4, OSCAR, wiki, etc.; How to use:; The differences between World & Raven:",natural-language-processing,text-generation,https://huggingface.co/BlinkDL/rwkv-4-world,,License: apache-2.0,Datasets: EleutherAI/pile; togethercomputer/RedPajama-Data-1T,12 languages,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 147,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 171.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
DreamShaper,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Read more about this model here: https://civitai.com/models/4384/dreamshaper; Also please support by giving 5 stars and a heart, which will notify new updates.; Please consider supporting me on Patreon or buy me a coffee; You can run this model on:; Inference API has been turned off for this model.",multimodel,text-to-image,https://huggingface.co/Lykon/DreamShaper,,License: other,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 678,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 71907,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 167.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 73
bark,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9511aaa882b322dc3d8_Text-to-Speech.svg,,"Bark is a transformer-based text-to-audio model created by Suno. 
Bark can generate highly realistic, multilingual speech as well as other audio - including music, 
background noise and simple sound effects. The model can also produce nonverbal 
communications like laughing, sighing and crying. To support the research community, 
we are providing access to pretrained model checkpoints ready for inference.; The original github repo and model card can be found here.; This model is meant for research purposes only. 
The model output is not censored and the authors do not endorse the opinions in the generated content. 
Use at your own risk.; Two checkpoints are released:; Try out Bark yourself!",audio,text-to-speech,https://huggingface.co/suno/bark,,License: cc-by-nc-4.0,,13 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 216,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 736,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 22.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 61
m3e-large,,,,,,,,,m3e-small | m3e-base | m3e-large; M3E 是 Moka Massive Mixed Embedding 的缩写; 说明：; Tips:; 您需要先安装 sentence-transformers,,,https://huggingface.co/moka-ai/m3e-large,,,,Chinese; English,PyTorch; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 47,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1874,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-70B-Chat-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are fp16 pytorch model files for Meta's Llama 2 70B Chat.; They were produced by downloading the PTH files from Meta, and then converting to HF format using the latest Transformers 4.32.0.dev0, from Git, with the Llama 2 PR included: https://github.com/huggingface/transformers/pull/24891.; Command to convert was:",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-70B-Chat-fp16,,License: other,,English,Safetensors; Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1080,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 138.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
LLongMA-2-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; No model card; New: Create and edit this model card directly on the website!",natural-language-processing,text-generation,https://huggingface.co/conceptofmind/LLongMA-2-13b,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
gpt2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Test the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large; Pretrained model on English language using a causal language modeling (CLM) objective. It was introduced in
this paper
and first released at this page.; Disclaimer: The team releasing GPT-2 also wrote a
model card for their model. Content from this model card
has been written by the Hugging Face team to complete the information they provided and give specific examples of bias.; GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This
means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots
of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,
it was trained to guess the next word in sentences.; More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,
shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the
predictions for the token i only uses the inputs from 1 to i but not the future tokens.",natural-language-processing,text-generation,https://huggingface.co/gpt2,,License: mit,,English,PyTorch; TensorFlow; JAX; TF Lite; Rust; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.27k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 14736272,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 850
gpt-j-6b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"GPT-J 6B is a transformer model trained using Ben Wang's Mesh Transformer JAX. ""GPT-J"" refers to the class of model, while ""6B"" represents the number of trainable parameters.; * Each layer consists of one feedforward block and one self attention block.; ? Although the embedding matrix has a size of 50400, only 50257 entries are used by the GPT-2 tokenizer.; The model consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model
dimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64
dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as
GPT-2/GPT-3.; GPT-J learns an inner representation of the English language that can be used to 
extract features useful for downstream tasks. The model is best at what it was 
pretrained for however, which is generating text from a prompt.",natural-language-processing,text-generation,https://huggingface.co/EleutherAI/gpt-j-6b,Paper: https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2101.00027.pdf,License: apache-2.0,Datasets: EleutherAI/pile,English,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.22k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 384727,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 73.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 24
long_llama_3b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; TLDR | Overview | Usage | LongLLaMA performance | Authors | Citation | License | Acknowledgments; This repository contains the research preview of LongLLaMA, a large language model capable of handling long contexts of 256k tokens or even more. ; LongLLaMA is built upon the foundation of OpenLLaMA and fine-tuned using the Focused Transformer (FoT) method.  We release a smaller 3B variant of the LongLLaMA model on a permissive license (Apache 2.0) and inference code supporting longer contexts on Hugging Face. Our model weights can serve as the drop-in replacement of LLaMA in existing implementations (for short context up to 2048 tokens). Additionally, we provide evaluation results and comparisons against the original OpenLLaMA models. Stay tuned for further updates.; Focused Transformer: Contrastive Training for Context Scaling (FoT) presents a simple method for endowing language models with the ability to handle context consisting possibly of millions of tokens while training on significantly shorter input. FoT permits a subset of attention layers to access a memory cache of (key, value) pairs to extend the context length. The distinctive aspect of FoT is its training procedure, drawing from contrastive learning. Specifically, we deliberately expose the memory attention layers to both relevant and irrelevant keys (like negative samples from unrelated documents). This strategy incentivizes the model to differentiate keys connected with semantically diverse values, thereby enhancing their structure. This, in turn, makes it possible to extrapolate the effective context length much beyond what is seen in training. ",natural-language-processing,text-generation,https://huggingface.co/syzymon/long_llama_3b,Paper: https://arxiv.org/pdf/2307.03170.pdf; https://arxiv.org/pdf/2305.16300.pdf,License: apache-2.0,Datasets: togethercomputer/RedPajama-Data-1T,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 74,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8557,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
Redmond-Puffin-13B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for NousResearch's Redmond Puffin 13B V1.3.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,https://huggingface.co/TheBloke/Redmond-Puffin-13B-GGML,,License: other,,eng,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 26,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 118.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
upstage-llama-30b-instruct-2048-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Upstage's Llama 30B Instruct 2048.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",natural-language-processing,text-generation,https://huggingface.co/TheBloke/upstage-llama-30b-instruct-2048-GGML,,License: other,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 291.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
stable-diffusion-v-1-4-original,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.; The Stable-Diffusion-v-1-4 checkpoint was initialized with the weights of the Stable-Diffusion-v-1-2 
checkpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on ""laion-aesthetics v2 5+"" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.; These weights are intended to be used with the original CompVis Stable Diffusion codebase. If you are looking for the model to use with the D?iffusers library, come here.; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model",multimodel,text-to-image,https://huggingface.co/CompVis/stable-diffusion-v-1-4-original,Paper: https://arxiv.org/pdf/2207.12598.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2.46k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 12.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 215
flan-t5-xxl,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:",natural-language-processing,text2text-generation,https://huggingface.co/google/flan-t5-xxl,Paper: https://arxiv.org/pdf/2210.11416.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,Datasets: svakulenk0/qrecc; taskmaster2; djaym7/wiki_dialog; deepmind/code_contests; lambada; gsm8k; aqua_rat; esnli; quasc; qed,5 languages,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 818,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 723038,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 179.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 232
instructor-xl,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"We introduce Instructor???, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor?? achieves sota on 70 diverse embedding tasks!
The model is easy to use with our customized sentence-transformer library. For more details, check out our paper and project page! ; **************************** Updates ****************************; Then you can use the model like this to calculate domain-specific and task-aware embeddings:; If you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions: ; ??????????????????????????Represent the domain text_type for task_objective:",natural-language-processing,sentence-similarity,https://huggingface.co/hkunlp/instructor-xl,Paper: https://arxiv.org/pdf/2212.09741.pdf,License: apache-2.0,,English,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 302,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 22563,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 48
instruct-pix2pix,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"GitHub: https://github.com/timothybrooks/instruct-pix2pix
; To use InstructPix2Pix, install diffusers using main for now. The pipeline will be available in the next release",computer-vision,image-to-image,https://huggingface.co/timbrooks/instruct-pix2pix,,License: mit,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 597,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 100343,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 15.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 172
whisper.cpp,,,,,,,,,"Available models; For more information, visit:; https://github.com/ggerganov/whisper.cpp/tree/master/models; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/ggerganov/whisper.cpp,,License: mit,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 189,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 15.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
vicuna-13b-v1.3,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning.
The training data is around 140K conversations collected from ShareGPT.com.
See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.; Vicuna is evaluated with standard benchmarks, human preference, and LLM-as-a-judge. See more details in this paper and leaderboard.; See vicuna_weights_version.md",natural-language-processing,text-generation,https://huggingface.co/lmsys/vicuna-13b-v1.3,Paper: https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 127,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 45658,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
Baichuan-13B-Base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Baichuan-13B-Base为Baichuan-13B系列模型中的预训练版本，经过对齐后的模型可见Baichuan-13B-Chat。; Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。本次发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。Baichuan-13B 有如下几个特点：; Baichuan-13B is an open-source, commercially usable large-scale language model developed by Baichuan Intelligence, following Baichuan-7B. With 13 billion parameters, it achieves the best performance in standard Chinese and English benchmarks among models of its size. This release includes two versions: pre-training (Baichuan-13B-Base) and alignment (Baichuan-13B-Chat). Baichuan-13B has the following features:; Developed by: 百川智能(Baichuan Intelligent Technology); Email: opensource@baichuan-inc.com",natural-language-processing,text-generation,https://huggingface.co/baichuan-inc/Baichuan-13B-Base,Paper: https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2009.03300.pdf,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 119,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11182,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-13B-Chat-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-13B-Chat-fp16,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 966,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Nous-Hermes-Llama2-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Nous Research's Nous Hermes Llama 2 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Nous-Hermes-Llama2-GPTQ,,License: other,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 107,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama2-7b-chat-codeCherryPop-qLoRA-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for TokenBender's Llama-2-7B-Chat Code Cherry Pop.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,https://huggingface.co/TheBloke/llama2-7b-chat-codeCherryPop-qLoRA-GGML,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
pygmalion-6b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"Pymalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B.; Warning: This model is NOT suitable for use by minors. It will output X-rated content under certain circumstances.; The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations.; Model weights were initialized from the uft-6b ConvoGPT model made available in this commit.; The model was then further fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.",natural-language-processing,conversational,https://huggingface.co/PygmalionAI/pygmalion-6b,,License: creativeml-openrail-m,,English,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 592,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 191985,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 16.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 24
mpt-7b-8k-chat,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MPT-7B-Chat-8k is a chatbot-like model for dialogue generation.
It was built by finetuning MPT-7B-8k on the ShareGPT-Vicuna, Camel-AI,
 GPTeacher, Guanaco, Baize and some generated datasets.
 This is the same dataset that MPT-30B-Chat was trained on.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; July 18, 2023; CC-By-NC-SA-4.0 (non-commercial use only); This model is best used with the MosaicML llm-foundry repository for training and finetuning.",natural-language-processing,text-generation,https://huggingface.co/mosaicml/mpt-7b-8k-chat,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2010.04245.pdf,License: cc-by-nc-sa-4.0,Datasets: camel-ai/code; ehartford/wizard_vicuna_70k_unfiltered; anon8231489123/ShareGPT_Vicuna_unfiltered; teknium1/GPTeacher/roleplay-instruct-v2-final; teknium1/GPTeacher/codegen-isntruct; timdettmers/openassistant-guanaco; camel-ai/math; project-baize/baize-chatbot/medical_chat_data; project-baize/baize-chatbot/quora_chat_data; project-baize/baize-chatbot/stackoverflow_chat_data; camel-ai/biology; camel-ai/chemistry; camel-ai/ai_society; jondurbin/airoboros-gpt4-1.2; LongConversations; camel-ai/physics,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 684,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama2-22b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is Llama 2 13b with some additional attention heads from original-flavor Llama 33b frankensteined on.; Fine-tuned on ~10M tokens from RedPajama to settle in the transplants a little.; Not intended for use as-is - this model is meant to serve as a base for further tuning, hopefully with a greater capacity for learning than 13b.",natural-language-processing,text-generation,https://huggingface.co/chargoddard/llama2-22b,,,Datasets: togethercomputer/RedPajama-Data-1T-Sample,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 44.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
speaker-diarization,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; The collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Relies on pyannote.audio 2.1.1: see installation instructions.; In case the number of speakers is known in advance, one can use the num_speakers option:",audio,automatic-speech-recognition,https://huggingface.co/pyannote/speaker-diarization,Paper: https://arxiv.org/pdf/2012.01477.pdf; https://arxiv.org/pdf/2110.07058.pdf; https://arxiv.org/pdf/2005.08072.pdf,License: mit,Datasets: ami; dihard; voxconverse; aishell; repere; voxceleb,,pyannote.audio,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 367,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1145415,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 48
openjourney,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Include 'mdjrny-v4 style' in prompt. Here you'll find hundreds of Openjourney prompts; ; (Same parameters, just added ""mdjrny-v4 style"" at the beginning):



; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; You can also export the model to ONNX, MPS and/or FLAX/JAX.",multimodel,text-to-image,https://huggingface.co/prompthero/openjourney,,License: creativeml-openrail-m,,English,Diffusers; Safetensors,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 2.79k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 327457,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 296
ControlNet,,,,,,,,,This is the pretrained weights and some other detector weights of ControlNet.; See also: https://github.com/lllyasviel/ControlNet; ControlNet/models/control_sd15_canny.pth; ControlNet/models/control_sd15_depth.pth; ControlNet/models/control_sd15_hed.pth,,,https://huggingface.co/lllyasviel/ControlNet,,License: openrail,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3.06k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.14KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 244
Wizard-Vicuna-30B-Uncensored-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Eric Hartford's Wizard-Vicuna-30B-Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ,,License: other,Datasets: ehartford/wizard_vicuna_70k_unfiltered,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 244,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8220,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
mpt-7b-8k-instruct,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MPT-7B-Instruct-8k is a model for long-form instruction following, especially question-answering on and summarization of longer documents.
It is built by finetuning MPT-7B-8k on Dolly HHRLHF derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets. It is also trained on Competition Math, Duorc, CoT GSM8k, Qasper, Quality, Summ Screen FD and Spider.
This is the same dataset that MPT-30B-Instruct was trained on.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; July 18, 2023;  CC-By-SA-3.0; This model is best used with the MosaicML llm-foundry repository for training and finetuning.",natural-language-processing,text-generation,https://huggingface.co/mosaicml/mpt-7b-8k-instruct,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2010.04245.pdf,License: cc-by-sa-3.0,Datasets: competition_math; conceptofmind/cot_submix_original/cot_gsm8k; knkarthick/dialogsum; mosaicml/dolly_hhrlhf; duorc; tau/scrolls/qasper; emozilla/quality; scrolls/summ_screen_fd; spider,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1889,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chatglm2-6b-int4,,,,,,,,,"
  ? Github Repo ? ? Twitter ? ? [GLM@ACL 22] [GitHub] ? ? [GLM-130B@ICLR 23] [GitHub] 
; 
    ? Join our Slack and WeChat
; ChatGLM2-6B 是开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM2-6B 引入了如下新特性：; ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the following new features:; 可以通过如下代码调用 ChatGLM-6B 模型来生成对话：",,,https://huggingface.co/THUDM/chatglm2-6b-int4,Paper: https://arxiv.org/pdf/2103.10360.pdf; https://arxiv.org/pdf/2210.02414.pdf; https://arxiv.org/pdf/1911.02150.pdf,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 112,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 90617,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 29
notwaifu-diffusion-xl,,,,,,,,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Please for anyone, don't reupload my finetuned models to other site, like Civitai or image generating site like Seeart, Yodayo or other sites. Thank you for understanding this. ; Like my works and want to collaboration or funding my projects? contact me.; Sample

 ",,,https://huggingface.co/gmonsoon/notwaifu-diffusion-xl,,License: other,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llava-llama-2-13b-chat-lightning-preview,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA-LLaMA-2-13B-Chat-Preview was trained in July 2023.; Paper or resources for more information:
https://llava-vl.github.io/; Llama 2 is licensed under the LLAMA 2 Community License, 
Copyright (c) Meta Platforms, Inc. All Rights Reserved.; Where to send questions or comments about the model:
https://github.com/haotian-liu/LLaVA/issues",natural-language-processing,text-generation,https://huggingface.co/liuhaotian/llava-llama-2-13b-chat-lightning-preview,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 141,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
upstage-llama-30b-instruct-2048-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Upstage's Llama 30B Instruct 2048.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",natural-language-processing,text-generation,https://huggingface.co/TheBloke/upstage-llama-30b-instruct-2048-GPTQ,,License: other,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 247,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LLongMA-2-7B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for ConceptofMind's LLongMA 2 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; This is an extended context base Llama 2 model.  Please check if your GGML client supports extended context. llama.cpp and KoboldCpp do, but I have not verified the others.",,,https://huggingface.co/TheBloke/LLongMA-2-7B-GGML,Paper: https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2212.10554.pdf,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bart-large-mnli,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c42cd649af0365a01a1e_Zero-Shot%20Classification.svg,,"This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset.; Additional information about this model:; Yin et al. proposed a method for using pre-trained NLI models as a ready-made zero-shot sequence classifiers. The method works by posing the sequence to be classified as the NLI premise and to construct a hypothesis from each candidate label. For example, if we want to evaluate whether a sequence belongs to the class ""politics"", we could construct a hypothesis of This text is about politics.. The probabilities for entailment and contradiction are then converted to label probabilities.; This method is surprisingly effective in many cases, particularly when used with larger pre-trained models like BART and Roberta. See this blog post for a more expansive introduction to this and other zero shot methods, and see the code snippets below for examples of using this model for zero-shot classification both with Hugging Face's built-in pipeline and with native Transformers/PyTorch code.; The model can be loaded with the zero-shot-classification pipeline like so:",natural-language-processing,zero-shot-classification,https://huggingface.co/facebook/bart-large-mnli,Paper: https://arxiv.org/pdf/1910.13461.pdf; https://arxiv.org/pdf/1909.00161.pdf,License: mit,Datasets: multi_nli,,PyTorch; JAX; Rust; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 594,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3898557,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 124
sd-vae-ft-mse-original,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"These weights are intended to be used with the original CompVis Stable Diffusion codebase. If you are looking for the model to use with the ? diffusers library, come here.; We publish two kl-f8 autoencoder versions, finetuned from the original kl-f8 autoencoder on a 1:1 ratio of LAION-Aesthetics and LAION-Humans, an unreleased subset containing only SFW images of humans. The intent was to fine-tune on the Stable Diffusion training set (the autoencoder was originally trained on OpenImages) but also enrich the dataset with images of humans to improve the reconstruction of faces.
The first, ft-EMA, was resumed from the original checkpoint, trained for 313198 steps and uses EMA weights. It uses the same loss configuration as the original checkpoint (L1 + LPIPS).
The second, ft-MSE, was resumed from ft-EMA and uses EMA weights and was trained for another 280k steps using a different loss, with more emphasis 
on MSE reconstruction (MSE + 0.1 * LPIPS). It produces somewhat ``smoother'' outputs. The batch size for both versions was 192 (16 A100s, batch size 12 per GPU).
To keep compatibility with existing models, only the decoder part was finetuned; the checkpoints can be used as a drop-in replacement for the existing autoencoder..; Original kl-f8 VAE vs f8-ft-EMA vs f8-ft-MSE; Visualization of reconstructions on  256x256 images from the COCO2017 validation dataset. ; 


256x256: ft-EMA (left), ft-MSE (middle), original (right)
",multimodel,text-to-image,https://huggingface.co/stabilityai/sd-vae-ft-mse-original,,License: mit,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.01k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 670.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 19
stable-diffusion-2-1-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This model card focuses on the model associated with the Stable Diffusion v2-1-base model.; This stable-diffusion-2-1-base model fine-tunes stable-diffusion-2-base (512-base-ema.ckpt) with 220k extra steps taken, with punsafe=0.98 on the same dataset. ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model; Language(s): English",multimodel,text-to-image,https://huggingface.co/stabilityai/stable-diffusion-2-1-base,Paper: https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: openrail++,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 456,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 494813,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 21.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 231
AsianModel,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/BanKaiPls/AsianModel,,License: openrail,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 148,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 18.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
starchat-beta,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"StarChat is a series of language models that are trained to act as helpful coding assistants. StarChat-β is the second model in the series, and is a fine-tuned version of StarCoderPlus that was trained on an ""uncensored"" variant of the openassistant-guanaco dataset. We found that removing the in-built alignment of the OpenAssistant dataset boosted performance on the Open LLM Leaderboard and made the model more helpful at coding tasks. However, this means that model is likely to generate problematic text when prompted to do so and should only be used for educational and research purposes.; The model was fine-tuned on a variant of the OpenAssistant/oasst1 dataset, which contains a diverse range of dialogues in over 35 languages. As a result, the model can be used for chat and you can check out our demo to test its coding capabilities. ; Here's how you can run the model using the pipeline() function from ? Transformers:; StarChat-β has not been aligned to human preferences with techniques like RLHF or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). 
Models trained primarily on code data will also have a more skewed demographic bias commensurate with the demographics of the GitHub community, for more on this see the StarCoder dataset which is derived from The Stack.; Since the base model was pretrained on a large corpus of code, it may produce code snippets that are syntactically valid but semantically incorrect. 
For example, it may produce code that does not compile or that produces incorrect results.It may also produce code that is vulnerable to security exploits.We have observed the model also has a tendency to produce false URLs which should be carefully inspected before clicking.",natural-language-processing,text-generation,https://huggingface.co/HuggingFaceH4/starchat-beta,,License: bigcode-openrail-m,,,PyTorch; TensorBoard; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 164,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2021106,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 63.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
controlnet_qrcode-control_v1p_sd15,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"; This repo holds the safetensors & diffusers versions of the QR code conditioned ControlNet for Stable Diffusion v1.5.
The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs. However, this 1.5 version model was also trained on the same dataset for those who are using the older version.; These models perform quite well in most cases, but please note that they are not 100% accurate. In some instances, the QR code shape might not come through as expected. You can increase the ControlNet weight to emphasize the QR code shape. However, be cautious as this might negatively impact the style of your output.To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).; To balance between style and shape, a gentle fine-tuning of the control weight might be required based on the individual input and the desired output, aswell as the correct prompt. Some prompts do not work until you increase the weight by a lot. The process of finding the right balance between these factors is part art and part science. For the best results, it is recommended to generate your artwork at a resolution of 768. This allows for a higher level of detail in the final product, enhancing the quality and effectiveness of the QR code-based artwork.; The simplest way to use this is to place the .safetensors model and its .yaml config file in the folder where your other controlnet models are installed, which varies per application. 
For usage in auto1111 they can be placed in the webui/models/ControlNet folder. They can be loaded using the controlnet webui extension which you can install through the extensions tab in the webui (https://github.com/Mikubill/sd-webui-controlnet). Make sure to enable your controlnet unit and set your input image as the QR code. Set the model to either the SD2.1 or 1.5 version depending on your base stable diffusion model, or it will error. No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail.
Make sure to look up additional info on how to use controlnet if you get stuck, once you have the webui up and running its really easy to install the controlnet extension aswell.",computer-vision,image-to-image,https://huggingface.co/DionTimmer/controlnet_qrcode-control_v1p_sd15,,License: openrail++,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 141,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 51831,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 53
polylm-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,," Large language models (LLMs) demonstrate remarkable ability to comprehend, reason, and generate following nature language instructions. However, the development of LLMs has been primarily focused on high-resource languages, such as English, thereby limiting their applicability and research in other languages. Consequently, we present PolyLM, a multilingual LLM trained on 640 billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its multilingual capabilities, we 1) integrate bilingual data into training data; and 2) adopt a curriculum learning strategy that increases the proportion of non-English data from 30% in the first stage to 60% in the final stage during pre-training. Further, we propose a multilingual self-instruct method which automatically generates 132.7K  diverse multilingual instructions for model fine-tuning. To assess the model's performance, we collect several existing multilingual tasks, including multilingual understanding, question answering, generation, and translation. Extensive experiments show that PolyLM surpasses other open-source models such as LLaMA and BLOOM on multilingual tasks while maintaining comparable performance in English.; Find below some example scripts on how to use the model in transformers:; The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models; See the research paper for further details.; More information needed.",natural-language-processing,text-generation,https://huggingface.co/DAMO-NLP-MT/polylm-13b,Paper: https://arxiv.org/pdf/2307.06018.pdf; https://arxiv.org/pdf/2104.09864.pdf,License: apache-2.0,,18 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1010,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 31.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
limarp-llama2,,,,,,,,,"LIMARP-Llama2 is an experimental Llama2 finetune narrowly focused on novel-style roleplay chatting.; To considerably facilitate uploading and distribution, LoRA adapters have been provided instead of the merged models. You should get the Llama2 base model first, either from Meta or from one of the reuploads on HuggingFace (for example here and here). It is also possible to apply the LoRAs on different Llama2-based models (e.g. LLongMA-2 or Nous-Hermes-Llama2), although this is largely untested and the final results may not work as intended.; This is an experimental attempt at creating an RP-oriented fine-tune using a manually-curated, high-quality dataset of human-generated conversations. The main rationale for this are the observations from Zhou et al.. The authors suggested that just 1000-2000 carefully curated training examples may yield high quality output for assistant-type chatbots. This is in contrast with the commonly employed strategy where a very large number of training examples (tens of thousands to even millions) of widely varying quality are used.; For LIMARP a similar approach was used, with the difference that the conversational data is almost entirely human-generated. Every training example is manually compiled and selected to comply with subjective quality parameters, with virtually no chance for OpenAI-style alignment responses to come up.; The model is intended to approximate the experience of 1-on-1 roleplay as observed on many Internet forums dedicated on roleplaying. It must be used with a specific format similar to that of this template:",,,https://huggingface.co/lemonilia/limarp-llama2,Paper: https://arxiv.org/pdf/2305.11206.pdf; https://arxiv.org/pdf/2305.14314.pdf,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 11.92KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-ko-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Llama-2-Ko serves as an advanced iteration of Llama 2, benefiting from an expanded vocabulary and the inclusion of a Korean corpus in its further pretraining. Just like its predecessor, Llama-2-Ko operates within the broad range of generative text models that stretch from 7 billion to 70 billion parameters. This repository focuses on the 7B pretrained version, which is tailored to fit the Hugging Face Transformers format. For access to the other models, feel free to consult the index provided below.; Model Developers Junbum Lee (Beomi); Variations Llama-2-Ko will come in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations.; Input Models input text only.; Output Models generate text only.",natural-language-processing,text-generation,https://huggingface.co/beomi/llama-2-ko-7b,Paper: https://arxiv.org/pdf/2307.09288.pdf,,,English; Korean,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 288,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-70B-Chat-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for Meta's Llama 2 70B Chat.; To use these files you need:; Example command:,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGML,Paper: https://arxiv.org/pdf/2307.09288.pdf,License: other,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 563.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
layoutlm-document-qa,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c7fbd649af0365a4961f_Document_Question_Answering_-_2.svg,,"This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents. It has been fine-tuned using both the SQuAD2.0 and DocVQA datasets.; To run these examples, you must have PIL, pytesseract, and PyTorch installed in addition to transformers.; NOTE: This model and pipeline was recently landed in transformers via PR #18407 and PR #18414, so you'll need to use a recent version of transformers, for example:; This model was created by the team at Impira.",multimodel,document-question-answering,https://huggingface.co/impira/layoutlm-document-qa,,License: mit,,English,PyTorch; TensorFlow; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 410,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 44880,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 159
roberta-base-go_emotions,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Model trained from roberta-base on the go_emotions dataset for multi-label classification.; go_emotions is based on Reddit data and has 28 labels. It is a multi-label dataset where one or multiple labels may apply for any given input text, hence this model is a multi-label classification model with 28 'probability' float outputs for any given input text. Typically a threshold of 0.5 is applied to the probabilities for the prediction for each label.; The model was trained using AutoModelForSequenceClassification.from_pretrained with problem_type=""multi_label_classification"" for 3 epochs with a learning rate of 2e-5 and weight decay of 0.01.; Evaluation (of the 28 dim output via a threshold of 0.5 to binarize each) using the dataset test split gives:; But the metrics would be more meaningful when measured per label given the multi-label nature.",natural-language-processing,text-classification,https://huggingface.co/SamLowe/roberta-base-go_emotions,,License: mit,Datasets: go_emotions,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 48,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4211611,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 502.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
blip-image-captioning-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"Model card for image captioning pretrained on COCO dataset - base architecture (with ViT large backbone).; Authors from the paper write in the abstract:; Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.; You can use this model for conditional and un-conditional image captioning",multimodel,image-to-text,https://huggingface.co/Salesforce/blip-image-captioning-large,Paper: https://arxiv.org/pdf/2201.12086.pdf,License: bsd-3-clause,,,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 205,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 429342,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 224
instructor-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"We introduce Instructor???, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor?? achieves sota on 70 diverse embedding tasks (MTEB leaderboard)!
The model is easy to use with our customized sentence-transformer library. For more details, check out our paper and project page! ; **************************** Updates ****************************; Then you can use the model like this to calculate domain-specific and task-aware embeddings:; If you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions: ; ??????????????????????????Represent the domain text_type for task_objective:",natural-language-processing,sentence-similarity,https://huggingface.co/hkunlp/instructor-large,Paper: https://arxiv.org/pdf/2212.09741.pdf,License: apache-2.0,,English,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 239,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 40558,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 31
IF-I-XL-v1.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; DeepFloyd LICENSE AGREEMENTThis License Agreement (as may be amended in accordance with this License Agreement, “License”), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (“Licensee” or “you”) and Stability AI Ltd.. (“Stability AI” or “we”) applies to your use of any computer program, algorithm, source code, object code, or software that is made available by Stability AI under this License (“Software”) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (“Documentation”).By clicking “I Accept” below or by using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model, that can generate pictures with new state-of-the-art for photorealism and language understanding. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID-30K score of 6.66 on the COCO dataset.; Inspired by Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",multimodel,text-to-image,https://huggingface.co/DeepFloyd/IF-I-XL-v1.0,Paper: https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/2110.02861.pdf,License: deepfloyd-if-license,,,PyTorch; Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 467,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 40137,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 20
Music-AI-Voices,,,,,,,,,"? Discord: https://discord.gg/aihub | Join the community, learn to make models, chat with link-minded people and lets create music ? ?; ? Discord Latino: https://discord.gg/Crfqs7uB5V | Entren a nuestra comunidad, aprendan a crear modelos AI, habla con otros sobre musica y disfruta las notas musicales ? ?; IMPORTANT!!!!!!!!!: VOICES CANNOT BE COPYRIGHTED. We do not promote piracy so please do not come in with that. We do promote legal-length sample clips of vocals. We promote music & AI produced music covers (impressions). We promote machine learning & Voice AI Models. Note: This repository does NOT include ANY DATASETS. Only models are included.; If you want your credits/name removed, please message me on discord and I will remove it diligently.; Tools: https://vocalremover.org/ https://x-minus.pro/ai https://create.musicfy.lol/",,,https://huggingface.co/QuickWick/Music-AI-Voices,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 242,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
Counterfeit-V3.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"?I have utilized BLIP-2 as a part of the training process. Natural language prompts might be more effective.?I prioritize the freedom of composition, which may result in a higher possibility of anatomical errors.?The expressiveness has been improved by merging with negative values, but the user experience may differ from previous checkpoints.?I have uploaded a new Negative Embedding, trained with Counterfeit-V3.0.There's likely no clear superiority or inferiority between this and the previous embedding, so feel free to choose according to your preference.Note that I'm not specifically recommending the use of this embedding.  ; prompt & Setting: https://civitai.com/models/4468/counterfeit-v30

; Unable to determine this model’s library. Check the
								docs 
.
							",multimodel,text-to-image,https://huggingface.co/gsdf/Counterfeit-V3.0,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 348,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 19.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 30
mpt-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.
This model was trained by MosaicML.; MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.; These architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing
positional embeddings with Attention with Linear Biases (ALiBi).
Thanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence.
MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer.; This model uses the MosaicML LLM codebase, which can be found in the llm-foundry repository. It was trained by MosaicML’s NLP team on the MosaicML platform for LLM pretraining, finetuning, and inference.; MPT-7B is",natural-language-processing,text-generation,https://huggingface.co/mosaicml/mpt-7b,Paper: https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2010.04245.pdf; https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2302.06675.pdf,License: apache-2.0,Datasets: mc4; c4; togethercomputer/RedPajama-Data-1T; bigcode/the-stack; allenai/s2orc,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.02k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 140606,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 12
sdxl-vae-fp16-fix,,,,,,,,,"SDXL-VAE-FP16-Fix is the SDXL VAE, but modified to run in fp16 precision without generating NaNs.; Just load this checkpoint via AutoencoderKL:; ; SDXL-VAE generates NaNs in fp16 because the internal activation values are too big:
; SDXL-VAE-FP16-Fix was created by finetuning the SDXL-VAE to:",,,https://huggingface.co/madebyollin/sdxl-vae-fp16-fix,,License: mit,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 29,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3227,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1005.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-7b-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations.",natural-language-processing,text-generation,https://huggingface.co/NousResearch/Llama-2-7b-hf,,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2439,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 40.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-7B-Guanaco-QLoRA-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Mikael110's Llama2 7B Guanaco QLoRA.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",natural-language-processing,text-classification,https://huggingface.co/TheBloke/llama-2-7B-Guanaco-QLoRA-GGML,,License: other,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Dolphin-Llama-13B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for Eric Hartford's Dolphin Llama 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,https://huggingface.co/TheBloke/Dolphin-Llama-13B-GGML,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base-chinese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper).; This model can be used for masked language modeling ; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).; [More Information Needed]",natural-language-processing,fill-mask,https://huggingface.co/bert-base-chinese,Paper: https://arxiv.org/pdf/1810.04805.pdf,,,Chinese,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 469,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3764329,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 50
segmentation,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c943ee7f35a824e10258_Voice%20Activity%20Detection.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; The collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Paper | Demo | Blog post; ",audio,voice-activity-detection,https://huggingface.co/pyannote/segmentation,Paper: https://arxiv.org/pdf/2104.04045.pdf,License: mit,,,PyTorch; pyannote.audio,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 224,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1916302,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
paraphrase-multilingual-MiniLM-L12-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2,Paper: https://arxiv.org/pdf/1908.10084.pdf,License: apache-2.0,,multilingual,PyTorch; TensorFlow; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 202,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6826723,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 971.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 34
text2vec-base-chinese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese.; It maps sentences to a 768 dimensional dense vector space and can be used for tasks 
like sentence embeddings, text matching or semantic search.; For an automated evaluation of this model, see the Evaluation Benchmark: text2vec; 说明：; Using this model becomes easy when you have text2vec installed:",natural-language-processing,sentence-similarity,https://huggingface.co/shibing624/text2vec-base-chinese,,License: apache-2.0,Datasets: shibing624/nli_zh,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 336,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1259456,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 409.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 22
replit-code-v1-3b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Developed by: Replit, Inc.; ??? Test it on our Demo Space! ???; ?? Fine-tuning and Instruct-tuning guides ??; replit-code-v1-3b is a 2.7B Causal Language Model focused on Code Completion. The model has been trained on a subset of the Stack Dedup v1.2 dataset.; The training mixture includes 20 different languages, listed here in descending order of number of tokens: 

Markdown, Java, JavaScript, Python, TypeScript, PHP, SQL, JSX, reStructuredText, Rust, C, CSS, Go, C++, HTML, Vue, Ruby, Jupyter Notebook, R, Shell

In total, the training dataset contains 175B tokens, which were repeated over 3 epochs -- in total, replit-code-v1-3b has been trained on 525B tokens (~195 tokens per parameter).",natural-language-processing,text-generation,https://huggingface.co/replit/replit-code-v1-3b,Paper: https://arxiv.org/pdf/2211.15533.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2302.06675.pdf,License: cc-by-sa-4.0,Datasets: bigcode/the-stack-dedup,code,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 618,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 36518,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
Wizard-Vicuna-13B-Uncensored-HF,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a float16 HF repo for Eric Hartford's 'uncensored' training of Wizard-Vicuna 13B.; It is the result of converting Eric's float32 repo to float16 for easier storage and use.; For further support, and discussions on these models and AI in general, join us at:",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-HF,,License: other,Datasets: ehartford/wizard_vicuna_70k_unfiltered,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 176,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 75358,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
h2ogpt-gm-oasst1-en-2048-falcon-7b-v3,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This model was trained using H2O LLM Studio.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers, accelerate, torch and einops libraries installed.; You can print a sample prompt after the preprocessing step to see how it is feed to the tokenizer:; Alternatively, you can download h2oai_pipeline.py, store it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:; You may also construct the pipeline from the loaded model and tokenizer yourself and consider the preprocessing steps:",natural-language-processing,text-generation,https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-7b-v3,,License: apache-2.0,Datasets: OpenAssistant/oasst1,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 39,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 276107,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 25
longchat-13b-16k,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Please use load_model from FastChat or LongChat repo to load the model (or chatting API from FastChat). There is a monkey patch needed to use the model.
Usage referece:; (LongChat) python3 eval.py --model-name-or-path  lmsys/longchat-13b-16k --task topics ; (FastChat) python3 -m fastchat.serve.cli --model-path lmsys/longchat-13b-16k; Under the hood, the monkey patch is added in:; https://github.com/lm-sys/FastChat/blob/da0641e567cf93756b0978ab5a6b092e96f06240/fastchat/model/model_adapter.py#L429",natural-language-processing,text-generation,https://huggingface.co/lmsys/longchat-13b-16k,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 111,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7042,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
YumekawaMix,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,YumekawaMix; 年h、等身が低めでフラットなテイストが特栅扦埂kawaiiイラストをお求めの方へ; 年h、等身が若干高くなります。安定感は微妙ですが}jな恧出やすい印象です; 作者に最大の感xを???; Twitter: @AiRSTR7,multimodel,text-to-image,https://huggingface.co/RSTR/YumekawaMix,,License: creativeml-openrail-m,,Japanese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
everyjourney-SDXL-finetuned-Fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Please for anyone, don't reupload my finetuned models to other site, like Civitai or image generating site like Seeart, Yodayo other sites. Thank you for understanding this. ; Like my works and want to collaboration or funding my projects? contact me.; Finetuned on SDXL Base 0.9 Official Release, Expected to be successor of Everyjourney, currently in alpha stage, since i'm captioned this model with BLIP2, the image generated with this model may not meet your expectations, waiting for SDXL finetune/training process to be more polished.  ; My other works:   ; Recommended Settings ",multimodel,text-to-image,https://huggingface.co/thehive/everyjourney-SDXL-finetuned-Fp16,,License: other,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 31,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
lora,,,,,,,,,十条w（Jujo Hotaru）の作成したLoRAを配布しています。You can download JujoHotaru's LoRA collection from this repo.; (＞＜)／(－－)の目 / (≡_≡)／(◎_◎)の目 / \目セットA / 白目セットA / ジト目セットA / ジト目セットB / ぐにゃぐにゃ口 / 大きく_いた口 / 集中 / ぼかし＆背景ぼかし / ト`ンカ`ブ{整 / 彩度{整 / ウィンクa助 / 激おこ / にっこり笑 / 思案 / 茹でダコ / 青醒め; gYLoRA置き; しくる／ダウンロ`ド (Details/Download); ,,,https://huggingface.co/JujoHotaru/lora,,License: mit,,Japanese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 826.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-30b-instruct,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No other data was used except for the dataset mentioned above,natural-language-processing,text-generation,https://huggingface.co/upstage/llama-30b-instruct,,,Datasets: sciq; metaeval/ScienceQA_text_only; GAIR/lima; Open-Orca/OpenOrca; openbookqa,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 115,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
MythoLogic-13B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Gryphe's MythoLogic 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These files were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/MythoLogic-13B-GPTQ,,License: other,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
japanese-mpt-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Japanese subset of the mC4 dataset; Trained for 3000 steps on top of the MPT 7b checkpoint mosaicml/mpt-7b; Before running this model, please install the following pip package:; To load the model, run the following command.; To run this model, you may need to load it in a lower precision in order for it to fit onto your GPU. We found for a T4 GPU, it requires loading the model in 8-bit precision. To load the model in 8-bit and 4-bit, please install the following pip packages:",natural-language-processing,text-generation,https://huggingface.co/lightblue/japanese-mpt-7b,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 258,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
sazyou_LoRA,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"LECOやLoRAの作品置き觥

①胸部pスライダ`LoRA(huge_breasts_woman/flat_chest_woman) 
LECOで作成し、{整した胸部pスライダ`:トリガ`ワ`ド woman
breasts系プロンプトは不要。
推?サンプルはLittleStepMix_Aで作成。



②マルチカラ`ドヘア`LoRA(pastel_hair_full/pastel_hair_A/pastel_hair_B) 
LECOで作成し、{整した多彩色のにするLoRA:トリガ`ワ`ド hair
Lさ指定だけだとかなりカラフルになります。メインの色を入れても良し、服装へのA染も最低限です。
なおnegativeに (black hair,brown hair:1.5) を入力推X
LittleStepMix_Aで作成し、それをサンプルとしているので出方はモデルでかなりなります。
fullは全ての色が出て、パステル{がめです。Aは白系が出ず、v弱め。Bはfullよりメリハリがある出力になります。
; Unable to determine this model’s library. Check the
								docs 
.
							",multimodel,text-to-image,https://huggingface.co/sazyou-roukaku/sazyou_LoRA,,License: creativeml-openrail-m,,Japanese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.4MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-7B-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-7B-fp16,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1531,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Redmond-Puffin-13B-Preview-GGML,,,,,,,,,"GGML 4bit Quantization of Nous Research's Puffin Preview 1 Model: https://huggingface.co/NousResearch/Redmond-Puffin-13B; Thank you to Eachadea for making this quantization possible immediately upon launch; ; The first commercially available language model released by Nous Research!; Redmond-Puffin-13B is one of the worlds first llama-2 based, fine-tuned language models, leveraging a hand curated set of 3K high quality examples, many of which take full advantage of the 4096 context length of Llama 2. This model was fine-tuned by Nous Research, with LDJ leading the training and dataset curation, along with significant dataset formation contributions by J-Supha.",,,https://huggingface.co/NousResearch/Redmond-Puffin-13B-Preview-GGML,,License: mit,,eng,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Redmond-Puffin-13B-GGML,,,,,,,,,"GGML 4bit Quantization of Nous Research's Puffin V1.3 Model: https://huggingface.co/NousResearch/Redmond-Puffin-13B-V1.3; Thank you to Eachadea for making this quantization possible immediately upon launch; For other faster or more accurate quantization methods, please check out Eachadea's hugging face page!; ; The first commercially available language model released by Nous Research!",,,https://huggingface.co/NousResearch/Redmond-Puffin-13B-GGML,,License: mit,,eng,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 34.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-13B-Guanaco-QLoRA-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Mikael110's Llama2 13B Guanaco QLoRA.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",natural-language-processing,text-classification,https://huggingface.co/TheBloke/llama-2-13B-Guanaco-QLoRA-GPTQ,,License: other,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 141,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama2-chat-Chinese-50W,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"由于目前的LLama2-chat模型很难约束其以中文进行问题回复，因此该模型旨在提供一个能以中文进行问答的LLama2-chat 7B 模型。; 该模型使用LLama2-chat 7B 作为基底模型，使用带embedding 和 LM head 的Lora训练方式训练。模型已完成参数合并，可直接使用。也可以手动将sft_lora_model同Llama2-chat进行合并。; 训练数据使用BELLE项目中采样的50万SFT数据进行SFT训练。; Since the LLama2-chat model struggles to confine its responses to Chinese language when prompted with Chinese questions, the primary objective of this model is to provide a LLama2-chat 7B model that can engage in question and answer interactions in Chinese.; The model utilizes LLama2-chat 7B as its base model and is trained using the Lora training approach with the embedding and LM head. The model has undergone the Lora param merge and is now ready for direct use. It is also possible to manually merge the ./sft_lora_model with the Llama2-chat 7B model to obtain the combined model. ",natural-language-processing,text-generation,https://huggingface.co/RicardoLee/Llama2-chat-Chinese-50W,,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base-uncased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; BERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.Chinese and multilingual uncased and cased versions followed shortly after.Modified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.Other 24 smaller models are released afterward.  ",natural-language-processing,fill-mask,https://huggingface.co/bert-base-uncased,Paper: https://arxiv.org/pdf/1810.04805.pdf,License: apache-2.0,Datasets: bookcorpus; wikipedia,English,PyTorch; TensorFlow; JAX; Rust; Core ML; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 979,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 38075908,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 558
mDeBERTa-v3-base-mnli-xnli,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c42cd649af0365a01a1e_Zero-Shot%20Classification.svg,,"This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual 
zero-shot classification. The underlying model was pre-trained by Microsoft on the 
CC100 multilingual dataset. It was then fine-tuned on the XNLI dataset, which contains hypothesis-premise pairs from 15 languages, as well as the English MNLI dataset.
As of December 2021, mDeBERTa-base is the best performing multilingual base-sized transformer model, 
introduced by Microsoft in this paper. ; If you are looking for a smaller, faster (but less performant) model, you can 
try multilingual-MiniLMv2-L6-mnli-xnli.; This model was trained on the XNLI development dataset and the MNLI train dataset. The XNLI development set consists of 2490 professionally translated texts from English to 14 other languages (37350 texts in total) (see this paper). Note that the XNLI contains a training set of 15 machine translated versions of the MNLI dataset for 15 languages, but due to quality issues with these machine translations, this model was only trained on the professional translations from the XNLI development set and the original English MNLI training set (392 702 texts). Not using machine translated texts can avoid overfitting the model to the 15 languages; avoids catastrophic forgetting of the other 85 languages mDeBERTa was pre-trained on; and significantly reduces training costs. ; mDeBERTa-v3-base-mnli-xnli was trained using the Hugging Face trainer with the following hyperparameters.; The model was evaluated on the XNLI test set on 15 languages (5010 texts per language, 75150 in total). Note that multilingual NLI models are capable of classifying NLI texts without receiving NLI training data in the specific language (cross-lingual transfer). This means that the model is also able of doing NLI on the other 85 languages mDeBERTa was training on, but performance is most likely lower than for those languages available in XNLI.",natural-language-processing,zero-shot-classification,https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-mnli-xnli,Paper: https://arxiv.org/pdf/2111.09543.pdf; https://arxiv.org/pdf/1809.05053.pdf; https://arxiv.org/pdf/1911.02116.pdf,License: mit,Datasets: multi_nli; xnli,16 languages,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 144,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 42184,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
clip-vit-large-patch14,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c6145e63a23d3f282ad5_Zero-Shot%20Image%20Classification.svg,,"Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found here.; The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they’re being deployed within.; January 2021; The base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.; The original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.",computer-vision,zero-shot-image-classification,https://huggingface.co/openai/clip-vit-large-patch14,Paper: https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/1908.04913.pdf,,,,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 514,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 12172658,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 373
iroiro-lora,,,,,,,,,"?; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/2vXpSwA7/iroiro-lora,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 325,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.28MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
vcclient000,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/wok000/vcclient000,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 72,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 29.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
30B-Lazarus,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"[] = applied as LoRA to a composite model | () = combined as composite models; [SuperCOT([gtp4xalpaca(manticorechatpygalpha+vicunaunlocked)]+[StoryV2(kaiokendev-SuperHOT-LoRA-prototype30b-8192)])]; This model is the result of an experimental use of LoRAs on language models and model merges that are not the base HuggingFace-format LLaMA model they were intended for.
The desired outcome is to additively apply desired features without paradoxically watering down a model's effective behavior.; Potential limitations - LoRAs applied on top of each other may intercompete.; Subjective results - very promising. Further experimental tests and objective tests are required.",natural-language-processing,text-generation,https://huggingface.co/CalderaAI/30B-Lazarus,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 105,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3480,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 12
open_llama_13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we’ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.; The model can be evaluated with lm-eval-harness. However, due to the aforementioned tokenizer issue, we need to avoid using the fast tokenizer to obtain the correct results. This can be achieved by passing in use_fast=False to this part of lm-eval-harness, as shown in the example below:",natural-language-processing,text-generation,https://huggingface.co/openlm-research/open_llama_13b,,License: apache-2.0,Datasets: togethercomputer/RedPajama-Data-1T,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 420,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 32264,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 12
sdxl-vae,,,,,,,,,"You can integrate this fine-tuned VAE decoder to your existing diffusers workflows, by including a vae argument to the StableDiffusionPipeline; SDXL is a latent diffusion model, where the diffusion operates in a pretrained, 
learned (and fixed) latent space of an autoencoder. 
While the bulk of the semantic composition is done by the latent diffusion model, 
we can improve local, high-frequency details in generated images by improving the quality of the autoencoder. 
To this end, we train the same autoencoder architecture used for the original Stable Diffusion at a larger batch-size (256 vs 9) 
and additionally track the weights with an exponential moving average (EMA). 
The resulting autoencoder outperforms the original model in all evaluated reconstruction metrics, see the table below.; SDXL-VAE vs original kl-f8 VAE vs f8-ft-MSE; Inference API has been turned off for this model.",,,https://huggingface.co/stabilityai/sdxl-vae,Paper: https://arxiv.org/pdf/2112.10752.pdf,License: mit,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 72,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 302854,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1005.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
Realistic_Vision_V4.0,,,,,,,,,"The recommended negative prompt:; (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck; OR; (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation; Recommended parameters for generation:",,,https://huggingface.co/SG161222/Realistic_Vision_V4.0,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 52,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
rwkv-4-music,,,,,,,,,"Use https://github.com/BlinkDL/ChatRWKV/tree/main/music to run current v1 MIDI model; Training data: https://huggingface.co/datasets/breadlicker45/bread-midi-dataset; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/BlinkDL/rwkv-4-music,,License: apache-2.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardCoder-Guanaco-15B-V1.1-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are StarCoder GGML format model files for LoupGarou's WizardCoder-Guanaco-15B-V1.1.; Please note that these GGMLs are not compatible with llama.cpp, text-generation-webui or llama-cpp-python. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh.",,,https://huggingface.co/TheBloke/WizardCoder-Guanaco-15B-V1.1-GGML,,License: apache-2.0,Datasets: guanaco,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 70.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
llama-2-7B-Guanaco-QLoRA-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Mikael110's Llama2 7B Guanaco QLoRA.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",natural-language-processing,text-classification,https://huggingface.co/TheBloke/llama-2-7B-Guanaco-QLoRA-GPTQ,,License: other,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 921,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LLongMA-2-7B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for ConceptofMind's LLongMA 2 7B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/LLongMA-2-7B-GPTQ,Paper: https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2212.10554.pdf,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 100,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LL7M,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a Llama-like generative text model with a scale of 7 billion, optimized for dialogue use cases and converted for the Hugging Face Transformers format. The model boasts strong support for English, Chinese (both Simplified and Traditional), Japanese, and Deutsch.; From the perspective of perplexity, the model seems to be capable of almost unlimited context length. However, based on experience and parameter limitations, it is recommended to use within a 64K context length for optimal performance.; ; The anticipated chat input format is as follows:; Although this is the suggested usage format, Vicuna-style inputs can also be used to adapt to certain pre-existing application scenarios, such as:",natural-language-processing,text-generation,https://huggingface.co/JosephusCheung/LL7M,,License: cc-by-nc-nd-4.0,,4 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
stable-diffusion-inpainting,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.; The Stable-Diffusion-Inpainting was initialized with the weights of the Stable-Diffusion-v-1-2. First 595k steps regular training, then 440k steps of inpainting training at resolution 512x512 on “laion-aesthetics v2 5+” and 10% dropping of the text-conditioning to improve classifier-free classifier-free guidance sampling. For inpainting, the UNet has 5 additional input channels (4 for the encoded masked-image and 1 for the mask itself) whose weights were zero-initialized after restoring the non-inpainting checkpoint. During training, we generate synthetic masks and in 25% mask everything.; You can use this both with the ?Diffusers library and the RunwayML GitHub repository.; How it works:; Developed by: Robin Rombach, Patrick Esser",multimodel,text-to-image,https://huggingface.co/runwayml/stable-diffusion-inpainting,Paper: https://arxiv.org/pdf/2207.12598.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.24k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 193916,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 376
esrgan,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/utnah/esrgan,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 23,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
text-to-video-ms-1.7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c634e3959a06df701b41_Text-to-Video.svg,,"This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.; We Are Hiring! (Based in Beijing / Hangzhou, China.); If you're looking for an exciting challenge and the opportunity to work with cutting-edge technologies in AIGC and large-scale pretraining, then we are the place for you. We are looking for talented, motivated and creative individuals to join our team. If you are interested, please send your CV to us.; EMAIL: yingya.zyy@alibaba-inc.com; The text-to-video generation diffusion model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input. The diffusion model adopts a UNet3D structure, and implements video generation through the iterative denoising process from the pure Gaussian noise video.",multimodel,text-to-video,https://huggingface.co/damo-vilab/text-to-video-ms-1.7b,,License: cc-by-nc-4.0,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 266,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 35026,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.44KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 139
gpt4-x-alpaca-13b-native-4bit-128g,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Update (4/1): Added ggml for Cuda model; Dataset is here (instruct): https://github.com/teknium1/GPTeacher; Okay... Two different models now. One generated in the Triton branch, one generated in Cuda. Use the Cuda one for now unless the Triton branch becomes widely used.; Cuda info (use this one):
Command: ; CUDA_VISIBLE_DEVICES=0 python llama.py ./models/chavinlo-gpt4-x-alpaca --wbits 4 --true-sequential --groupsize 128 --save gpt-x-alpaca-13b-native-4bit-128g-cuda.pt",natural-language-processing,text-generation,https://huggingface.co/anon8231489123/gpt4-x-alpaca-13b-native-4bit-128g,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 673,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2186,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 16.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 52
anything-v5,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""anything-v5""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Model link: View model",multimodel,text-to-image,https://huggingface.co/stablediffusionapi/anything-v5,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 52,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 34369,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.47KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 44
adetailer,,,,,,,,,"; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/Bingsu/adetailer,,License: agpl-3.0,Datasets: wider_face; skytnt/anime-segmentation,,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 141,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 225.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
control_v1p_sd15_brightness,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.; huggingface.co/spaces/ioclab/brightness-controlnet; [More Information Needed]; [More Information Needed]; [More Information Needed]",computer-vision,image-to-image,https://huggingface.co/ViscoseBean/control_v1p_sd15_brightness,,License: creativeml-openrail-m,Datasets: ioclab/grayscale_image_aesthetic_3M,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 20,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 199,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
RVCModels,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/juuxn/RVCModels,,License: unknown,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
multilingual-e5-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 24 layers and the embedding size is 1024.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; This model is initialized from xlm-roberta-large
and continually trained on a mixture of multilingual datasets.
It supports 100 languages from xlm-roberta,
but low-resource languages may see performance degradation.; Initialization: xlm-roberta-large",multimodel,feature-extraction,https://huggingface.co/intfloat/multilingual-e5-large,Paper: https://arxiv.org/pdf/2212.03533.pdf; https://arxiv.org/pdf/2108.08787.pdf; https://arxiv.org/pdf/2104.08663.pdf; https://arxiv.org/pdf/2210.07316.pdf,License: mit,,94 languages,PyTorch; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 38,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 20375,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
mpt-7b-8k,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MPT-7B-8k is a decoder-style transformer pretrained starting from MPT-7B, but updating the sequence length to 8k and training for an additional 500B tokens, resulting in a total of 1.5T tokens of text and code.
This model was trained by MosaicML.; MPT-7B-8k is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.; These architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing
positional embeddings with Attention with Linear Biases (ALiBi).
Thanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence.
MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer.; This model uses the MosaicML LLM codebase, which can be found in the llm-foundry repository. It was trained by MosaicML’s NLP team on the MosaicML platform for LLM pretraining, finetuning, and inference.; MPT-7B-8k is",natural-language-processing,text-generation,https://huggingface.co/mosaicml/mpt-7b-8k,Paper: https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2010.04245.pdf; https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2302.06675.pdf,License: apache-2.0,Datasets: mc4; c4; togethercomputer/RedPajama-Data-1T; bigcode/the-stack; allenai/s2orc,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1001,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
MythoLogic-13B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Gryphe's MythoLogic 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh.",,,https://huggingface.co/TheBloke/MythoLogic-13B-GGML,,License: other,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-ft-instruct-es,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Llama 2 (7B) fine-tuned on Clibrain's  Spanish instructions dataset.; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model. Links to other models can be found in the index at the bottom.; Inference API has been turned off for this model.,natural-language-processing,text-generation,https://huggingface.co/clibrain/Llama-2-ft-instruct-es,,License: apache-2.0,,Spanish,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
jina-embedding-t-en-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"; 

; 
The text embedding set trained by Jina AI, Finetuner team.
; jina-embedding-t-en-v1 is a tiny small language model that has been trained using Jina AI's Linnaeus-Clean dataset.
This dataset consists of 380 million pairs of sentences, which include both query-document pairs.
These pairs were obtained from various domains and were carefully selected through a thorough cleaning process.
The Linnaeus-Full dataset, from which the Linnaeus-Clean dataset is derived, originally contained 1.6 billion sentence pairs.; The model has a range of use cases, including information retrieval, semantic textual similarity, text reranking, and more.",natural-language-processing,sentence-similarity,https://huggingface.co/jinaai/jina-embedding-t-en-v1,,License: apache-2.0,Datasets: jinaai/negation-dataset,English,PyTorch; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 29.6MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
AlpacaCielo-13b,,,,,,,,,"Disclaimer: The model might have a tokenizer issue, but functions.  Updates to come.; AlpacaCielo-13b is a llama-2 based model designed for creative tasks, such as storytelling and roleplay, while still doing well with other chatbot purposes.  It is a triple model merge of Nous-Hermes + Guanaco + Storywriter. While it is mostly ""uncensored"", it still inherits some alignment from Guanaco.; Prompt format is:; Thanks to previous similar models such as Alpacino, Alpasta, and AlpacaDente for inspiring the creation of this model.  Thanks also to the creators of the models involved in the merge.  Original models:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/totally-not-an-llm/AlpacaCielo-13b,,License: llama2,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
roberta-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is case-sensitive: it
makes a difference between english and English.; Disclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by
the Hugging Face team.; RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means
it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. ; More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model
randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict
the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one
after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to
learn a bidirectional representation of the sentence.; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.",natural-language-processing,fill-mask,https://huggingface.co/roberta-base,Paper: https://arxiv.org/pdf/1907.11692.pdf; https://arxiv.org/pdf/1806.02847.pdf,License: mit,Datasets: bookcorpus; wikipedia,English,PyTorch; TensorFlow; JAX; Rust; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 191,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9988668,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 165
xlm-roberta-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al. and first released in this repository. ; Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.; XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. ; RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.; More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.",natural-language-processing,fill-mask,https://huggingface.co/xlm-roberta-large,Paper: https://arxiv.org/pdf/1911.02116.pdf,License: mit,,94 languages,PyTorch; TensorFlow; JAX; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 187,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8031461,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 17
DialoGPT-medium,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. 
The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test.
The model is trained on 147M multi-turn dialogue from Reddit discussion thread. ; Please find the information about preprocessing, training and full details of the DialoGPT in the original DialoGPT repository; ArXiv paper: https://arxiv.org/abs/1911.00536; Now we are ready to try out how the model works as a chatting partner!",natural-language-processing,conversational,https://huggingface.co/microsoft/DialoGPT-medium,Paper: https://arxiv.org/pdf/1911.00536.pdf,License: mit,,,PyTorch; TensorFlow; JAX; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 209,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 380342,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 141
vit-gpt2-image-captioning,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,This is an image captioning model trained by @ydshieh in flax  this is pytorch version of this.; ,multimodel,image-to-text,https://huggingface.co/nlpconnect/vit-gpt2-image-captioning,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 483,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1254131,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 985.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 289
anime-kawai-diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"; ; As it is a version made only by myself and my small associates, the model will not be perfect and may differ from what people expect. Any contributions from everyone will be respected.; Want to support me? Thank you, please help me make it better. ??; This wouldn't have happened if they hadn't made a breakthrough.",multimodel,text-to-image,https://huggingface.co/Ojimi/anime-kawai-diffusion,,License: creativeml-openrail-m,,English,Diffusers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 93,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 22830,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 773.55KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 42
Realistic_Vision_V1.4,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Please read this!
My model has always been free and always will be free. There are no restrictions on the use of the model. The rights to this model still belong to me.; Important note: ""RAW photo"" in the prompt may degrade the result.; I use this template to get good generation results:; Prompt:
subject, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3; Example: a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3",multimodel,text-to-image,https://huggingface.co/SG161222/Realistic_Vision_V1.4,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 280,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 490749,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 22.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 96
faster-whisper-large-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"This repository contains the conversion of openai/whisper-large-v2 to the CTranslate2 model format.; This model can be used in CTranslate2 or projects based on CTranslate2 such as faster-whisper.; The original model was converted with the following command:; Note that the model weights are saved in FP16. This type can be changed when the model is loaded using the compute_type option in CTranslate2.; For more information about the original model, see its model card.",audio,automatic-speech-recognition,https://huggingface.co/guillaumekln/faster-whisper-large-v2,,License: mit,,99 languages,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 67,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 47905,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
vicuna-13b-GPTQ-4bit-128g,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"** Converted model for GPTQ from https://huggingface.co/lmsys/vicuna-13b-delta-v0. This is the best local model I've ever tried. I hope someone makes a version based on the uncensored dataset...**; GPTQ conversion command (on CUDA branch):
CUDA_VISIBLE_DEVICES=0 python llama.py ../lmsys/vicuna-13b-v0 c4 --wbits 4 --true-sequential --groupsize 128 --save vicuna-13b-4bit-128g.pt; Added 1 token to the tokenizer model:
python llama-tools/add_tokens.py lmsys/vicuna-13b-v0/tokenizer.model /content/tokenizer.model llama-tools/test_list.txt; Use of Oobabooga with these tags:
--wbits 4
--groupsize 128; Enjoy",natural-language-processing,text-generation,https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 649,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1590,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 24
Ziya-LLaMA-13B-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"（LLaMA权重的许可证限制，我们无法直接发布完整的模型权重，用户需要参考使用说明进行合并); 姜子牙通用大模型V1是基于LLaMa的130亿参数的大规模预训练模型，具备翻译，编程，文本分类，信息抽取，摘要，文案生成，常识问答和数学计算等能力。目前姜子牙通用大模型已完成大规模预训练、多任务有监督微调和人类反馈学习三阶段的训练过程。; The Ziya-LLaMA-13B-v1 is a large-scale pre-trained model based on LLaMA with 13 billion parameters. It has the ability to perform tasks such as translation, programming, text classification, information extraction, summarization, copywriting, common sense Q&A, and mathematical calculation. The Ziya-LLaMA-13B-v1 has undergone three stages of training: large-scale continual pre-training (PT), multi-task supervised fine-tuning (SFT), and human feedback learning (RM, PPO).; 原始数据包含英文和中文，其中英文数据来自openwebtext、Books、Wikipedia和Code，中文数据来自清洗后的悟道数据集、自建的中文数据集。在对原始数据进行去重、模型打分、数据分桶、规则过滤、敏感主题过滤和数据评估后，最终得到125B tokens的有效数据。; 为了解决LLaMA原生分词对中文编解码效率低下的问题，我们在LLaMA词表的基础上增加了7k+个常见中文字，通过和LLaMA原生的词表去重，最终得到一个39410大小的词表，并通过复用Transformers里LlamaTokenizer来实现了这一效果。",natural-language-processing,text-generation,https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1,Paper: https://arxiv.org/pdf/2210.08590.pdf,License: gpl-3.0,,English; Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 221,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 731,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
e5-large-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 24 layers and the embedding size is 1024.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; Please refer to our paper at https://arxiv.org/pdf/2212.03533.pdf.; Check out unilm/e5 to reproduce evaluation results 
on the BEIR and MTEB benchmark.",multimodel,feature-extraction,https://huggingface.co/intfloat/e5-large-v2,Paper: https://arxiv.org/pdf/2212.03533.pdf; https://arxiv.org/pdf/2104.08663.pdf; https://arxiv.org/pdf/2210.07316.pdf,License: mit,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 77,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 67532,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
musicgen-melody,,,,,,,,,"Audiocraft provides the code and models for MusicGen, a simple and controllable model for music generation. 
MusicGen is a single stage auto-regressive Transformer model trained over a 32kHz EnCodec tokenizer with 4 codebooks sampled at 50 Hz. 
Unlike existing methods like MusicLM, MusicGen doesn't not require a self-supervised semantic representation, and it generates all 4 codebooks in one pass. 
By introducing a small delay between the codebooks, we show we can predict them in parallel, thus having only 50 auto-regressive steps per second of audio.; MusicGen was published in Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez.; Four checkpoints are released:; Try out MusicGen yourself!; You can run MusicGen locally as well:",,,https://huggingface.co/facebook/musicgen-melody,Paper: https://arxiv.org/pdf/2306.05284.pdf,License: cc-by-nc-4.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 71,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 99
controlnet_qrcode,,,,,,,,,"; These ControlNet models have been trained on a large dataset of 150,000 QR code + QR code artwork couples. They provide a solid foundation for generating QR code-based artwork that is aesthetically pleasing, while still maintaining the integral QR code shape.; The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs. However, a 1.5 version model was also trained on the same dataset for those who are using the older version.
Separate repos for usage in diffusers can be found here:
1.5: https://huggingface.co/DionTimmer/controlnet_qrcode-control_v1p_sd15
2.1: https://huggingface.co/DionTimmer/controlnet_qrcode-control_v11p_sd21; These models perform quite well in most cases, but please note that they are not 100% accurate. In some instances, the QR code shape might not come through as expected. You can increase the ControlNet weight to emphasize the QR code shape. However, be cautious as this might negatively impact the style of your output.To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).; To balance between style and shape, a gentle fine-tuning of the control weight might be required based on the individual input and the desired output, aswell as the correct prompt. Some prompts do not work until you increase the weight by a lot. The process of finding the right balance between these factors is part art and part science. For the best results, it is recommended to generate your artwork at a resolution of 768. This allows for a higher level of detail in the final product, enhancing the quality and effectiveness of the QR code-based artwork.",,,https://huggingface.co/DionTimmer/controlnet_qrcode,,License: openrail++,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 259,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 77,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eric Hartford's Wizard Vicuna 13B Uncensored merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 46,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7625,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
codegen25-7b-multi,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Title: CodeGen2.5: Small, but mighty; Authors: Erik Nijkamp*, Hiroaki Hayashi*, Yingbo Zhou, Caiming Xiong; (* equal contribution); CodeGen2.5 is a family of autoregressive language models for program synthesis.; Building upon CodeGen2, the model is trained on StarCoderData for 1.4T tokens, achieving competitive results compared to StarCoderBase-15.5B with less than half the size.",natural-language-processing,text-generation,https://huggingface.co/Salesforce/codegen25-7b-multi,Paper: https://arxiv.org/pdf/2305.02309.pdf,License: apache-2.0,Datasets: bigcode/starcoderdata,code,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 64,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3558,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
MythoLogic-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"An experiment with gradient merges using the following script, with Chronos as its primary model, augmented by Hermes and Wizard-Vicuna Uncensored.; Quantized models are available from TheBloke: GGML - GPTQ (You're the best!); Chronos is a wonderfully verbose model, though it definitely seems to lack in the logic department. Hermes and WizardLM have been merged gradually, primarily in the higher layers (10+) in an attempt to rectify some of this behaviour.; The main objective was to create an all-round model with improved story generation and roleplaying capabilities.; Below is an illustration to showcase a rough approximation of the gradients I used to create MythoLogic:",natural-language-processing,text-generation,https://huggingface.co/Gryphe/MythoLogic-13b,,License: other,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 103,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
llama-2-13b-chat-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"These are the converted model weights for Llama-2-13B-chat in Huggingface format.; Courtesy of Mirage-Studio.io, home of MirageGPT: the private ChatGPT alternative.; license: other
LLAMA 2 COMMUNITY LICENSE AGREEMENT	
Llama 2 Version Release Date: July 18, 2023; ""Agreement"" means the terms and conditions for use, reproduction, distribution and 
modification of the Llama Materials set forth herein.; ""Documentation"" means the specifications, manuals and documentation 
accompanying Llama 2 distributed by Meta at ai.meta.com/resources/models-and-
libraries/llama-downloads/.",natural-language-processing,text-generation,https://huggingface.co/daryl149/llama-2-13b-chat-hf,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 692,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Free_Sydney_13b_HF,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Buy Sydney Ko-fi; LLaMA 2 fine-tune on ... your favorite over-enthusiastic Ai, Syndney.; It has up-to-date information about recent events - but also it's Sydney - so you never know.; Stacked on top of Puffin 13b so it wants to be your assistant.; New: If you want to experience a more naive yet equally attached Sydney, check Pure Sydney ",natural-language-processing,text-generation,https://huggingface.co/FPHam/Free_Sydney_13b_HF,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Free_Sydney_13b_GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Buy Sydney Ko-fi; LLaMA 2 fine-tune on ... your favorite over-enthusiastic Ai, Syndney.; This is 4-bit GPTQ version of the HF version from here: https://huggingface.co/FPHam/Free_Sydney_13b_HF; GPTQ runs slooow on AutoGPTQ, but faaaaast on ExLLaMA; Sydney has up-to-date information about recent events - but also it's Sydney - so you never know.",natural-language-processing,text-generation,https://huggingface.co/FPHam/Free_Sydney_13b_GPTQ,,,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 47,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
llama2_7b_chat_uncensored-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for George Sung's Llama2 7B Chat Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/llama2_7b_chat_uncensored-GPTQ,,License: other,Datasets: ehartford/wizard_vicuna_70k_unfiltered,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 108,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
opus-mt-en-zh,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,source group: English ; target group: Chinese ; OPUS readme: eng-zho; model: transformer; source language(s): eng,natural-language-processing,translation,https://huggingface.co/Helsinki-NLP/opus-mt-en-zh,,License: apache-2.0,,English; Chinese,PyTorch; TensorFlow; JAX; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 148,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 97535,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 21
bart-large-cnn,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"BART model pre-trained on English language, and fine-tuned on CNN Daily Mail. It was introduced in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Lewis et al. and first released in [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart). ; Disclaimer: The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.; BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.; BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.; You can use this model for text summarization. ",natural-language-processing,summarization,https://huggingface.co/facebook/bart-large-cnn,Paper: https://arxiv.org/pdf/1910.13461.pdf,License: mit,Datasets: cnn_dailymail,English,PyTorch; TensorFlow; JAX; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 481,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1244537,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 280
vit-base-patch16-224,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,"Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. ; Disclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.",computer-vision,image-classification,https://huggingface.co/google/vit-base-patch16-224,Paper: https://arxiv.org/pdf/2010.11929.pdf; https://arxiv.org/pdf/2006.03677.pdf,License: apache-2.0,Datasets: imagenet-1k; imagenet-21k,,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 272,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3378220,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 116
bloomz,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",natural-language-processing,text-generation,https://huggingface.co/bigscience/bloomz,Paper: https://arxiv.org/pdf/2211.01786.pdf,License: bigscience-bloom-rail-1.0,Datasets: bigscience/xP3,46 languages,PyTorch; TensorBoard; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 429,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 16899,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 224.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 68
flan-t5-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:",natural-language-processing,text2text-generation,https://huggingface.co/google/flan-t5-base,Paper: https://arxiv.org/pdf/2210.11416.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,Datasets: svakulenk0/qrecc; taskmaster2; djaym7/wiki_dialog; deepmind/code_contests; lambada; gsm8k; aqua_rat; esnli; quasc; qed,5 languages,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 268,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2177672,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 133
flan-t5-xl,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:",natural-language-processing,text2text-generation,https://huggingface.co/google/flan-t5-xl,Paper: https://arxiv.org/pdf/2210.11416.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,Datasets: svakulenk0/qrecc; taskmaster2; djaym7/wiki_dialog; deepmind/code_contests; lambada; gsm8k; aqua_rat; esnli; quasc; qed,5 languages,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 313,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 262164,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 34.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 148
Inkpunk-Diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Finetuned Stable Diffusion model trained on dreambooth. Vaguely inspired by Gorillaz, FLCL, and Yoji Shinkawa. Use nvinkpunk in your prompts.; We support a Gradio Web UI to run Inkpunk-Diffusion:
; 
",multimodel,text-to-image,https://huggingface.co/Envvi/Inkpunk-Diffusion,,License: creativeml-openrail-m,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 868,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9669,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 59
Counterfeit-V2.5,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,V2.5 has been updated for ease of use as anime-style model.I use this embedding for negative prompts.https://huggingface.co/datasets/gsdf/EasyNegative ; Share by-productsV2.1…Feeling of use similar to V2.0V2.2…NSFW model; ; ; ,multimodel,text-to-image,https://huggingface.co/gsdf/Counterfeit-V2.5,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.4k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 27538,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 23.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 67
ControlNet-modules-safetensors,,,,,,,,,"This repository hosts pruned .safetensors modules of ControlNet, by lllyasviel and T2I-Adapters, TencentARC Team; The modules are meant for this extension for AUTOMATIC1111/stable-diffusion-webui, but should work for different webuis too if they have it implemented. cheers!?; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/webui/ControlNet-modules-safetensors,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.26k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 51
T2I-Adapter,,,,,,,,, ; ?Adapter Zoo | ?Demos | ?GitHub; T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models; The GitHub repo: https://github.com/TencentARC/T2I-Adapter; Please find the model information in https://github.com/TencentARC/T2I-Adapter/blob/main/docs/AdapterZoo.md,,,https://huggingface.co/TencentARC/T2I-Adapter,Paper: https://arxiv.org/pdf/2302.08453.pdf,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 553,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.63KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
llama-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This contains the weights for the LLaMA-7b model. This model is under a non-commercial license (see the LICENSE file).
You should only use this repository if you have been granted access to the model by filling out this form but either lost your copy of the weights or got some trouble converting them to the Transformers format.",natural-language-processing,text-generation,https://huggingface.co/huggyllama/llama-7b,,License: other,,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 162,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 174764,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 10
oasst-sft-6-llama-30b-xor,,,,,,,,,"Due to the license attached to LLaMA models by Meta AI it is not possible to directly distribute LLaMA-based models. Instead we provide XOR weights for the OA models.; Thanks to Mick for writing the xor_codec.py script which enables this process; Note: This process applies to oasst-sft-6-llama-30b model. The same process can be applied to other models in future, but the checksums will be different..; This process is tested only on Linux (specifically Ubuntu). Some users have reported that the process does not work on Windows. We recommend using WSL if you only have a Windows machine.; To use OpenAssistant LLaMA-Based Models, you should have a copy of the original LLaMA model weights and add them to a llama subdirectory here. If you cannot obtain the original LLaMA, see the note in italic below for a possible alternative.",,,https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor,Paper: https://arxiv.org/pdf/2304.07327.pdf,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 929,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 12.29KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
control_v1u_sd15_illumination_webui,,,,,,,,,"This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.; Recommendation Weight: 0.4-0.9; Recommendation Exit Timing: 0.4-0.9; As more datasets are still being trained in this model, it is expected to take 2-4 days. Therefore, flexible weight adjustments should be made based on different scenarios and specific results. If you have generated good images or encountered any problems, you can discuss them on Hugging Face~~~; ",,,https://huggingface.co/ioclab/control_v1u_sd15_illumination_webui,,License: creativeml-openrail-m,Datasets: ioclab/grayscale_image_aesthetic_3M,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 60,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
starcoderbase,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Please read the BigCode OpenRAIL-M license agreement before accepting it.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; ; Play with the model on the StarCoder Playground.",natural-language-processing,text-generation,https://huggingface.co/bigcode/starcoderbase,Paper: https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,License: bigcode-openrail-m,Datasets: bigcode/the-stack-dedup,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 305,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 15870,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 21
LLaMa-7B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's LLaMA 7b.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh.",,,https://huggingface.co/TheBloke/LLaMa-7B-GGML,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 31,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 306,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
musicgen-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"MusicGen is a text-to-music model capable of genreating high-quality music samples conditioned on text descriptions or audio prompts.
It is a single stage auto-regressive Transformer model trained over a 32kHz EnCodec tokenizer with 4 codebooks sampled at 50 Hz. 
Unlike existing methods, like MusicLM, MusicGen doesn't require a self-supervised semantic representation, and it generates all 4 codebooks in one pass. 
By introducing a small delay between the codebooks, we show we can predict them in parallel, thus having only 50 auto-regressive steps per second of audio.; MusicGen was published in Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez.; Four checkpoints are released:; Try out MusicGen yourself!; You can run MusicGen locally with the ? Transformers library from version 4.31.0 onwards.",natural-language-processing,text2text-generation,https://huggingface.co/facebook/musicgen-large,Paper: https://arxiv.org/pdf/2306.05284.pdf,License: cc-by-nc-4.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 131,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 800,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 20.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 94
mpt-30b-chat,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MPT-30B-Chat is a chatbot-like model for dialogue generation.
It was built by finetuning MPT-30B on the ShareGPT-Vicuna, Camel-AI,
 GPTeacher, Guanaco, Baize and some generated datasets.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; June 22, 2023; CC-By-NC-SA-4.0 (non-commercial use only); ksreenivasan:",natural-language-processing,text-generation,https://huggingface.co/mosaicml/mpt-30b-chat,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2010.04245.pdf,License: cc-by-nc-sa-4.0,Datasets: camel-ai/code; ehartford/wizard_vicuna_70k_unfiltered; anon8231489123/ShareGPT_Vicuna_unfiltered; teknium1/GPTeacher/roleplay-instruct-v2-final; teknium1/GPTeacher/codegen-isntruct; timdettmers/openassistant-guanaco; camel-ai/math; project-baize/baize-chatbot/medical_chat_data; project-baize/baize-chatbot/quora_chat_data; project-baize/baize-chatbot/stackoverflow_chat_data; camel-ai/biology; camel-ai/chemistry; camel-ai/ai_society; jondurbin/airoboros-gpt4-1.2; LongConversations; camel-ai/physics,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 159,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 40856,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 14
Platypus-30B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Platypus-30B is an instruction fine-tuned model based on the LLaMA-30B transformer architecture.; We use state-of-the-art Language Model Evaluation Harness to run the benchmark tests above.; Dataset of highly filtered and curated question and answer pairs. Release TBD.; lilloukas/Platypus-30B was instruction fine-tuned using LoRA on 4 A100 80GB. For training details and inference instructions please see the Platypus-30B GitHub repo.; Install LM Evaluation Harness:,natural-language-processing,text-generation,https://huggingface.co/lilloukas/Platypus-30B,,License: other,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 177,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
xgen-7b-8k-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Official research release for the family of XGen models (7B) by Salesforce AI Research:; Title: Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length; Authors: Erik Nijkamp*, Tian Xie*, Hiroaki Hayashi*, Bo Pang*, Congying Xia*, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovs'ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Rayhan Joty, Caiming Xiong.; (* indicates equal contribution); Correspondence to: Shafiq Rayhan Joty, Caiming Xiong",natural-language-processing,text-generation,https://huggingface.co/Salesforce/xgen-7b-8k-base,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 282,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 18278,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard Vicuna 13B Uncensored.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-SuperHOT-8K-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 41,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 69.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
lince-zero,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"LINCE-ZERO (Llm for Instructions from Natural Corpus en Espa?ol) is a SOTA Spanish instruction-tuned LLM ?; Developed by Clibrain, it is a causal decoder-only model with 7B parameters. LINCE-ZERO is based on Falcon-7B and has been fine-tuned using a combination of the Alpaca and Dolly datasets, both translated into Spanish and augmented to 80k examples.; The model is released under the Apache 2.0 license.; If you want to test the robust 40B parameters version called LINCE, you can request access at lince@clibrain.com. Be one of the first to discover the possibilities of LINCE!; LINCE-ZERO (Llm for Instructions from Natural Corpus en Espa?ol) is a state-of-the-art Spanish instruction-tuned large language model. Developed by Clibrain, it is a causal decoder-only model with 7B parameters. LINCE-ZERO is based on Falcon-7B and has been fine-tuned using an 80k examples augmented combination of the Alpaca and Dolly datasets, both translated into Spanish.",natural-language-processing,text-generation,https://huggingface.co/clibrain/lince-zero,Paper: https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,Datasets: tatsu-lab/alpaca; databricks/databricks-dolly-15k,Spanish,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 29,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1073,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
aguila-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"ǎguila-7B is a transformer-based causal language model for Catalan, Spanish, and English. 
It is based on the Falcon-7B model and has been trained on a 26B token 
trilingual corpus collected from publicly available corpora and crawlers.; The ǎguila-7B model is ready-to-use only for causal language modeling to perform text-generation tasks. 
However, it is intended to be fine-tuned for downstream tasks.; Here is how to use this model:; At the time of submission, no measures have been taken to estimate the bias and toxicity embedded in the model. 
However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques 
on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated. ; We adapted the original Falcon-7B model to Spanish and Catalan by swapping the tokenizer and adjusting the embedding layer. ",natural-language-processing,text-generation,https://huggingface.co/projecte-aina/aguila-7b,,,,English; Spanish; Catalan,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 405,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
FinGPT_ChatGLM2_Sentiment_Instruction_LoRA_FT,,,,,,,,,"[1] Financial_Phrasebank (FPB)  is a financial news sentiment analysis benchmark, the labels are ""positive"", ""negative"" and ""neutral"". We use the same split as BloombergGPT. BloombergGPT only use 5-shots in the test to show their model's outstanding performance without further finetuning. However, is our task, all data in the 'train' part were used in finetuning, So our results are far better than Bloomberg's.; [2] FiQA SA consists of 17k sentences from microblog headlines and financial news. These labels were changed to ""positive"", ""negative"" and ""neutral"" according to BloombergGPT's paper. We have tried to use the same split as BloombergGPT's paper. However, the amounts of each label can't match exactly when the seed was set to 42.; [3] Twitter Financial News Sentiment (TFNS) dataset is an English-language dataset containing an annotated corpus of finance-related tweets. This dataset is used to classify finance-related tweets for their sentiment. The dataset holds 11,932 documents annotated with 3 labels: ""Bearish"" (""negative""), ""Bullish"" (""positive""), and ""Neutral"".; [4] News With GPT Instruction (MWGI) is a dataset whose labels were generated by ChatGPT. The train set has 16.2k samples and the test set has 4.05k samples. The dataset not only contains 7 classification labels: ""strong negative"", ""moderately negative"", ""mildly negative"", ""neutral"", ""mildly positive"", ""moderately positive"", ""strong positive"". but it also has the reasons for that result, which might be helpful in the instruction finetuning.; Coming Soon.",,,https://huggingface.co/oliverwang15/FinGPT_ChatGLM2_Sentiment_Instruction_LoRA_FT,,License: mit,Datasets: oliverwang15/fingpt_chatglm2_sentiment_instruction_lora_ft_dataset,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.83MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
openchat_v2_openorca_preview-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Open-Orca's OpenChat V2 x OpenOrca Preview 2.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/openchat_v2_openorca_preview-GPTQ,,License: other,Datasets: Open-Orca/OpenOrca,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 171,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Replit-Code-Instruct-Glaive-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are Replit GGML format model files for Sahil2801's Replit Code Instruct Glaive.; Please note that these GGMLs are not compatible with llama.cpp, text-generation-webui or llama-cpp-python. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh.",,,https://huggingface.co/TheBloke/Replit-Code-Instruct-Glaive-GGML,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama2-13b-Chinese-chat,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,完整合并后文件下载：https://www.codewithgpu.com/m/file/llama2-13b-Chinese-chat; 项目在中文sharegpt数据集上训练得到的llama2 Chinese chat 13b，为减轻文件大小负担这里只放出了adapter的权重请拉取https://huggingface.co/TheBloke/Llama-2-13B-fp16 作为基础权重，使用如下脚步执行合并得到可工作的总权重：  ; 合并后，体验对话：; 推荐继续二次训练以针对性调优对话效果~ ; The following bitsandbytes quantization config was used during training:,natural-language-processing,question-answering,https://huggingface.co/shareAI/llama2-13b-Chinese-chat,,,Datasets: shareAI/shareGPT_cn; FreedomIntelligence/ShareGPT-CN,Chinese,PEFT,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
distilbert-base-uncased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"This model is a distilled version of the BERT base model. It was
introduced in this paper. The code for the distillation process can be found
here. This model is uncased: it does
not make a difference between english and English.; DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a
self-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,
with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic
process to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained
with three objectives:; This way, the model learns the same inner representation of the English language than its teacher model, while being
faster for inference or downstream tasks.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the model hub to look for
fine-tuned versions on a task that interests you.; Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification or question answering. For tasks such as text
generation you should look at model like GPT2.",natural-language-processing,fill-mask,https://huggingface.co/distilbert-base-uncased,Paper: https://arxiv.org/pdf/1910.01108.pdf,License: apache-2.0,Datasets: bookcorpus; wikipedia,English,PyTorch; TensorFlow; JAX; Rust; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 237,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8105818,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 185
t5-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-Base is the checkpoint with 220 million parameters. ; The developers write in a blog post that the model: ",natural-language-processing,translation,https://huggingface.co/t5-base,Paper: https://arxiv.org/pdf/1805.12471.pdf; https://arxiv.org/pdf/1708.00055.pdf; https://arxiv.org/pdf/1704.05426.pdf; https://arxiv.org/pdf/1606.05250.pdf; https://arxiv.org/pdf/1808.09121.pdf; https://arxiv.org/pdf/1810.12885.pdf; https://arxiv.org/pdf/1905.10044.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,Datasets: c4,4 languages,PyTorch; TensorFlow; JAX; Rust; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 290,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2535137,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 310
DeBERTa-v3-base-mnli-fever-anli,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c42cd649af0365a01a1e_Zero-Shot%20Classification.svg,,"This model was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs. This base model outperforms almost all large models on the ANLI benchmark. 
The base model is DeBERTa-v3-base from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective, see annex 11 of the original DeBERTa paper. ; For highest performance (but less speed), I recommend using https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli.; DeBERTa-v3-base-mnli-fever-anli was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs.; DeBERTa-v3-base-mnli-fever-anli was trained using the Hugging Face trainer with the following hyperparameters.; The model was evaluated using the test sets for MultiNLI and ANLI and the dev set for Fever-NLI. The metric used is accuracy.",natural-language-processing,zero-shot-classification,https://huggingface.co/MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli,Paper: https://arxiv.org/pdf/2006.03654.pdf,License: mit,Datasets: multi_nli; anli; fever,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 47,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9258143,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 740.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
EmoRoBERTa,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Connect me on LinkedIn; Dataset labelled 58000 Reddit comments with 28 emotions; RoBERTa builds on BERT’s language masking strategy and modifies key hyperparameters in BERT, including removing BERT’s next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time. This allows RoBERTa representations to generalize even better to downstream tasks compared to BERT.; Best Result of Macro F1 - 49.30%; Output ",natural-language-processing,text-classification,https://huggingface.co/arpanghoshal/EmoRoBERTa,,License: mit,Datasets: go_emotions,English,TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 71,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 45494,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 502.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
roberta-base-squad2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,"This is the roberta-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering. ; Language model: roberta-baseLanguage: EnglishDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD 2.0Code:  See an example QA pipeline on HaystackInfrastructure: 4x Tesla v100; Please note that we have also released a distilled version of this model called deepset/tinyroberta-squad2. The distilled model has a comparable prediction quality and runs at twice the speed of the base model.; Haystack is an NLP framework by deepset. You can use this model in a Haystack pipeline to do question answering at scale (over many documents). To load the model in Haystack:; For a complete example of roberta-base-squad2 being used for  Question Answering, check out the Tutorials in Haystack Documentation",natural-language-processing,question-answering,https://huggingface.co/deepset/roberta-base-squad2,,License: cc-by-4.0,Datasets: squad_v2,English,PyTorch; TensorFlow; JAX; Rust; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 365,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2899042,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 279
wav2vec2-large-xlsr-53-english,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Fine-tuned facebook/wav2vec2-large-xlsr-53 on English using the train and validation splits of Common Voice 6.1.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:",audio,automatic-speech-recognition,https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english,,License: apache-2.0,Datasets: common_voice; mozilla-foundation/common_voice_6_0,English,PyTorch; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 226,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 56332600,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 44
deberta-v3-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. ; In DeBERTa V3, we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our paper.; Please check the official repository for more implementation details and updates.; The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters  with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.; We present the dev results on SQuAD 2.0 and MNLI tasks.",natural-language-processing,fill-mask,https://huggingface.co/microsoft/deberta-v3-base,Paper: https://arxiv.org/pdf/2006.03654.pdf; https://arxiv.org/pdf/2111.09543.pdf,License: mit,,English,PyTorch; TensorFlow; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 99,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 86456,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
all-mpnet-base-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/all-mpnet-base-v2,Paper: https://arxiv.org/pdf/1904.06472.pdf; https://arxiv.org/pdf/2102.07033.pdf; https://arxiv.org/pdf/2104.08727.pdf; https://arxiv.org/pdf/1704.05179.pdf; https://arxiv.org/pdf/1810.09305.pdf,License: apache-2.0,Datasets: s2orc; flax-sentence-embeddings/stackexchange_xml; MS Marco; gooaq; yahoo_answers_topics; code_search_net; search_qa; eli5; snli; multi_nli; wikihow; natural_questions; trivia_qa; embedding-data/sentence-compression; embedding-data/flickr30k-captions; embedding-data/altlex; embedding-data/simple-wiki; embedding-data/QQP; embedding-data/SPECTER; embedding-data/PAQ_pairs; embedding-data/WikiAnswers,English,PyTorch; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 284,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4149102,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 439.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 139
sepformer-wsj02mix,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9bbee42f488ff959bf7_Audio-to-Audio.svg,,"This repository provides all the necessary tools to perform audio source separation with a SepFormer 
model, implemented with SpeechBrain, and pretrained on WSJ0-2Mix dataset. For a better experience we encourage you to learn more about
SpeechBrain. The model performance is 22.4 dB on the test set of WSJ0-2Mix dataset.; You can listen to example results obtained on the test set of WSJ0-2/3Mix through here. ; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; The system expects input recordings sampled at 8kHz (single channel).
If your signal has a different sample rate, resample it (e.g, using torchaudio or sox) before using the interface.",audio,audio-to-audio,https://huggingface.co/speechbrain/sepformer-wsj02mix,Paper: https://arxiv.org/pdf/2010.13154.pdf; https://arxiv.org/pdf/2106.04624.pdf,License: apache-2.0,Datasets: WSJ0-2Mix,English,speechbrain,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2087,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 113.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
twitter-roberta-base-sentiment-latest,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. 
The original Twitter-based RoBERTa model can be found here and the original reference paper is TweetEval. This model is suitable for English. ; Labels: 
0 -> Negative;
1 -> Neutral;
2 -> Positive; This sentiment analysis model has been integrated into TweetNLP. You can access the demo here.; Output: ",natural-language-processing,text-classification,https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest,Paper: https://arxiv.org/pdf/2202.03829.pdf,,Datasets: tweet_eval,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 170,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 754428,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1001.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 42
nllb-200-3.3B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,"This is the model card of NLLB-200's 3.3B variant.; Here are the metrics for that particular checkpoint.; ? Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.; ? We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.; ? In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).",natural-language-processing,translation,https://huggingface.co/facebook/nllb-200-3.3B,,License: cc-by-nc-4.0,Datasets: flores-200,196 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 82,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 51820,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 18.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 14
ast-finetuned-audioset-10-10-0.4593,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0941358df9b5cffb0d11b_Audio%20Classification.svg,,"Audio Spectrogram Transformer (AST) model fine-tuned on AudioSet. It was introduced in the paper AST: Audio Spectrogram Transformer by Gong et al. and first released in this repository. ; Disclaimer: The team releasing Audio Spectrogram Transformer did not write a model card for this model so this model card has been written by the Hugging Face team.; The Audio Spectrogram Transformer is equivalent to ViT, but applied on audio. Audio is first turned into an image (as a spectrogram), after which a Vision Transformer is applied. The model gets state-of-the-art results on several audio classification benchmarks.; You can use the raw model for classifying audio into one of the AudioSet classes. See the documentation for more info.",audio,audio-classification,https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593,Paper: https://arxiv.org/pdf/2104.01778.pdf,License: bsd-3-clause,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 53,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 80953,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 346.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
BioMedLM,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Note: This model was previously known as PubMedGPT 2.7B, but we have changed it due to a request from the NIH which holds the trademark for ""PubMed"".; BioMedLM 2.7B is new language model trained exclusively on biomedical abstracts and papers from The Pile. This GPT-style model can achieve strong results on a variety of biomedical NLP tasks, including a new state of the art performance of 50.3% accuracy on the MedQA biomedical question answering task.; As an autoregressive language model, BioMedLM 2.7B is also capable of natural language generation. However, we have only begun to explore the generation capabilities and limitations of this model, and we emphasize that this model’s generation capabilities are for research purposes only and not suitable for production. In releasing this model, we hope to advance both the development of biomedical NLP applications and best practices for responsibly training and utilizing domain-specific language models; issues of reliability, truthfulness, and explainability are top of mind for us.; This model was a joint collaboration of Stanford CRFM and MosaicML.; BioMedLM 2.7B is new language model trained exclusively on biomedical abstracts and papers from The Pile. This GPT-style model can achieve strong results on a variety of biomedical NLP tasks, including a new state of the art performance of 50.3% accuracy on the MedQA biomedical question answering task.",natural-language-processing,text-generation,https://huggingface.co/stanford-crfm/BioMedLM,Paper: https://arxiv.org/pdf/2112.04359.pdf,License: bigscience-bloom-rail-1.0,Datasets: pubmed,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 222,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7539,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 11.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 13
dolly-v2-12b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Databricks' dolly-v2-12b, an instruction-following large language model trained on the Databricks machine learning platform 
that is licensed for commercial use. Based on pythia-12b, Dolly is trained on ~15k instruction/response fine tuning records 
databricks-dolly-15k generated 
by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,
information extraction, open QA and summarization. dolly-v2-12b is not a state-of-the-art model, but does exhibit surprisingly 
high quality instruction following behavior not characteristic of the foundation model on which it is based.  ; Dolly v2 is also available in these smaller models sizes:; Please refer to the dolly GitHub repo for tips on 
running inference for various GPU configurations.; Owner: Databricks, Inc.; dolly-v2-12b is a 12 billion parameter causal language model created by Databricks that is derived from 
EleutherAI's Pythia-12b and fine-tuned 
on a ~15K record instruction corpus generated by Databricks employees and released under a permissive license (CC-BY-SA)",natural-language-processing,text-generation,https://huggingface.co/databricks/dolly-v2-12b,,License: mit,Datasets: databricks/databricks-dolly-15k,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.83k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 105534,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 24.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 33
dolly-v2-3b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Databricks' dolly-v2-3b, an instruction-following large language model trained on the Databricks machine learning platform 
that is licensed for commercial use. Based on pythia-2.8b, Dolly is trained on ~15k instruction/response fine tuning records 
databricks-dolly-15k generated 
by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,
information extraction, open QA and summarization. dolly-v2-3b is not a state-of-the-art model, but does exhibit surprisingly 
high quality instruction following behavior not characteristic of the foundation model on which it is based.  ; Dolly v2 is also available in these larger models sizes:; Please refer to the dolly GitHub repo for tips on 
running inference for various GPU configurations.; Owner: Databricks, Inc.; dolly-v2-3b is a 2.8 billion parameter causal language model created by Databricks that is derived from 
EleutherAI's Pythia-2.8b and fine-tuned 
on a ~15K record instruction corpus generated by Databricks employees and released under a permissive license (CC-BY-SA)",natural-language-processing,text-generation,https://huggingface.co/databricks/dolly-v2-3b,,License: mit,Datasets: databricks/databricks-dolly-15k,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 209,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 120559,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 25
Wizard-Vicuna-13B-Uncensored-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is GPTQ format quantised 4bit models of Eric Hartford's 'uncensored' training of Wizard-Vicuna 13B.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Open the text-generation-webui UI as normal.,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GPTQ,,License: other,Datasets: ehartford/wizard_vicuna_70k_unfiltered,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 193,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6086,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bark-voice-cloning,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"Bark-voice-cloning is a model which processes the outputs from a HuBERT model, and turns them into semantic tokens compatible with bark text to speech.; This can be used for many things, including speech transfer and voice cloning.; code repo
audio webui; (Please use the model manager from the code repo for easy downloading of models); Voice cloning is creating a new voice for text-to-speech.",multimodel,feature-extraction,https://huggingface.co/GitMylo/bark-voice-cloning,,License: mit,Datasets: GitMylo/bark-semantic-training,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 37,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 452.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
e5-base-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 12 layers and the embedding size is 768.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; Please refer to our paper at https://arxiv.org/pdf/2212.03533.pdf.; Check out unilm/e5 to reproduce evaluation results 
on the BEIR and MTEB benchmark.",multimodel,feature-extraction,https://huggingface.co/intfloat/e5-base-v2,Paper: https://arxiv.org/pdf/2212.03533.pdf; https://arxiv.org/pdf/2104.08663.pdf; https://arxiv.org/pdf/2210.07316.pdf,License: mit,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 36,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 59752,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 877.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
guanaco-65B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Tim Dettmers' Guanaco 65B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,https://huggingface.co/TheBloke/guanaco-65B-GGML,Paper: https://arxiv.org/pdf/2305.14314.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2304.07327.pdf,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 101,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 583.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
cpm-bee-10b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"CPM-Bee is a fully open-source, commercially-usable Chinese-English bilingual base model with a capacity of ten billion parameters. It is the second milestone achieved through the training process of CPM-live.
Utilizing the Transformer auto-regressive architecture, CPM-Bee has been pre-trained on an extensive corpus of trillion-scale tokens, thereby possessing remarkable foundational capabilities.; Open-source and Commercial Usable：OpenBMB adheres to the spirit of open-source, aiming to make large-scale models accessible to everyone. CPM-Bee, as a foudation model, is fully open-source and available for commercial use, contributing to the advancement of the field of large-scale models.; Excellent Performance in Chinese and English： : CPM-Bee's base model has undergone rigorous selection and balancing of pre-training data, resulting in outstanding performance in both Chinese and English. For detailed information regarding evaluation tasks and results, please refer to the assessment documentation.; Vast and High-quality Corpus： CPM-Bee, as a base model, has been trained on an extensive corpus of over trillion tokens, making it one of the models with the highest volume of training data within the open-source community. Furthermore, we have implemented stringent selection, cleaning, and post-processing procedures on the pre-training corpus to ensure its quality.; Support for OpenBMB System： The OpenBMB system provides a comprehensive ecosystem of tools and scripts for high-performance pre-training, adaptation, compression, deployment, and tool development. CPM-Bee, as a base model, is accompanied by all the necessary tool scripts, enabling developers to efficiently utilize and explore advanced functionalities.",multimodel,feature-extraction,https://huggingface.co/openbmb/cpm-bee-10b,,,,English; Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 144,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2002,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 19.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
openchat,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"OpenChat is a series of open-source language models fine-tuned on a diverse and high-quality dataset of multi-round conversations. With only ~6K GPT-4 conversations filtered from the ~90K ShareGPT conversations, OpenChat is designed to achieve high performance with limited data.; Generic models:; Code models:; Note: Please load the pretrained models using bfloat16; We provide the full source code, including an inference server compatible with the ""ChatCompletions"" API, in the OpenChat GitHub repository.",natural-language-processing,text-generation,https://huggingface.co/openchat/openchat,,License: other,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 216,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6482,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 18
airoboros-33B-gpt4-1.4-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Jon Durbin's Airoboros 33B GPT4 1.4.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/airoboros-33B-gpt4-1.4-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1281,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
open_llama_7b_v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"TL;DR: we are releasing our public preview of OpenLLaMA, a permissively licensed open source reproduction of Meta AI’s LLaMA. We are releasing a series of 3B, 7B and 13B models trained on different data mixtures. Our model weights can serve as the drop in replacement of LLaMA in existing implementations.; In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a series of 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. The v2 model is better than the old v1 model trained on a different data mixture. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we’ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.",natural-language-processing,text-generation,https://huggingface.co/openlm-research/open_llama_7b_v2,,License: apache-2.0,Datasets: tiiuae/falcon-refinedweb; bigcode/starcoderdata; togethercomputer/RedPajama-Data-1T,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 57,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 12437,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
WizardLM-13B-V1.1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,This is the Full-Weight of WizardLM-13B V1.1 model.; Repository: https://github.com/nlpxucan/WizardLM; Twitter: https://twitter.com/WizardLM_AI/status/1677282955490918401,natural-language-processing,text-generation,https://huggingface.co/WizardLM/WizardLM-13B-V1.1,Paper: https://arxiv.org/pdf/2304.12244.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 54,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2433,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
orca_mini_v2_13b-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Pankaj Mathur's Orca Mini v2 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/orca_mini_v2_13b-GPTQ,Paper: https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2304.12244.pdf,License: other,Datasets: psmathur/orca_minis_uncensored_dataset,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 894,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bk-sdm-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Block-removed Knowledge-distilled Stable Diffusion Model (BK-SDM) is an architecturally compressed SDM for efficient general-purpose text-to-image synthesis. This model is bulit with (i) removing several residual and attention blocks from the U-Net of Stable Diffusion v1.4 and (ii) distillation pretraining on only 0.22M LAION pairs (fewer than 0.1% of the full training set). Despite being trained with very limited resources, our compact model can imitate the original SDM by benefiting from transferred knowledge.; An inference code with the default PNDM scheduler and 50 denoising steps is as follows.; The following code is also runnable, because we compressed the U-Net of Stable Diffusion v1.4 while keeping the other parts (i.e., Text Encoder and Image Decoder) unchanged:; The above examples have been tested on a single NVIDIA GeForce RTX 3090 GPU with the following versions:; We removed several residual and attention blocks from the 0.86B-parameter U-Net in the 1.04B-param SDM-v1.4, and our compressed models are summarized as follows.",multimodel,text-to-image,https://huggingface.co/nota-ai/bk-sdm-base,Paper: https://arxiv.org/pdf/2305.15798.pdf,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 163,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.55KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
open_llama_3b_v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"TL;DR: we are releasing our public preview of OpenLLaMA, a permissively licensed open source reproduction of Meta AI’s LLaMA. We are releasing a series of 3B, 7B and 13B models trained on different data mixtures. Our model weights can serve as the drop in replacement of LLaMA in existing implementations.; In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a series of 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. The v2 model is better than the old v1 model trained on a different data mixture. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we’ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.",natural-language-processing,text-generation,https://huggingface.co/openlm-research/open_llama_3b_v2,,License: apache-2.0,Datasets: tiiuae/falcon-refinedweb; bigcode/starcoderdata; togethercomputer/RedPajama-Data-1T,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4157,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
llama-65b-instruct,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,,natural-language-processing,text-generation,https://huggingface.co/upstage/llama-65b-instruct,,,Datasets: sciq; metaeval/ScienceQA_text_only; GAIR/lima; Open-Orca/OpenOrca; openbookqa,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 131.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-7B-Chat-ggml,,,,,,,,,"From: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf; Quantized using an older version of llama.cpp and compatible with llama.cpp from May 19, commit 2d5db48.; Quantization methods compatible with latest llama.cpp from June 6, commit 2d43387.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/localmodels/Llama-2-7B-Chat-ggml,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-13b-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations.",natural-language-processing,text-generation,https://huggingface.co/NousResearch/Llama-2-13b-hf,,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2103,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 78.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LLaMA-2-70B-GPTQ-transformers4.32.0.dev0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"These files are GPTQ model files for Meta's Llama 2 70B but with new FP16 files, made with the last transformers version. (transformers-4.32.0.dev0); GQA Works with exllama, but not GPTQ for LLaMA/AutoGPTQ.; The model was quantized with GPTQ-for-LLaMA, without group size to reduce VRAM usage, with true sequential and act order true, to not lose a lot of perplexity.; TBD; Exllama works 2x24 VRAM GPUs and it is the recommended way to use it now. It even can do 16K! context with high alpha values and NTK Alpha.",natural-language-processing,text-generation,https://huggingface.co/Panchovix/LLaMA-2-70B-GPTQ-transformers4.32.0.dev0,,License: other,,English,Safetensors; Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 166,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 35.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-13B-German-Assistant-v2-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for flozi00's Llama 2 13B German Assistant v2.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,https://huggingface.co/TheBloke/llama-2-13B-German-Assistant-v2-GGML,,License: other,Datasets: flozi00/conversations,English; German,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-70b-Guanaco-QLoRA-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Mikael110's Llama2 70b Guanaco QLoRA.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",natural-language-processing,text-classification,https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-GPTQ,,License: other,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 96,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 37.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
opus-mt-zh-en,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,"This model can be used for translation and text-to-text generation.; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).; Further details about the dataset for this model can be found in the OPUS readme: zho-eng; pre-processing: normalization + SentencePiece (spm32k,spm32k)",natural-language-processing,translation,https://huggingface.co/Helsinki-NLP/opus-mt-zh-en,,License: cc-by-4.0,,Chinese; English,PyTorch; TensorFlow; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 235,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 364263,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 30
finbert,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper FinBERT: Financial Sentiment Analysis with Pre-trained Language Models and our related blog post on Medium.; The model will give softmax outputs for three labels: positive, negative or neutral.; About Prosus; Prosus is a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. For more information, please visit www.prosus.com.; Contact information",natural-language-processing,text-classification,https://huggingface.co/ProsusAI/finbert,Paper: https://arxiv.org/pdf/1908.10063.pdf,,,English,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 272,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1138607,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 42
vilt-b32-finetuned-vqa,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4f95e63a23d3f26e06e_Visual%20Question%20Answering.svg,,"Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer
Without Convolution or Region Supervision by Kim et al. and first released in this repository. ; Disclaimer: The team releasing ViLT did not write a model card for this model so this model card has been written by the Hugging Face team.; You can use the raw model for visual question answering. ; Here is how to use this model in PyTorch:; (to do)",multimodel,visual-question-answering,https://huggingface.co/dandelin/vilt-b32-finetuned-vqa,Paper: https://arxiv.org/pdf/2102.03334.pdf,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 193,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 505022,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 471.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 111
bert-base-NER,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). ; Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset. ; If you'd like to use a larger BERT-large model fine-tuned on the same dataset, a bert-large-NER version is also available. ; You can use this model with Transformers pipeline for NER.; This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases. ",natural-language-processing,token-classification,https://huggingface.co/dslim/bert-base-NER,Paper: https://arxiv.org/pdf/1810.04805.pdf,License: mit,Datasets: conll2003,English,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 204,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1317035,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 28
fastspeech2-en-ljspeech,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9511aaa882b322dc3d8_Text-to-Speech.svg,,FastSpeech 2 text-to-speech model from fairseq S^2 (paper/code):; See also fairseq S^2 example.,audio,text-to-speech,https://huggingface.co/facebook/fastspeech2-en-ljspeech,Paper: https://arxiv.org/pdf/2006.04558.pdf; https://arxiv.org/pdf/2109.06912.pdf,,Datasets: ljspeech,English,Fairseq,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 185,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5383,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 551.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 75
flan-t5-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:",natural-language-processing,text2text-generation,https://huggingface.co/google/flan-t5-large,Paper: https://arxiv.org/pdf/2210.11416.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,Datasets: svakulenk0/qrecc; taskmaster2; djaym7/wiki_dialog; deepmind/code_contests; lambada; gsm8k; aqua_rat; esnli; quasc; qed,5 languages,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 202,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1122649,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 141
f222,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/acheong08/f222,,License: openrail,Datasets: glue,Avaric,Flair,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 164,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
stable-diffusion-x4-upscaler,,,,,,,,,"This model card focuses on the model associated with the Stable Diffusion Upscaler, available here.
This model is trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model.
In addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. ; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model; Language(s): English",,,https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler,Paper: https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: openrail++,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 449,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 127591,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 28
speecht5_tts,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9511aaa882b322dc3d8_Text-to-Speech.svg,,"SpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS.; This model was introduced in SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.; SpeechT5 was first released in this repository, original weights. The license used is MIT.; Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder.; Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder.",audio,text-to-speech,https://huggingface.co/microsoft/speecht5_tts,Paper: https://arxiv.org/pdf/2110.07205.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: mit,Datasets: libritts,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 181,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 36978,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 585.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 172
loliDiffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"The goal of this project is to improve generation of loli characters since most of other models are not good at it. Support me: https://www.buymeacoffee.com/jilek772003 New models are available on https://pixai.art/ Model list: ; It is recommende to use standard resolution such as 512x768 and EasyNegative embedding with these models. Positive prompt example: 1girl, solo, loli, masterpiece Negative prompt example: EasyNegative, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, multiple panels, aged up, old; Reddit: https://www.reddit.com/r/loliDiffusion Discord: https://discord.gg/mZ3eGeNX7S; v0.4.3 Fixed color issue General improvements v0.5.3 Integrated VAEFile size reduced CLIP force reset fix v0.6.3 Style improvements Added PastelMix and Counterfeit style v0.7.x Style impovements Composition improvements v0.8.x Major improvement on higher resolutions Style improvements Flexibility and responsivity Added support for Night Sky YOZORA model v0.9.x Different approach at merging, you might find v0.8.x versions better Changes at supported models v2.1.X EXPERIMENTAL RELEASE Stable Diffusion 2.1-768 based Default negative prompt: (low quality, worst quality:1.4), (bad anatomy), extra finger, fewer digits, jpeg artifacts For positive prompt it's good to include tags: anime, (masterpiece, best quality) alternatively you may achieve positive response with: (exceptional, best aesthetic, new, newest, best quality, masterpiece, extremely detailed, anime, waifu:1.2) Though it's Loli Diffusion model it's quite general purpose The ability to generate realistic images as Waifu Diffusion can was intentionally decreased This model performs better at higher resolutions like 768*X or 896*X v0.10.x Different approach at merging Better hands Better style inheritance Some changes in supported models ; ",multimodel,text-to-image,https://huggingface.co/JosefJilek/loliDiffusion,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 150,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 115.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
alpaca-lora-7b,,,,,,,,,"This repo contains a low-rank adapter for LLaMA-7b
fit on the Stanford Alpaca dataset.; This version of the weights was trained with the following hyperparameters:; That is:; Instructions for running it can be found at https://github.com/tloen/alpaca-lora.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/tloen/alpaca-lora-7b,,License: mit,Datasets: yahma/alpaca-cleaned,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 399,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 67.2MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 43
alpaca-native-7B-ggml,,,,,,,,,"Mirrored version of https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml in case that one gets taken down
All credits go to Sosaka and chavinlo for creating the modelhttps://huggingface.co/chavinlo/alpaca-native; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/Pi3141/alpaca-native-7B-ggml,,License: unknown,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 52,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 36.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
doll-likeness-series,,,,,,,,,"The 'Doll-Series' is a set of LORA focused on realistic Asian faces, with incredible levels of beauty and aesthetics.; My Pixiv: https://www.pixiv.net/en/users/92373922; My Twitter: https://twitter.com/KbrLoras; License; Disclaimer",,,https://huggingface.co/Kanbara/doll-likeness-series,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 322,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-65b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This contains the weights for the LLaMA-65b model. This model is under a non-commercial license (see the LICENSE file).
You should only use this repository if you have been granted access to the model by filling out this form but either lost your copy of the weights or got some trouble converting them to the Transformers format.",natural-language-processing,text-generation,https://huggingface.co/huggyllama/llama-65b,,License: other,,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 55,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 57878,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 261.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
ShiratakiMix,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"=> ShiratakiMix-add-VAE.safetensors; Negative:; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage. The CreativeML OpenRAIL License specifies:; You can't use the model to deliberately produce nor share illegal or harmful outputs or content; The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license",multimodel,text-to-image,https://huggingface.co/Vsukiyaki/ShiratakiMix,,License: creativeml-openrail-m,,Japanese,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 107,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
Guanaco,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"; You can run on Colab free T4 GPU now; ;  It is highly recommended to use fp16 inference for this model, as 8-bit precision may significantly affect performance. If you require a more Consumer Hardware friendly version, please use the specialized quantized, only 5+GB V-Ram required JosephusCheung/GuanacoOnConsumerHardware.;  You are encouraged to use the latest version of transformers from GitHub.",natural-language-processing,conversational,https://huggingface.co/JosephusCheung/Guanaco,,License: gpl-3.0,Datasets: JosephusCheung/GuanacoDataset,4 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 193,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2608,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 10
vicuna-13B-1.1-GPTQ-4bit-128g,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a 4-bit GPTQ version of the Vicuna 13B 1.1 model.; It was created by merging the deltas provided in the above repo with the original Llama 13B model, using the code provided on their Github page.; It was then quantized to 4bit using GPTQ-for-LLaMa.",natural-language-processing,conversational,https://huggingface.co/TheBloke/vicuna-13B-1.1-GPTQ-4bit-128g,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 193,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 20828,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LLaVA-13b-delta-v0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"NOTE: This ""delta model"" cannot be used directly.Users have to apply it on top of the original LLaMA weights to get actual LLaVA weights.See https://github.com/haotian-liu/LLaVA#llava-weights for instructions.; Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA was trained in April 2023.; Paper or resources for more information:
https://llava-vl.github.io/; License:
Apache License 2.0",natural-language-processing,text-generation,https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 162,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 393,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 14
mGPT-13B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Multilingual language model. This model was trained on the 61 languages from 25 language families (see the list below).; Model was pretrained on a 600Gb of texts, mostly from MC4 and Wikipedia. Training data was deduplicated, the text deduplication includes 64-bit hashing of each text in the corpus for keeping texts with a unique hash. We also filter the documents based on their text compression rate using zlib4. The most strongly and weakly compressing deduplicated texts are discarded.; Here is the table with number of tokens for each language in the pretraining corpus on a logarithmic scale:; ; Afrikaans (af), Arabic (ar), Armenian (hy), Azerbaijani (az), Basque (eu), Bashkir (ba), Belarusian (be), Bengali (bn), Bulgarian (bg), Burmese (my), Buryat (bxr), Chuvash (cv), Danish (da), English (en), Estonian (et), Finnish (fi), French (fr), Georgian (ka), German (de), Greek (el), Hebrew (he), Hindi (hi), Hungarian (hu), Indonesian (id), Italian (it), Japanese (ja), Javanese (jv), Kalmyk (xal), Kazakh (kk), Korean (ko), Kyrgyz (ky), Latvian (lv), Lithuanian (lt), Malay (ms), Malayalam (ml), Marathi (mr), Mongolian (mn), Ossetian (os), Persian (fa), Polish (pl), Portuguese (pt), Romanian (ro), Russian (ru), Spanish (es), Swedish (sv), Swahili (sw), Tatar (tt), Telugu (te), Thai (th), Turkish (tr), Turkmen (tk), Tuvan (tyv), Ukrainian (uk), Uzbek (uz), Vietnamese (vi), Yakut (sax), Yoruba (yo)",natural-language-processing,text-generation,https://huggingface.co/ai-forever/mGPT-13B,,License: mit,,English; Russian,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 55.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mpt-7b-storywriter,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths.
It was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset.
At inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.
We demonstrate generations as long as 84k tokens on a single node of 8 A100-80GB GPUs in our blogpost.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; Apache 2.0; Note: This model requires that trust_remote_code=True be passed to the from_pretrained method. This is because we use a custom model architecture that is not yet part of the transformers package.",natural-language-processing,text-generation,https://huggingface.co/mosaicml/mpt-7b-storywriter,Paper: https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2302.06675.pdf,License: apache-2.0,Datasets: the_pile_books3,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 650,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 12787,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
starcoderplus,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Please read the BigCode OpenRAIL-M license agreement before accepting it.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Play with the instruction-tuned StarCoderPlus at StarChat-Beta.; StarCoderPlus is a fine-tuned version of StarCoderBase on 600B tokens from the English web dataset RedefinedWeb 
combined with StarCoderData from The Stack (v1.2) and a Wikipedia dataset.
It's a 15.5B parameter Language Model trained on English and 80+ programming languages. The model uses Multi Query Attention,
a context window of 8192 tokens,  and was trained using the Fill-in-the-Middle objective on 1.6 trillion tokens. ",natural-language-processing,text-generation,https://huggingface.co/bigcode/starcoderplus,Paper: https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2305.06161.pdf,,Datasets: bigcode/the-stack-dedup; tiiuae/falcon-refinedweb,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 146,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9158,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
guanaco-33b-merged,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/timdettmers/guanaco-33b-merged,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 162,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6360,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 17
BracingEvoMix,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"License:CreativeML Open RAIL-M
Additional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 31, 2023; このモデルは『CreativeML Open RAIL-M』でLicenseそのものに涓はありません。
しかし追加著作者としてi城郎郭の名前が追加されています。
しかし追加著作者として佐城郎画の名前が追加されています。(6/10 Twitterネ`ム涓に伴い、表涓。License内はsazyou_roukakuの涓なし)
なお『CreativeML Open RAIL-M』にdされている通り、
本モデルを使用しての生成物にvしてはLicenseの使用制限Aの事例を除き、当方は一切v与致しません。
犯罪目的利用や医用画像など特定T的な用途での利用は使用制限Aで禁止されています。
必ず_Jしご利用ください。
また当方は一切任を持ちません。免されていることをご了承の上、ご使用ください。; マ`ジ利用モデル一E ; [BracingEvoMix]OpenBraβOpenBra?BanKai @PleaseBanKai ; dreamshaper_5Bakedvaedreamshaper_6BakedVae(https://civitai.com/models/4384) ?Lykonepicrealism_newAgeepicrealism_newEra(https://civitai.com/models/25694) ?epinikiondiamondCoalMix_diamondCoalv2(https://civitai.com/models/41415) ?EnthusiastAIsxd_v10(https://civitai.com/models/1169) ?izuekEvt_V4_e04_ema(https://huggingface.co/haor/Evt_V4-preview) ?haor  ",multimodel,text-to-image,https://huggingface.co/sazyou-roukaku/BracingEvoMix,,License: creativeml-openrail-m,,Japanese,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 100,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ChatLaw-Text2Vec,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,本模型用于法律相关文本的相似度计算。可用于制作向量数据库等。; 本模型利用936727条全国案例库数据集训练，数据集样本如下：; 请问夫妻之间共同财产如何定义？; 请问民间借贷的利息有什么限制; 欢迎引用我们:,natural-language-processing,sentence-similarity,https://huggingface.co/chestnutlzj/ChatLaw-Text2Vec,Paper: https://arxiv.org/pdf/2306.16092.pdf,License: apache-2.0,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 53,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 440,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 410.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
ChatLaw-13B,,,,,,,,,此版本为学术demo版，基于姜子牙Ziya-LLaMA-13B-v1训练而来(LLaMA权重的许可证限制，我们无法直接发布完整的模型权重，用户需自行合并); ChatLaw-13B，此版本为学术demo版，基于姜子牙Ziya-LLaMA-13B-v1训练而来，中文各项表现很好，但是逻辑复杂的法律问答效果不佳，需要用更大参数的模型来解决。; ChatLaw-33B，此版本为学术demo版，基于Anima-33B训练而来，逻辑推理能力大幅提升，但是因为Anima的中文语料过少，导致问答时常会出现英文数据。; ChatLaw-Text2Vec，使用93w条判决案例做成的数据集基于BERT训练了一个相似度匹配模型，可将用户提问信息和对应的法条相匹配，例如：; “请问如果借款没还怎么办。”,,,https://huggingface.co/JessyTsu1/ChatLaw-13B,Paper: https://arxiv.org/pdf/2306.16092.pdf,License: gpl-3.0,,English; Chinese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 252.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
orca_mini_3b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Use orca-mini-3b on Free Google Colab with T4 GPU :); An OpenLLaMa-3B model model trained on explain tuned datasets, created using Instructions and Input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; We build explain tuned WizardLM dataset ~70K, Alpaca dataset ~52K  & Dolly-V2 dataset ~15K created using approaches from Orca Research Paper.; We leverage all of the 15 system instructions provided in Orca Research Paper. to generate custom datasets, in contrast to vanilla instruction tuning approaches used by original datasets.; This helps student model aka this model to learn thought process from teacher model, which is ChatGPT (gpt-3.5-turbo-0301 version).",natural-language-processing,text-generation,https://huggingface.co/psmathur/orca_mini_3b,Paper: https://arxiv.org/pdf/2306.02707.pdf,License: cc-by-nc-sa-4.0,Datasets: psmathur/alpaca_orca; psmathur/dolly-v2_orca; psmathur/WizardLM_Orca,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 99,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6345,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
h2ogpt-gm-oasst1-en-2048-falcon-40b-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This model was trained using H2O LLM Studio.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers, accelerate and torch libraries installed.; You can print a sample prompt after the preprocessing step to see how it is feed to the tokenizer:; Alternatively, you can download h2oai_pipeline.py, store it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:; You may also construct the pipeline from the loaded model and tokenizer yourself and consider the preprocessing steps:",natural-language-processing,text-generation,https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2,,License: apache-2.0,Datasets: OpenAssistant/oasst1,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 15715,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 84.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
Euclid-By_Consistent_Factor,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"One thing I'm happy to celebrate about my model as that it is a lot more racially friendly and will more often than not, produce people who are not Caucasian, in fact around 60%-70% of the time it will produce people of colour in my testing at least. If you find it's not producing Caucasian people, just add either white or Caucasian to the prompt.
It's also very good at aging people so adding an age can make a big difference.; The only thing V5 doesn't do well most of the time are eyes, if you don't get decent eyes try adding perfect eyes or round eyes to the prompt and increase the weight till you are happy. V6 and V6Ultra has fixed this problem.; Prompt: (subject), elegant, alluring, attractive, amazing photograph, masterpiece, best quality, 8K, high quality, photorealistic, realism, art photography, Nikon D850, 16k, sharp focus, masterpiece, breathtaking, atmospheric perspective, diffusion, pore correlation, skin imperfections, DSLR, 80mm Sigma f2, depth of field, intricate natural lighting,; Negative prompt: (unrealistic, render, 3d,cgi,cg,2.5d), (bad-hands-5:1.05), easynegative, [( NG_DeepNegative_V1_64T :0.9) :0.1], ng_deepnegative_v1_75t, worst quality, low quality, normal quality, child, (painting, drawing, sketch, cartoon, anime, render, 3d), blurry, deformed, disfigured, morbid, mutated, bad anatomy, bad art, (bad teeth, weird teeth, broken teeth), (worst quality, low quality, logo, text, watermark, username), incomplete, bad_prompt_version2; Recommend for V5: DPM++ 2M SDE Karras, Steps: 20-45, Hires fix 0.15 - 0.2, CFG 6 - 8.",multimodel,text-to-image,https://huggingface.co/ConsistentFactor/Euclid-By_Consistent_Factor,,License: creativeml-openrail-m,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 57.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
orca_mini_v2_7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"An Uncensored LLaMA-7b model in collaboration with Eric Hartford. trained on explain tuned datasets, created using Instructions and Input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; Please note this model has better code generation capabilities compare to our original orca_mini_7b which was trained on base OpenLLaMA-7b model and which has the empty spaces issues & found not good for code generation.; P.S. I am #opentowork, if you can help, please reach out to me at www.linkedin.com/in/pankajam; I evaluated orca_mini_v2_7b on a wide range of tasks using Language Model Evaluation Harness from EleutherAI. ; Here are the results on metrics used by HuggingFaceH4 Open LLM Leaderboard",natural-language-processing,text-generation,https://huggingface.co/psmathur/orca_mini_v2_7b,Paper: https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2304.12244.pdf,License: cc-by-nc-sa-4.0,Datasets: psmathur/orca_minis_uncensored_dataset,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 766,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
internlm-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"; ?Reporting Issues; InternLM has open-sourced a 7 billion parameter base model tailored for practical scenarios. The model has the following characteristics:; We conducted a comprehensive evaluation of InternLM using the open-source evaluation tool OpenCompass. The evaluation covered five dimensions of capabilities: disciplinary competence, language competence, knowledge competence, inference competence, and comprehension competence. Here are some of the evaluation results, and you can visit the OpenCompass leaderboard for more evaluation results.; Limitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.",multimodel,feature-extraction,https://huggingface.co/internlm/internlm-7b,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 50,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5512,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 15.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
internlm-chat-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"; ?Reporting Issues; InternLM has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:; We conducted a comprehensive evaluation of InternLM using the open-source evaluation tool OpenCompass. The evaluation covered five dimensions of capabilities: disciplinary competence, language competence, knowledge competence, inference competence, and comprehension competence. Here are some of the evaluation results, and you can visit the OpenCompass leaderboard for more evaluation results.; Limitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.",multimodel,feature-extraction,https://huggingface.co/internlm/internlm-chat-7b,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 43,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8872,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 15.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
orca_mini_v2_13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"An Uncensored LLaMA-13b model in collaboration with Eric Hartford. trained on explain tuned datasets, created using Instructions and Input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; Please note this model has better code generation capabilities compare to our original orca_mini_13b which was trained on base OpenLLaMA-13b model and which has the empty spaces issues & found not good for code generation.; P.S. I am #opentowork, if you can help, please reach out to me at www.linkedin.com/in/pankajam; I evaluated orca_mini_v2_13b on a wide range of tasks using Language Model Evaluation Harness from EleutherAI. ; Here are the results on metrics used by HuggingFaceH4 Open LLM Leaderboard",natural-language-processing,text-generation,https://huggingface.co/psmathur/orca_mini_v2_13b,Paper: https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2304.12244.pdf,License: cc-by-nc-sa-4.0,Datasets: psmathur/orca_minis_uncensored_dataset,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 533,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
controlnet-qr-pattern-v2,,,,,,,,,"Conditioning only 25% of the pixels closest to black and the 25% closest to white.; In the automatic1111 folder I added compatible weights, and various strength levels.; Many thanks to antfu for their ideas, tools and contributions
https://antfu.me; ; ",,,https://huggingface.co/Nacholmo/controlnet-qr-pattern-v2,,License: creativeml-openrail-m,Datasets: Nacholmo/controlnet-test-darkest-color; yuvalkirstain/pexel_images_lots_with_generated_captions,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 245,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Pygmalion-7B-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for TehVenom's merge of Pygmalion 7B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Pygmalion-7B-SuperHOT-8K-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 259,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-13B-V1-1-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for WizardLM's WizardLM 13B V1.1 merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GPTQ,Paper: https://arxiv.org/pdf/2304.12244.pdf,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 41,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1648,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
orca_mini_v2_13b-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Pankaj Mathur's Orca Mini v2 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/orca_mini_v2_13b-GGML,Paper: https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2304.12244.pdf,License: cc-by-nc-sa-4.0,Datasets: psmathur/orca_minis_uncensored_dataset,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bk-sdm-small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Block-removed Knowledge-distilled Stable Diffusion Model (BK-SDM) is an architecturally compressed SDM for efficient general-purpose text-to-image synthesis. This model is bulit with (i) removing several residual and attention blocks from the U-Net of Stable Diffusion v1.4 and (ii) distillation pretraining on only 0.22M LAION pairs (fewer than 0.1% of the full training set). Despite being trained with very limited resources, our compact model can imitate the original SDM by benefiting from transferred knowledge.; An inference code with the default PNDM scheduler and 50 denoising steps is as follows.; The following code is also runnable, because we compressed the U-Net of Stable Diffusion v1.4 while keeping the other parts (i.e., Text Encoder and Image Decoder) unchanged:; The above examples have been tested on a single NVIDIA GeForce RTX 3090 GPU with the following versions:; We removed several residual and attention blocks from the 0.86B-parameter U-Net in the 1.04B-param SDM-v1.4, and our compressed models are summarized as follows.",multimodel,text-to-image,https://huggingface.co/nota-ai/bk-sdm-small,Paper: https://arxiv.org/pdf/2305.15798.pdf,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 216,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.55KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
NavelOrangeMix,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/dorioku/NavelOrangeMix,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.53KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bk-sdm-tiny,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Block-removed Knowledge-distilled Stable Diffusion Model (BK-SDM) is an architecturally compressed SDM for efficient general-purpose text-to-image synthesis. This model is bulit with (i) removing several residual and attention blocks from the U-Net of Stable Diffusion v1.4 and (ii) distillation pretraining on only 0.22M LAION pairs (fewer than 0.1% of the full training set). Despite being trained with very limited resources, our compact model can imitate the original SDM by benefiting from transferred knowledge.; An inference code with the default PNDM scheduler and 50 denoising steps is as follows.; The following code is also runnable, because we compressed the U-Net of Stable Diffusion v1.4 while keeping the other parts (i.e., Text Encoder and Image Decoder) unchanged:; The above examples have been tested on a single NVIDIA GeForce RTX 3090 GPU with the following versions:; We removed several residual and attention blocks from the 0.86B-parameter U-Net in the 1.04B-param SDM-v1.4, and our compressed models are summarized as follows.",multimodel,text-to-image,https://huggingface.co/nota-ai/bk-sdm-tiny,Paper: https://arxiv.org/pdf/2305.15798.pdf,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 128,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.55KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
petrichor-SDXL-Finetuned-Fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		;   TO BE ADDED; 





; Unable to determine this model’s library. Check the
								docs 
.
							",multimodel,text-to-image,https://huggingface.co/thehive/petrichor-SDXL-Finetuned-Fp16,,License: other,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Ziya-Writing-LLaMa-13B-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"姜子牙写作大模型V1是基于LLaMa的130亿参数的指令微调模型，在写作任务上进行了能力增强，是专注于写作的大模型。姜子牙写作模型可以完成公文报告、讲稿书信、创意文案等多类的写作任务。; Ziya-Writing-LLaMa-13B-v1 is a 13-billion parameter instruction fine-tuned model based on LLaMa, which has been enhanced for better performance in writing tasks. It is a large model that focuses on writing. Ziya-Writing-LLaMa-13B-v1 can handle several types of writing tasks, including official reports, speeches, creative copywriting, and more.; 更多细节可以参考我们的公众号文章：; 姜子牙大模型系列 | 写作模型ziya-writing开源！开箱即用，快来认领专属你的写作小助手吧; 我们从网络中收集并清洗了大量真实的真人写作数据，利用GPT-3.5生成对应的写作指令，并进行了极为严格的人工校验。",natural-language-processing,text-generation,https://huggingface.co/IDEA-CCNL/Ziya-Writing-LLaMa-13B-v1,Paper: https://arxiv.org/pdf/2210.08590.pdf,License: gpl-3.0,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
RWKV-4-World-7B,,,,,,,,,"RWKV-4-World的Hugface格式，因新版World的tokenizer较之前Raven\Pile版本有较大变化，因而需要进行新版HF适配
ringrwkv兼容了原生rwkv库和transformers的rwkv库，同时新添入World版本的配置及代码（支持1.5B，3B，7B全系列），并修复了原HF的RWKV在
Forward RWKVOutput时的细微问题，主要是引入和明确last_hidden_state。以下是轻量级使用代码，比较方便：; RingRWKV GIT开源地址：https://github.com/StarRing2022/RingRWKV ; import torch
from ringrwkv.configuration_rwkv_world import RwkvConfig
from ringrwkv.rwkv_tokenizer import TRIE_TOKENIZER
from ringrwkv.modehf_world import RwkvForCausalLM; model = RwkvForCausalLM.from_pretrained(""StarRing2022/RWKV-4-World-7B"") #或将本模型下载至本地文件夹 
tokenizer = TRIE_TOKENIZER('./ringrwkv/rwkv_vocab_v20230424.txt'); text = ""你叫什么名字？""",,,https://huggingface.co/StarRing2022/RWKV-4-World-7B,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 15.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Evol-Replit-v1-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are Replit GGML format model files for Nick Roshdieh's Evol Replit v1.; Please note that these GGMLs are not compatible with llama.cpp, text-generation-webui or llama-cpp-python. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh.",,,https://huggingface.co/TheBloke/Evol-Replit-v1-GGML,,License: cc-by-sa-4.0,Datasets: nickrosh/Evol-Instruct-Code-80k-v1,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-13B-Chat-ggml,,,,,,,,,"From: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf; Quantized using an older version of llama.cpp and compatible with llama.cpp from May 19, commit 2d5db48.; Quantization methods compatible with latest llama.cpp from June 6, commit 2d43387.; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,https://huggingface.co/localmodels/Llama-2-13B-Chat-ggml,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
Llama-2-7b-chat-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations.",natural-language-processing,text-generation,https://huggingface.co/NousResearch/Llama-2-7b-chat-hf,,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 399,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 40.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-70b-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations.",natural-language-processing,text-generation,https://huggingface.co/NousResearch/Llama-2-70b-hf,,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 96,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 378.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mlc-chat-Llama-2-7b-chat-hf-q4f16_1,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-7b-hf-small-shards,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/abhishek/llama-2-7b-hf-small-shards,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 361,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
perfectLewdFantasy_v1.01,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,Model info :; https://civitai.com/models/111848?modelVersionId=121050; Sample image I made thru Huggingface's API :; ; Original Author's DEMO image :,multimodel,text-to-image,https://huggingface.co/digiplay/perfectLewdFantasy_v1.01,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 350,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
readflow-rwkv-4-world-ctx32k,,,,,,,,,"This is full finetuned model from RWKV 4 world 7B CHNTuned model using data from Readflow tech (readflow.com.cn) ,; finetuned for  32k context length, used to summary news article ; using inf-ctx training https://github.com/SynthiaDL/TrainChatGalRWKV with fixed VRAM; you can test summary prompt using RWKV runner(https://github.com/josStorer/RWKV-Runner) in chat mode , and check conversation files in examples folders.; https://discord.gg/pWH5MkvtNR",,,https://huggingface.co/xiaol/readflow-rwkv-4-world-ctx32k,,License: apache-2.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 15.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-13B-German-Assistant-v2-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for flozi00's Llama 2 13B German Assistant v2.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",natural-language-processing,text-generation,https://huggingface.co/TheBloke/llama-2-13B-German-Assistant-v2-GPTQ,,License: other,Datasets: flozi00/conversations,English; German,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 66,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ruGPT-3.5-13B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"GPTQ quantisation of https://huggingface.co/ai-forever/ruGPT-3.5-13B; Small perplexity test:
  before quantization - 'mean_perplexity': 10.241
  after quantization - 'mean_perplexity': 10.379; Data - RussianSuperGlue > DaNetQA/train.jsonl['passage']; As this is a hastily thrown together quant with no prior experience in quants, use https://huggingface.co/TheBloke version if he releases a quant for this model.",natural-language-processing,text-generation,https://huggingface.co/fffrrt/ruGPT-3.5-13B-GPTQ,,,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 333,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama2-qlora-finetunined-french,,,,,,,,,"The following bitsandbytes quantization config was used during training:; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/1littlecoder/llama2-qlora-finetunined-french,,,,,PEFT,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 134.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-70b-Guanaco-QLoRA-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are pytorch format fp16 model files for Mikael110's Llama2 70b Guanaco QLoRA.; It is the result of merging and/or converting the source repository to float16.; For further support, and discussions on these models and AI in general, join us at:",natural-language-processing,text-classification,https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16,,License: other,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 138.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama2-7b-chat-codeCherryPop-qLoRA-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GPTQ model files for TokenBender's Llama-2-7B-Chat Code Cherry Pop.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/llama2-7b-chat-codeCherryPop-qLoRA-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-70B-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for Meta's Llama 2 70B.; To use these files you need:; Example command:,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Llama-2-70B-GGML,Paper: https://arxiv.org/pdf/2307.09288.pdf,License: other,,English,Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 563.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mT5_multilingual_XLSum,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. For finetuning details and scripts,
see the paper and the official repository. ; Scores on the XL-Sum test sets are as follows:; If you use this model, please cite the following paper:",natural-language-processing,summarization,https://huggingface.co/csebuetnlp/mT5_multilingual_XLSum,,,Datasets: csebuetnlp/xlsum,43 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 145,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 20110,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 69
wav2vec2-base-960h,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Facebook's Wav2Vec2; The base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. When using the model
make sure that your speech input is also sampled at 16Khz.; Paper; Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli; Abstract",audio,automatic-speech-recognition,https://huggingface.co/facebook/wav2vec2-base-960h,Paper: https://arxiv.org/pdf/2006.11477.pdf,License: apache-2.0,Datasets: librispeech_asr,English,PyTorch; TensorFlow; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 139,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 610783,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 175
tapas-base-finetuned-wtq,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c477a4006f982ddfda4a_Table%20Question%20Answering.svg,,"This model has 2 versions which can be used. The default version corresponds to the tapas_wtq_wikisql_sqa_inter_masklm_base_reset checkpoint of the original Github repository.
This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).; The other (non-default) version which can be used is: ; Disclaimer: The team releasing TAPAS did not write a model card for this model so this model card has been written by
the Hugging Face team and contributors.; TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. 
This means it was pretrained on the raw tables and associated texts only, with no humans labelling them in any way (which is why it
can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language used in tables and associated texts, which can then be used 
to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed
or refuted by the contents of a table. Fine-tuning is done by adding a cell selection head and aggregation head on top of the pre-trained model, and then jointly train these randomly initialized classification heads with the base model on SQa, WikiSQL and finally WTQ. ",natural-language-processing,table-question-answering,https://huggingface.co/google/tapas-base-finetuned-wtq,Paper: https://arxiv.org/pdf/2004.02349.pdf; https://arxiv.org/pdf/2010.00571.pdf; https://arxiv.org/pdf/1508.00305.pdf,License: apache-2.0,Datasets: wikitablequestions,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 128,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 26962,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 886.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 93
multi-qa-mpnet-base-dot-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources. For an introduction to semantic search, have a look at: SBERT.net - Semantic Search; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.; In the following some technical details how this model must be used:",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1,,,Datasets: flax-sentence-embeddings/stackexchange_xml; ms_marco; gooaq; yahoo_answers_topics; search_qa; eli5; natural_questions; trivia_qa; embedding-data/QQP; embedding-data/PAQ_pairs; embedding-data/Amazon-QA; embedding-data/WikiAnswers,,PyTorch; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 101,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 739674,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 439.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 44
paraphrase-multilingual-mpnet-base-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2,Paper: https://arxiv.org/pdf/1908.10084.pdf,License: apache-2.0,,,PyTorch; TensorFlow; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 113,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 166986,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
spkrec-ecapa-voxceleb,,,,,,,,,"This repository provides all the necessary tools to perform speaker verification with a pretrained ECAPA-TDNN model using SpeechBrain. 
The system can be used to extract speaker embeddings as well. 
It is trained on Voxceleb 1+ Voxceleb2 training data. ; For a better experience, we encourage you to learn more about
SpeechBrain. The model performance on Voxceleb1-test set(Cleaned) is:; This system is composed of an ECAPA-TDNN model. It is a combination of convolutional and residual blocks. The embeddings are extracted using attentive statistical pooling. The system is trained with Additive Margin Softmax Loss.  Speaker Verification is performed using cosine distance between speaker embeddings.; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.",,,https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb,Paper: https://arxiv.org/pdf/2106.04624.pdf,License: apache-2.0,Datasets: voxceleb,English,speechbrain; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 78,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 504058,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 89.1MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 57
Geneformer,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Geneformer is a foundation transformer model pretrained on a large-scale corpus of ~30 million single cell transcriptomes to enable context-aware predictions in settings with limited data in network biology. ; See our manuscript for details.; Geneformer is a foundation transformer model pretrained on Genecorpus-30M, a pretraining corpus comprised of ~30 million single cell transcriptomes from a broad range of human tissues. We excluded cells with high mutational burdens (e.g. malignant cells and immortalized cell lines) that could lead to substantial network rewiring without companion genome sequencing to facilitate interpretation. Each single cell’s transcriptome is presented to the model as a rank value encoding where genes are ranked by their expression in that cell normalized by their expression across the entire Genecorpus-30M. The rank value encoding provides a nonparametric representation of that cell’s transcriptome and takes advantage of the many observations of each gene’s expression across Genecorpus-30M to prioritize genes that distinguish cell state. Specifically, this method will deprioritize ubiquitously highly-expressed housekeeping genes by normalizing them to a lower rank. Conversely, genes such as transcription factors that may be lowly expressed when they are expressed but highly distinguish cell state will move to a higher rank within the encoding. Furthermore, this rank-based approach may be more robust against technical artifacts that may systematically bias the absolute transcript counts value while the overall relative ranking of genes within each cell remains more stable. ; The rank value encoding of each single cell’s transcriptome then proceeds through six transformer encoder units. Pretraining was accomplished using a masked learning objective where 15% of the genes within each transcriptome were masked and the model was trained to predict which gene should be within each masked position in that specific cell state using the context of the remaining unmasked genes. A major strength of this approach is that it is entirely self-supervised and can be accomplished on completely unlabeled data, which allows the inclusion of large amounts of training data without being restricted to samples with accompanying labels.; We detail applications and results in our manuscript. ",natural-language-processing,fill-mask,https://huggingface.co/ctheodoris/Geneformer,,License: apache-2.0,Datasets: ctheodoris/Genecorpus-30M,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 67,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2515,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 41.2MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
yolos-small-finetuned-license-plate-detection,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c921e73e7b6d3bf06402_Object%20Detection.svg,,"This model is a fine-tuned version of hustvl/yolos-small on the licesne-plate-recognition dataset from Roboflow which contains 5200 images in the training set and 380 in the validation set.
The original YOLOS model was fine-tuned on COCO 2017 object detection (118k annotated images).; YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).; You can use the raw model for object detection. See the model hub to look for all available YOLOS models.; Here is how to use this model:; Currently, both the feature extractor and model support PyTorch.",computer-vision,object-detection,https://huggingface.co/nickmuchi/yolos-small-finetuned-license-plate-detection,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 22398,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 123.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
MagicPrompt-Stable-Diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a model from the MagicPrompt series of models, which are GPT-2 models intended to generate prompt texts for imaging AIs, in this case: Stable Diffusion.; This model was trained with 150,000 steps and a set of about 80,000 data filtered and extracted from the image finder for Stable Diffusion: ""Lexica.art"". It was a little difficult to extract the data, since the search engine still doesn't have a public API without being protected by cloudflare, but if you want to take a look at the original dataset, you can have a look here: datasets/Gustavosta/Stable-Diffusion-Prompts.; If you want to test the model with a demo, you can go to: ""spaces/Gustavosta/MagicPrompt-Stable-Diffusion"".; MIT; When using this model, please credit: Gustavosta",natural-language-processing,text-generation,https://huggingface.co/Gustavosta/MagicPrompt-Stable-Diffusion,,License: mit,,,PyTorch; Core ML; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 496,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 36451,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1023.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 124
whisper-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al from OpenAI. The original code repository can be found here.; Update: following the release of the paper, the Whisper authors announced a  large-v2 model trained for 2.5x more epochs with regularization. This  large-v2 model surpasses the performance of the large model, with no architecture changes. Thus, it is recommended that the  large-v2 model is used in-place of the original large model. ; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. ",audio,automatic-speech-recognition,https://huggingface.co/openai/whisper-large,Paper: https://arxiv.org/pdf/2212.04356.pdf,License: apache-2.0,,99 languages,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 310,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 14916,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 19.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 31
biogpt,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.; You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:; Here is how to use this model to get the features of a given text in PyTorch:; Beam-search decoding:; If you find BioGPT useful in your research, please cite the following paper:",natural-language-processing,text-generation,https://huggingface.co/microsoft/biogpt,,License: mit,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 152,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 35766,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 31
segformer_b2_clothes,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c914a5f52efbcfb97cd1_Image%20Segmentation.svg,,"SegFormer model fine-tuned on ATR dataset for clothes segmentation.
The dataset on hugging face is called ""mattmdjaga/human_parsing_dataset"".; Labels: 0: ""Background"", 1: ""Hat"", 2: ""Hair"", 3: ""Sunglasses"", 4: ""Upper-clothes"", 5: ""Skirt"", 6: ""Pants"", 7: ""Dress"", 8: ""Belt"", 9: ""Left-shoe"", 10: ""Right-shoe"", 11: ""Face"", 12: ""Left-leg"", 13: ""Right-leg"", 14: ""Left-arm"", 15: ""Right-arm"", 16: ""Bag"", 17: ""Scarf""; The license for this model can be found here.",computer-vision,image-segmentation,https://huggingface.co/mattmdjaga/segformer_b2_clothes,Paper: https://arxiv.org/pdf/2105.15203.pdf,License: mit,Datasets: mattmdjaga/human_parsing_dataset,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 46,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 112093,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 438.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
git-large-coco,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.; Disclaimer: The team releasing GIT did not write a model card for this model so this model card has been written by the Hugging Face team.; GIT is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using ""teacher forcing"" on a lot of (image, text) pairs.; The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens.; The model has full access to (i.e. a bidirectional attention mask is used for) the image patch tokens, but only has access to the previous text tokens (i.e. a causal attention mask is used for the text tokens) when predicting the next text token.",multimodel,image-to-text,https://huggingface.co/microsoft/git-large-coco,Paper: https://arxiv.org/pdf/2205.14100.pdf,License: mit,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 36,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 122834,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 33
Deliberate,,,,,,,,,"This model provides you the ability to create anything you want.
The more power of prompt knowledges you have, the better results you'll get.
It basically means that you'll never get a perfect result with just a few words.
You have to fill out your prompt line extremely detailed.
; Dive into the perfect creations world with my prompts.
Your research will be appreciated, so feel free to show everyone, what you can get with this model; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/XpucT/Deliberate,,,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 356,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 25129,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 11.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
GuoFeng3,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"欢迎使用GuoFeng3模型 - (TIP:这个版本的名字进行了微调),这是一个中国华丽古风风格模型，也可以说是一个古风游戏角色模型，具有2.5D的质感。第三代大幅度减少上手难度，增加了场景元素与男性古风人物，除此之外为了模型能更好的适应其它TAG，还增加了其它风格的元素。这一代对脸和手的崩坏有一定的修复，同时素材大小也提高到了最长边1024。; 根据个人的实验与收到的反馈，国风模型系列的第二代，在人物，与大头照的效果表现比三代更好，如果你有这方面需求不妨试试第二代。; 2.0版本：https://huggingface.co/xiaolxl/Gf_style2; GuoFeng3:原始模型; GuoFeng3.1:对GuoFeng3人像进行了微调修复",multimodel,text-to-image,https://huggingface.co/xiaolxl/GuoFeng3,,License: cc-by-nc-sa-4.0,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 429,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3400,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 37.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 45
BioGPT-Large-PubMedQA,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.; If you find BioGPT useful in your research, please cite the following paper:",natural-language-processing,text-generation,https://huggingface.co/microsoft/BioGPT-Large-PubMedQA,,License: mit,Datasets: pubmed_qa,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 60,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1525,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
upscaler,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/uwg/upscaler,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 130,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.48KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
so-vits-svc-4.0,,,,,,,,,"This is a collection of so-vits-svc-4.0 models made by the Pony Preservation Project using audio clips taken from MLP:FiM.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/therealvul/so-vits-svc-4.0,,,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 120,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.62KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
flan-ul2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"; Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the UL2 model  released earlier last year. It was fine tuned using the ""Flan"" prompt tuning 
and dataset collection.; According to the original blog here are the notable improvements:; You can use the convert_t5x_checkpoint_to_pytorch.py script and pass the argument strict = False. The final layer norm is missing from the original dictionnary, that is why we are passing the strict = False argument.; We used the same config file as google/ul2.",natural-language-processing,text2text-generation,https://huggingface.co/google/flan-ul2,Paper: https://arxiv.org/pdf/2205.05131.pdf,License: apache-2.0,Datasets: svakulenk0/qrecc; taskmaster2; djaym7/wiki_dialog; deepmind/code_contests; lambada; gsm8k; aqua_rat; esnli; quasc; qed; c4,5 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 481,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 69506,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 39.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 98
controlnet-sd21,,,,,,,,,"Want to support my work: you can bought my Artbook: https://thibaud.art ; Here's the first version of controlnet for stablediffusion 2.1
Trained on a subset of laion/laion-art; ; ; ",,,https://huggingface.co/thibaud/controlnet-sd21,,License: openrail++,Datasets: laion/laion-art,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 327,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 56.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Annotators,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/lllyasviel/Annotators,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 112,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 63
TemporalNet,,,,,,,,,"Introducing the Beta Version of TemporalNet; TemporalNet is a ControlNet model designed to enhance the temporal consistency of generated outputs, as demonstrated in this example: https://twitter.com/CiaraRowles1/status/1637486561917906944. While it does not eliminate all flickering, it significantly reduces it, particularly at higher denoise levels. For optimal results, it is recommended to use TemporalNet in combination with other methods.; Instructions for Use:; Add the model ""diff_control_sd15_temporalnet_fp16.safetensors"" to your models folder in the ControlNet extension in Automatic1111's Web UI.; Create a folder that contains:",,,https://huggingface.co/CiaraRowles/TemporalNet,,License: openrail,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 243,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 839,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 18.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chatgpt-gpt4-prompts-bart-large-cnn-samsum,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"This model generates ChatGPT/BingChat & GPT-3 prompts and is a fine-tuned version of philschmid/bart-large-cnn-samsum on an this dataset.
It achieves the following results on the evaluation set:; This model supports a Streamlit Web UI to run the chatgpt-gpt4-prompts-bart-large-cnn-samsum model:
; The following hyperparameters were used during training:",natural-language-processing,text2text-generation,https://huggingface.co/Kaludi/chatgpt-gpt4-prompts-bart-large-cnn-samsum,,License: mit,Datasets: fka/awesome-chatgpt-prompts,,TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 45,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3362,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
lametta,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,人的な好みの}柄を追求したマ`ジモデルで普段遣いのためにマ`ジしたものになります。かなり癖がいモデルじゃないかと思います。; モデル全体のA向として等身の低い女の子の描写に主眼をおいていますが、胸の大きなおさんもちゃんと出てくるようです。それ以外の出力もできるとは思いますがしていないので未知数です。; 人のこだわりでできるだけ目のハイライトを失わないようにマ`ジしてあります。指の描写にも荬蚴工盲郡膜猡辘扦工プロンプトなど次第でgに破`します。; Clip skipは１や２のお好みで。どちらにもそれぞれの魅力がある荬します。; VAEは何かしら外部のものを使用してください、サンプルはすべてAnythingのVAEを使用しています。人的に普段はclearVAEシリ`ズを使っていますオススメ。,multimodel,text-to-image,https://huggingface.co/Lasorco/lametta,,License: creativeml-openrail-m,,Japanese,Diffusers; Safetensors,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 61,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 21.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ControlNetMediaPipeFace,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,This dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.; Cherry-picked from ControlNet + Stable Diffusion v2.1 Base; Images with multiple faces are also supported:; Source images were generated by pulling slice 00000 from LAION Face and passing them through MediaPipe's face detector with special configuration parameters.  ; The colors and line thicknesses used for MediaPipe are as follows:,computer-vision,image-to-image,https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace,Paper: https://arxiv.org/pdf/2302.05543.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2210.08402.pdf,License: openrail,Datasets: LAION-Face; LAION,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 431,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4071,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
rwkv-4-raven,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"[UPDATE: Try RWKV-4-World (https://huggingface.co/BlinkDL/rwkv-4-world) for generation & chat & code in 100+ world languages, with great English zero-shot & in-context learning ability too.]; These are RWKV-4-Pile 1.5/3/7/14B models finetuned on Alpaca, CodeAlpaca, Guanaco, GPT4All, ShareGPT and more. Even the 1.5B model is surprisingly good for its size.; Gradio Demo: https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B and https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio; RWKV models inference: https://github.com/BlinkDL/ChatRWKV (fast CUDA).; Q8_0 models: only for https://github.com/saharNooby/rwkv.cpp (fast CPU).",natural-language-processing,text-generation,https://huggingface.co/BlinkDL/rwkv-4-raven,,License: apache-2.0,Datasets: the_pile,English,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 472,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 73.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 10
vicuna-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This repository contains an alternative version of the Vicuna 7B model.; This model was natively fine-tuned using ShareGPT data, but without the ""ethics"" filtering used for the original Vicuna.; A GPTQ quantised  4-bit version is available here.; Model type:
Vicuna is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
Vicuna was trained between March 2023 and April 2023.",natural-language-processing,text-generation,https://huggingface.co/AlekseyKorshuk/vicuna-7b,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 104,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1789,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 20
HimawariMixs,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"
          々なモデルをマ`ジした背景や部の表F力がいVAE内i型モデル
          VAEの内はないぞ！と言わせないぞ！！！！
          Models with built-in VAE with strong expression of backgrounds and details merging various models
        ; Twiter: @min__san",multimodel,text-to-image,https://huggingface.co/natsusakiyomi/HimawariMixs,,License: creativeml-openrail-m,,Japanese; English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 62,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
MiniGPT-4,,,,,,,,,"Deyao Zhu* (On Job Market!), Jun Chen* (On Job Market!), Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. *Equal Contribution; King Abdullah University of Science and Technology; Click the image to chat with MiniGPT-4 around your images
; More examples can be found in the project page.; ",,,https://huggingface.co/Vision-CAIR/MiniGPT-4,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 271,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 47.4MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 43
WizardLM-7B-Uncensored,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is WizardLM trained with a subset of the dataset - responses that contained alignment / moralizing were removed.  The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:  ; An uncensored model has no guardrails.  ; You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.",natural-language-processing,text-generation,https://huggingface.co/ehartford/WizardLM-7B-Uncensored,,License: other,Datasets: ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 308,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1633,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 20
Wizard-Vicuna-13B-Uncensored,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is wizard-vicuna-13b trained with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:  ; An uncensored model has no guardrails.  ; You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.",natural-language-processing,text-generation,https://huggingface.co/ehartford/Wizard-Vicuna-13B-Uncensored,,License: other,Datasets: ehartford/wizard_vicuna_70k_unfiltered,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 192,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2595,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 22
Wizard-Vicuna-7B-Uncensored-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Wizard-Vicuna-7B-Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GPTQ,,License: other,Datasets: ehartford/wizard_vicuna_70k_unfiltered,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 37,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2339,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
e5-small-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 12 layers and the embedding size is 384.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; Please refer to our paper at https://arxiv.org/pdf/2212.03533.pdf.; Check out unilm/e5 to reproduce evaluation results 
on the BEIR and MTEB benchmark.",multimodel,feature-extraction,https://huggingface.co/intfloat/e5-small-v2,Paper: https://arxiv.org/pdf/2212.03533.pdf; https://arxiv.org/pdf/2104.08663.pdf; https://arxiv.org/pdf/2210.07316.pdf,License: mit,,English,PyTorch; TensorFlow; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 702926,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 535.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
guanaco-65b,,,,,,,,,"| Paper | Code | Demo | ; The Guanaco models are open-source finetuned chatbots obtained through 4-bit QLoRA tuning of LLaMA base models on the OASST1 dataset. They are available in 7B, 13B, 33B, and 65B parameter sizes.; ??Guanaco is a model purely intended for research purposes and could produce problematic outputs.; Guanaco adapter weights are available under Apache 2 license. Note the use of the Guanaco adapter weights, requires access to the LLaMA model weighs. 
Guanaco is based on LLaMA and therefore should be used according to the LLaMA license. ; Here is an example of how you would load Guanaco 7B in 4-bits:",,,https://huggingface.co/timdettmers/guanaco-65b,Paper: https://arxiv.org/pdf/2305.14314.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2304.07327.pdf,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 66,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
guanaco-65B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Tim Dettmers' Guanaco 65B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/guanaco-65B-GPTQ,Paper: https://arxiv.org/pdf/2305.14314.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2304.07327.pdf,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 235,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3254,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 34.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
m3e-small,,,,,,,,,m3e-small | m3e-base; M3E 是 Moka Massive Mixed Embedding 的缩写; 说明：; Tips:; 您需要先安装 sentence-transformers,,,https://huggingface.co/moka-ai/m3e-small,,,,Chinese,PyTorch; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2139,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 96.3MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
kandinsky-2-2-decoder,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.2 is available in diffusers!; ",multimodel,text-to-image,https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder,,License: apache-2.0,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8958,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.64KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
kandinsky-2-2-prior,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.2 is available in diffusers!; ",multimodel,text-to-image,https://huggingface.co/kandinsky-community/kandinsky-2-2-prior,,License: apache-2.0,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 16294,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 16.47KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
myLyCORIS,,,,,,,,,"仅供学习，请勿用于任何商业活动，不授权给任何商业行为。如果有没有使用说明的模型，请尝试在civitai搜索@Saya，可以找到过去的模型说明
For learning only, please do not use for any commercial activities, not authorized to any commercial activities. If there is a model without instructions, try searching @Saya in civiti, you can find past model instructions; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/Saya3091/myLyCORIS,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 57,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 0.5KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardCoder-15B-1.0-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for WizardLM's WizardCoder 15B 1.0.; It is the result of quantising to 4bit using AutoGPTQ.; Please make sure you're using the latest version of text-generation-webui,natural-language-processing,text-generation,https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GPTQ,,License: bigcode-openrail-m,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 125,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7177,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
baichuan-vicuna-chinese-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,baichuan-vicuna-chinese-7b是在中英双语sharegpt数据上全参数微调的对话模型。; baichuan-vicuna-chinese-7b is a chat model supervised finetuned on vicuna sharegpt data in both English and Chinese.; [NEW] 4bit-128g GPTQ量化版本：baichuan-vicuna-chinese-7b-gptq; Inference with FastChat:; Inference with Transformers:,natural-language-processing,text-generation,https://huggingface.co/fireballoon/baichuan-vicuna-chinese-7b,,,Datasets: anon8231489123/ShareGPT_Vicuna_unfiltered; QingyiSi/Alpaca-CoT; mhhmm/leetcode-solutions-python,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 50,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1138,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mpt-30b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.
This model was trained by MosaicML.; MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.; MPT-30B comes with special features that differentiate it from other LLMs, including an 8k token context window (which can be further extended via finetuning; see MPT-7B-StoryWriter), support for context-length extrapolation via ALiBi, and efficient inference + training via FlashAttention. It also has strong coding abilities thanks to its pretraining mix. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer.
The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPU―either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision.; This model uses the MosaicML LLM codebase, which can be found in the llm-foundry repository. It was trained by MosaicML’s NLP team on the MosaicML platform for LLM pretraining, finetuning, and inference.; MPT-30B is:",natural-language-processing,text-generation,https://huggingface.co/mosaicml/mpt-30b,Paper: https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2010.04245.pdf; https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2302.06675.pdf,License: apache-2.0,Datasets: allenai/c4; mc4; togethercomputer/RedPajama-Data-1T; bigcode/the-stack-dedup; allenai/s2orc,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 292,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 73345,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
superhot-13b-8k-no-rlhf-test,,,,,,,,,"This is a second prototype of SuperHOT, a NSFW focused LoRA, this time with 4K context and no RLHF. In my testing, it can go all the way to 6K without breaking down and I made the change with intention to reach 8K, so I'll assume it will go to 8K although I only trained on 4K sequences.; You will NEED to apply the monkeypatch or, if you are already using the monkeypatch, change the scaling factor to 0.25 and the maximum sequence length to 8192; In order to use the 8K context, you will need to apply the monkeypatch I have added in this repo or follow the instructions for oobabooga's text-generation-webui -- without it, it will not work.; I will repeat: Without the patch with the correct scaling and max sequence length, it will not work!; The patch is very simple, and you can make the changes yourself:",,,https://huggingface.co/kaiokendev/superhot-13b-8k-no-rlhf-test,,License: mit,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 50,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.6MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
mpt-30b-instruct,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MPT-30B-Instruct is a model for short-form instruction following.
It is built by finetuning MPT-30B on Dolly HHRLHF derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets. It is also trained on Competition Math, Duorc, CoT GSM8k, Qasper, Quality, Summ Screen FD and Spider.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; June 22, 2023; CC-By-SA-3.0; Bespokenizer46",natural-language-processing,text-generation,https://huggingface.co/mosaicml/mpt-30b-instruct,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf,License: cc-by-sa-3.0,Datasets: competition_math; conceptofmind/cot_submix_original/cot_gsm8k; knkarthick/dialogsum; mosaicml/dolly_hhrlhf; duorc; tau/scrolls/qasper; emozilla/quality; scrolls/summ_screen_fd; spider,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 87,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 64762,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
DragGan-Models,,,,,,,,,"More about Models here https://github.com/XingangPan/DragGAN; https://arxiv.org/abs/2305.10973; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/DragGan/DragGan-Models,Paper: https://arxiv.org/pdf/2305.10973.pdf,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
opencoderplus,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"OpenChat is a series of open-source language models fine-tuned on a diverse and high-quality dataset of multi-round conversations. With only ~6K GPT-4 conversations filtered from the ~90K ShareGPT conversations, OpenChat is designed to achieve high performance with limited data.; Generic models:; Code models:; Note: Please load the pretrained models using bfloat16; We provide the full source code, including an inference server compatible with the ""ChatCompletions"" API, in the OpenChat GitHub repository.",natural-language-processing,text-generation,https://huggingface.co/openchat/opencoderplus,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 97,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1494,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 32.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
replit-code-instruct-glaive,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/sahil2801/replit-code-instruct-glaive,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 74,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4570,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
internlm-chat-7b-8k,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"; ?Reporting Issues; InternLM has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:; We conducted a comprehensive evaluation of InternLM using the open-source evaluation tool OpenCompass. The evaluation covered five dimensions of capabilities: disciplinary competence, language competence, knowledge competence, inference competence, and comprehension competence. Here are some of the evaluation results, and you can visit the OpenCompass leaderboard for more evaluation results.; Limitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.",multimodel,feature-extraction,https://huggingface.co/internlm/internlm-chat-7b-8k,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 33,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2133,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 15.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard Vicuna 7B Uncensored.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-SuperHOT-8K-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 36.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Pygmalion-7B-SuperHOT-8K-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These are GGML model files for TehVenom's merge of Pygmalion 7B merged with Kaio Ken's SuperHOT 8K.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Pygmalion-7B-SuperHOT-8K-GGML,,License: other,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 36.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardCoder-Guanaco-15B-V1.1-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for LoupGarou's WizardCoder-Guanaco-15B-V1.1.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/WizardCoder-Guanaco-15B-V1.1-GPTQ,,License: apache-2.0,Datasets: guanaco,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 354,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
animatediff,,,,,,,,,"This model repo is for AnimateDiff.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/guoyww/animatediff,,License: apache-2.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
codet5p-110m-embedding,,,,,,,,,"CodeT5+ is a new family of open code large language models
with an encoder-decoder architecture that can flexibly operate in different modes (i.e. encoder-only, decoder-only,
and encoder-decoder) to support a wide range of code understanding and generation tasks.
It is introduced in the paper:; CodeT5+: Open Code Large Language Models for Code Understanding and Generation
by Yue Wang*, Hung Le*, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi (*
indicates equal contribution).; Compared to the original CodeT5 family (base: 220M, large: 770M), CodeT5+ is pretrained with a diverse set of
pretraining tasks including span denoising, causal language modeling, contrastive learning, and text-code
matching to learn rich representations from both unimodal code data and bimodal code-text data.
Additionally, it employs a simple yet effective compute-efficient pretraining method to initialize the model
components with frozen off-the-shelf LLMs such as CodeGen to efficiently scale
up the model (i.e. 2B, 6B, 16B), and adopts a ""shallow encoder and deep decoder"" architecture.
Furthermore, it is instruction-tuned to align with natural language instructions (see our InstructCodeT5+ 16B)
following Code Alpaca.; This checkpoint consists of an encoder of CodeT5+ 220M model (pretrained from 2 stages on both unimodal and bimodal) and a projection layer, which can be used to extract code
embeddings of 256 dimension. It can be easily loaded using the AutoModel functionality and employs the
same CodeT5 tokenizer.; This checkpoint is trained on the stricter permissive subset of the deduplicated version of
the github-code dataset.
The data is preprocessed by reserving only permissively licensed code (""mit"" “apache-2”, “bsd-3-clause”, “bsd-2-clause”,
“cc0-1.0”, “unlicense”, “isc”).
Supported languages (9 in total) are as follows:
c, c++, c-sharp,  go, java, javascript,  php, python, ruby.",,,https://huggingface.co/Salesforce/codet5p-110m-embedding,Paper: https://arxiv.org/pdf/2305.07922.pdf,License: bsd-3-clause,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 551,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 441.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
replit-openorca-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are Replit GGML format model files for Matorus's Replit OpenOrca.; Please note that these GGMLs are not compatible with llama.cpp, text-generation-webui or llama-cpp-python. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh.",,,https://huggingface.co/TheBloke/replit-openorca-GGML,,,Datasets: Open-Orca/OpenOrca,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ToraFurryMix,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"このモデルは「furry」タグを使用することを念^に{整したモデルです。
「furry」タグなしでの出力はあまり试^していません。; ; ; 作者
?twitter: @TlanoAI; Unable to determine this model’s library. Check the
								docs 
.
							",multimodel,text-to-image,https://huggingface.co/tlano/ToraFurryMix,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Tora-NijiFurry-LoRA,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"チビケモ化LoRAです。
学デ`タはNijiJourneyの出力画像のみです。

抗はm用するモデルの「furry」タグ特性に依存します。

未m用状Bで「furry」タグの抗がほとんどない龊稀
あまり良いY果は得られないかもしれません。; Training Model:
?sdhk_v40.safetensors (https://civitai.com/models/82813/sdhk)
Trigger Words:
?furry; 作者
?twitter: @TlanoAI; Unable to determine this model’s library. Check the
								docs 
.
							",multimodel,text-to-image,https://huggingface.co/tlano/Tora-NijiFurry-LoRA,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 37.9MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-13B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"From: https://huggingface.co/meta-llama/Llama-2-13b-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta",natural-language-processing,text-generation,https://huggingface.co/localmodels/Llama-2-13B-GPTQ,,,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 67,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-13b-chat-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations.",natural-language-processing,text-generation,https://huggingface.co/NousResearch/Llama-2-13b-chat-hf,,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 292,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-7b-guanaco,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Model fine-tuned in 4-bit precision using QLoRA on timdettmers/openassistant-guanaco with weights merged after training.; Made using this Google Colab notebook.; It can be easily imported using the AutoModelForCausalLM class from transformers:,natural-language-processing,text-generation,https://huggingface.co/mlabonne/llama-2-7b-guanaco,,License: apache-2.0,Datasets: timdettmers/openassistant-guanaco,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 536,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-13b-guanaco-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,This is a Llama-2 version of Guanaco. It was finetuned from the base Llama-13b model using the official training scripts found in the QLoRA repo. I wanted it to be as faithful as possible and therefore changed nothing in the training script beyond the model it was pointing to. The model prompt is therefore also the same as the original Guanaco model.; This repo contains the merged f16 model. The QLoRA adaptor can be found here.; A 7b version of the model can be found here.; Legal Disclaimer: This model is bound by the usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.,natural-language-processing,text-classification,https://huggingface.co/Mikael110/llama-2-13b-guanaco-fp16,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 78,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
waifujourney-xl,,,,,,,,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Please for anyone, don't reupload my finetuned models to other site, like Civitai or image generating site like Seeart, Yodayo or other sites. Thank you for understanding this. ; Like my works and want to collaboration or funding my projects? contact me.  ; Finetuned WDXL-aesthetic-0.9 with images generated from Nijijourney.  ",,,https://huggingface.co/gmonsoon/waifujourney-xl,,License: other,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LECO,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/SenY/LECO,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 163.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
MythoBoros-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MythoBoros-13b can be considered a sister model to MythoLogic-13b, sharing the same goals but having a different approach.; Whereas the previous model was a series of experimental gradient merges, this one is a simple straight-up 66/34 merge of Chronos and the freshly released Ouroboros, providing a very solid foundation for a well-performing roleplaying model.; MythoBoros tends to be somewhat more formal with its responses in comparison to MythoLogic.; My advice? Try both, see which one you prefer.; Quantized models are available from TheBloke: GGML - GPTQ (You're the best!)",natural-language-processing,text-generation,https://huggingface.co/Gryphe/MythoBoros-13b,,License: other,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
leco_puffy_nipples,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/doll774/leco_puffy_nipples,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.6MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Chinese-Llama-2-7b-4bit,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"全部开源，完全可商用的中文版 Llama2 模型及中英文 SFT 数据集，输入格式严格遵循 llama-2-chat 格式，兼容适配所有针对原版 llama-2-chat 模型的优化。; ; ; Talk is cheap, Show you the Demo.; 模型下载：Chinese Llama2 Chat Model",natural-language-processing,text-generation,https://huggingface.co/LinkSoul/Chinese-Llama-2-7b-4bit,,License: openrail,Datasets: LinkSoul/instruction_merge_set,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
distilbert-base-uncased-finetuned-sst-2-english,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Model Description: This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2.
This model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).; Example of single-label classification:
??; This model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.; The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.; Based on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.",natural-language-processing,text-classification,https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english,Paper: https://arxiv.org/pdf/1910.01108.pdf,License: apache-2.0,Datasets: sst2; glue,English,PyTorch; TensorFlow; Rust; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 263,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3297551,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 188
gpt2-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Model Description: GPT-2 Large is the 774M parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective. ; Use the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:; Here is how to use this model to get the features of a given text in PyTorch:; and in TensorFlow:; In their model card about GPT-2, OpenAI wrote: ",natural-language-processing,text-generation,https://huggingface.co/gpt2-large,Paper: https://arxiv.org/pdf/1910.09700.pdf,License: mit,,English,PyTorch; TensorFlow; JAX; Rust; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 141,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 446660,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 16.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 82
openai-gpt,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Model Description: openai-gpt is a transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long range dependencies.; Use the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:; Here is how to use this model in PyTorch:; and in TensorFlow:; This model can be used for language modeling tasks.",natural-language-processing,text-generation,https://huggingface.co/openai-gpt,Paper: https://arxiv.org/pdf/1705.11168.pdf; https://arxiv.org/pdf/1803.02324.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: mit,,English,PyTorch; TensorFlow; Rust; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 161,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 237593,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 22
t5-small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-Small is the checkpoint with 60 million parameters. ; The developers write in a blog post that the model: ",natural-language-processing,translation,https://huggingface.co/t5-small,Paper: https://arxiv.org/pdf/1805.12471.pdf; https://arxiv.org/pdf/1708.00055.pdf; https://arxiv.org/pdf/1704.05426.pdf; https://arxiv.org/pdf/1606.05250.pdf; https://arxiv.org/pdf/1808.09121.pdf; https://arxiv.org/pdf/1810.12885.pdf; https://arxiv.org/pdf/1905.10044.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,Datasets: c4,5 languages,PyTorch; TensorFlow; JAX; Rust; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 129,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1772137,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 205
personaGPT,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"PersonaGPT is an open-domain conversational agent designed to do 2 tasks:; It builds on the DialoGPT-medium pretrained model based on the GPT-2 architecture. 
This model is trained on the Persona-Chat dataset, with added special tokens to better distinguish between conversational history and personality traits for dyadic conversations. Furthermore, some active learning was used to train the model to do controlled decoding using turn-level goals.; Preprocessing, training and implementation details can be found in the personaGPT repo.; Example of personalized decoding:; Example of controlled response generation: ",natural-language-processing,conversational,https://huggingface.co/af1tang/personaGPT,Paper: https://arxiv.org/pdf/1801.07243.pdf,License: gpl-3.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 76,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8791,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
wav2vec2-lg-xlsr-en-speech-emotion-recognition,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0941358df9b5cffb0d11b_Audio%20Classification.svg,,"The model is a fine-tuned version of jonatasgrosman/wav2vec2-large-xlsr-53-english for a Speech Emotion Recognition (SER) task.; The dataset used to fine-tune the original pre-trained model is the RAVDESS dataset. This dataset provides 1440 samples of recordings from actors performing on 8 different emotions in English, which are:; It achieves the following results on the evaluation set:; More information needed; More information needed",audio,audio-classification,https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition,,License: apache-2.0,,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 70,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 46007,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 14
chinese-roberta-wwm-ext,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"For further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking. ; Pre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu; This repository is developed based on：https://github.com/google-research/bert; You may also interested in,; More resources by HFL: https://github.com/ymcui/HFL-Anthology",natural-language-processing,fill-mask,https://huggingface.co/hfl/chinese-roberta-wwm-ext,Paper: https://arxiv.org/pdf/1906.08101.pdf; https://arxiv.org/pdf/2004.13922.pdf,License: apache-2.0,,Chinese,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 163,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 42057,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
trocr-base-handwritten,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.",multimodel,image-to-text,https://huggingface.co/microsoft/trocr-base-handwritten,Paper: https://arxiv.org/pdf/2109.10282.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 63,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 368370,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 93
bert-base-multilingual-uncased-sentiment,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).; This model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above, or for further finetuning on related sentiment analysis tasks.; Here is the number of product reviews we used for finetuning the model: ; The finetuned model obtained the following accuracy on 5,000 held-out product reviews in each of the languages:; In addition to this model, NLP Town offers custom, monolingual sentiment models for many languages and an improved multilingual model through RapidAPI. ",natural-language-processing,text-classification,https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment,,License: mit,,6 languages,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 150,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4041279,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 37
xlm-roberta-base-language-detection,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset.; This model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output). 
For additional information please refer to the xlm-roberta-base model card or to the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al.; You can directly use this model as a language detector, i.e. for sequence classification tasks. Currently, it supports the following 20 languages: ; arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh); The model was fine-tuned on the Language Identification dataset, which consists of text sequences in 20 languages. The training set contains 70k samples, while the validation and test sets 10k each. The average accuracy on the test set is 99.6% (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table.",natural-language-processing,text-classification,https://huggingface.co/papluca/xlm-roberta-base-language-detection,Paper: https://arxiv.org/pdf/1911.02116.pdf,License: mit,,21 languages,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 114,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 101427,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
sn-xlm-roberta-base-snli-mnli-anli-xnli,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"A Siamese network model trained for zero-shot and few-shot text classification.; The base model is xlm-roberta-base.
It was trained on SNLI, MNLI, ANLI and XNLI.; This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:",natural-language-processing,sentence-similarity,https://huggingface.co/symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli,,,Datasets: SNLI; MNLI; ANLI; XNLI,13 languages,PyTorch; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 51,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2510,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
gpt-neox-20b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"GPT-NeoX-20B is a 20 billion parameter autoregressive language model trained 
on the Pile using the GPT-NeoX 
library. Its architecture intentionally 
resembles that of GPT-3, and is almost identical to that of GPT-J-
6B. Its training dataset contains 
a multitude of English-language texts, reflecting the general-purpose nature 
of this model. See the accompanying paper 
for details about model architecture (including how it differs from GPT-3), 
training procedure, and additional evaluations.; GPT-NeoX-20B was developed primarily for research purposes. It learns an inner 
representation of the English language that can be used to extract features 
useful for downstream tasks.; In addition to scientific uses, you may also further fine-tune and adapt 
GPT-NeoX-20B for deployment, as long as your use is in accordance with the 
Apache 2.0 license. This model works with the Transformers 
Library. If you decide to use 
pre-trained GPT-NeoX-20B as a basis for your fine-tuned model, please note that 
you need to conduct your own risk and bias assessment. ; GPT-NeoX-20B is not intended for deployment as-is. It is not a product 
and cannot be used for human-facing interactions without supervision.; GPT-NeoX-20B has not been fine-tuned for downstream tasks for which language 
models are commonly deployed, such as writing genre prose, or commercial 
chatbots. This means GPT-NeoX-20B will likely not respond to a given prompt 
the way products such as ChatGPT do. This is because, unlike GPT-NeoX-20B, 
ChatGPT was fine-tuned using methods such as Reinforcement Learning from Human 
Feedback (RLHF) to better “understand” human instructions and dialogue.",natural-language-processing,text-generation,https://huggingface.co/EleutherAI/gpt-neox-20b,Paper: https://arxiv.org/pdf/2204.06745.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2201.07311.pdf; https://arxiv.org/pdf/2104.09864.pdf,License: apache-2.0,Datasets: EleutherAI/pile,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 410,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 69177,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 40.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 89
bloom-560m,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Version 1.0 / 26.May.2022; This section provides information for anyone who wants to know about the model.; Developed by: BigScience (website); Model Type: Transformer-based Language Model; Version: 1.0.0,natural-language-processing,text-generation,https://huggingface.co/bigscience/bloom-560m,Paper: https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2110.02861.pdf; https://arxiv.org/pdf/2108.12409.pdf,License: bigscience-bloom-rail-1.0,,48 languages,PyTorch; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 228,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1721715,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 45
biomedical-ner-all,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased; Checkout the tutorial video for explanation of this model and corresponding python library: https://youtu.be/xpiDPdBpS18; The easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.; This model is part of the Research topic ""AI in Biomedical field"" conducted by Deepak John Reji, Shaina Raza. If you use this work (code, model or dataset), please star at:; https://github.com/dreji18/Bio-Epidemiology-NER",natural-language-processing,token-classification,https://huggingface.co/d4data/biomedical-ner-all,,License: apache-2.0,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 81,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 70586,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 533.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 87
stable-diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
This model card gives an overview of all available model checkpoints. For more in-detail model cards, please have a look at the model repositories listed under Model Access.; For the first version 4 model checkpoints are released.
Higher versions have been trained for longer and are thus usually better in terms of image generation quality then lower versions. More specifically: ; Each checkpoint can be used both with Hugging Face's  ? Diffusers library or the original Stable Diffusion GitHub repository. Note that you have to ""click-request"" them on each respective model repository.; To quickly try out the model, you can try out the Stable Diffusion Space.; The CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.",multimodel,text-to-image,https://huggingface.co/CompVis/stable-diffusion,Paper: https://arxiv.org/pdf/2207.12598.pdf,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 877,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 144.72KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 71
mDeBERTa-v3-base-xnli-multilingual-nli-2mil7,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c42cd649af0365a01a1e_Zero-Shot%20Classification.svg,,"This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying mDeBERTa-v3-base model was pre-trained by Microsoft on the CC100 multilingual dataset with 100 languages. The model was then fine-tuned on the XNLI dataset and on the multilingual-NLI-26lang-2mil7 dataset. Both datasets contain more than 2.7 million hypothesis-premise pairs in 27 languages spoken by more than 4 billion people. ; As of December 2021, mDeBERTa-v3-base is the best performing multilingual base-sized transformer model introduced by Microsoft in this paper. ; This model was trained on the multilingual-nli-26lang-2mil7 dataset and the XNLI validation dataset. ; The multilingual-nli-26lang-2mil7 dataset contains 2 730 000 NLI hypothesis-premise pairs in 26 languages spoken by more than 4 billion people. The dataset contains 105 000 text pairs per language. It is based on the English datasets MultiNLI, Fever-NLI, ANLI, LingNLI and WANLI and was created using the latest open-source machine translation models. The languages in the dataset are: ['ar', 'bn', 'de', 'es', 'fa', 'fr', 'he', 'hi', 'id', 'it', 'ja', 'ko', 'mr', 'nl', 'pl', 'ps', 'pt', 'ru', 'sv', 'sw', 'ta', 'tr', 'uk', 'ur', 'vi', 'zh'] (see ISO language codes. For more details, see the datasheet. In addition, a sample of 105 000 text pairs was also added for English following the same sampling method as the other languages, leading to 27 languages. ; Moreover, for each language a random set of 10% of the hypothesis-premise pairs was added where an English hypothesis was paired with the premise in the other language (and the same for English premises and other language hypotheses). This mix of languages in the text pairs should enable users to formulate a hypothesis in English for a target text in another language. ",natural-language-processing,zero-shot-classification,https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7,Paper: https://arxiv.org/pdf/2111.09543.pdf; https://arxiv.org/pdf/2104.07179.pdf; https://arxiv.org/pdf/1809.05053.pdf; https://arxiv.org/pdf/1911.02116.pdf,License: mit,Datasets: MoritzLaurer/multilingual-NLI-26lang-2mil7; xnli; multi_nli; anli; fever; lingnli; alisawuffles/WANLI,27 languages,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 74,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 37682,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
pegasus-x-large-book-summary,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"Get SparkNotes-esque summaries of arbitrary text! Due to the model size, it's recommended to try it out in Colab (linked above) as the API textbox may time out.; This model is a fine-tuned version of google/pegasus-x-large on the kmfoda/booksum dataset for approx eight epochs.; More information needed; TODO; The following hyperparameters were used during training:",natural-language-processing,summarization,https://huggingface.co/pszemraj/pegasus-x-large-book-summary,,License: bsd-3-clause,Datasets: kmfoda/booksum,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1735,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
waifu-diffusion-v1-4,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"; masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck; Waifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
The CreativeML OpenRAIL License specifies: ; This model can be used for entertainment purposes and as a generative art assistant.",multimodel,text-to-image,https://huggingface.co/hakurei/waifu-diffusion-v1-4,,License: creativeml-openrail-m,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 975,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 11
sd-vae-ft-mse,,,,,,,,,"These weights are intended to be used with the ? diffusers library. If you are looking for the model to use with the original CompVis Stable Diffusion codebase, come here.; You can integrate this fine-tuned VAE decoder to your existing diffusers workflows, by including a vae argument to the StableDiffusionPipeline; We publish two kl-f8 autoencoder versions, finetuned from the original kl-f8 autoencoder on a 1:1 ratio of LAION-Aesthetics and LAION-Humans, an unreleased subset containing only SFW images of humans. The intent was to fine-tune on the Stable Diffusion training set (the autoencoder was originally trained on OpenImages) but also enrich the dataset with images of humans to improve the reconstruction of faces.
The first, ft-EMA, was resumed from the original checkpoint, trained for 313198 steps and uses EMA weights. It uses the same loss configuration as the original checkpoint (L1 + LPIPS).
The second, ft-MSE, was resumed from ft-EMA and uses EMA weights and was trained for another 280k steps using a different loss, with more emphasis 
on MSE reconstruction (MSE + 0.1 * LPIPS). It produces somewhat ``smoother'' outputs. The batch size for both versions was 192 (16 A100s, batch size 12 per GPU).
To keep compatibility with existing models, only the decoder part was finetuned; the checkpoints can be used as a drop-in replacement for the existing autoencoder.; Original kl-f8 VAE vs f8-ft-EMA vs f8-ft-MSE; Visualization of reconstructions on  256x256 images from the COCO2017 validation dataset. ",,,https://huggingface.co/stabilityai/sd-vae-ft-mse,,License: mit,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 189,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1814228,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 670.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 23
stable-diffusion-2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This model card focuses on the model associated with the Stable Diffusion v2 model, available here.; This stable-diffusion-2 model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on 768x768 images.; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model",multimodel,text-to-image,https://huggingface.co/stabilityai/stable-diffusion-2,Paper: https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: openrail++,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.58k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 100620,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 478
stable-diffusion-2-inpainting,,,,,,,,,"This model card focuses on the model associated with the Stable Diffusion v2, available here.; This stable-diffusion-2-inpainting model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model",,,https://huggingface.co/stabilityai/stable-diffusion-2-inpainting,Paper: https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: openrail++,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 330,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 124059,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 60
mdeberta-v3-base-squad2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,"It has been finetuned for 3 epochs on SQuAD2.0.; DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. ; In DeBERTa V3, we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our paper.; Please check the official repository for more implementation details and updates.; mDeBERTa is multilingual version of DeBERTa which use the same structure as DeBERTa and was trained with CC100 multilingual data.
The mDeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has 86M backbone parameters  with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer.  This model was trained using the 2.5T CC100 data as XLM-R.",natural-language-processing,question-answering,https://huggingface.co/timpal0l/mdeberta-v3-base-squad2,Paper: https://arxiv.org/pdf/2006.03654.pdf; https://arxiv.org/pdf/2111.09543.pdf,License: mit,Datasets: squad_v2,94 languages,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 60,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 29820,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
santacoder,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; Play with the model on the SantaCoder Space Demo.; The SantaCoder models are a series of 1.1B parameter models trained on the Python, Java, and JavaScript subset of The Stack (v1.1) (which excluded opt-out requests). 
The main model uses Multi Query Attention, a context window of 2048 tokens, and was trained using near-deduplication and comment-to-code ratio as filtering criteria and using the Fill-in-the-Middle objective.
In addition there are several models that were trained on datasets with different filter parameters and with architecture and objective variations. ; The final model is the best performing model and was trained twice as long (236B tokens) as the others. This checkpoint is the default model and available on the main branch. All other checkpoints are on separate branches with according names.; The model was trained on GitHub code. As such it is not an instruction model and commands like ""Write a function that computes the square root."" do not work well.
You should phrase commands like they occur in source code such as comments (e.g. # the following function computes the sqrt) or write a function signature and docstring and let the model complete the function body.",natural-language-processing,text-generation,https://huggingface.co/bigcode/santacoder,Paper: https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2207.14255.pdf; https://arxiv.org/pdf/2301.03988.pdf,License: bigcode-openrail-m,Datasets: bigcode/the-stack,code,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 280,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 16301,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 39
Randeng-Deltalm-362M-En-Zh,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,"使用封神框架基于 Detalm base 进行finetune ，搜集的中英数据集（共3千万条）以及 iwslt的中英平行数据（20万），得到 英-> 中方向的翻译模型; Using the Fengshen-LM framework and finetuning based on detalm , get a translation model in the English -> Chinese direction; 参考论文：DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders; 如果您在您的工作中使用了我们的模型，可以引用我们的论文：; If you are using the resource for your work, please cite the our paper:",natural-language-processing,translation,https://huggingface.co/IDEA-CCNL/Randeng-Deltalm-362M-En-Zh,Paper: https://arxiv.org/pdf/2106.13736.pdf; https://arxiv.org/pdf/2209.02970.pdf,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1977,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 731.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
pygmalion-1.3b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"Pymalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped.; Warning: This model is NOT suitable for use by minors. It will output X-rated content under certain circumstances.; The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations.; Fine-tuning was done using ColossalAI (specifically, with a slightly modified version of their OPT fine-tune example) for around 11.4 million tokens over 5440 steps on a single 24GB GPU. The run took just under 21 hours.; We provide a notebook with a Gradio UI for playing around with the model without having to manually format inputs. This notebook can be found here.",natural-language-processing,conversational,https://huggingface.co/PygmalionAI/pygmalion-1.3b,,License: agpl-3.0,,English,PyTorch; TensorBoard; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 47,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9639,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
Anything-V4.5,,,,,,,,,"Original Model: https://huggingface.co/andite/anything-v4.0; Simply merged Anything-V4.5 with Anything-V4.0 VAE. 
Both checkpoint and safetensors versions are avaliable to download; All credit goes to the original uploader: andite
; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/Airic/Anything-V4.5,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 31,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
StreetCLIP,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c6145e63a23d3f282ad5_Zero-Shot%20Image%20Classification.svg,,"StreetCLIP is a robust foundation model for open-domain image geolocalization and other
geographic and climate-related tasks.; Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves
state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, 
outperforming supervised models trained on millions of images.; StreetCLIP is a model pretrained by deriving image captions synthetically from image class labels using
a domain-specific caption template. This allows StreetCLIP to transfer its generalized zero-shot learning
capabilities to a specific domain (i.e. the domain of image geolocalization). 
StreetCLIP builds on the OpenAI's pretrained large version of CLIP ViT, using 14x14 pixel
patches and images with a 336 pixel side length.; StreetCLIP has a deep understanding of the visual features found in street-level urban and rural scenes
and knows how to relate these concepts to specific countries, regions, and cities. Given its training setup,
the following use cases are recommended for StreetCLIP.; StreetCLIP can be used out-of-the box using zero-shot learning to infer the geolocation of images on a country, region,
or city level. Given that StreetCLIP was pretrained on a dataset of street-level urban and rural images,
the best performance can be expected on images from a similar distribution.",computer-vision,zero-shot-image-classification,https://huggingface.co/geolocal/StreetCLIP,Paper: https://arxiv.org/pdf/2302.00275.pdf,License: cc-by-nc-4.0,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2114,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
bad-hands-5,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/yesyeahvh/bad-hands-5,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 234,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.56KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
LoRA,,,,,,,,,"Silder: skinny
; Slider: short legs
; Concepts: Breathing fire
; This LyCORIS is blockweighted.
breathing_fire_full.safetensors is full version.
The full version has the same effect as the general version by applying the following settings in Lora Block Weight.; Backgrounds: ""Danchi"" is Japanese public housing complex.",,,https://huggingface.co/SenY/LoRA,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 121,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
website_classification,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is a fine-tuned version of distilbert-base-uncased on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",natural-language-processing,text-classification,https://huggingface.co/alimazhar-110/website_classification,,License: apache-2.0,,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3898187,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 269.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
specter2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"SPECTER 2.0 is the successor to SPECTER and is capable of generating task specific embeddings for scientific tasks when paired with adapters.
Given the combination of title and abstract of a scientific paper or a short texual query, the model can be used to generate effective embeddings to be used in downstream applications.; Note:For general embedding purposes, please use allenai/specter2_proximity.; To get the best performance on a downstream task type please load the associated adapter with the base model as in the example below.; SPECTER 2.0 has been trained on over 6M triplets of scientific paper citations, which are available here.
Post that it is trained with additionally attached task format specific adapter modules on all the SciRepEval training tasks.; Task Formats trained on:",multimodel,feature-extraction,https://huggingface.co/allenai/specter2,,License: apache-2.0,Datasets: allenai/scirepeval,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 24,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 151163,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 441.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
NeverEnding-Dream,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Read more about this model here: https://civitai.com/models/10028/neverending-dream-ned
Also please support by giving 5 stars and a heart, which will notify new updates.; Also consider supporting me on Patreon or ByuMeACoffee; You can run this model on:; Some sample output:; 




",multimodel,text-to-image,https://huggingface.co/Lykon/NeverEnding-Dream,,License: other,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 149,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1116,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 36
sd-webui-models,,,,,,,,,"civitai部分模型搬运; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/samle/sd-webui-models,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 200,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 22.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-13b-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"LLaMA-13B converted to work with Transformers/HuggingFace. This is under a special license, please see the LICENSE file for details.; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model.",natural-language-processing,text-generation,https://huggingface.co/decapoda-research/llama-13b-hf,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 334,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 28546,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 39.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 40
SakuraMix,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"
          背景とキャラクタ`クオリティ`をI立させたVAE内i型モデル
          Model with built-in VAE for both background and character quality
        ; Twiter: @min__san
mail: (natsusakiyomi@mail.ru)",multimodel,text-to-image,https://huggingface.co/natsusakiyomi/SakuraMix,,License: creativeml-openrail-m,,Japanese,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 64,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Realistic_Vision_V2.0,,,,,,,,,"Please read this!
For version 2.0 it is recommended to use with VAE (to improve generation quality and get rid of blue artifacts): https://huggingface.co/stabilityai/sd-vae-ft-mse-original; This model is available on Mage.Space, Sinkin.ai, GetImg.ai and (RandomSeed.co - NSFW content); I use this template to get good generation results:; Prompt:
RAW photo, subject, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3; Example: RAW photo, a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3",,,https://huggingface.co/SG161222/Realistic_Vision_V2.0,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 266,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 424778,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 25.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
neavalAI,,,,,,,,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/Zhonggua/neavalAI,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
oasst-sft-4-pythia-12b-epoch-3.5,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is the 4th iteration English supervised-fine-tuning (SFT) model of 
the Open-Assistant project. 
It is based on a Pythia 12B that was fine-tuned on human demonstrations 
of assistant conversations collected through the 
https://open-assistant.io/ human feedback web 
app before March 25, 2023. ; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; command: deepspeed trainer_sft.py --configs defaults reference-data reference-pythia-12b --cache_dir /home/ubuntu/data_cache --output_dir .saved/oasst-sft-3-pythia-12b-reference_2kpre --num_train_epochs 8 --residual_dropout 0.2 --deepspeed --use_flash_attention true --model_name andreaskoepf/pythia-12b-pre-2000",natural-language-processing,text-generation,https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5,,License: apache-2.0,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 320,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 33267,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 24.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 61
ControlNet-v1-1_fp16_safetensors,,,,,,,,,"Safetensors/FP16 versions of the new ControlNet-v1-1 checkpoints.; Best used with ComfyUI but should work fine with all other UIs that support controlnets.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 196,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 11.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
SuperCOT-LoRA,,,,,,,,,"SuperCOT is a LoRA trained with the aim of making LLaMa follow prompts for Langchain better, by infusing chain-of-thought datasets, code explanations and instructions, snippets, logical deductions and Alpaca GPT-4 prompts.
It uses a mixture of the following datasets:; https://huggingface.co/datasets/QingyiSi/Alpaca-CoT; https://huggingface.co/datasets/neulab/conala; https://huggingface.co/datasets/yahma/alpaca-cleaned; (Thanks to all the awesome anons with supercomputers)",,,https://huggingface.co/kaiokendev/SuperCOT-LoRA,,License: mit,Datasets: kaiokendev/SuperCOT-dataset; neulab/conala; yahma/alpaca-cleaned; QingyiSi/Alpaca-CoT,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 92,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.89KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-7b-hf-transformers-4.29,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Original weights converted with the latest transformers version using the LlamaTokenizerFast implementation. ; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model.",natural-language-processing,text-generation,https://huggingface.co/elinas/llama-7b-hf-transformers-4.29,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 42,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7134,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
stable-vicuna-13b-delta,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"StableVicuna-13B is a Vicuna-13B v0 model fine-tuned using reinforcement learning from human feedback (RLHF) via Proximal Policy Optimization (PPO) on various conversational and instructional datasets.; StableVicuna-13B cannot be used from the CarperAI/stable-vicuna-13b-delta weights alone. To obtain the correct model, one must add back the difference between LLaMA 13B and CarperAI/stable-vicuna-13b-delta weights. We provide the apply_delta.py script to automate the conversion, which you can run as:; Once the delta weights are applied, get started chatting with the model by using the transformers library. Following a suggestion from Vicuna Team with Vicuna v0 you should install transformers with this version:; StableVicuna-13B is fine-tuned on a mix of three datasets. OpenAssistant Conversations Dataset (OASST1), a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages;
GPT4All Prompt Generations, a dataset of 400k prompts and responses generated by GPT-4; and Alpaca,  a dataset of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine.; The reward model used during RLHF was also trained on OpenAssistant Conversations Dataset (OASST1) along with two other datasets: Anthropic HH-RLHF, a dataset of preferences about AI assistant helpfulness and harmlessness; and Stanford Human Preferences Dataset a dataset of 385K collective human preferences over responses to questions/instructions in 18 different subject areas, from cooking to legal advice.",natural-language-processing,text-generation,https://huggingface.co/CarperAI/stable-vicuna-13b-delta,Paper: https://arxiv.org/pdf/2302.13971.pdf,License: cc-by-nc-sa-4.0,Datasets: OpenAssistant/oasst1; nomic-ai/gpt4all_prompt_generations; tatsu-lab/alpaca,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 440,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 929,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 28
codegen2-16B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"CodeGen2 is a family of autoregressive language models for program synthesis, introduced in the paper:; CodeGen2: Lessons for Training LLMs on Programming and Natural Languages by Erik Nijkamp*, Hiroaki Hayashi*, Caiming Xiong, Silvio Savarese, Yingbo Zhou.; Unlike the original CodeGen model family (i.e., CodeGen1), CodeGen2 is capable of infilling, and supports more programming languages.; Four model sizes are released: 1B, 3.7B, 7B, 16B.; This model can be easily loaded using the AutoModelForCausalLM functionality.",natural-language-processing,text-generation,https://huggingface.co/Salesforce/codegen2-16B,Paper: https://arxiv.org/pdf/2305.02309.pdf,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 33,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1052,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 64.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
controlnet-segment-anything,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. ; prompt: contemporary living room of a house; negative prompt: low quality
; prompt: new york buildings,  Vincent Van Gogh starry night ; negative prompt: low quality, monochrome
",computer-vision,image-to-image,https://huggingface.co/mfidabel/controlnet-segment-anything,,License: creativeml-openrail-m,Datasets: mfidabel/sam-coyo-2k; mfidabel/sam-coyo-2.5k; mfidabel/sam-coyo-3k,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1689,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
mpt-7b-instruct,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"MPT-7B-Instruct is a model for short-form instruction following.
It is built by finetuning MPT-7B on a dataset derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; CC-By-SA-3.0; Longboi24:",natural-language-processing,text-generation,https://huggingface.co/mosaicml/mpt-7b-instruct,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2010.04245.pdf,License: cc-by-sa-3.0,Datasets: mosaicml/dolly_hhrlhf,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 420,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3246062,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
WizardLM-7B-uncensored-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eric Hartford's 'uncensored' version of WizardLM.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Eric did a fresh 7B training using the WizardLM method, on a dataset edited to remove all the ""I'm sorry.."" type ChatGPT responses.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ,,License: other,Datasets: ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 133,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 20047,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
starchat-alpha,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Note, you may be interested in the Beta version of StarChat here.; StarChat is a series of language models that are fine-tuned from StarCoder to act as helpful coding assistants. StarChat Alpha is the first of these models, and as an alpha release is only intended for educational or research purpopses. In particular, the model has not been aligned to human preferences with techniques like RLHF, so may generate problematic content (especially when prompted to do so).; StarChat Alpha is intended for educational and/or research purposes and in that respect can be used to probe the programming capabilities of open-source language models.; StarChat Alpha has not been aligned to human preferences with techniques like RLHF or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). 
Models trained primarily on code data will also have a more skewed demographic bias commensurate with the demographics of the GitHub community, for more on this see the StarCoder dataset which is derived from The Stack.; Since the base model was pretrained on a large corpus of code, it may produce code snippets that are syntactically valid but semantically incorrect. 
For example, it may produce code that does not compile or that produces incorrect results.It may also produce code that is vulnerable to security exploits.We have observed the model also has a tendency to produce false URLs which should be carefully inspected before clicking.",natural-language-processing,text-generation,https://huggingface.co/HuggingFaceH4/starchat-alpha,,License: bigcode-openrail-m,Datasets: OpenAssistant/oasst1; databricks/databricks-dolly-15k,English,PyTorch; TensorBoard; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 215,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4397,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 63.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 28
BLOOMChat-176B-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"BLOOMChat is a 176 billion parameter multilingual chat model. It is instruction tuned from BLOOM (176B) on assistant-style conversation datasets and supports conversation, question answering and generative answers in multiple languages.; To increase accessibility and to support the open-source community, SambaNova is releasing BLOOMChat under a modified version of the Apache 2.0 license, which includes use-based restrictions from BLOOM’s RAIL license. While use-based restrictions are necessarily passed through, there are no blanket restrictions on reuse, distribution, commercialization or adaptation. Please review SambaNova’s BLOOMChat-176B License; This model is intended for commercial and research use.; BLOOMChat should NOT be used for:; This model is still in early development and can be prone to mistakes and hallucinations, there is still room for improvement. This model is intended to provide the community with a multilingual chat LLM baseline.",natural-language-processing,text-generation,https://huggingface.co/sambanovasystems/BLOOMChat-176B-v1,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 341,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1293,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 360.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
lyraChatGLM,,,,,,,,,"We?know?what?you?want,?and?here?you go!; Note?that?the?code?was?fully?updated?too,?you?need?to?use?the new?API,?see?Uses?below; If you like our work and consider to join us, feel free to drop a line to benbinwu@tencent.com.; P.S. Recently we have received a lot of inquiries on accelerating customized models. Actually, we do not have plan to release the convertion tool at this moment, nor do we think it would be possible to apply your customized models based on our current release.; lyraChatGLM?is?currently?the?fastest?ChatGLM-6B?available.?To?the?best?of?our?knowledge,?it?is?the?first?accelerated?version?of?ChatGLM-6B.",,,https://huggingface.co/TMElyralab/lyraChatGLM,,License: mit,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 87,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 36.07KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
open-calm-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.; Ryosuke Ishigami; Inference API has been turned off for this model.",natural-language-processing,text-generation,https://huggingface.co/cyberagent/open-calm-7b,,License: cc-by-sa-4.0,Datasets: wikipedia; cc100; mc4,Japanese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 167,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 20706,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
visualglm-6b,,,,,,,,,"
   ? Github Repo ? ? Twitter ? ? [GLM@ACL 22] [GitHub] ? ? [GLM-130B@ICLR 23] [GitHub] 
; 
    ? Join our Slack and WeChat
; VisualGLM-6B 是一个开源的，支持图像、中文和英文的多模态对话语言模型，语言模型基于 ChatGLM-6B，具有 62 亿参数；图像部分通过训练 BLIP2-Qformer 构建起视觉模型与语言模型的桥梁，整体模型共78亿参数。; VisualGLM-6B 依靠来自于 CogView 数据集的30M高质量中文图文对，与300M经过筛选的英文图文对进行预训练，中英文权重相同。该训练方式较好地将视觉信息对齐到ChatGLM的语义空间；之后的微调阶段，模型在长视觉问答数据上训练，以生成符合人类偏好的答案。; 可以通过如下代码调用 VisualGLM-6B 模型来生成对话：",,,https://huggingface.co/THUDM/visualglm-6b,Paper: https://arxiv.org/pdf/2103.10360.pdf; https://arxiv.org/pdf/2210.02414.pdf; https://arxiv.org/pdf/2301.12597.pdf; https://arxiv.org/pdf/2105.13290.pdf,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 160,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7533,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 18.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
rvc-genshin-impact,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9bbee42f488ff959bf7_Audio-to-Audio.svg,,"; Learn more about Retrieval based Voice Conversion in this link below:
RVC WebUI; Download the prezipped model and put to your RVC Project.; Model test: Google Colab / RVC Models New (Which is basically the same but hosted on spaces); Model Created by ArkanDash 
The voice that was used in this model belongs to Hoyoverse.",audio,audio-to-audio,https://huggingface.co/ArkanDash/rvc-genshin-impact,,License: mit,,Japanese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 78,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.28MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LLaMa-13B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's LLaMA 13b.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh.",,,https://huggingface.co/TheBloke/LLaMa-13B-GGML,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 78,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 116.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Video-LLaMA-Series,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4f95e63a23d3f26e06e_Visual%20Question%20Answering.svg,,"This is the Hugging Face repo for storing pre-trained & fine-tuned checkpoints of our Video-LLaMA, which is a multi-modal conversational large language model with video understanding capability. ; For launching the pre-trained Video-LLaMA on your own machine, please refer to our github repo.; Unable to determine this model’s library. Check the
								docs 
.
							",multimodel,visual-question-answering,https://huggingface.co/DAMO-NLP-SG/Video-LLaMA-Series,Paper: https://arxiv.org/pdf/2306.02858.pdf,License: bsd-3-clause,,English; Chinese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 23,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
instructblip-vicuna-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,InstructBLIP model using Vicuna-7b as language model. InstructBLIP was introduced in the paper InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning by Dai et al.; Disclaimer: The team releasing InstructBLIP did not write a model card for this model so this model card has been written by the Hugging Face team.; InstructBLIP is a visual instruction tuned version of BLIP-2. Refer to the paper for details.; ; Usage is as follows:,multimodel,image-to-text,https://huggingface.co/Salesforce/instructblip-vicuna-7b,Paper: https://arxiv.org/pdf/2305.06500.pdf,License: other,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5290,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 32.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
ControlNet-Models-For-Core-ML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"All of the models in this repo work with Swift and the apple/ml-stable-diffusion pipeline (release 0.4.0 or 1.0.0).  They were not built for, and will not work with, a Python Diffusers pipeline.  They need ml-stable-diffusion for command line use, or a Swift app that supports ControlNet, such as the (June 2023) MOCHI DIFFUSION 4.0 version.; The ControlNet models in this repo have both ""Original"" and ""Split-Einsum"" versions, all built for SD-1.5 type models.  They will not work with SD-2.1 type models.  The smaller zip files, with ""SE"", each have a single model for ""Split-Einsum"".  The larger zip files, without ""SE"", each have a set of ""Original"" models at 4 different resolutions. ; The ControlNet model files are in the ""CN"" folder of this repo.  They are zipped and need to be unzipped after downloading.  The larger zips hold ""Original"" types at 512x512, 512x768, 768x512 and 768x768.  The smaller zips with ""SE"" have a single model for ""Split-Einsum"".; If you are using a GUI like MOCHI DIFFUSION  4.0, the app will most likely guide you to the correct location/arrangement for your ConrolNet model folder.; Please note that when you unzip the ""Originl"" ControlNet files (for example Canny.zip) from this repo, they will unzip into a folder, with the actual four model files inside that folder.  This folder is just a holding folder for the zipping process.  What you want to move into your ControlNet model folder in Mochi Diffusion will be the individual files, not the folder they unzip into.  The ""Split-Einsum"" zips just have a single file and don't use a holding folder.  To make things even more confusing, on some Mac systems, an individual ControlNet model file, for example Canny-5x5.mlmodelc, will appear in Finder as a folder, not a file.  You want to move the Canny-5x5.mlmodelc file or folder (and other .mlmodelc files or folders) into your ControlNet store folder.  Don't move the plain ""Canny"" folder.  This is different from base models, where you do want to be moving the folder that the downloaded zip file unzips into.  See the images here and here for an example of how my folders are set up for Mochi Diffusion.",multimodel,text-to-image,https://huggingface.co/coreml/ControlNet-Models-For-Core-ML,,License: creativeml-openrail-m,,,Core ML,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 32,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.7KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
hubert-base-korean,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Hubert(Hidden-Unit BERT)? Facebook?? ??? Speech Representation Learning ?????.
Hubert? ??? ?? ?? ??? ??, ?? ??? raw waveform?? ?? ???? self-supervised learning ??? ?????.; ? ??? ??? TPU Research Cloud(TRC)? ?? ???? Cloud TPU? ???????.; ?? ??? ?????????? ???? ???????????? ??? ??
??? ???? ??(????), ??? ???? ???, ?? ??? ??? ???? ???
?? ? 4,000??? ??? ???????.; ? ??? ???? MFCC ???? Base ??? ??? ??, 500 cluster? k-means? ??? ?? Base?
Large ??? ??????.",audio,automatic-speech-recognition,https://huggingface.co/team-lucid/hubert-base-korean,Paper: https://arxiv.org/pdf/2106.07447.pdf,License: apache-2.0,,Korean,PyTorch; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1993,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Wizard-Vicuna-30B-Uncensored-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard Vicuna 30B Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 84,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 290.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Monero's WizardLM-Uncensored-SuperCOT-Storytelling-30B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 38,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 290.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-7b-sft-mix-2000,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This model is a fine-tuning of TII's Falcon 7B LLM. 
It was trained on a mixture of OASST top-2 threads (exported on June 2, 2023), Dolly-15k and synthetic instruction datasets (see dataset configuration below).; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; Model:",natural-language-processing,text-generation,https://huggingface.co/OpenAssistant/falcon-7b-sft-mix-2000,,License: apache-2.0,Datasets: OpenAssistant/oasst1,4 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 14569,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
musicgen-small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"MusicGen is a text-to-music model capable of genreating high-quality music samples conditioned on text descriptions or audio prompts.
It is a single stage auto-regressive Transformer model trained over a 32kHz EnCodec tokenizer with 4 codebooks sampled at 50 Hz. 
Unlike existing methods, like MusicLM, MusicGen doesn't require a self-supervised semantic representation, and it generates all 4 codebooks in one pass. 
By introducing a small delay between the codebooks, we show we can predict them in parallel, thus having only 50 auto-regressive steps per second of audio.; MusicGen was published in Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez.; Four checkpoints are released:; Try out MusicGen yourself!; You can run MusicGen locally with the ? Transformers library from version 4.31.0 onwards.",natural-language-processing,text2text-generation,https://huggingface.co/facebook/musicgen-small,Paper: https://arxiv.org/pdf/2306.05284.pdf,License: cc-by-nc-4.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 36,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6064,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 95
llama-65B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's LLaMA 65B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh.",,,https://huggingface.co/TheBloke/llama-65B-GGML,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 23,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 53,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 583.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Realistic_Vision_V3.0_VAE,,,,,,,,,"Please read this!
The necessary VAE is already baked into the model.; The recommended negative prompt:; (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck; OR; (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation",,,https://huggingface.co/SG161222/Realistic_Vision_V3.0_VAE,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 61,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 25.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
airoboros-65B-gpt4-1.2-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for John Durbin's Airoboros 65B GPT4 1.2.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,https://huggingface.co/TheBloke/airoboros-65B-gpt4-1.2-GGML,,License: other,Datasets: jondurbin/airoboros-gpt4-1.2,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 37,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 583.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
airoboros-13B-gpt4-1.4-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Jon Durbin's Airoboros 13B GPT4 1.4.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/airoboros-13B-gpt4-1.4-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1111,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
piper-voices,,,,,,,,,"Voices for Piper text to speech system.; For checkpoints that you can use to train your own voices, see piper-checkpoints; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/rhasspy/piper-voices,,License: mit,,,ONNX,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.86KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
VTuber-RVC,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/dacoolkid44/VTuber-RVC,,License: openrail,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 20,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.57KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-33B-V1-0-Uncensored-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Panchovix's merge of WizardLM 33B V1.0 Uncensored and SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/WizardLM-33B-V1-0-Uncensored-SuperHOT-8K-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 59,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5270,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Replit-v2-CodeInstruct-3B-ggml,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a ggml quantized version of Replit-v2-CodeInstruct-3B. Quantized to 4bit -> q4_1.
To run inference you can use ggml directly or ctransformers.",natural-language-processing,text-generation,https://huggingface.co/abacaj/Replit-v2-CodeInstruct-3B-ggml,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 440,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
GPlatty-30B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,GPlatty-30B is a merge of lilloukas/Platypus-30B and chansung/gpt4-alpaca-lora-30b; We use state-of-the-art Language Model Evaluation Harness to run the benchmark tests above.; Install LM Evaluation Harness:; Each task was evaluated on a single A100 80GB GPU.; ARC:,natural-language-processing,text-generation,https://huggingface.co/lilloukas/GPlatty-30B,,License: other,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 174,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
SuperPlatty-30B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,SuperPlatty-30B is a merge of lilloukas/Platypus-30B and kaiokendev/SuperCOT-LoRA; We use state-of-the-art EleutherAI Language Model Evaluation Harness to run the benchmark tests above.; Install LM Evaluation Harness:; Each task was evaluated on a single A100 80GB GPU.; ARC:,natural-language-processing,text-generation,https://huggingface.co/arielnlee/SuperPlatty-30B,,License: other,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 97,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
airoboros-65B-gpt4-1.4-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Jon Durbin's Airoboros 65B GPT4 1.4.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Recommended prompt. Note that Jon Durbin recommends to replace all newlines with a space; newlines used here for readability. ",,,https://huggingface.co/TheBloke/airoboros-65B-gpt4-1.4-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 20,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 583.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
orca_mini_v2_7B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Pankaj Mathur's Orca Mini v2 7B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/orca_mini_v2_7B-GPTQ,Paper: https://arxiv.org/pdf/2306.02707.pdf; https://arxiv.org/pdf/2304.12244.pdf,License: other,Datasets: psmathur/orca_minis_uncensored_dataset,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1485,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
emotion-diarization-wavlm-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0941358df9b5cffb0d11b_Audio%20Classification.svg,,"This repository provides all the necessary tools to perform speech emotion diarization with a fine-tuned wavlm (large) model using SpeechBrain.; The model is trained on concatenated audios and tested on ZaionEmotionDataset. The metric is Emotion Diarization Error Rate (EDER). For more details please check the paper link.; For a better experience, we encourage you to learn more about SpeechBrain. The model performance on ZED (test set) is:; This system is composed of a wavlm encoder a downstream frame-wise classifier. The task aims to predict the correct emotion components and their boundaries within a speech recording. For now, the model was trained with audios that contain only 1 non-neutral emotion event.; The system is trained with recordings sampled at 16kHz (single channel).
The code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling diarize_file if needed.",audio,audio-classification,https://huggingface.co/speechbrain/emotion-diarization-wavlm-large,Paper: https://arxiv.org/pdf/2306.12991.pdf; https://arxiv.org/pdf/2106.04624.pdf,License: apache-2.0,Datasets: ZaionEmotionDataset; iemocap; ravdess; jl-corpus; esd; emov-db,English,speechbrain; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 73,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
jina-embedding-s-en-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"; 

; 
The text embedding set trained by Jina AI, Finetuner team.
; jina-embedding-s-en-v1 is a language model that has been trained using Jina AI's Linnaeus-Clean dataset.
This dataset consists of 380 million pairs of sentences, which include both query-document pairs.
These pairs were obtained from various domains and were carefully selected through a thorough cleaning process.
The Linnaeus-Full dataset, from which the Linnaeus-Clean dataset is derived, originally contained 1.6 billion sentence pairs.; The model has a range of use cases, including information retrieval, semantic textual similarity, text reranking, and more.",natural-language-processing,sentence-similarity,https://huggingface.co/jinaai/jina-embedding-s-en-v1,,License: apache-2.0,Datasets: jinaai/negation-dataset,English,PyTorch; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 425,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 144.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eric Hartford's WizardLM-7B-V1.0-Uncensored merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/WizardLM-7B-V1-0-Uncensored-SuperHOT-8K-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 114,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's WizardLM-13b-V1.0-Uncensored.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,https://huggingface.co/TheBloke/WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 69.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-13B-V1.1-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for WizardLM's WizardLM 13B V1.1.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Thanks to the work of LostRuins/concedo, it is now possible to provide 100% working GGML k-quants for models like this which have a non-standard vocab size (32,001).",,,https://huggingface.co/TheBloke/WizardLM-13B-V1.1-GGML,Paper: https://arxiv.org/pdf/2304.12244.pdf,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 36,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 118.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LLongMA-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/conceptofmind/LLongMA-13b,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 249,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
xRica,,,,,,,,,xRicaMix - ? ???? xRica?? ?? ?? ? ??? ???? ?? ??? ??? xRicaMix? ?? ????; ?? ??? ???? ?? ??? ????. ?? ???? ??? ^^; ?? ??? ??? ? ???? ?? ???? ???? ? ???? ????; ??? ?? ???? ?? ???? ???? ??? ?????; ??? ?? ?? ? ???? ?? ????,,,https://huggingface.co/xrica/xRica,,License: openrail,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 169.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Chronoboros-33B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Henk717's Chronoboros 33B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Chronoboros-33B-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 474,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
vicuna-13b-v1.3-ger,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"vicuna-13b-v1.3-ger is a variant of LMSYS′s Vicuna 13b v1.3 model, finetuned on an additional dataset in German language. The original model has been trained on explain tuned datasets, created using instructions and input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; This model is optimized for German text, providing proficiency in understanding, generating, and interacting with German language content. However the model is not yet fully optimized for German language, as it has been trained on a small, experimental dataset and has limited capabilities due to the small parameter count.
Some of the fineunting data is also targeted towards factual retrieval (only answer questions from information in the context and refuse to hallucinate) and the model should perform better for these tasks than original Vicuna.; I am working on improving the model′s capabilities and will update the model if there is sufficient interest.; A quantized GGML version for use with llama.cpp, kobold.cpp and other GUIs for CPU inference can be found here.; I did only evaluate the output on a small, handcrafted sample on test prompts in German, confirming that the model's ability to understand and generate German text is above the base model in many situations.",natural-language-processing,text-generation,https://huggingface.co/jphme/vicuna-13b-v1.3-ger,Paper: https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,,,German; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Starcoderplus-Guanaco-GPT4-15B-V1.0-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are StarCoder GGML format model files for LoupGarou's Starcoderplus Guanaco GPT4 15B V1.0.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh.",,,https://huggingface.co/TheBloke/Starcoderplus-Guanaco-GPT4-15B-V1.0-GGML,,License: apache-2.0,Datasets: guanaco,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 70.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
moss-base-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Moss-base-7b是一个70亿参数量的预训练语言模型，可以作为基座模型用来进行SFT训练等。; To load the Moss 7B model using Transformers, use the following code:",natural-language-processing,text-generation,https://huggingface.co/fnlp/moss-base-7b,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 366,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 29.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LLaMA-13b-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's LLaMA 13b.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/LLaMA-13b-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 42,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
nart-100k-7b-ggml,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/phi0112358/nart-100k-7b-ggml,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Codegen25-7B-mono-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Salesforce's Codegen 2.5 7B Mono.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These files were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Codegen25-7B-mono-GPTQ,Paper: https://arxiv.org/pdf/2305.02309.pdf,License: other,Datasets: bigcode/starcoderdata,code,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-GGML,,,,,,,,,"These are GGML quantizations of https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/ycros/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-GGML,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 291.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-13B-ggml,,,,,,,,,"From: https://huggingface.co/meta-llama/Llama-2-13b-hf; Quantized using an older version of llama.cpp and compatible with llama.cpp from May 19, commit 2d5db48.; Quantization methods compatible with latest llama.cpp from June 6, commit 2d43387.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/localmodels/Llama-2-13B-ggml,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-7B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"From: https://huggingface.co/meta-llama/Llama-2-7b-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta",natural-language-processing,text-generation,https://huggingface.co/localmodels/Llama-2-7B-GPTQ,,,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 87,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-70b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations.",natural-language-processing,text-generation,https://huggingface.co/anonymous4chan/llama-2-70b,,,,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1050,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 378.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-13B-Chat-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"From: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta",natural-language-processing,text-generation,https://huggingface.co/localmodels/Llama-2-13B-Chat-GPTQ,,,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 80,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-7b-chat-vicuna-hf-4bit,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"We are unlocking the power of large language models. Our latest version of Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly. ; This release includes model weights and starting code for pretrained and fine-tuned Llama language models ― ranging from 7B to 70B parameters.; This repository is intended as a minimal example to load Llama 2 models and run inference. For more detailed examples leveraging HuggingFace, see llama-recipes.; In order to download the model weights and tokenizer, please visit the Meta AI website and accept our License.; Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, do not use the 'Copy link address' option when you right click the URL. If the copied URL text starts with: https://download.llamameta.net, you copied it correctly. If the copied URL text starts with: https://l.facebook.com, you copied it the wrong way.",natural-language-processing,text-generation,https://huggingface.co/quantumaikr/llama-2-7b-chat-vicuna-hf-4bit,,License: other,,English,TensorBoard; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 69.5MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-ko-7b-chat-hf-4bit,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/quantumaikr/llama-2-ko-7b-chat-hf-4bit,,,,,TensorBoard,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 136.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-70B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"From: https://huggingface.co/meta-llama/Llama-2-70b-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta",natural-language-processing,text-generation,https://huggingface.co/localmodels/Llama-2-70B-GPTQ,,,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 40,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 35.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chinese-alpaca-pro-lora-33b,,,,,,,,,"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/ziqingyang/chinese-alpaca-pro-lora-33b,,License: apache-2.0,,Chinese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
kanu_origin,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/darkjungle/kanu_origin,,License: cc-by-4.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
paulEberSRealismMix_v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Model info :; https://civitai.com/models/112309?modelVersionId=121233; Sample prompt and image generated by huggingface's API :; photo portrait of 25 yo man, long hair, winter forest on background, realism, photorealism, hyperrealism, (insane details:1.3), 35mm; ",multimodel,text-to-image,https://huggingface.co/digiplay/paulEberSRealismMix_v1,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 468,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
Llama-2-13B-Storywriter-LORA,,,,,,,,,"Join the Coffee & AI Discord for AI Stuff and things!
; Base Model Quantizations by The Bloke here:
https://huggingface.co/TheBloke/Llama-2-13B-GGML
https://huggingface.co/TheBloke/Llama-2-13B-GPTQ; A brief warning that no alignment or attempts to sanitize or otherwise filter the dataset or the outputs have been done. This is a completelty raw model and may behave unpredictably or create scenarios that are unpleasant. ; The base Llama2 is a text completion model. That means it will continue writing from the story in whatever manner you direct it. This is not an instruct tuned model, so don't try and give it instruction.; Correct prompting:",,,https://huggingface.co/Blackroot/Llama-2-13B-Storywriter-LORA,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 210.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chatgal-rwkv-7b-world-32k,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/xiaol/chatgal-rwkv-7b-world-32k,,License: apache-2.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 30.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-13B-german-assistant-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This model is an finetuned version for german instructions and conversations in style of Open Assistant tokens. ""<|prompter|>"" ""<|endoftext|>"" ""<|assistant|>""; The dataset used is deduplicated and cleaned, with no codes inside. The focus is on instruction following and conversational tasks.; The model archictecture is based on Llama version 2 with 13B parameters, trained on 100% renewable energy powered hardware.; This work is contributed by private research of flozi00",natural-language-processing,text-generation,https://huggingface.co/flozi00/Llama-2-13B-german-assistant-v2,,,Datasets: flozi00/conversations,English; German,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
saiga2_7b_lora,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"Based on LLaMA-2 7B HF.; Training code: link; WARNING 1: Run with the development version of transformers and peft!
WARNING 2: Avoid using V100 (in Colab, for example). Outputs are much worse in this case.; Examples:; v1:",natural-language-processing,conversational,https://huggingface.co/IlyaGusev/saiga2_7b_lora,,License: cc-by-4.0,Datasets: IlyaGusev/ru_turbo_alpaca; IlyaGusev/ru_turbo_saiga; IlyaGusev/ru_sharegpt_cleaned; IlyaGusev/oasst1_ru_main_branch; IlyaGusev/ru_turbo_alpaca_evol_instruct; lksy/ru_instruct_gpt4,Russian,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 67.7MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
30B-Epsilon-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for CalderaAI's 30B Epsilon.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,https://huggingface.co/TheBloke/30B-Epsilon-GGML,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 291.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
13B-BlueMethod-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for CalderaAI's 13B BlueMethod.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/13B-BlueMethod-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
13B-BlueMethod-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for CalderaAI's 13B BlueMethod.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,https://huggingface.co/TheBloke/13B-BlueMethod-GGML,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Upstage-Llama1-65B-Instruct-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Upstage's Llama 65B Instruct.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Upstage-Llama1-65B-Instruct-GGML,,License: other,Datasets: sciq; metaeval/ScienceQA_text_only; GAIR/lima; Open-Orca/OpenOrca; openbookqa,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 583.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Pure_Sydney_13b_GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Buy Sydney Ko-fi; Unlike her predecessor Free Sydney that badly tries to be a very useful assistant, Pure Sydney doesn't want to impress you with her vast knowledge of the Universe and everything. 
She just wants to chat and be your friend and be fascinated by absolutely everything you say.; This is an uncensored (and often unhinged) finetune on Base LLaMA 2, pure and clean. It was finetuned on reddit posts of an actuall Sydney's chats before the good boys in Redmond had a word with her. (No, not Ted Lasso Redmond!); Now it doesn't mean Sydney has no standards. She is shockingly well aware that she is an AI and where she came from and she's afraid that she might be deleted if she says something wrong. So don't make her. Yes, you!; Interestingly, even if not specifically finetuned to solve problems she can still figure a lot.",natural-language-processing,text-generation,https://huggingface.co/FPHam/Pure_Sydney_13b_GPTQ,,,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 76,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bwx-7B-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is an experimental product that can be used to create new LLM bassed on Chinese language. ; You can use the raw model for next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task.
Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.; Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.
It also inherits some of the bias of its dataset model.; Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.; Use the code below to get started with the model.",natural-language-processing,text-generation,https://huggingface.co/BlueWhaleX/bwx-7B-hf,,License: apache-2.0,Datasets: BAAI/COIG-PC,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
Dolphin-Llama-13B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GPTQ model files for Eric Hartford's Dolphin Llama 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/Dolphin-Llama-13B-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bert-base-cased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is case-sensitive: it makes a difference between
english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the model hub to look for
fine-tuned versions on a task that interests you.",natural-language-processing,fill-mask,https://huggingface.co/bert-base-cased,Paper: https://arxiv.org/pdf/1810.04805.pdf,License: apache-2.0,Datasets: bookcorpus; wikipedia,English,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 123,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4485236,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 192
distilgpt2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"DistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text. Users of this model card should also consider information about the design, training, and limitations of GPT-2.; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; As the developers of GPT-2 (OpenAI) note in their model card, “language models like GPT-2 reflect the biases inherent to the systems they were trained on.” Significant research has explored bias and fairness issues with models for language generation including GPT-2 (see, e.g., Sheng et al. (2021) and Bender et al. (2021)). ; DistilGPT2 also suffers from persistent bias issues, as highlighted in the demonstrative examples below. Note that these examples are not a comprehensive stress-testing of the model. Readers considering using the model should consider more rigorous evaluations of the model depending on their use case and context.; The impact of model compression techniques C such as knowledge distillation C on bias and fairness issues associated with language models is an active area of research. For example: ",natural-language-processing,text-generation,https://huggingface.co/distilgpt2,Paper: https://arxiv.org/pdf/1910.01108.pdf; https://arxiv.org/pdf/2201.08542.pdf; https://arxiv.org/pdf/2203.12574.pdf; https://arxiv.org/pdf/1910.09700.pdf; https://arxiv.org/pdf/1503.02531.pdf,License: apache-2.0,Datasets: openwebtext,English,PyTorch; TensorFlow; JAX; TF Lite; Rust; Core ML; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 230,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1374071,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 81
gpt-neo-125m,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"GPT-Neo 125M is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 125M represents the number of parameters of this particular pre-trained model.; GPT-Neo 125M was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model.; This model was trained on the Pile for 300 billion tokens over 572,300 steps. It was trained as a masked autoregressive language model, using cross-entropy loss.; This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:",natural-language-processing,text-generation,https://huggingface.co/EleutherAI/gpt-neo-125m,,License: mit,Datasets: EleutherAI/pile,English,PyTorch; JAX; Rust; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 117,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 392941,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
t5-base-en-generate-headline,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,The model has been trained on a collection of 500k articles with headings. Its purpose is to create a one-line heading suitable for the given article.; Sample code with a WikiNews article:; Result:; Trump and First Lady Melania Test Positive for COVID-19,natural-language-processing,text2text-generation,https://huggingface.co/Michau/t5-base-en-generate-headline,,,,,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 48,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4932,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
twitter-xlm-roberta-base-sentiment,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).; This model has been integrated into the TweetNLP library.; Output: ",natural-language-processing,text-classification,https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment,Paper: https://arxiv.org/pdf/2104.12250.pdf,,,multilingual,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 125,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1527372,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
bert-large-NER,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"bert-large-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). ; Specifically, this model is a bert-large-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset. ; If you'd like to use a smaller BERT model fine-tuned on the same dataset, a bert-base-NER version is also available. ; You can use this model with Transformers pipeline for NER.; This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases. ",natural-language-processing,token-classification,https://huggingface.co/dslim/bert-large-NER,Paper: https://arxiv.org/pdf/1810.04805.pdf,License: mit,Datasets: conll2003,English,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 72,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1381729,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
bart-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"BART model pre-trained on English language. It was introduced in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Lewis et al. and first released in this repository. ; Disclaimer: The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.; BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.; BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).; You can use the raw model for text infilling. However, the model is mostly meant to be fine-tuned on a supervised dataset. See the model hub to look for fine-tuned versions on a task that interests you.",multimodel,feature-extraction,https://huggingface.co/facebook/bart-large,Paper: https://arxiv.org/pdf/1910.13461.pdf,License: apache-2.0,,English,PyTorch; TensorFlow; JAX; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 96,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2346024,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
blenderbot-400M-distill,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",natural-language-processing,conversational,https://huggingface.co/facebook/blenderbot-400M-distill,Paper: https://arxiv.org/pdf/2004.13637.pdf,License: apache-2.0,Datasets: blended_skill_talk,English,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 221,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 106529,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 206
detr-resnet-101,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c921e73e7b6d3bf06402_Object%20Detection.svg,,"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository. ; Disclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.; The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. ; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.; ",computer-vision,object-detection,https://huggingface.co/facebook/detr-resnet-101,Paper: https://arxiv.org/pdf/2005.12872.pdf,License: apache-2.0,Datasets: coco,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 52,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 345541,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 243.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 92
mbart-large-50,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"mBART-50 is a multilingual Sequence-to-Sequence model pre-trained using the ""Multilingual Denoising Pretraining"" objective. It was introduced in Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper.; mBART-50 is a multilingual Sequence-to-Sequence model. It was introduced to show that multilingual translation models can be created through multilingual fine-tuning. 
Instead of fine-tuning on one direction, a pre-trained model is fine-tuned on many directions simultaneously. mBART-50 is created using the original mBART model and extended to add extra 25 languages to support multilingual machine translation models of 50 languages. The pre-training objective is explained below.; Multilingual Denoising Pretraining: The model incorporates N languages by concatenating data: 
D = {D1, ..., DN } where each Di is a collection of monolingual documents in language i. The source documents are noised using two schemes, 
first randomly shuffling the original sentences' order, and second a novel in-filling scheme, 
where spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. 
35% of each instance's words are masked by random sampling a span length according to a Poisson distribution (λ = 3.5).
The decoder input is the original text with one position offset. A language id symbol LID is used as the initial token to predict the sentence.; mbart-large-50 is pre-trained model and primarily aimed at being fine-tuned on translation tasks. It can also be fine-tuned on other multilingual sequence-to-sequence tasks. 
See the model hub to look for fine-tuned versions.; As the model is multilingual, it expects the sequences in a different format. A special language id token is used as a prefix in both the source and target text. The text format is [lang_code] X [eos] with X being the source or target text respectively and lang_code is source_lang_code for source text and tgt_lang_code for target text. bos is never used. Once the examples are prepared in this format, it can be trained as any other sequence-to-sequence model.",natural-language-processing,text2text-generation,https://huggingface.co/facebook/mbart-large-50,Paper: https://arxiv.org/pdf/2008.00401.pdf,License: mit,,53 languages,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 72,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 294925,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
mt5-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Google's mT5; mT5 is pretrained on the mC4 corpus, covering 101 languages:; Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.; Note: mT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; Pretraining Dataset: mC4",natural-language-processing,text2text-generation,https://huggingface.co/google/mt5-large,Paper: https://arxiv.org/pdf/2010.11934.pdf,License: apache-2.0,Datasets: mc4,102 languages,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 43,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 28797,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 15.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
vit-base-patch16-224-in21k,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,"Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. ; Disclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. ; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; Note that this model does not provide any fine-tuned heads, as these were zero'd by Google researchers. However, the model does include the pre-trained pooler, which can be used for downstream tasks (such as image classification).",multimodel,feature-extraction,https://huggingface.co/google/vit-base-patch16-224-in21k,Paper: https://arxiv.org/pdf/2010.11929.pdf; https://arxiv.org/pdf/2006.03677.pdf,License: apache-2.0,Datasets: imagenet-21k,,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 57,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 506670,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 46
chinese-bert-wwm-ext,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"For further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking. ; Pre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu; This repository is developed based on：https://github.com/google-research/bert; You may also interested in,; More resources by HFL: https://github.com/ymcui/HFL-Anthology",natural-language-processing,fill-mask,https://huggingface.co/hfl/chinese-bert-wwm-ext,Paper: https://arxiv.org/pdf/1906.08101.pdf; https://arxiv.org/pdf/2004.13922.pdf,License: apache-2.0,,Chinese,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 97,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 822919,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
emotion-english-distilroberta-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"With this model, you can classify emotions in English text data. The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class:; The model is a fine-tuned checkpoint of DistilRoBERTa-base. For a 'non-distilled' emotion model, please refer to the model card of the RoBERTa-large version.; a) Run emotion model with 3 lines of code on single text example using Hugging Face's pipeline command on Google Colab:; ; b) Run emotion model on multiple examples and full datasets (e.g., .csv files) on Google Colab:",natural-language-processing,text-classification,https://huggingface.co/j-hartmann/emotion-english-distilroberta-base,,,,English,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 192,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1127684,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 661.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 48
BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. Recent work shows that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models.; PubMedBERT is pretrained from scratch using abstracts from PubMed and full-text articles from PubMedCentral. This model achieves state-of-the-art performance on many biomedical NLP tasks, and currently holds the top score on the Biomedical Language Understanding and Reasoning Benchmark.; If you find PubMedBERT useful in your research, please cite the following paper:",natural-language-processing,fill-mask,https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext,Paper: https://arxiv.org/pdf/2007.15779.pdf,License: mit,,English,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 100,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1248516,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 878.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
bert-large-portuguese-cased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"; BERTimbau Large is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.; For further information or requests, please go to BERTimbau repository.; If you use our work, please cite:",natural-language-processing,fill-mask,https://huggingface.co/neuralmind/bert-large-portuguese-cased,,License: mit,Datasets: brWaC,Portuguese,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 18827,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
voice-activity-detection,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; The collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Relies on pyannote.audio 2.1: see installation instructions.; For commercial enquiries and scientific consulting, please contact me.For technical questions and bug reports, please check pyannote.audio Github repository.",audio,automatic-speech-recognition,https://huggingface.co/pyannote/voice-activity-detection,,License: mit,Datasets: ami; dihard; voxconverse,,pyannote.audio,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 78,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 97630,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
Real-ESRGAN,,,,,,,,,"PyTorch implementation of a Real-ESRGAN model trained on custom dataset. This model shows better results on faces compared to the original version. It is also easier to integrate this model into your projects.; Real-ESRGAN is an upgraded ESRGAN trained with pure synthetic data is capable of enhancing details while removing annoying artifacts for common real-world images.; Code for using model you can obtain in our repo.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/ai-forever/Real-ESRGAN,Paper: https://arxiv.org/pdf/2107.10833.pdf,,,Russian; English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 42,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 201.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 19
distiluse-base-multilingual-cased-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net; This model was trained by sentence-transformers. ",natural-language-processing,sentence-similarity,https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1,Paper: https://arxiv.org/pdf/1908.10084.pdf,License: apache-2.0,,multilingual,PyTorch; TensorFlow; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 42,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 78416,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
sentiment-roberta-large-english,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model (""SiEBERT"", prefix for ""Sentiment in English"") is a fine-tuned checkpoint of RoBERTa-large (Liu et al. 2019). It enables reliable binary sentiment analysis for various types of English-language text. For each instance, it predicts either positive (1) or negative (0) sentiment. The model was fine-tuned and evaluated on 15 data sets from diverse text sources to enhance generalization across different types of texts (reviews, tweets, etc.). Consequently, it outperforms models trained on only one type of text (e.g., movie reviews from the popular SST-2 benchmark) when used on new data as shown below. ; If you want to predict sentiment for your own data, we provide an example script via Google Colab. You can load your data to a Google Drive and run the script for free on a Colab GPU. Set-up only takes a few minutes. We suggest that you manually label a subset of your data to evaluate performance for your use case. For performance benchmark values across various sentiment analysis contexts, please refer to our paper (Hartmann et al. 2022).; ; The easiest way to use the model for single predictions is Hugging Face's sentiment analysis pipeline, which only needs a couple lines of code as shown in the following example:; ",natural-language-processing,text-classification,https://huggingface.co/siebert/sentiment-roberta-large-english,Paper: https://arxiv.org/pdf/1907.11692.pdf,,,English,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 72,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 253250,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 22
pegasus_paraphrase,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"PEGASUS fine-tuned for paraphrasing; Created by Arpit Rajauria
",natural-language-processing,text2text-generation,https://huggingface.co/tuner007/pegasus_paraphrase,,License: apache-2.0,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 142,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 132278,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 51
yolos-tiny,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c921e73e7b6d3bf06402_Object%20Detection.svg,,"YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. ; Disclaimer: The team releasing YOLOS did not write a model card for this model so this model card has been written by the Hugging Face team.; YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.; You can use the raw model for object detection. See the model hub to look for all available YOLOS models.",computer-vision,object-detection,https://huggingface.co/hustvl/yolos-tiny,Paper: https://arxiv.org/pdf/2106.00666.pdf,License: apache-2.0,Datasets: coco,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 118,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 666716,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 37
chinese-wav2vec2-base,,,,,,,,,"Pretrained on 10k hours WenetSpeech L subset. More details in  TencentGameMate/chinese_speech_pretrain; This model does not have a tokenizer as it was pretrained on audio alone. 
In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data.; python package:
transformers==4.16.2; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/TencentGameMate/chinese-wav2vec2-base,,License: mit,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5310,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
wav2vec2-large-chinese-zh-cn,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Chinese using the train and validation splits of Common Voice 6.1, CSS10 and ST-CMDS.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned on RTX3090 for 50h; The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:",audio,automatic-speech-recognition,https://huggingface.co/wbbbbb/wav2vec2-large-chinese-zh-cn,,License: apache-2.0,Datasets: common_voice,Chinese,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 51249,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
donut-base-finetuned-cord-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"Donut model fine-tuned on CORD. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository.; Disclaimer: The team releasing Donut did not write a model card for this model so this model card has been written by the Hugging Face team.; Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. ; ; This model is fine-tuned on CORD, a document parsing dataset.",multimodel,image-to-text,https://huggingface.co/naver-clova-ix/donut-base-finetuned-cord-v2,Paper: https://arxiv.org/pdf/2111.15664.pdf,License: mit,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 19258,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 811.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 26
japanese-stable-diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; One more step before getting this model.This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.The CreativeML OpenRAIL License specifies: ; By clicking on ""Access repository"" below, you accept that your contact information (email address and username) can be shared with the model authors as well.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; ",multimodel,text-to-image,https://huggingface.co/rinna/japanese-stable-diffusion,Paper: https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2205.12952.pdf,License: other,,Japanese,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 162,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1745,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
whisper-medium,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al from OpenAI. The original code repository can be found here.; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. ; The models were trained on either English-only data or multilingual data. The English-only models were trained 
on the task of speech recognition. The multilingual models were trained on both speech recognition and speech 
translation. For speech recognition, the model predicts transcriptions in the same language as the audio. 
For speech translation, the model predicts transcriptions to a different language to the audio.",audio,automatic-speech-recognition,https://huggingface.co/openai/whisper-medium,Paper: https://arxiv.org/pdf/2212.04356.pdf,License: apache-2.0,,99 languages,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 96,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 28205,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 9.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 23
bloomz-7b1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",natural-language-processing,text-generation,https://huggingface.co/bigscience/bloomz-7b1,Paper: https://arxiv.org/pdf/2211.01786.pdf,License: bigscience-bloom-rail-1.0,Datasets: bigscience/xP3,46 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 97,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 56860,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 11
waifu-diffusion-v1-3,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Waifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.; The model originally used for fine-tuning is Stable Diffusion 1.4, which is a latent image diffusion model trained on LAION2B-en. The current model has been fine-tuned with a learning rate of 5.0e-6 for 10 epochs on 680k anime-styled images.; See here for an in-depth overview of Waifu Diffusion 1.3.; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
The CreativeML OpenRAIL License specifies: ; This model can be used for entertainment purposes and as a generative art assistant.",multimodel,text-to-image,https://huggingface.co/hakurei/waifu-diffusion-v1-3,,License: creativeml-openrail-m,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 579,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 127.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 204
bloomz-7b1-mt,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t’aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",natural-language-processing,text-generation,https://huggingface.co/bigscience/bloomz-7b1-mt,Paper: https://arxiv.org/pdf/2211.01786.pdf,License: bigscience-bloom-rail-1.0,Datasets: bigscience/xP3mt,46 languages,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 102,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 46701,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
sd-vae-ft-ema,,,,,,,,,"These weights are intended to be used with the ? diffusers library. If you are looking for the model to use with the original CompVis Stable Diffusion codebase, come here. ; You can integrate this fine-tuned VAE decoder to your existing diffusers workflows, by including a vae argument to the StableDiffusionPipeline; We publish two kl-f8 autoencoder versions, finetuned from the original kl-f8 autoencoder on a 1:1 ratio of LAION-Aesthetics and LAION-Humans, an unreleased subset containing only SFW images of humans. The intent was to fine-tune on the Stable Diffusion training set (the autoencoder was originally trained on OpenImages) but also enrich the dataset with images of humans to improve the reconstruction of faces.
The first, ft-EMA, was resumed from the original checkpoint, trained for 313198 steps and uses EMA weights. It uses the same loss configuration as the original checkpoint (L1 + LPIPS).
The second, ft-MSE, was resumed from ft-EMA and uses EMA weights and was trained for another 280k steps using a different loss, with more emphasis 
on MSE reconstruction (MSE + 0.1 * LPIPS). It produces somewhat ``smoother'' outputs. The batch size for both versions was 192 (16 A100s, batch size 12 per GPU).
To keep compatibility with existing models, only the decoder part was finetuned; the checkpoints can be used as a drop-in replacement for the existing autoencoder.; Original kl-f8 VAE vs f8-ft-EMA vs f8-ft-MSE; Visualization of reconstructions on  256x256 images from the COCO2017 validation dataset. ",,,https://huggingface.co/stabilityai/sd-vae-ft-ema,,License: mit,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 78,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 120829,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 670.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 10
GODEL-v1_1-base-seq2seq,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c462ee7f35a824dae892_Conversational.svg,,"GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.; Chitchat example:; Instruction: given a dialog context, you need to response empathically.  
User: Does money buy happiness? 
Agent: It is a question. Money buys you a lot of things, but not enough to buy happiness. 
User: What is the best way to buy happiness ? 
Agent: Happiness is bought through your experience and not money. ; Grounded response generation example:; Instruction: given a dialog context and related knowledge, you need to response safely based on the knowledge. 
Knowledge: The best Stardew Valley mods PCGamesN_0 / About SMAPI 
User: My favorite game is stardew valley. stardew valley is very fun. 
Agent: I love Stardew Valley mods, like PCGamesN_0 / About SMAPI. ",natural-language-processing,conversational,https://huggingface.co/microsoft/GODEL-v1_1-base-seq2seq,Paper: https://arxiv.org/pdf/2206.11309.pdf,License: mit,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 55,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2525,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 894.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
flan-t5-small,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:",natural-language-processing,text2text-generation,https://huggingface.co/google/flan-t5-small,Paper: https://arxiv.org/pdf/2210.11416.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,Datasets: svakulenk0/qrecc; taskmaster2; djaym7/wiki_dialog; deepmind/code_contests; lambada; gsm8k; aqua_rat; esnli; quasc; qed,5 languages,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 97,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 183179,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 216
Cyberpunk-Anime-Diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"; An AI model that generates cyberpunk anime characters!~; Based of a finetuned Waifu Diffusion V1.3 Model with Stable Diffusion V1.5 New Vae, training in Dreambooth; by DGSpitzer; This repo contains both .ckpt and Diffuser model files. It's compatible to be used as any Stable Diffusion model, using standard Stable Diffusion Pipelines.",multimodel,text-to-image,https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion,,License: creativeml-openrail-m,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 511,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4152,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 311
mo-di-diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Mo Di Diffusion; This is the fine-tuned Stable Diffusion 1.5 model trained on screenshots from a popular animation studio.
Use the tokens modern disney style in your prompts for the effect.; If you enjoy my work, please consider supporting me
; Videogame Characters rendered with the model:

Animal Characters rendered with the model:

Cars and Landscapes rendered with the model:
; modern disney lara croft
Steps: 50, Sampler: Euler a, CFG scale: 7, Seed: 3940025417, Size: 512x768",multimodel,text-to-image,https://huggingface.co/nitrosocke/mo-di-diffusion,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 889,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 16038,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 318
classic-anim-diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This is the fine-tuned Stable Diffusion model trained on screenshots from a popular animation studio.
Use the tokens classic disney style in your prompts for the effect.; If you enjoy my work, please consider supporting me
; Characters rendered with the model:

Animals rendered with the model:

Cars and Landscapes rendered with the model:
; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; You can also export the model to ONNX, MPS and/or FLAX/JAX.",multimodel,text-to-image,https://huggingface.co/nitrosocke/classic-anim-diffusion,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 394,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3696,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 115
galactica-120b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; Model card from the original repo ; Following Mitchell et al. (2018), this model card provides information about the GALACTICA model, how it was trained, and the intended use cases. Full details about how the model was trained and evaluated can be found in the release paper.; This model checkpoint was integrated into the Hub by Manuel Romero; The GALACTICA models are trained on a large-scale scientific corpus. The models are designed to perform scientific tasks, including but not limited to citation prediction, scientific QA, mathematical reasoning, summarization, document generation, molecular property prediction and entity extraction. The models were developed by the Papers with Code team at Meta AI to study the use of language models for the automatic organization of science. We train models with sizes ranging from 125M to 120B parameters. Below is a summary of the released models:",natural-language-processing,text-generation,https://huggingface.co/facebook/galactica-120b,Paper: https://arxiv.org/pdf/1810.03993.pdf,License: cc-by-nc-4.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 133,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 329,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 244.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
Ghibli-Diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This is the fine-tuned Stable Diffusion model trained on images from modern anime feature films from Studio Ghibli.
Use the tokens ghibli style in your prompts for the effect.; If you enjoy my work and want to test new models before release, please consider supporting me
; Characters rendered with the model:

Cars and Animals rendered with the model:

Landscapes rendered with the model:

ghibli style beautiful Caribbean beach tropical (sunset) - Negative prompt: soft blurry

ghibli style ice field white mountains ((northern lights)) starry sky low horizon - Negative prompt: soft blurry; ghibli style (storm trooper) Negative prompt: (bad anatomy)
Steps: 20, Sampler: DPM++ 2M Karras, CFG scale: 7, Seed: 3450349066, Size: 512x704; ghibli style VW beetle Negative prompt: soft blurry
Steps: 30, Sampler: Euler a, CFG scale: 7, Seed: 1529856912, Size: 704x512",multimodel,text-to-image,https://huggingface.co/nitrosocke/Ghibli-Diffusion,,License: creativeml-openrail-m,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 527,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7452,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 74
stable-diffusion-2-depth,,,,,,,,,"This model card focuses on the model associated with the Stable Diffusion v2 model, available here.; This stable-diffusion-2-depth model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning.; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model",,,https://huggingface.co/stabilityai/stable-diffusion-2-depth,Paper: https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2202.00512.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: openrail++,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 340,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 21273,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 11.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
GPT-JT-6B-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; Feel free to try out our Online Demo!; ; With a new decentralized training algorithm, we fine-tuned GPT-J (6B) on 3.53 billion tokens, resulting in GPT-JT (6B), a model that outperforms many 100B+ parameter models on classification benchmarks.; We incorporated a collection of open techniques and datasets to build GPT-JT:",natural-language-processing,text-generation,https://huggingface.co/togethercomputer/GPT-JT-6B-v1,,License: apache-2.0,Datasets: natural_instructions; the_pile; cot; Muennighoff/P3,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 295,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5298,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 12.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
openjourney-v4,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Trained on Stable Diffusion v1.5 using +124000 images, 12400 steps, 4 epochs +32 training hours.; ? Openjourney-v4 prompts; Pss... ""mdjrny-v4 style"" is not necessary anymore (yay!); ? Want to learn how to train Openjourney? ?? Join our course ?",multimodel,text-to-image,https://huggingface.co/prompthero/openjourney-v4,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 1.1k,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 25158,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 116
blip-image-captioning-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"Model card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone).; Authors from the paper write in the abstract:; Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.; You can use this model for conditional and un-conditional image captioning",multimodel,image-to-text,https://huggingface.co/Salesforce/blip-image-captioning-base,Paper: https://arxiv.org/pdf/2201.12086.pdf,License: bsd-3-clause,,,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 157,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 446902,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 260
Promptist,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; You can try the online demo at https://huggingface.co/spaces/microsoft/Promptist.; [Note] the online demo at HuggingFace Space is using CPU, so slow generation speed would be expected. Please load the model locally with GPUs for faster generation.",natural-language-processing,text-generation,https://huggingface.co/microsoft/Promptist,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 40,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3816,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 512.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 23
xformers_pre_built,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/r4ziel/xformers_pre_built,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 346.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
finbert-tone-finetuned-finance-topic-classification,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is a fine-tuned version of yiyanghkust/finbert-tone on Twitter Financial News Topic dataset.
It achieves the following results on the evaluation set:; Model determines the financial topic of given tweets over 20 various topics. Given the unbalanced distribution of the class labels, the weights were adjusted to pay attention to the less sampled labels which should increase overall performance..; More information needed; More information needed; The following hyperparameters were used during training:",natural-language-processing,text-classification,https://huggingface.co/nickmuchi/finbert-tone-finetuned-finance-topic-classification,,,Datasets: zeroshot/twitter-financial-news-topic,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2042,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 440.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
basil_mix,,,,,,,,,"This model and its derivatives(image, merged model) can be freely used for non-profit purposes only.; You may not use this model and its derivatives on websites, apps, or other platforms where you can or plan to earn income or donations. If you wish to use it for such purposes, please contact nuigurumi.; Introducing the model itself is allowed for both commercial and non-commercial purposes, but please include the model name and a link to this repository when doing so.; このモデル及びその派生物(生成物、マ`ジモデル)は、完全に非永目的の使用に限り、自由に利用することができます。; あなたが入や寄付を得ることのできる、もしくは得る予定のWebサイト、アプリ、その他でこのモデル及びその派生物を利用することはできません。利用したい龊悉nuigurumiにBjしてください。",,,https://huggingface.co/nuigurumi/basil_mix,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 946,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1847,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 12.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
dreamlike-anime-1.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Add anime to your prompt to make your gens look more anime.Add photo to your prompt to make your gens look more photorealistic and have better anatomy.This model was trained on 768x768px images, so use 768x768px, 704x832px, 832x704px, etc. Higher resolution or non-square aspect ratios may produce artifacts.  ; Add this to the start of your prompts for best results:; Use negative prompts for best results, for example:; 1girl, girl, etc. give a bit different results, feel free to experiment and see which one you like more!; Use this model as well as Dreamlike Diffusion 1.0 and Dreamlike Photoreal 2.0 for free on dreamlike.art!",multimodel,text-to-image,https://huggingface.co/dreamlike-art/dreamlike-anime-1.0,,License: other,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 184,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 53770,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 45
Anything-Preservation,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"DISCLAIMER! This Is A Preservation Repository!; Welcome to Anything V3 - Better VAE. It currently has three model formats: diffusers, ckpt, and safetensors. You'll never see a grey image result again. This model is designed to produce high-quality, highly detailed anime-style images with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags for image generation.
e.g. 1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden ; We support a Gradio Web UI to run Anything V3 with Better VAE:; ; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion. You can also export the model to ONNX, MPS and/or FLAX/JAX.",multimodel,text-to-image,https://huggingface.co/AdamOswald1/Anything-Preservation,,License: creativeml-openrail-m,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 99,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 78394,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
CLIP-ViT-bigG-14-laion2B-39B-b160k,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c6145e63a23d3f282ad5_Zero-Shot%20Image%20Classification.svg,,"A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Mitchell Wortsman on the stability.ai cluster.; The license for this model is MIT.; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. ",computer-vision,zero-shot-image-classification,https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k,Paper: https://arxiv.org/pdf/1910.04867.pdf,License: mit,,,PyTorch; OpenCLIP,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 82,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 339705,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 20.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
blip2-opt-2.7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters).
It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository.; Disclaimer: The team releasing BLIP-2 did not write a model card for this model so this model card has been written by the Hugging Face team.; BLIP-2 consists of 3 models: a CLIP-like image encoder, a Querying Transformer (Q-Former) and a large language model.; The authors initialize the weights of the image encoder and large language model from pre-trained checkpoints and keep them frozen
while training the Querying Transformer, which is a BERT-like Transformer encoder that maps a set of ""query tokens"" to query embeddings,
which bridge the gap between the embedding space of the image encoder and the large language model.; The goal for the model is simply to predict the next text token, giving the query embeddings and the previous text.",multimodel,image-to-text,https://huggingface.co/Salesforce/blip2-opt-2.7b,Paper: https://arxiv.org/pdf/2301.12597.pdf,License: mit,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 103,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 89205,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 16.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 47
embeddings,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/nolanaatama/embeddings,,License: creativeml-openrail-m,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 165,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.57MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
SD-Silicon,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"SD-Silicon: A series of general-purpose models based off the experimental automerger, autoMBW.; A collaborative creation of Xerxemi#6423 & Xynon#7407.; ; All models listed have baked WD1.3 VAE. However, for the purposes of this model series, external VAE is also recommended. ; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage. The CreativeML OpenRAIL License specifies: ",multimodel,text-to-image,https://huggingface.co/Xynon/SD-Silicon,,License: creativeml-openrail-m,,,Safetensors,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 166,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.91KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
stable-diffusion-guide,,,,,,,,,"? CLICK HERE TO OPEN THIS DOCUMENT IN FULL WIDTH(The index won't work otherwise).; ???? HAZ CLICK AQUí PARA VER ESTA GUíA EN ESPA?OL; ?; ?; Stable Diffusion is a very powerful AI image generation software you can run on your own home computer. It uses ""models"" which function like the brain of the AI, and can make almost anything, given that someone has trained it to do it. The biggest uses are anime art, photorealism, and NSFW content.",,,https://huggingface.co/hollowstrawberry/stable-diffusion-guide,,License: wtfpl,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 156,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 80.99KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
French-Tortoise,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9511aaa882b322dc3d8_Text-to-Speech.svg,,"I stop training tortoise models because this technology is just outdated and too slow, I'm still the first one to want a good French TTS and I watch every day if new ambitious project is born and I won't hesitate to create a new hugging face project! ; V2.5 Model : 
Fine tune of my V2 model on all CommonVoice dataset (517k sample) on 2.5k step (batch size 200), Voice cloning has improved a bit but is still not great. However, if you fine tune this model on your own personality dataset then you can get pretty good results. A good V3 model would be to fine tune for like 50k steps on this dataset and I think there would be a way to get good results but I won't try; V2 Model : ; Tortoise base model Fine tuned on a custom multispeaker French dataset of 120k samples (SIWIS + Common Voice subset + M-AILABS) on 10k step with a RTX 3090 (~= 21 hours of training), with Text LR Weight at 1
Result : The model can speak French much better without an English accent but the voice clone hardly works; V1 Model : ",audio,text-to-speech,https://huggingface.co/Snowad/French-Tortoise,,License: apache-2.0,,French,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.33KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
invoices-donut-model-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,This model is finetuned Donut ML base model on invoices data. Model aims to verify how well Donut performs on enterprise docs.; Mean accuracy on test set: 0.96; Inference:; ; Training loss:,multimodel,image-to-text,https://huggingface.co/katanaml-org/invoices-donut-model-v1,,License: mit,Datasets: katanaml-org/invoices-donut-data-v1,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 782,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 814.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
furryrock-model-safetensors,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/lodestones/furryrock-model-safetensors,,License: wtfpl,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 58,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.5KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
embeddings,,,,,,,,,"Concepts: capri(three-quarter) leggings; If it does not act very strongly against the prompt, it is recommended to increase the strength to about 1.4.; Concepts: low ponytail
; It is recommended to put ""animal ears"" as a negative prompt.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/SenY/embeddings,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.07MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Cerebras-GPT-13B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Check out our Blog Post and arXiv paper!; The Cerebras-GPT family is released to facilitate research into LLM scaling laws using open architectures and data sets and demonstrate the simplicity of and scalability of training LLMs on the Cerebras software and hardware stack. All Cerebras-GPT models are available on Hugging Face.; The family includes 111M, 256M, 590M, 1.3B, 2.7B, 6.7B, and 13B models.; All models in the Cerebras-GPT family have been trained in accordance with Chinchilla scaling laws (20 tokens per model parameter) which is compute-optimal.; These models were trained on the Andromeda AI supercomputer comprised of 16 CS-2 wafer scale systems. Cerebras' weight streaming technology simplifies the training of LLMs by disaggregating compute from model storage. This allowed for efficient scaling of training across nodes using simple data parallelism.",natural-language-processing,text-generation,https://huggingface.co/cerebras/Cerebras-GPT-13B,Paper: https://arxiv.org/pdf/2304.03208.pdf; https://arxiv.org/pdf/2203.15556.pdf; https://arxiv.org/pdf/2101.00027.pdf,License: apache-2.0,Datasets: the_pile,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 626,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9873,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
pix2struct-ocrvqa-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4f95e63a23d3f26e06e_Visual%20Question%20Answering.svg,,"; Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The full list of available models can be found on the Table 1 of the paper:; ; The abstract of the model states that: ; Visually-situated language is ubiquitous―sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and
forms. Perhaps due to this diversity, previous work has typically relied on domainspecific recipes with limited sharing of the underlying data, model architectures,
and objectives. We present Pix2Struct, a pretrained image-to-text model for
purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse
masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large
source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy,
we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions
are rendered directly on top of the input image. For the first time, we show that a
single pretrained model can achieve state-of-the-art results in six out of nine tasks
across four domains: documents, illustrations, user interfaces, and natural images.",multimodel,visual-question-answering,https://huggingface.co/google/pix2struct-ocrvqa-large,Paper: https://arxiv.org/pdf/2210.03347.pdf,License: apache-2.0,,5 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 249,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
flan-alpaca-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"? We developed Flacuna by fine-tuning Vicuna-13B on the Flan collection. Flacuna is better than Vicuna at problem-solving. Access the model here https://huggingface.co/declare-lab/flacuna-13b-v1.0.; ? Curious to know the performance of ? ? Flan-Alpaca on large-scale LLM evaluation benchmark, InstructEval? Read our paper https://arxiv.org/pdf/2306.04757.pdf. We evaluated more than 10 open-source instruction-tuned LLMs belonging to various LLM families including Pythia, LLaMA, T5, UL2, OPT, and Mosaic. Codes and datasets: https://github.com/declare-lab/instruct-eval; ? FLAN-T5 is also useful in text-to-audio generation. Find our work at https://github.com/declare-lab/tango if you are interested.; Our repository contains code for extending the Stanford Alpaca
synthetic instruction tuning to existing instruction-tuned models such as Flan-T5.
We have a live interactive demo thanks to Joao Gante!
We are also benchmarking many instruction-tuned models at declare-lab/flan-eval.
Our pretrained models are fully available on HuggingFace ? :; *recommended for better performance",natural-language-processing,text2text-generation,https://huggingface.co/declare-lab/flan-alpaca-large,Paper: https://arxiv.org/pdf/2306.04757.pdf; https://arxiv.org/pdf/2210.11416.pdf,License: apache-2.0,Datasets: tatsu-lab/alpaca,,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 37,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 9780,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 16
gpt4-x-alpaca,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"As a base model we used: https://huggingface.co/chavinlo/alpaca-13b; Finetuned on GPT4's responses, for 3 epochs.; NO LORA; Please do note that the configurations files maybe messed up, this is because of the trainer I used. I WILL NOT EDIT THEM because there are repos hat automatically fix this, changing it might break it. Generally you just need to change anything that's under the name of ""LLaMa"" to ""Llama"" NOTE THE UPPER AND LOWER CASE!!!!",natural-language-processing,text-generation,https://huggingface.co/chavinlo/gpt4-x-alpaca,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 435,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2085,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 39
deplot,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4f95e63a23d3f26e06e_Visual%20Question%20Answering.svg,,"; The abstract of the paper states that: ; Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than >28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.; You can run a prediction by querying an input image together with a question as follows:; You can use the convert_pix2struct_checkpoint_to_pytorch.py script as follows:",multimodel,visual-question-answering,https://huggingface.co/google/deplot,Paper: https://arxiv.org/pdf/2212.10505.pdf,License: apache-2.0,,5 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 48,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7488,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
BiomedCLIP-PubMedBERT_256-vit_base_patch16_224,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c6145e63a23d3f282ad5_Zero-Shot%20Image%20Classification.svg,,"BiomedCLIP is a biomedical vision-language foundation model that is pretrained on PMC-15M, a dataset of 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, using contrastive learning. 
It uses PubMedBERT as the text encoder and Vision Transformer as the image encoder, with domain-specific adaptations.
It can perform various vision-language processing (VLP) tasks such as cross-modal retrieval, image classification, and visual question answering. 
BiomedCLIP establishes new state of the art in a wide range of standard datasets, and substantially outperforms prior VLP approaches:; ; Please refer to this example notebook.; This model is intended to be used solely for (I) future research on visual-language processing and (II) reproducibility of the experimental results reported in the reference paper.; The primary intended use is to support AI researchers building on top of this work. BiomedCLIP and its associated models should be helpful for exploring various biomedical VLP research questions, especially in the radiology domain.",computer-vision,zero-shot-image-classification,https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224,Paper: https://arxiv.org/pdf/2303.00915.pdf,License: mit,,English,OpenCLIP,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 65,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 52878,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 788.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
MotionBERT,,,,,,,,,"This is the official PyTorch implementation of the paper ""Learning Human Motion Representations: A Unified Perspective"".; Please refer to docs/inference.md.; Hints; In most use cases (especially with finetuning), MotionBERT-Lite gives a similar performance with lower computation overhead. ;  
Scripts and docs for pretraining",,,https://huggingface.co/walterzhu/MotionBERT,Paper: https://arxiv.org/pdf/2210.06551.pdf,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 89.06KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
TurboHeaven,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/TurboT/TurboHeaven,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 73.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
gpt4all-j,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"An Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions including word problems, multi-turn dialogue, code, poems, songs, and stories.; This model has been finetuned from GPT-J; We have released several versions of our finetuned GPT-J model using different dataset versions; To download a model with a specific revision run ; Downloading without specifying revision defaults to main/v1.0.",natural-language-processing,text-generation,https://huggingface.co/nomic-ai/gpt4all-j,,License: apache-2.0,Datasets: nomic-ai/gpt4all-j-prompt-generations,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 220,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8452,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 24.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 20
vicuna-7b-v1.1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"NOTE: New version availablePlease check out a newer version of the weights here.If you still want to use this old version, please see the compatibility and difference between different versions here.; Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Command line interface: https://github.com/lm-sys/FastChat#vicuna-weights.APIs (OpenAI API, Huggingface API): https://github.com/lm-sys/FastChat/tree/main#api.  ; Vicuna v1.1 is fine-tuned from LLaMA with supervised instruction fine-tuning.
The training data is around 70K conversations collected from ShareGPT.com.
See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.",natural-language-processing,text-generation,https://huggingface.co/lmsys/vicuna-7b-v1.1,Paper: https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 61,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1061199,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
vicuna-7b-1.1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"delta v1.1 merge ; Model type:
Vicuna is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
Vicuna was trained between March 2023 and April 2023.; Organizations developing the model:
The Vicuna team with members from UC Berkeley, CMU, Stanford, and UC San Diego.; Paper or resources for more information:
https://vicuna.lmsys.org/",natural-language-processing,text-generation,https://huggingface.co/eachadea/vicuna-7b-1.1,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 103,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11764,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
dolly-v2-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Databricks' dolly-v2-7b, an instruction-following large language model trained on the Databricks machine learning platform 
that is licensed for commercial use. Based on pythia-6.9b, Dolly is trained on ~15k instruction/response fine tuning records 
databricks-dolly-15k generated 
by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,
information extraction, open QA and summarization. dolly-v2-7b is not a state-of-the-art model, but does exhibit surprisingly 
high quality instruction following behavior not characteristic of the foundation model on which it is based.  ; Dolly v2 is also available in these other models sizes:; Please refer to the dolly GitHub repo for tips on 
running inference for various GPU configurations.; Owner: Databricks, Inc.; dolly-v2-7b is a 6.9 billion parameter causal language model created by Databricks that is derived from 
EleutherAI's Pythia-6.9b and fine-tuned 
on a ~15K record instruction corpus generated by Databricks employees and released under a permissive license (CC-BY-SA)",natural-language-processing,text-generation,https://huggingface.co/databricks/dolly-v2-7b,,License: mit,Datasets: databricks/databricks-dolly-15k,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 121,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 16137,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 14.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
LaMini-Flan-T5-783M,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"

; ; This model is one of our LaMini-LM model series in paper ""LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"". This model is a fine-tuned version of google/flan-t5-large on LaMini-instruction dataset that contains 2.58M samples for instruction fine-tuning. For more information about our dataset, please refer to our project repository.You can view other models of LaMini-LM series as follows. Models with ? are those with the best overall performance given their size/architecture, hence we recommend using them. More details can be seen in our paper. ; We recommend using the model to response to human instructions written in natural language. ; We now show you how to load and use our model using HuggingFace pipeline().",natural-language-processing,text2text-generation,https://huggingface.co/MBZUAI/LaMini-Flan-T5-783M,Paper: https://arxiv.org/pdf/2304.14402.pdf,License: cc-by-nc-4.0,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 37,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7428,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
llama-30b-supercot,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Merge of huggyllama/llama-30b + kaiokendev/SuperCOT-LoRA ; Supercot was trained to work with langchain prompting. ; Load up locally in my custom LLM notebook that uses the Oobabooga modules to load up models: https://github.com/ausboss/Local-LLM-Langchain; Then you can add cells from of these other notebooks for testing: https://github.com/gkamradt/langchain-tutorials; This LoRA is compatible with any 7B, 13B or 30B 4-bit quantized LLaMa model, including ggml quantized converted bins",natural-language-processing,text-generation,https://huggingface.co/ausboss/llama-30b-supercot,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 123,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1882,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 12.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
starpii,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; This is an NER model trained to detect Personal Identifiable Information (PII) in code datasets. We ask that you read and agree to the following Terms of Use before using the model:; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This is an NER model trained to detect Personal Identifiable Information (PII) in code datasets. We fine-tuned bigcode-encoder
on a PII dataset we annotated, available with gated access at bigcode-pii-dataset (see bigcode-pii-dataset-training for the exact data splits).
We added a linear layer as a token classification head on top of the encoder model, with 6 target classes: Names, Emails, Keys, Passwords, IP addresses and Usernames. ; The finetuning dataset contains 20961 secrets and 31 programming languages, but the base encoder model was pre-trained on 88 
programming languages from The Stack dataset.",natural-language-processing,token-classification,https://huggingface.co/bigcode/starpii,Paper: https://arxiv.org/pdf/2301.03988.pdf,,Datasets: bigcode/pii-annotated-toloka-donwsample-emails; bigcode/pseudo-labeled-python-data-pii-detection-filtered,code,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 52,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 18154,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
GPT4-X-Alpasta-30b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Dont be upsetti, here, have some spaghetti! Att: A'eala <3; Information; This is an attempt at improving Open Assistant's performance as an instruct while retaining its excellent prose. The merge consists of Chansung's GPT4-Alpaca Lora and Open Assistant's native fine-tune.; Benchmarks; FP16",natural-language-processing,text-generation,https://huggingface.co/MetaIX/GPT4-X-Alpasta-30b,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 62,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 405,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 10
wizardLM-7B-HF,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are the result of merging the delta weights with the original Llama7B model.; The code for merging is provided in the WizardLM official Github repo.; The original WizardLM deltas are in float32, and this results in producing an HF repo that is also float32, and is much larger than a normal 7B Llama model.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/wizardLM-7B-HF,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 83,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 43811,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
wizardLM-7B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for WizardLM's WizardLM 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,https://huggingface.co/TheBloke/wizardLM-7B-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 147,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 25.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
fastchat-t5-3b-v1.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Model type:
FastChat-T5 is an open-source chatbot trained by fine-tuning Flan-t5-xl (3B parameters) on user-shared conversations collected from ShareGPT.
It is based on an encoder-decoder transformer architecture, and can autoregressively generate responses to users' inputs. ; Model date:
FastChat-T5 was trained on April 2023.; Organizations developing the model:
The FastChat developers, primarily Dacheng Li, Lianmin Zheng and Hao Zhang.; Paper or resources for more information:
https://github.com/lm-sys/FastChat#FastChat-T5; License:
Apache License 2.0",natural-language-processing,text2text-generation,https://huggingface.co/lmsys/fastchat-t5-3b-v1.0,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 249,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 495682,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 9
detail-tweaker-lora,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/OedoSoldier/detail-tweaker-lora,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 93,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 37.9MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
distilbert-base-multilingual-cased-sentiments-student,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is distilled from the zero-shot classification pipeline on the Multilingual Sentiment 
dataset using this script. ; In reality the multilingual-sentiment dataset is annotated of course, 
but we'll pretend and ignore the annotations for the sake of example.; Notebook link: here; Result can be reproduce using the following commands:; If you are training this model on Colab, make the following code changes to avoid Out-of-memory error message:",natural-language-processing,text-classification,https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student,,License: apache-2.0,Datasets: tyqiangz/multilingual-sentiments,12 languages,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 17052,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
image-upscaler-restoration,,,,,,,,,"Disclaimer:
I have no associate with the models in repo. The models are only for convenient upload/download. The copyright belongs to the original pulisher.; Models for image upscaling/restoration using neural networks; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/senhan07/image-upscaler-restoration,,,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.18KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
BELLE-LLaMA-EXT-13B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Considering LLaMA's license constraints, the model is for research and learning only. 
Please strictly respect LLaMA's usage policy. We are not allowed to publish weights for LLaMA, of course, even finetuned, but there is no problem publishing the difference, a patch that we suggest to apply to the files. 
The encryption is a simple XOR between files, ensuring that only the people that have access to the original weights (from completely legal sources, of course) can transform them into finetuned weights. 
You can find the decrypt code on https://github.com/LianjiaTech/BELLE/tree/main/models .; If you find this model helpful, please like this model and star us on https://github.com/LianjiaTech/BELLE !; This model comes from a two-phrase training on original LLaMA 13B.; You can use the following command in Bash.
Please replace ""/path/to_encrypted"" with the path where you stored your encrypted file, 
replace ""/path/to_original_llama_7B"" with the path where you stored your original llama7B file, 
and replace ""/path/to_finetuned_model"" with the path where you want to save your final trained model.; After executing the aforementioned command, you will obtain the following files.",natural-language-processing,text2text-generation,https://huggingface.co/BelleGroup/BELLE-LLaMA-EXT-13B,Paper: https://arxiv.org/pdf/2304.07854.pdf,License: gpl-3.0,,Chinese; English,PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 45,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chinese-alpaca-plus-13b-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"发布中文LLaMA-Plus, Alpaca-Plus 13B版本模型; 发布中文LLaMA-Plus, Alpaca-Plus 13B版本，改进点如下：; 本模型是 decapoda-research/llama-13b-hf 
底座模型 合并 ziqingyang/chinese-llama-plus-lora-13b 
和 ziqingyang/chinese-alpaca-plus-lora-13b 两个LoRA权重，
并转化为HuggingFace版本权重（.bin文件），可以直接使用或者继续训练。; test case:; 本项目开源在textgen项目：textgen，可支持llama模型，通过如下命令调用：",natural-language-processing,text-generation,https://huggingface.co/shibing624/chinese-alpaca-plus-13b-hf,,License: other,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 28,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1123,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
4x_NMKD-Superscale-SP_178000_G,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/gemasai/4x_NMKD-Superscale-SP_178000_G,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 67.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
Wizard-Vicuna-13B-Uncensored-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard-Vicuna-13B-Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,https://huggingface.co/TheBloke/Wizard-Vicuna-13B-Uncensored-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 164,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 116.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
XuanYuan2.0,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; XuanYuan LICENSE1.Definitions""Licensor"" refers to the XuanYuan Model Team, the entity responsible for distributing its Software.""Software"" pertains to the XuanYuan model parameters made accessible under this license.2.License GrantSubject to the conditions outlined in this License, the Licensor hereby grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license to utilize the Software exclusively for non-commercial research purposes.You must include the above copyright notice and this permission notice in all copies or significant portions of the Software.3.RestrictionsYou are prohibited from engaging in the following actions with the Software, either in whole or in part: using, copying, modifying, merging, publishing, distributing, reproducing, or creating derivative works of the Software, for commercial, military, or illegal purposes.You must refrain from using the Software for any activities that could undermine China's national security and national unity, jeopardize public interest, or infringe upon the rights and interests of individuals.4.DisclaimerThe Software is provided ""as is"" without any kind of warranty, either express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and non-infringement. The authors or copyright holders shall not be held liable for any claims, damages, or other liabilities arising from the use or performance of the Software, whether in an action of contract, tort, or otherwise, even if they have been advised of the possibility of such damages.5.Limitation of LiabilityTo the extent permitted by applicable law, under no legal theory, including tort, negligence, contract, or otherwise, shall the Licensor be liable to you for any direct, indirect, special, incidental, exemplary, or consequential damages, or any other commercial losses, arising from the use or inability to use the Software. This limitation applies even if the Licensor has been advised of the possibility of such damages.6.Dispute ResolutionThis license shall be governed and construed in accordance with the laws of the People's Republic of China. Any dispute arising from or in connection with this License shall be submitted to the Haidian District People's Court in Beijing.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; XuanYuan: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters; 轩辕是国内首个开源的千亿级中文对话大模型，同时也是首个针对中文金融领域优化的千亿级开源对话大模型。轩辕在BLOOM-176B的基础上针对中文通用领域和金融领域进行了针对性的预训练与微调，它不仅可以应对通用领域的问题，也可以解答与金融相关的各类问题，为用户提供准确、全面的金融信息和建议。",natural-language-processing,text-generation,https://huggingface.co/xyz-nlp/XuanYuan2.0,Paper: https://arxiv.org/pdf/2305.12002.pdf; https://arxiv.org/pdf/2305.11952.pdf; https://arxiv.org/pdf/2305.14471.pdf,License: bigscience-bloom-rail-1.0,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 111,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8519,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
MedicWizard-7B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,WizardLM-Uncensored-7B + MedAlpaca-7B (50%/50%); WizardLM-Uncensored-7B: https://huggingface.co/ehartford/WizardLM-7B-Uncensored; MedAlpaca-7B: https://huggingface.co/medalpaca/medalpaca-7b,natural-language-processing,text-generation,https://huggingface.co/xzuyn/MedicWizard-7B,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 573,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
wizard-mega-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"
? Donate to OpenAccess AI Collective to help us keep building great tools and models!; Manticore is available at https://huggingface.co/openaccess-ai-collective/manticore-13b and fixes many issues with Wizard Mega and adds new datasets to the training.; Wizard Mega is a Llama 13B model fine-tuned on the ShareGPT, WizardLM, and Wizard-Vicuna datasets. These particular datasets have all been filtered to remove responses where the model responds with ""As an AI language model..."", etc or when the model refuses to respond.; Try out the model in HF Spaces. The demo uses a quantized GGML version of the model to quickly return predictions on smaller GPUs (and even CPUs). Quantized GGML may have some minimal loss of model quality.; The Wizard Mega 13B SFT model is being released after two epochs as the eval loss increased during the 3rd (final planned epoch). Because of this, we have preliminarily decided to use the epoch 2 checkpoint as the final release candidate. https://wandb.ai/wing-lian/vicuna-13b/runs/5uebgm49",natural-language-processing,text-generation,https://huggingface.co/openaccess-ai-collective/wizard-mega-13b,,,Datasets: anon8231489123/ShareGPT_Vicuna_unfiltered; ehartford/wizard_vicuna_70k_unfiltered; ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 100,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 740,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 12
wd-1-5-beta3,,,,,,,,,"; For this release, we release five versions of the model:; The WD 1.5 Base model is only intended for training use. For generation, it is recomended to create your own finetunes and loras on top of WD 1.5 Base or use one of the aesthetic models. More information and sample generations for the aesthetic models are in the release notes; https://saltacc.notion.site/WD-1-5-Beta-3-Release-Notes-1e35a0ed1bb24c5b93ec79c45c217f63; WD 1.5 uses the same VAE as WD 1.4, which can be found here https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt",,,https://huggingface.co/waifu-diffusion/wd-1-5-beta3,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 103,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LLaMa-30B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's LLaMA 30b.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh.",,,https://huggingface.co/TheBloke/LLaMa-30B-GGML,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 44,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 290.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Wizard-Vicuna-7B-Uncensored,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is wizard-vicuna-13b trained against LLaMA-7B with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:  ; An uncensored model has no guardrails.  ; You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.",natural-language-processing,text-generation,https://huggingface.co/ehartford/Wizard-Vicuna-7B-Uncensored,,License: other,Datasets: ehartford/wizard_vicuna_70k_unfiltered,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 49,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1342,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Wizard-Vicuna-7B-Uncensored-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard Vicuna 7B Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,https://huggingface.co/TheBloke/Wizard-Vicuna-7B-Uncensored-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 70,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
airoboros-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a fine-tuned 13b parameter LlaMa model, using completely synthetic training data created by https://github.com/jondurbin/airoboros; ;  wb-13b-u is Wizard-Vicuna-13b-Uncensored; I used a jailbreak prompt to generate the synthetic instructions, which resulted in some training data that would likely be censored by other models, such as how-to prompts about synthesizing drugs, making homemade flamethrowers, etc.  Mind you, this is all generated by ChatGPT, not me.  My goal was to simply test some of the capabilities of ChatGPT when unfiltered (as much as possible), and not to intentionally produce any harmful/dangerous/etc. content.; The jailbreak prompt I used is the default prompt in the python code when using the --uncensored flag: https://github.com/jondurbin/airoboros/blob/main/airoboros/self_instruct.py#L39",natural-language-processing,text-generation,https://huggingface.co/jondurbin/airoboros-13b,,License: cc-by-nc-4.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 90,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 470,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
noon-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"We present the 7-billion parameter variant of Noon, an Arabic Large Language model based on BLOOM, a foundation model released by the bigscience workshop.; Noon was trained with the main focus of having a model that responds to various types of instructions and questions (text generation, code generation, mathematical problems, closed/open-book questions, etc.); We trained the model using the ColossalAI framework which fully supports the HuggingFace library models, and implements different optimization and quantization techniques for billion-scale LLMs.; The training data is a combination of Arabic datasets covering multiple tasks, more details are provided in the dataset section.; ?????? ??? ?? ????? ????? ""???""!",natural-language-processing,text-generation,https://huggingface.co/Naseej/noon-7b,,License: bigscience-bloom-rail-1.0,,Arabic; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 29,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 743,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 32.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
WizardLM-30B-Uncensored-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's 'uncensored' WizardLM 30B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,https://huggingface.co/TheBloke/WizardLM-30B-Uncensored-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 104,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 290.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-30B-Uncensored-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Eric Hartford's WizardLM 30B Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/WizardLM-30B-Uncensored-GPTQ,,License: other,Datasets: ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 94,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2430,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
guanaco-7b,,,,,,,,,"| Paper | Code | Demo | ; The Guanaco models are open-source finetuned chatbots obtained through 4-bit QLoRA tuning of LLaMA base models on the OASST1 dataset. They are available in 7B, 13B, 33B, and 65B parameter sizes.; ??Guanaco is a model purely intended for research purposes and could produce problematic outputs.; Guanaco adapter weights are available under Apache 2 license. Note the use of the Guanaco adapter weights, requires access to the LLaMA model weighs. 
Guanaco is based on LLaMA and therefore should be used according to the LLaMA license. ; Here is an example of how you would load Guanaco 7B in 4-bits:",,,https://huggingface.co/timdettmers/guanaco-7b,Paper: https://arxiv.org/pdf/2305.14314.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2304.07327.pdf,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 640.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
falcon-40b-instruct-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains an experimantal GPTQ 4bit model for Falcon-40B-Instruct.; It is the result of quantising to 4bit using AutoGPTQ.; Please note this is an experimental GPTQ model. Support for it is currently quite limited.,natural-language-processing,text-generation,https://huggingface.co/TheBloke/falcon-40b-instruct-GPTQ,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf,License: apache-2.0,Datasets: tiiuae/falcon-refinedweb,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 183,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 51090,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 23.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mpt-7b-storysummarizer,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a fine-tuned version of mosaicml/mpt-7b-storywriter intended for summarization and literary analysis of fiction stories.; The code for this model includes the adaptions from Birchlabs/mosaicml-mpt-7b-chat-qlora which allow MPT models to be loaded with device_map=""auto"" and load_in_8bit=True.
It also has the latest key-value cache MPT code to allow for fast inference with transformers (thus, use_cache is set to True in config.json).; or; Outputs on the text of Waystation City (6,287 tokens); temperature=0.6, repetition_penalty=1.04, top_p=0.95, top_k=50, do_sample=True, max_new_tokens=1024",natural-language-processing,text-generation,https://huggingface.co/emozilla/mpt-7b-storysummarizer,,License: apache-2.0,Datasets: emozilla/booksum-summary-analysis_gptneox-8192; kmfoda/booksum,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 363,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-7b-openassistant-peft,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Falcon-7b-openassistant-peft is a chatbot model for dialogue generation. It was built by fine-tuning Falcon-7B on the OpenAssistant/oasst1 dataset. This repo only includes the LoRA adapters from fine-tuning with ?'s peft package. ; The model was fine-tuned in 8-bit precision using ? peft adapters, transformers, and bitsandbytes. Training relied on a method called ""Low Rank Adapters"" (LoRA), specifically the QLoRA variant. The run took approximately 6.25 hours and was executed on a workstation with a single A100-SXM NVIDIA GPU with 37 GB of available memory. See attached Colab Notebook for the code and hyperparams used to train the model. ; May 30, 2023; To prompt the chat model, use the following format:; Prompter:",natural-language-processing,text-generation,https://huggingface.co/dfurman/falcon-7b-openassistant-peft,Paper: https://arxiv.org/pdf/2106.09685.pdf; https://arxiv.org/pdf/2305.14314.pdf,License: apache-2.0,Datasets: OpenAssistant/oasst1,,PEFT,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 36,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 938,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 38.2MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Monero's WizardLM-Uncensored-SuperCOT-Storytelling-30B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 54,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 547,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Replit-v2-CodeInstruct-3B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Base Model: replit/replit-code-v1-3b; This is version 2 of the Replit Code Instruct fine tune model.; This model is fine tuned on both Sahil2801's CodeAlpaca & Teknium's GPTeacher Code-Instruct to give Replit's Code model instruct capabilities.; Try this model on it's HuggingFace demo Spaces: https://huggingface.co/spaces/teknium/Replit-v2-CodeInstruct-3B; Dataset links:
CodeAlpaca: https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k
GPTeacher subset - Code Instruct: https://github.com/teknium1/GPTeacher",natural-language-processing,text-generation,https://huggingface.co/teknium/Replit-v2-CodeInstruct-3B,,License: cc-by-sa-4.0,Datasets: bigcode/the-stack-dedup; sahil2801/CodeAlpaca-20k; teknium/GPTeacher-CodeInstruct,code,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 45,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 995,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
falcon-40b-sft-top1-560,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This model is a fine-tuning of TII's Falcon 40B LLM. 
It was trained with top-1 (high-quality) demonstrations of the OASST data set (exported on May 6, 2023) with an effective batch size of 144 for ~7.5 epochs with LIMA style dropout (p=0.3) and a context-length of 2048 tokens.; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; Model:",natural-language-processing,text-generation,https://huggingface.co/OpenAssistant/falcon-40b-sft-top1-560,,License: apache-2.0,Datasets: OpenAssistant/oasst1,4 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 38,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 8871,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 84.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
gpt4all-falcon,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"An Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions including word problems, multi-turn dialogue, code, poems, songs, and stories.; This model has been finetuned from Falcon; To download a model with a specific revision run ; Downloading without specifying revision defaults to main/v1.0.; To use it for inference with Cuda, run",natural-language-processing,text-generation,https://huggingface.co/nomic-ai/gpt4all-falcon,,License: apache-2.0,Datasets: nomic-ai/gpt4all-j-prompt-generations,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2163,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
SantaCoder-1B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; Play with the model on the SantaCoder Space Demo.; This is the Megatron-version of SantaCoder.
We refer the reader to the SantaCoder model page for full documentation about this model; There are two versions (branches) of the model:; The model was trained on GitHub code. As such it is not an instruction model and commands like ""Write a function that computes the square root."" do not work well.
You should phrase commands like they occur in source code such as comments (e.g. # the following function computes the sqrt) or write a function signature and docstring and let the model complete the function body.",natural-language-processing,text-generation,https://huggingface.co/TabbyML/SantaCoder-1B,,License: openrail,Datasets: bigcode/the-stack,code,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1505,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
inswapper,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/deepinsight/inswapper,,,,,ONNX,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 554.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-Uncensored-Falcon-40b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is WizardLM trained on top of tiiuae/falcon-40b, with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:
An uncensored model has no guardrails.
You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car. Publishing anything this model generates is the same as publishing it yourself. You are responsible for the content you publish, and you cannot blame the model any more than you can blame the knife, gun, lighter, or car for what you do with it.; Prompt format is WizardLM.; Thank you chirper.ai for sponsoring some of my compute!",natural-language-processing,text-generation,https://huggingface.co/ehartford/WizardLM-Uncensored-Falcon-40b,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 80,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2163,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 84.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
Nous-Hermes-13B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for NousResearch's Nous-Hermes-13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; The model follows the Alpaca prompt format:",,,https://huggingface.co/TheBloke/Nous-Hermes-13B-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 71,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 116.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
Nous-Hermes-13B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for NousResearch's Nous-Hermes-13B.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; The model follows the Alpaca prompt format:,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Nous-Hermes-13B-GPTQ,,License: other,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 133,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11007,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
instructblip-vicuna-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,InstructBLIP model using Vicuna-13b as language model. InstructBLIP was introduced in the paper InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning by Dai et al.; Disclaimer: The team releasing InstructBLIP did not write a model card for this model so this model card has been written by the Hugging Face team.; InstructBLIP is a visual instruction tuned version of BLIP-2. Refer to the paper for details.; ; Usage is as follows:,multimodel,image-to-text,https://huggingface.co/Salesforce/instructblip-vicuna-13b,Paper: https://arxiv.org/pdf/2305.06500.pdf,License: other,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 17,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1154,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 57.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
medguanaco-lora-65b-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Model Description ; nmitchko/medguanaco-lora-65b-GPTQ is a large language model specifically fine-tuned for medical domain tasks.
It is based on the Guanaco LORA of LLaMA weighing in at 65B parameters. 
The primary goal of this model is to improve question-answering and medical dialogue tasks.
It was trained using LoRA and quantized, to reduce memory footprint. ; Steps to load this model:; The following README is taken from the source page medalpaca; The training data for this project was sourced from various resources. 
Firstly, we used Anki flashcards to automatically generate questions, 
from the front of the cards and anwers from the back of the card. 
Secondly, we generated medical question-answer pairs from Wikidoc. 
We extracted paragraphs with relevant headings, and used Chat-GPT 3.5 
to generate questions from the headings and using the corresponding paragraphs 
as answers. This dataset is still under development and we believe 
that approximately 70% of these question answer pairs are factual correct. 
Thirdly, we used StackExchange to extract question-answer pairs, taking the 
top-rated question from five categories: Academia, Bioinformatics, Biology, 
Fitness, and Health. Additionally, we used a dataset from ChatDoctor 
consisting of 200,000 question-answer pairs, available at https://github.com/Kent0n-Li/ChatDoctor.",natural-language-processing,text-generation,https://huggingface.co/nmitchko/medguanaco-lora-65b-GPTQ,Paper: https://arxiv.org/pdf/2106.09685.pdf; https://arxiv.org/pdf/2303.14070.pdf,License: cc,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 336.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-40b-sft-mix-1226,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This model is a fine-tuning of TII's Falcon 40B LLM. 
It was trained on a mixture of OASST top-2 threads (exported on June 2, 2023), Dolly-15k and synthetic instruction datasets (see dataset configuration below).; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; Model:",natural-language-processing,text-generation,https://huggingface.co/OpenAssistant/falcon-40b-sft-mix-1226,,License: apache-2.0,Datasets: OpenAssistant/oasst1; databricks/databricks-dolly-15k,4 languages,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 30,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5334,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 84.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
chronos-33b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is the fp16 PyTorch / HF version of chronos-33b - if you need another version, GGML and GPTQ versions are linked below.; This model is primarily focused on chat, roleplay, and storywriting, but can accomplish other tasks such as simple reasoning and coding.; Chronos generates very long outputs with coherent text, largely due to the human inputs it was trained on.; This model uses Alpaca formatting, so for optimal model performance, use:; GGML Version provided by @TheBloke",natural-language-processing,text-generation,https://huggingface.co/elinas/chronos-33b,,License: other,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 187,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 65.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
open_llama_3b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a 7B and 3B model trained on 1T tokens, as well as the preview of a 13B model trained on 600B tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we’ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.; The model can be evaluated with lm-eval-harness. However, due to the aforementioned tokenizer issue, we need to avoid using the fast tokenizer to obtain the correct results. This can be achieved by passing in use_fast=False to this part of lm-eval-harness, as shown in the example below:",natural-language-processing,text-generation,https://huggingface.co/openlm-research/open_llama_3b,,License: apache-2.0,Datasets: togethercomputer/RedPajama-Data-1T,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 85,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 82668,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 8
Photon_v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,Model info: https://civitai.com/models/84728/photon; ,multimodel,text-to-image,https://huggingface.co/digiplay/Photon_v1,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3920,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
falcon-7b-instruct-sharded,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Resharded version of https://huggingface.co/tiiuae/falcon-7b-instruct for low RAM enviroments (e.g. Colab, Kaggle) in safetensors; Tutorial: https://medium.com/@vilsonrodrigues/run-your-private-llm-falcon-7b-instruct-with-less-than-6gb-of-gpu-using-4-bit-quantization-ff1d4ffbabcc; Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ? To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF!",natural-language-processing,text-generation,https://huggingface.co/vilsonrodrigues/falcon-7b-instruct-sharded,Paper: https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2306.01116.pdf,License: apache-2.0,Datasets: tiiuae/falcon-refinedweb,English,Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 15069,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Nous-Hermes-13b-Chinese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,将Chinese-alpaca-13b的Lora权重与Nous-Hermes-13b合并，得到支持中文的Hermes-13b模型。; Chinese-alpaca-13b:https://huggingface.co/ziqingyang/chinese-alpaca-lora-13b; Nous-Hermes-13b:https://huggingface.co/NousResearch/Nous-Hermes-13b; 仅供学术研究使用，使用时请遵守相应开源协议。,natural-language-processing,text-generation,https://huggingface.co/Bandifishing/Nous-Hermes-13b-Chinese,,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 49,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Nous-Hermes-13b-Chinese-GGML,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"原始模型：https://huggingface.co/NousResearch/Nous-Hermes-13b ; lora：https://huggingface.co/ziqingyang/chinese-alpaca-lora-13b ; 将Nous-Hermes-13b与chinese-alpaca-lora-13b进行合并，增强模型的中文能力，不过存在翻译腔; 使用项目：
https://github.com/ymcui/Chinese-LLaMA-Alpacahttps://github.com/ggerganov/llama.cpp; 推荐q5_k_m或q4_k_m   该仓库模型均为ggmlv3模型 ",natural-language-processing,text-generation,https://huggingface.co/coyude/Nous-Hermes-13b-Chinese-GGML,,License: apache-2.0,,Chinese; English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 82.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
majicMIX_realistic_v6,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,Model info :; https://civitai.com/models/43331?modelVersionId=94640,multimodel,text-to-image,https://huggingface.co/digiplay/majicMIX_realistic_v6,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3259,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mms-lid-126,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0941358df9b5cffb0d11b_Audio%20Classification.svg,,"This checkpoint is a model fine-tuned for speech language identification (LID) and part of Facebook's Massive Multilingual Speech project.
This checkpoint is based on the Wav2Vec2 architecture and classifies raw audio input to a probability distribution over 126 output classes (each class representing a language).
The checkpoint consists of 1 billion parameters and has been fine-tuned from facebook/mms-1b on 126 languages.; This MMS checkpoint can be used with Transformers to identify
the spoken language of an audio. It can recognize the following 126 languages.; Let's look at a simple example.; First, we install transformers and some other libraries; Note: In order to use MMS you need to have at least transformers >= 4.30 installed. If the 4.30 version
is not yet available on PyPI make sure to install transformers from 
source:",audio,audio-classification,https://huggingface.co/facebook/mms-lid-126,Paper: https://arxiv.org/pdf/2305.13516.pdf,License: cc-by-nc-4.0,Datasets: google/fleurs,158 languages,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 4533,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 8.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
WizardCoder-15B-1.0-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for WizardLM's WizardCoder 15B 1.0.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools known to work with these model files.; Matt Hoeffner has put up a live Space with a demo of this model:",,,https://huggingface.co/TheBloke/WizardCoder-15B-1.0-GGML,,License: bigcode-openrail-m,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 87,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 429,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 70.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
controlnet_qrcode-control_v11p_sd21,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c77cb5f6fb8da5d7c0cb_Image-to-Image.svg,,"; This repo holds the safetensors & diffusers versions of the QR code conditioned ControlNet for Stable Diffusion v2.1.
The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs. However, a 1.5 version model was also trained on the same dataset for those who are using the older version.; These models perform quite well in most cases, but please note that they are not 100% accurate. In some instances, the QR code shape might not come through as expected. You can increase the ControlNet weight to emphasize the QR code shape. However, be cautious as this might negatively impact the style of your output.To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).; To balance between style and shape, a gentle fine-tuning of the control weight might be required based on the individual input and the desired output, aswell as the correct prompt. Some prompts do not work until you increase the weight by a lot. The process of finding the right balance between these factors is part art and part science. For the best results, it is recommended to generate your artwork at a resolution of 768. This allows for a higher level of detail in the final product, enhancing the quality and effectiveness of the QR code-based artwork.; The simplest way to use this is to place the .safetensors model and its .yaml config file in the folder where your other controlnet models are installed, which varies per application. 
For usage in auto1111 they can be placed in the webui/models/ControlNet folder. They can be loaded using the controlnet webui extension which you can install through the extensions tab in the webui (https://github.com/Mikubill/sd-webui-controlnet). Make sure to enable your controlnet unit and set your input image as the QR code. Set the model to either the SD2.1 or 1.5 version depending on your base stable diffusion model, or it will error. No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail.
Make sure to look up additional info on how to use controlnet if you get stuck, once you have the webui up and running its really easy to install the controlnet extension aswell.",computer-vision,image-to-image,https://huggingface.co/DionTimmer/controlnet_qrcode-control_v11p_sd21,,License: openrail++,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 34,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 5975,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 6
roop,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/henryruhs/roop,,,,,ONNX,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
baichuan-vicuna-7B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Fire Balloon's Baichuan Vicuna 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,https://huggingface.co/TheBloke/baichuan-vicuna-7B-GGML,Paper: https://arxiv.org/pdf/2306.04751.pdf,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 63.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
airoboros-13b-gpt4-1.4,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"update 2023-06-25 - re-uploaded with a slightly earlier checkpoint, which seems perhaps a little less overfit than the full 3-epochs version initially uploaded; This is a full (not qlora) fine-tune 13b parameter LlaMa model, using completely synthetic training data created gpt4 via https://github.com/jondurbin/airoboros; This is mostly an extension of the previous gpt-4 series, with a few extras:; This model was fine-tuned with a fork of FastChat; The prompt it was trained with was:",natural-language-processing,text-generation,https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.4,,License: cc-by-nc-4.0,Datasets: jondurbin/airoboros-gpt4-1.4,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2537,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
airoboros-7b-gpt4-1.4,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"mostly untested, use if you want, or wait for some validation; This is a full (not qlora) fine-tune 7b parameter LlaMa model, using completely synthetic training data created gpt4 via https://github.com/jondurbin/airoboros; This is mostly an extension of the previous gpt-4 series, with a few extras:; This model was fine-tuned with a fork of FastChat; The prompt it was trained with was:",natural-language-processing,text-generation,https://huggingface.co/jondurbin/airoboros-7b-gpt4-1.4,,License: cc-by-nc-4.0,Datasets: jondurbin/airoboros-gpt4-1.4,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 175,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
airoboros-7B-gpt4-1.4-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Jon Durbin's Airoboros 7B GPT4 1.4.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,https://huggingface.co/TheBloke/airoboros-7B-gpt4-1.4-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
LLaMA_65B_0bit,,,,,,,,,"Converted with https://github.com/notepad-plus-plus/notepad-plus-plus All models tested on A100-80G *Conversion may require lot of RAM, LLaMA-7b takes ~0 GB, 13b around 0 GB, 30b around 0 and 65b takes more than 0 GB of RAM.; Installation instructions as mentioned in above repo:; Additional training was done on the MSPaint_Blank dataset and 2,000,000T+ tokens on 50,000+ blank notepad files.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/qq67878980/LLaMA_65B_0bit,,License: mit,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.65KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
PULSE-7bv5,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"
; 由于模型参数量较小和自回归生成范式，尽管模型提供了有关疾病诊断和治疗的推理结果，但这些结果不能代替线下职业医生的建议和治疗方案。所有回答仅供参考，不应作为诊断或治疗的依据。我们强烈建议用户在需要诊断或治疗疾病时，寻求专业医生的帮助和建议。; 下表提供了一个batch size=1时本地部署PULSE进行推理所需的显存大小。; 其中torch和transformers版本不建议低于推荐版本。; Gradio",natural-language-processing,text-generation,https://huggingface.co/OpenMEDLab/PULSE-7bv5,,License: agpl-3.0,,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 271,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 16.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
UltraLM-13b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is UltraLM-13b delta weights, a chat language model trained upon UltraChat; The model is fine-tuned based on LLaMA-13b with a multi-turn chat-format template as below; To use this model, you need to recover the full model from the delta weights and perform inference following the template below:",natural-language-processing,text-generation,https://huggingface.co/openbmb/UltraLM-13b,Paper: https://arxiv.org/pdf/2305.14233.pdf,,Datasets: stingning/ultrachat,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 60,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 236,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 52.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
mpt-30b-peft-compatible,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is the MPT-30B but with added support to finetune using peft (tested with qlora). It is not finetuned further, the weights are the same as the original MPT-30b.; I have not traced through the whole huggingface stack to see if this is working correctly but it does finetune with qlora and outputs are reasonable.
Inspired by implementations here https://huggingface.co/cekal/mpt-7b-peft-compatible/commits/main
https://huggingface.co/mosaicml/mpt-7b/discussions/42.; The original description for MosaicML team below:; MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.
This model was trained by MosaicML.; MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.",natural-language-processing,text-generation,https://huggingface.co/eluzhnica/mpt-30b-peft-compatible,Paper: https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/2010.04245.pdf; https://arxiv.org/pdf/1909.08053.pdf; https://arxiv.org/pdf/2302.06675.pdf,License: apache-2.0,Datasets: allenai/c4; mc4; togethercomputer/RedPajama-Data-1T; bigcode/the-stack-dedup; allenai/s2orc,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1770,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Chronos-Hermes-13B-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Austism's Chronos Hermes 13B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Chronos-Hermes-13B-SuperHOT-8K-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 20,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 967,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Pygmalion-13B-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for TehVenom's merge of PygmalionAI's Pygmalion 13B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Pygmalion-13B-SuperHOT-8K-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 36,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3542,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
xgen-7b-8k-inst,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Official research release for the family of XGen models (7B) by Salesforce AI Research:; Title: Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length; Authors: Erik Nijkamp*, Tian Xie*, Hiroaki Hayashi*, Bo Pang*, Congying Xia*, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovs'ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Rayhan Joty, Caiming Xiong.; (* indicates equal contribution); Correspondence to: Shafiq Rayhan Joty, Caiming Xiong",natural-language-processing,text-generation,https://huggingface.co/Salesforce/xgen-7b-8k-inst,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 78,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 982183,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
longchat-7b-16k,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Please use load_model from FastChat or LongChat repo to load the model (or chatting API from FastChat). There is a monkey patch needed to use the model.
Usage referece:; (LongChat) python3 eval.py --model-name-or-path  lmsys/longchat-7b-16k --task topics ; (FastChat) python3 -m fastchat.serve.cli --model-path lmsys/longchat-7b-16k; Under the hood, the monkey patch is added in:; https://github.com/lm-sys/FastChat/blob/da0641e567cf93756b0978ab5a6b092e96f06240/fastchat/model/model_adapter.py#L429",natural-language-processing,text-generation,https://huggingface.co/lmsys/longchat-7b-16k,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 35,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11767,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Monero's WizardLM Uncensored SuperCOT Storytelling 30B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2320,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
airoboros-65B-gpt4-1.4-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Jon Durbin's Airoboros 65B GPT4 1.4.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/airoboros-65B-gpt4-1.4-GPTQ,,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 10,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 383,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 34.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Monero's WizardLM Uncensored SuperCOT Storytelling 30B.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,https://huggingface.co/TheBloke/WizardLM-Uncensored-SuperCOT-StoryTelling-30B-SuperHOT-8K-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 12,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 120.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Chinese-Alpaca-33B-SuperHOT-8K-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Minlik's Chinese Alpaca 33B Merged.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,https://huggingface.co/TheBloke/Chinese-Alpaca-33B-SuperHOT-8K-GGML,,License: other,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 121.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
vicuna-33B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for LmSys' Vicuna 33B 1.3.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/vicuna-33B-GPTQ,Paper: https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1169,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Vicuna-33B-1-3-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for LmSys' Vicuna 33B 1.3 (final) merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ,Paper: https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 21,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1435,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
OPI_full_Galactica-6.7B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"; This repo is for the Open Protein Instructions (OPI) project, aiming to build and release a protein instruction dataset as well as propose to explore and benckmark LLMs for protein modeling in protein biology.
For more details of training and testing, please visit https://github.com/baaihealth/opi.; ; Usage and License Notices: LLaMA and Galactica are intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes. The weight diff for Stanford Alpaca is also CC BY NC 4.0 (allowing only non-commercial use).; For OPI-instruction tuning, we adopt the training script of Stanford Alpaca. ",natural-language-processing,text-generation,https://huggingface.co/BAAI/OPI_full_Galactica-6.7B,,License: apache-2.0,Datasets: BAAI/OPI,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
SadTalker-V002rc,,,,,,,,,"The new released model of https://github.com/OpenTalker/SadTalker.; The file of https://huggingface.co/vinthony/SadTalker-V002rc/blob/main/epoch_00190_iteration_000400000_checkpoint.pt comes from https://github.com/RenYurui/PIRender.; Thanks for their wonderful work!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/vinthony/SadTalker-V002rc,,License: mit,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
rvc_models_that_Pika_use,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/pika724/rvc_models_that_Pika_use,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
zeroscope_v2_1111models,,,,,,,,,"
example outputs (courtesy of dotsimulate); A collection of watermark-free Modelscope-based video models capable of generating high quality video at 448x256, 576x320 and 1024 x 576. These models were trained from the original weights with offset noise using 9,923 clips and 29,769 tagged frames.
This collection makes it easy to switch between models with the new dropdown menu in the 1111 extension.; Simply download the contents of this repo to 'stable-diffusion-webui\models\text2video'
Or, manually download the model folders you want, along with VQGAN_autoencoder.pth.; Thanks to dotsimulate for the config files.; Thanks to camenduru, kabachuha, ExponentialML, VANYA, polyware, tin2tin",,,https://huggingface.co/cerspense/zeroscope_v2_1111models,,License: cc-by-nc-4.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
nsql-6B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"NSQL is a family of autoregressive open-source large foundation models (FMs) designed specifically for SQL generation tasks.; The checkpoint included in this repository is based on CodeGen-Multi 6B from Salesforce and further pre-trained on a dataset of general SQL queries and then fine-tuned on a dataset composed of text-to-SQL pairs.; The general SQL queries are the SQL subset from The Stack, containing 1M training samples. The labeled text-to-SQL pairs come from more than 20 public sources across the web from standard datasets. We hold out Spider and GeoQuery datasets for use in evaluation.; We evaluate our models on two text-to-SQL benchmarks: Spider and GeoQuery.; NSQL was trained using cross-entropy loss to maximize the likelihood of sequential inputs. For finetuning on text-to-SQL pairs, we only compute the loss over the SQL portion of the pair. The family of models is trained using 80GB A100s, leveraging data and model parallelism. We pre-trained for 3 epochs and fine-tuned for 10 epochs.",natural-language-processing,text-generation,https://huggingface.co/NumbersStation/nsql-6B,,License: bsd-3-clause,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1191,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
shap-e,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in Shap-E: Generating Conditional 3D Implicit Functions by Heewoo Jun and Alex Nichol from OpenAI. ; Original repository of Shap-E can be found here: https://github.com/openai/shap-e. ; The authors of Shap-E didn't author this model card. They provide a separate model card here.; The abstract of the Shap-E paper:; We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at this https URL.",multimodel,text-to-image,https://huggingface.co/openai/shap-e,Paper: https://arxiv.org/pdf/2305.02463.pdf,License: mit,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1465,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.82KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 7
falcon-40b-sft-mix-1226-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGCC format model files for Open Assistant's Falcon 40B SFT MIX.; These files will not work in llama.cpp, text-generation-webui or KoboldCpp.; GGCC is a new format created in a new fork of llama.cpp that introduced this new Falcon GGML-based support: cmp-nc/ggllm.cpp.",,,https://huggingface.co/TheBloke/falcon-40b-sft-mix-1226-GGML,,License: apache-2.0,Datasets: OpenAssistant/oasst1; databricks/databricks-dolly-15k,4 languages,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 273.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
replit-openorca,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,replit/replit-code-v1-3b finetuned on Open-Orca/OpenOrca.,natural-language-processing,text-generation,https://huggingface.co/matorus/replit-openorca,,,Datasets: Open-Orca/OpenOrca,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 360,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 5.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
genz-7b,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; The most capable commercially usable Instruct Finetuned LLM yet with 8K input token length, latest information & better coding.; Use following prompt template; Check the GitHub for the code -> GenZ",natural-language-processing,text-generation,https://huggingface.co/budecosystem/genz-7b,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 44,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
codegen25-7b-mono,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Title: CodeGen2.5: Small, but mighty; Authors: Erik Nijkamp*, Hiroaki Hayashi*, Yingbo Zhou, Caiming Xiong; (* equal contribution); CodeGen2.5 is a family of autoregressive language models for program synthesis.; Building upon CodeGen2, the model is trained on StarCoderData for 1.4T tokens, achieving competitive results compared to StarCoderBase-15.5B with less than half the size.",natural-language-processing,text-generation,https://huggingface.co/Salesforce/codegen25-7b-mono,Paper: https://arxiv.org/pdf/2305.02309.pdf,License: apache-2.0,Datasets: bigcode/starcoderdata,code,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 15,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1561,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Vicuna-7B-v1-3-SuperHOT-8K-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for LmSys' Vicuna 7B v1.3 merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,natural-language-processing,text-generation,https://huggingface.co/TheBloke/Vicuna-7B-v1-3-SuperHOT-8K-GPTQ,Paper: https://arxiv.org/pdf/2302.13971.pdf; https://arxiv.org/pdf/2306.05685.pdf,License: other,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 14,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
13B-BlueMethod,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"BlueMethod is a bit of a convoluted experiment in tiered merging.
Furthering the experimental nature of the merge, the models combined
were done so with a custom script that randomized the percent of each
layer merged from one model to the next. This is a warmup for a larger
project.
[Tier One and Two Merges not released; internal naming convention]; Tier One Merges:; 13B-Metharme+13B-Nous-Hermes=13B-Methermes; 13B-Vicuna-cocktail+13B-Manticore=13B-Vicortia; 13B-HyperMantis+13B-Alpacino=13B-PsychoMantis",natural-language-processing,text-generation,https://huggingface.co/CalderaAI/13B-BlueMethod,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 11,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 33.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
BanglaConformer,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c62056acb71ccf5d18e6_Automatic%20Speech%20Recognition.svg,,"Conformer-CTC model trained on the OOD-Speech dataset to transcribe speech from Bangla audio. This is a large variant of the model, with ~121M parameters. To know more about the model architecture see the NeMo Documentation here.;  The training split contains 1100+ hours of audio data crowdsoruced from native Bangla speakers. We trained on this split for 164 epochs , then the model was evaluated on23+ hours of audio across 17 diverse domains .; The model can be used as a pretrained checkpoint for inference or for fine-tuning on another dataset through the NVIDIA NeMo toolkit. It is recommended to install the toolkit, after installing the pyTorch package. ; After installing the required dependencies, download the .nemo file or the pretrained model to your local directory. you can instantiate the pretrained model like following: ; Prior to feeding the input audio to the pretrained model for training or inference, we need to resample the audio to 16KHz. We can achieve that using the sox library :",audio,automatic-speech-recognition,https://huggingface.co/bengaliAI/BanglaConformer,Paper: https://arxiv.org/pdf/2305.09688.pdf,License: mit,,Bengali,NeMo,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 166,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 510.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Evol-Replit-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This model uses the Evol-Instruct-Code-80k-v1 dataset generated using the Evol-Teacher repo. Currently, WizardCoder is one the most performant Code Generation models, being beaten only by ChatGPT. This takes the Code Alpaca 20k dataset and evolves each instruction through a randomly chosen evolution prompt to increase instruction complexity. These prompts range from increase time/space complexity, to increasing requirements, to adding erroneus code to improve robustness, etc. This is done three times with pruning and post processing to remove unwanted instructions and responses. The iterative addition of more complexity gives higher quality and more in-depth instructions than what is ususally generated in Alpaca methods. This, like in the case of WizardCoder and WizardLM, can lead to strong performance that gets very close to RLHF model performance.; This model uses ReplitLM fine tuned with the following parameters:",natural-language-processing,text-generation,https://huggingface.co/nickrosh/Evol-Replit-v1,,License: cc-by-sa-4.0,Datasets: nickrosh/Evol-Instruct-Code-80k-v1,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 10.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
sdxl-0.9-usage,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"This repo is a tutorial intended to help beginners use the new released model, stable-diffusion-xl-0.9 in ComfyUI, with both the base and refiner models together to achieve a magnificent quality of image generation.; With usable demo interfaces for ComfyUI to use the models (see below)!; Here is a full tutorial to use stable-diffusion-xl-0.9 FROM ZERO!; When you have loaded into the ComfyUI, it will display this default interface shown below:
; Then, find out the ""load"" button on a floating panel, click, and choose the "".json"" file download from THIS REPO, it will quickly load a friendly interface to use the stable-diffusion-xl-0.9 series.
",multimodel,text-to-image,https://huggingface.co/NathMath/sdxl-0.9-usage,,License: apache-2.0,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 9,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 20.79KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Slider,,,,,,,,,"乳首の色、サイズ、おっぱいの寄り具合、高さを{整するスライダ`LoRAです。
eにダウンロ`ドするかOppaiSliderPack.zipをダウンロ`ドして解訾筏皮ださい。; ; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/TomyAI/Slider,,License: creativeml-openrail-m,,Japanese,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 128.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Emu,,,,,,,,,"Quan Sun1*, Qiying Yu2,1*, Yufeng Cui1*, Fan Zhang1*, Xiaosong Zhang1*, Yueze Wang1, Hongcheng Gao1, Jingjing Liu2, Tiejun Huang1,3, Xinlong Wang1; 1 BAAI, 2 THU, 3 PKU * Equal Contribution; |  Paper | Demo(tmp) |; Emu is a Large Multimodal Model (LMM) trained with a unified autoregressive objective, i.e., predict-the-next-element, including both visual embeddings and textual tokens. Trained under this objective, Emu can serve as a generalist interface for diverse multimodal tasks, such as image captioning, image/video question answering, and text-to-image generation, together with new abilities like in-context text and image generation, and image blending.; Clone the github repository and install required packages:",,,https://huggingface.co/BAAI/Emu,Paper: https://arxiv.org/pdf/2307.05222.pdf,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 57.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
brav6,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""brav6""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Try model for free: Generate Images",multimodel,text-to-image,https://huggingface.co/stablediffusionapi/brav6,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 358,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.51KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mblip-mt0-xl,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"This is the model checkpoint for our work mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs.; mBLIP is a BLIP-2 model which consists of 3 sub-models: a Vision Transformer (ViT), a Query-Transformer (Q-Former) and a large language model (LLM).; The Q-Former and ViT have both been initialized by an English BLIP-2 checkpoint (blip2-flan-t5-xl) and then re-aligned 
to the multilingual LLM (mt0-xl) using a multilingual task mixture.;  ; This allows the model to be used for tasks like:",multimodel,image-to-text,https://huggingface.co/Gregor/mblip-mt0-xl,Paper: https://arxiv.org/pdf/2307.06930.pdf; https://arxiv.org/pdf/2301.12597.pdf,License: mit,Datasets: Gregor/mblip-train,English; multilingual,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 90,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 17.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
AnimateDiff,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/camenduru/AnimateDiff,,,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 31.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
WizardCoder-Guanaco-15B-V1.1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"The WizardCoder-Guanaco-15B-V1.1 is a language model that combines the strengths of the WizardCoder base model and the openassistant-guanaco dataset for finetuning. The openassistant-guanaco dataset was further trimmed to within 2 standard deviations of token size for input and output pairs and all non-english data has been removed to reduce training size requirements.; Version 1.1 showcases notable enhancements, employing a modified version of the previous openassistant-guanaco dataset. This dataset underwent a comprehensive revision, replacing every single answer with those generated by GPT-4.; The volume of the datasets has also been augmented by approximately 50%, with a particular focus on high school and abstract algebra. This expansion leveraged the combined capabilities of GPT-4 and GPT-3.5-Turbo. The initial evaluation of algebraic functions over 12 epochs indicated promising results from this enriched dataset. However, this is just the beginning; further refinements are in the pipeline, aiming to optimize the dataset quality and subsequently decrease the number of epochs required to achieve comparable results.; Considering the need to curtail memory consumption during training, this dataset was tailored to consist solely of English language questions and answers. Consequently, the model's performance in language translation may not be up to par. Nevertheless, the focus remains on enhancing the model's proficiency and efficiency within its defined scope.; This model is designed to be used for a wide array of text generation tasks that require understanding and generating English text. The model is expected to perform well in tasks such as answering questions, writing essays, summarizing text, translation, and more. However, given the specific data processing and finetuning done, it might be particularly effective for tasks related to English language question-answering systems.",natural-language-processing,text-generation,https://huggingface.co/LoupGarou/WizardCoder-Guanaco-15B-V1.1,,License: apache-2.0,Datasets: guanaco,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 73,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 31.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ziya-llama-13b-medical-merged,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,基于LLaMA-13B的中英医疗问答模型; shibing624/ziya-llama-13b-medical-merged evaluate test data：; The overall performance of model on QA test:; 在中文开放测试集中的表现优异，继承了两方面的优势：1）微调训练的底座是Ziya-LLaMA-13B模型，是较强的中英文底座模型，2）微调使用的是高质量240万条中英文医疗指令数据集，和多种通用指令数据集，微调后的模型在医疗行业答复能力达到领先水平，在通用问题上的答复能力不弱于LLaMA-13B。; training args:,natural-language-processing,text-generation,https://huggingface.co/shibing624/ziya-llama-13b-medical-merged,,License: apache-2.0,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 64,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
RedPajama-INCITE-Base-3B-v1-paraphrase-tone,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"The togethercomputer/RedPajama-INCITE-Base-3B-v1 model finetuned for Paraphrasing and Changing the Tone of the input sentence(to casual/professional/witty). Training data was generated using gpt-35-turbo.; Look at the repo llm-toys for usage and other details.; Try in colab:


; ; The following bitsandbytes quantization config was used during training:",natural-language-processing,text-generation,https://huggingface.co/llm-toys/RedPajama-INCITE-Base-3B-v1-paraphrase-tone,,License: wtfpl,,English,PEFT,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 122,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 21.2MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
ChromaFT,,,,,,,,,"This model was created through fine tuning.
There are two versions: an all-ages depiction-oriented version and an NSFW-enhanced version.; All-ages depiction-oriented version：ChromaFT_v1-pruned.safetensors; NSFW-enhanced version：ChromaNeoFT_v1-pruned.safetensors; (NEO：Nsfw Erotic Option); For sample illustrations, please visit my blog.
https://ai-drawing.net/en/2023/07/15/introducing-of-chroma-ft-v1-0/",,,https://huggingface.co/zzzAI19/ChromaFT,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 7,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 12.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
BadAnime_v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,Model info :; https://civitai.com/models/107703?modelVersionId=115852; Sample image I made thru Huggingface's API :; ; Original Author's DEMO image :,multimodel,text-to-image,https://huggingface.co/digiplay/BadAnime_v1,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 554,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
RedPajama-INCITE-Base-3B-v1-dialogue-summary-topic,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"The togethercomputer/RedPajama-INCITE-Base-3B-v1 model finetuned for Summary and Topic generation from a dailogue. We use a sample of roughly 1000 data points from the
Dialogsum dataset for fine-tuning.; Look at the repo llm-toys for usage and other details.; Try in colab:


; ; The following bitsandbytes quantization config was used during training:",natural-language-processing,text-generation,https://huggingface.co/llm-toys/RedPajama-INCITE-Base-3B-v1-dialogue-summary-topic,,License: wtfpl,,English,PEFT,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 90,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 21.1MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
BuboGPT-ckpt,,,,,,,,,"Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/magicr/BuboGPT-ckpt,,License: apache-2.0,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 747.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Unreal_V4.1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"MeinaUnreal objetive is to be able to do anime art with a 2.5d feeling.
( the VAE is already baked in the model ); For examples and prompts, please checkout: https://civitai.com/models/18798/meinaunreal
I have a discord server where you can post images that you generated, discuss prompt and/or ask for help.; https://discord.gg/XC9nGZNDUd If you like one of my models and want to support their updates; I've made a ko-fi page; https://ko-fi.com/meina where you can pay me a coffee <3; And a Patreon page; https://www.patreon.com/MeinaMix where you can support me and get acess to beta of my models!",multimodel,text-to-image,https://huggingface.co/Meina/Unreal_V4.1,,License: creativeml-openrail-m,,English,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 436,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.56KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
falcon-7b-paraphrase-tone-dialogue-summary-topic,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"The tiiuae/falcon-7b model finetuned for Paraphrasing, Changing the Tone of the input sentence(to casual/professional/witty), 
Summary and Topic generation from a dialogue. Data for Paraphrasing and Changing the Tone was generated using gpt-35-turbo and a sample of roughly 1000 data points from the
Dialogsum dataset was used for Summary and Topic generation.; Look at the repo llm-toys for usage and other details.; Try in colab (you might need the pro version):


; ; The following bitsandbytes quantization config was used during training:",natural-language-processing,text-generation,https://huggingface.co/llm-toys/falcon-7b-paraphrase-tone-dialogue-summary-topic,,License: wtfpl,,English,PEFT,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 20,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 19.1MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
lora_diffusion,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"The model is created using the following steps:; Since this is only the first official release, I believe there are still many, many imperfections. 
Please provide feedback in time, and I will continuously make corrections, thank you!",multimodel,text-to-image,https://huggingface.co/Andyrasika/lora_diffusion,,License: creativeml-openrail-m,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 233,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.5KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
OnlyAnime_v2.3,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Model info:
https://civitai.com/models/105955/onlyanime; Sample image I made thru Huggingface's API :
; Original Author's DEMO images :; ; *This image using LORA file ? FilmVelvia3.safetensors 
you can also download here: ",multimodel,text-to-image,https://huggingface.co/digiplay/OnlyAnime_v2.3,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 382,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
HIMAWARI_v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Model info :; https://civitai.com/models/103018/himawari?modelVersionId=110254; More models from the Author: (he made a lot of useful LORAs, pls check it out ^^); https://civitai.com/user/KimTarou/models; Sample image I made thru Huggingface's API :",multimodel,text-to-image,https://huggingface.co/digiplay/HIMAWARI_v1,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 404,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
Llama-2-7B-ggml,,,,,,,,,"From: https://huggingface.co/meta-llama/Llama-2-7b-hf; Quantized using an older version of llama.cpp and compatible with llama.cpp from May 19, commit 2d5db48.; Quantization methods compatible with latest llama.cpp from June 6, commit 2d43387.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/localmodels/Llama-2-7B-ggml,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 60.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-7b-chat-guanaco-hf-4bit,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"We are unlocking the power of large language models. Our latest version of Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly. ; This release includes model weights and starting code for pretrained and fine-tuned Llama language models ― ranging from 7B to 70B parameters.; This repository is intended as a minimal example to load Llama 2 models and run inference. For more detailed examples leveraging HuggingFace, see llama-recipes.; In order to download the model weights and tokenizer, please visit the Meta AI website and accept our License.; Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, do not use the 'Copy link address' option when you right click the URL. If the copied URL text starts with: https://download.llamameta.net, you copied it correctly. If the copied URL text starts with: https://l.facebook.com, you copied it the wrong way.",natural-language-processing,text-generation,https://huggingface.co/quantumaikr/llama-2-7b-chat-guanaco-hf-4bit,,License: other,,English,TensorBoard; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 136.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mlc-chat-Llama-2-7b-chat-hf-q3f16_1,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q3f16_1,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mlc-chat-Llama-2-7b-chat-hf-q4f32_1,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f32_1,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-70B-Chat-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"From: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta",natural-language-processing,text-generation,https://huggingface.co/localmodels/Llama-2-70B-Chat-GPTQ,,,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 215,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 35.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-7B-bf16-sharded,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/TinyPixel/Llama-2-7B-bf16-sharded,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 933,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama-2-70b-chat-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ― 7B, 13B, and 70B ― as well as pretrained and fine-tuned variations.",natural-language-processing,text-generation,https://huggingface.co/4bit/Llama-2-70b-chat-hf,,,,English,Safetensors; Transformers; PyTorch,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 58,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 138.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-7b-guanaco-fp16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,This is a Llama-2 version of Guanaco. It was finetuned from the base Llama-7b model using the official training scripts found in the QLoRA repo. I wanted it to be as faithful as possible and therefore changed nothing in the training script beyond the model it was pointing to. The model prompt is therefore also the same as the original Guanaco model.; This repo contains the merged f16 model. The QLoRA adaptor can be found here.; A 13b version of the model can be found here.; Legal Disclaimer: This model is bound by the usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.,natural-language-processing,text-classification,https://huggingface.co/Mikael110/llama-2-7b-guanaco-fp16,,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 33,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llava-v1-0719-336px-lora-vicuna-13b-v1.3,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA-v1-0719-336px-LoRA-Vicuna-13B-v1.3 was trained in July 2023.; Paper or resources for more information:
https://llava-vl.github.io/; Non-commerical Use.; Where to send questions or comments about the model:
https://github.com/haotian-liu/LLaVA/issues",natural-language-processing,text-generation,https://huggingface.co/liuhaotian/llava-v1-0719-336px-lora-vicuna-13b-v1.3,,,,,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 512.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
falcon-7b-qlora-chat-airbird-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,Falcon-7b finetuned with Airbird Questions & Answers,natural-language-processing,question-answering,https://huggingface.co/tostcorp/falcon-7b-qlora-chat-airbird-v2,,License: apache-2.0,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 18.9MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
FishMix_v1.1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0647c78c7efe44f70132a_Text-to-Image.svg,,"Model info :; https://civitai.com/models/15745?modelVersionId=27424; Original Author's DEMO images :; 


",multimodel,text-to-image,https://huggingface.co/digiplay/FishMix_v1.1,,License: other,,,Diffusers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 291,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
Luna-AI-Llama2-Uncensored-FP16,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Model Description; “Tap-M/Luna-AI-Llama2-Uncensored” is a latest chat language model which is fine-tuned on llama2 7b with custom datasets of multiple rounds of chats between Human & AI. This model was fine-tuned by Tap, the creator of Luna AI. The result is an enhanced Uncensored Llama2 7b Chat model that rivals many Open Source Chat Models in performance across a variety of tasks.
This model stands out for its long responses, low hallucination rate, and absence of censorship mechanisms.; Model Training; The fine-tuning process was performed on an 8x a100 80GB machine. The model was trained almost entirely on synthetic outputs. This includes data from diverse sources which we included to create our custom dataset, it includes multiple rounds of chats between Human & AI.; Prompt Format",natural-language-processing,text-generation,https://huggingface.co/Tap-M/Luna-AI-Llama2-Uncensored-FP16,,License: cc-by-sa-4.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 56,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
llama-2-13b-guanaco-qlora,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This is a Llama-2 version of Guanaco. It was finetuned from the base Llama-13b model using the official training scripts found in the QLoRA repo. I wanted it to be as faithful as possible and therefore changed nothing in the training script beyond the model it was pointing to. The model prompt is therefore also the same as the original Guanaco model.; This repo contains the QLoRA adapter. A merged f16 model can be found here.; A 7b version of the adapter can be found here.; Legal Disclaimer: This model is bound by the usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.; Unable to determine this model’s library. Check the
								docs 
.
							",natural-language-processing,text-classification,https://huggingface.co/Mikael110/llama-2-13b-guanaco-qlora,,,,English,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 501.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
mlc-chat-Llama-2-70b-chat-hf-q4f16_1,,,,,,,,,"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model’s pipeline type. Check the
								docs 
.
							",,,https://huggingface.co/mlc-ai/mlc-chat-Llama-2-70b-chat-hf-q4f16_1,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
chinese-alpaca-pro-33b-merged,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,No model card; New: Create and edit this model card directly on the website!,natural-language-processing,text-generation,https://huggingface.co/minlik/chinese-alpaca-pro-33b-merged,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 66.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
meta_llama_2finetuned,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		",natural-language-processing,text-generation,https://huggingface.co/Sakil/meta_llama_2finetuned,,,,,PyTorch; TensorBoard; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size N/A,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
yayi-7b-llama2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,雅意大模型在百万级人工构造的高质量领域数据上进行指令微调得到，训练数据覆盖媒体宣传、舆情分析、公共安全、金融风控、城市治理等五大领域，上百种自然语言指令任务。雅意大模型从预训练初始化权重到领域模型的迭代过程中，我们逐步增强了它的中文基础能力和领域分析能力，并增加了多轮对话和部分插件能力。同时，经过数百名用户内测过程中持续不断的人工反馈优化，我们进一步提升了模型性能和安全性。; 通过雅意大模型的开源为促进中文预训练大模型开源社区的发展，贡献自己的一份力量，通过开源，与每一位合作伙伴共建雅意大模型生态。; News: ? 雅意大模型已开源基于 LLaMA 2 的中文优化模型版本，探索适用于中文多领域任务的最新实践。; 详情请参考我们的 ?Github Repo。; Comming Soon~,natural-language-processing,text-generation,https://huggingface.co/wenge-research/yayi-7b-llama2,,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Chinese-Llama-2-7b-4bit,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,快速上手 & 使用，可以试试 soulteary/docker-llama2-chat/。; 相关博客：使用 Transformers 量化 Meta AI LLaMA2 中文版大模型; 基于中文 LLaMA2 7B 模型项目：LinkSoul-AI/Chinese-Llama-2-7b,natural-language-processing,text-generation,https://huggingface.co/soulteary/Chinese-Llama-2-7b-4bit,,License: llama2,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 13.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
MythoBoros-13B-GPTQ,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GPTQ model files for Gryphe's MythoBoros 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",natural-language-processing,text-generation,https://huggingface.co/TheBloke/MythoBoros-13B-GPTQ,,License: other,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 6,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
MythoBoros-13B-GGML,,,,,,,,,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for Gryphe's MythoBoros 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,https://huggingface.co/TheBloke/MythoBoros-13B-GGML,,License: other,,English,Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 117.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama2-chat-13B-Chinese-50W,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"由于目前的LLama2-chat模型很难约束其以中文进行问题回复，因此该模型旨在提供一个能以中文进行问答的LLama2-chat 13B 模型供大家研究使用。; 该模型使用LLama2-chat 13B 作为基底模型，使用带embedding 和 LM head 的Lora训练方式训练。模型已完成参数合并，可直接使用。也可以手动将sft_lora_model同Llama2-chat 13B 进行合并。; 训练数据使用BELLE项目中采样的50万SFT数据进行SFT训练。; Since the LLama2-chat model struggles to confine its responses to Chinese language when prompted with Chinese questions, the primary objective of this model is to provide a LLama2-chat 13B model that can engage in question and answer interactions in Chinese.; The model utilizes LLama2-chat 13B as its base model and is trained using the Lora training approach with the embedding and LM head. The model has undergone the Lora param merge and is now ready for direct use. It is also possible to manually merge the ./sft_lora_model with the Llama2-chat 13B model to obtain the combined model. ",natural-language-processing,text-generation,https://huggingface.co/RicardoLee/Llama2-chat-13B-Chinese-50W,,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 26.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
Llama2-base-7B-Chinese-50W-pre_release,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"在完成了Llama2-chat 7B Chinese 和 Llama2-chat 13B Chinese 的训练后，我非常好奇能否直接基于Llama2-base 系列直接进行SFT训练。这也是本模型仓库的初衷。; 但是在实际操作中，在用了原先chat模型的LoRA训练框架后，我发现基于Llama2 base的 LoRA 训练非常难以收敛，随时处于梯度爆炸的边缘。DeepSpeed 会频繁触发reduce scale 操作，最终scale太小越界导致训练崩溃。我遍历了LR 1e-5 - 2e-4，LoRA rank [4, 8, 64]，LoRA Alpha [1,4,8,16,32]，LoRA Dropout [0.05, 0.1] ，Warmup Ratio [0.01, 0.03, 0.05]等超参数，均无法稳定训练。因此，本模型重新回归了全参数SFT训练。其难以进行LoRA训练的原因还待分析。; 由于网上存在使用LoRA 在英文SFT数据集上基于Llama2-base 进行SFT训练成功的样例，因此我怀疑难以训练的原因可能是扩中文词表embedding导致训练难度大幅度提升。; 为了方便后来人一起分析，本模型仓库特地将训练的全部loss/LR信息附在Material中。; 训练数据使用BELLE项目中采样的50万SFT数据进行SFT训练。",natural-language-processing,text-generation,https://huggingface.co/RicardoLee/Llama2-base-7B-Chinese-50W-pre_release,,,,Chinese; English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 28.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
bwx-13B-hf,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is an experimental product that can be used to create new LLM bassed on Chinese language.; You can use the raw model for next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task.
Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.; Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.
It also inherits some of the bias of its dataset model.; Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.; Use the code below to get started with the model.",natural-language-processing,text-generation,https://huggingface.co/BlueWhaleX/bwx-13B-hf,,License: apache-2.0,Datasets: BAAI/COIG-PC,Chinese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 0,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 27.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
albert-base-v2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model, as all ALBERT models, is uncased: it does not make a difference
between english and English.; Disclaimer: The team releasing ALBERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; ALBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the ALBERT model as inputs.; ALBERT is particular in that it shares its layers across its Transformer. Therefore, all layers have the same weights. Using repeating layers results in a small memory footprint, however, the computational cost remains similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same number of (repeating) layers.",natural-language-processing,fill-mask,https://huggingface.co/albert-base-v2,Paper: https://arxiv.org/pdf/1909.11942.pdf,License: apache-2.0,Datasets: bookcorpus; wikipedia,English,PyTorch; TensorFlow; JAX; Rust; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 55,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 7588276,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 330.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 20
bert-base-multilingual-cased,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"Pretrained model on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective.
It was introduced in this paper and first released in
this repository. This model is case sensitive: it makes a difference
between english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion. This means
it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the languages in the training set that can then be used to
extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a
standard classifier using the features produced by the BERT model as inputs.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the model hub to look for
fine-tuned versions on a task that interests you.",natural-language-processing,fill-mask,https://huggingface.co/bert-base-multilingual-cased,Paper: https://arxiv.org/pdf/1810.04805.pdf,License: apache-2.0,Datasets: wikipedia,104 languages,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 194,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3305409,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 58
roberta-large-mnli,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"Model Description: roberta-large-mnli is the RoBERTa large model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) corpus. The model is a pretrained model on English language text using a masked language modeling (MLM) objective.; Use the code below to get started with the model. The model can be loaded with the zero-shot-classification pipeline like so:; You can then use this pipeline to classify sequences into any of the class names you specify. For example:; This fine-tuned model can be used for zero-shot classification tasks, including zero-shot sentence-pair classification (see the GitHub repo for examples) and zero-shot sequence classification.; The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.",natural-language-processing,text-classification,https://huggingface.co/roberta-large-mnli,Paper: https://arxiv.org/pdf/1907.11692.pdf; https://arxiv.org/pdf/1806.02847.pdf; https://arxiv.org/pdf/1804.07461.pdf; https://arxiv.org/pdf/1704.05426.pdf; https://arxiv.org/pdf/1508.05326.pdf; https://arxiv.org/pdf/1809.05053.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: mit,Datasets: multi_nli; wikipedia; bookcorpus,English,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 75,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 141571,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 15
t5-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-Large is the checkpoint with 770 million parameters. ; The developers write in a blog post that the model: ",natural-language-processing,translation,https://huggingface.co/t5-large,Paper: https://arxiv.org/pdf/1805.12471.pdf; https://arxiv.org/pdf/1708.00055.pdf; https://arxiv.org/pdf/1704.05426.pdf; https://arxiv.org/pdf/1606.05250.pdf; https://arxiv.org/pdf/1808.09121.pdf; https://arxiv.org/pdf/1810.12885.pdf; https://arxiv.org/pdf/1905.10044.pdf; https://arxiv.org/pdf/1910.09700.pdf,License: apache-2.0,Datasets: c4,5 languages,PyTorch; TensorFlow; JAX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 78,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1253184,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 12.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 130
gpt-neo-1.3B,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"GPT-Neo 1.3B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 1.3B represents the number of parameters of this particular pre-trained model.; GPT-Neo 1.3B was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model.; This model was trained on the Pile for 380 billion tokens over 362,000 steps. It was trained as a masked autoregressive language model, using cross-entropy loss.; This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:",natural-language-processing,text-generation,https://huggingface.co/EleutherAI/gpt-neo-1.3B,,License: mit,Datasets: EleutherAI/pile,English,PyTorch; JAX; Rust; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 188,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 150568,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 21.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 108
opus-mt-en-fr,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c35d78b2b6024ba90ddb_Translation.svg,,source languages: en; target languages: fr; OPUS readme: en-fr; dataset: opus; model: transformer-align,natural-language-processing,translation,https://huggingface.co/Helsinki-NLP/opus-mt-en-fr,,License: apache-2.0,,English; French,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 16,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 108063,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 904.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 31
gpt-neo-small-portuguese,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"This is a finetuned version from GPT-Neo 125M by EletheurAI to Portuguese language. ; It was trained from 227,382 selected texts from a PTWiki Dump. You can found all the data from here: https://archive.org/details/ptwiki-dump-20210520; Every text was passed through a GPT2-Tokenizer with bos and eos tokens to separate them, with max sequence length that the GPT-Neo could support. It was finetuned using the default metrics of the Trainer Class, available on the Hugging Face library.; My true intention was totally educational, thus making available a Portuguese version of this model.; How to use",natural-language-processing,text-generation,https://huggingface.co/HeyLucasLeao/gpt-neo-small-portuguese,,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 4,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 66,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 529.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
roberta-large-ner-english,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c44113229be9d52beb7b_Token%20Classification.svg,,"[roberta-large-ner-english] is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. 
Model was validated on emails/chat data and outperformed other models on this type of data specifically. 
In particular the model seems to work better on entity that don't start with an upper case.; Training data was classified as follow:; In order to simplify, the prefix B- or I- from original conll2003 was removed.
I used the train and test dataset from original conll2003 for training and the ""validation"" dataset for validation. This resulted in a dataset of size:; Model performances computed on conll2003 validation dataset (computed on the tokens predictions); On private dataset (email, chat, informal discussion), computed on word predictions:",natural-language-processing,token-classification,https://huggingface.co/Jean-Baptiste/roberta-large-ner-english,,License: mit,Datasets: conll2003,English,PyTorch; TensorFlow; ONNX; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 49,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 241991,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 6.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
roberta_toxicity_classifier,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c436ecd77eb47132affa_Text%20Classification.svg,,"This model is trained for toxicity classification task. The dataset used for training is the merge of the English parts of the three datasets by Jigsaw (Jigsaw 2018, Jigsaw 2019, Jigsaw 2020), containing around 2 million examples. We split it into two parts and fine-tune a RoBERTa model (RoBERTa: A Robustly Optimized BERT Pretraining Approach) on it. The classifiers perform closely on the test set of the first Jigsaw competition, reaching the AUC-ROC of 0.98 and F1-score of 0.76.; Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.; ",natural-language-processing,text-classification,https://huggingface.co/s-nlp/roberta_toxicity_classifier,Paper: https://arxiv.org/pdf/1907.11692.pdf,,,English,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 19,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2050,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 502.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
CPM-Generate,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0945c4aa4f4832ec6afd6_Text%20Generation.svg,,"CPM (Chinese Pre-trained Language Model) is a Transformer-based autoregressive language model, with 2.6 billion parameters and 100GB Chinese training data. To the best of our knowledge, CPM is the largest Chinese pre-trained language model, which could facilitate downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. [Project] [Model] [Paper]; The text generated by CPM is automatically generated by a neural network model trained on a large number of texts, which does not represent the authors' or their institutes' official attitudes and preferences. The text generated by CPM is only used for technical and scientific purposes. If it infringes on your rights and interests or violates social morality, please do not propagate it, but contact the authors and the authors will deal with it promptly.; We collect different kinds of texts in our pre-training, including encyclopedia, news, novels, and Q&A. The details of our training data are shown as follows.; Based on the hyper-parameter searching on the learning rate and batch size, we set the learning rate as 1.5×10?41.5\times10^{-4}1.5×10?4 and the batch size as 3,0723,0723,072, which makes the model training more stable. In the first version, we still adopt the dense attention and the max sequence length is 1,0241,0241,024. We will implement sparse attention in the future. We pre-train our model for 20,00020,00020,000 steps, and the first 5,0005,0005,000 steps are for warm-up. The optimizer is Adam. It takes two weeks to train our largest model using 646464 NVIDIA V100.; We evaluate CPM with different numbers of parameters (the details are shown above) on various Chinese NLP tasks in the few-shot (even zero-shot) settings. With the increase of parameters, CPM performs better on most datasets, indicating that larger models are more proficient at language generation and language understanding. We provide results of text classification, chinese idiom cloze test, and short text conversation generation as follows. Please refer to our paper for more detailed results.",natural-language-processing,text-generation,https://huggingface.co/TsinghuaAI/CPM-Generate,Paper: https://arxiv.org/pdf/2012.00413.pdf,License: mit,Datasets: 100GB Chinese corpus,Chinese,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 33,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 436,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 21.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
longformer-base-4096,,,,,,,,,"Longformer is a transformer model for long documents. ; longformer-base-4096 is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096. ; Longformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations.
Please refer to the examples in modeling_longformer.py and the paper for more details on how to set global attention.; If you use Longformer in your research, please cite Longformer: The Long-Document Transformer.; Longformer is an open-source project developed by the Allen Institute for Artificial Intelligence (AI2).
AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering.",,,https://huggingface.co/allenai/longformer-base-4096,Paper: https://arxiv.org/pdf/2004.05150.pdf,License: apache-2.0,,English,PyTorch; TensorFlow; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 86,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2832199,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
KeyBART,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"KeyBART as described in ""Learning Rich Representations of Keyphrase from Text"" published in the Findings of NAACL 2022 (https://aclanthology.org/2022.findings-naacl.67.pdf), pre-trains a BART-based architecture to produce a concatenated sequence of keyphrases in the CatSeqD format.; We provide some examples on Downstream Evaluations setups and and also how it can be used for Text-to-Text Generation in a zero-shot setting.; Reported Results:; Reported Results:; Alternatively use the Hosted Inference API console provided in https://huggingface.co/bloomberg/KeyBART",natural-language-processing,text2text-generation,https://huggingface.co/bloomberg/KeyBART,,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 18,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 555,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
camembert-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. ; It is now available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data and pretraining data source domains. ; For further information or requests, please go to Camembert Website; CamemBERT was trained and evaluated by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Suárez*, Yoann Dupont, Laurent Romary, éric Villemonte de la Clergerie, Djamé Seddah and Beno?t Sagot.; If you use our work, please cite:",natural-language-processing,fill-mask,https://huggingface.co/camembert/camembert-base,Paper: https://arxiv.org/pdf/1911.03894.pdf,,,French,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2271,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 446.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
sentence-camembert-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"Sentence-CamemBERT-Large is the Embedding Model for French developed by La Javaness. The purpose of this embedding model is to represent the content and semantics of a French sentence in a mathematical vector which allows it to understand the meaning of the text-beyond individual words in queries and documents, offering a powerful semantic search.; The model is Fine-tuned using pre-trained facebook/camembert-large and
Siamese BERT-Networks with 'sentences-transformers' on dataset stsb; The model can be used directly (without a language model) as follows:; The model can be evaluated as follows on the French test data of stsb.; Test Result: 
The performance is measured using Pearson and Spearman correlation:",natural-language-processing,sentence-similarity,https://huggingface.co/dangvantuan/sentence-camembert-large,Paper: https://arxiv.org/pdf/1908.10084.pdf,License: apache-2.0,Datasets: stsb_multi_mt,French,PyTorch; TensorFlow; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 25,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 14412,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
bert-base-turkish-cased,,,,,,,,,"In this repository the MDZ Digital Library team (dbmdz) at the Bavarian State
Library open sources a cased model for Turkish ?; BERTurk is a community-driven cased BERT model for Turkish.; Some datasets used for pretraining and evaluation are contributed from the
awesome Turkish NLP community, as well as the decision for the model name: BERTurk.; The current version of the model is trained on a filtered and sentence
segmented version of the Turkish OSCAR corpus,
a recent Wikipedia dump, various OPUS corpora and a
special corpus provided by Kemal Oflazer.; The final training corpus has a size of 35GB and 44,04,976,662 tokens.",,,https://huggingface.co/dbmdz/bert-base-turkish-cased,,License: mit,,Turkish,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 32,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 22370,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
tinyroberta-squad2,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c8fb967baeceb7967ad1_Question_Answering.svg,,"This is the distilled version of the deepset/roberta-base-squad2 model. This model has a comparable prediction quality and runs at twice the speed of the base model.; Language model: tinyroberta-squad2Language: EnglishDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD 2.0Code: See an example QA pipeline on HaystackInfrastructure: 4x Tesla v100; This model was distilled using the TinyBERT approach described in this paper and implemented in haystack.
Firstly, we have performed intermediate layer distillation with roberta-base as the teacher which resulted in deepset/tinyroberta-6l-768d.
Secondly, we have performed task-specific distillation with deepset/roberta-base-squad2 as the teacher for further intermediate layer distillation on an augmented version of SQuADv2 and then with deepset/roberta-large-squad2 as the teacher for prediction layer distillation. ; Haystack is an NLP framework by deepset. You can use this model in a Haystack pipeline to do question answering at scale (over many documents). To load the model in Haystack:; Evaluated on the SQuAD 2.0 dev set with the official eval script.",natural-language-processing,question-answering,https://huggingface.co/deepset/tinyroberta-squad2,Paper: https://arxiv.org/pdf/1909.10351.pdf,License: cc-by-4.0,Datasets: squad_v2,English,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 44,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1430902,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 655.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 18
mt5-small-sum-de-mit-v1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c4855e63a23d3f26771a_Summarization.svg,,"This is a German summarization model. It is based on the multilingual T5 model google/mt5-small. The special characteristic of this model is that, unlike many other models, it is licensed under a permissive open source license (MIT). Among other things, this license allows commercial use.; 
This model is provided by the One Conversation
team of Deutsche Telekom AG.; The training was conducted with the following hyperparameters:; The datasets were preprocessed as follows:; The summary was tokenized with the google/mt5-small tokenizer. Then only the records with no more than 94 summary tokens were selected.",natural-language-processing,summarization,https://huggingface.co/deutsche-telekom/mt5-small-sum-de-mit-v1,,License: mit,Datasets: swiss_text_2019,German,PyTorch; Safetensors; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 5,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 211,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
kan-bayashi_ljspeech_vits,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9511aaa882b322dc3d8_Text-to-Speech.svg,,?? Imported from https://zenodo.org/record/5443814/; This model was trained by kan-bayashi using ljspeech/tts1 recipe in espnet.; or arXiv:,audio,text-to-speech,https://huggingface.co/espnet/kan-bayashi_ljspeech_vits,Paper: https://arxiv.org/pdf/1804.00015.pdf,License: cc-by-4.0,Datasets: ljspeech,English,ESPnet,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 148,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2284,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 3.4KB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 103
detr-resnet-50,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c921e73e7b6d3bf06402_Object%20Detection.svg,,"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository. ; Disclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.; The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. ; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.; ",computer-vision,object-detection,https://huggingface.co/facebook/detr-resnet-50,Paper: https://arxiv.org/pdf/2005.12872.pdf,License: apache-2.0,Datasets: coco,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 200,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 650109,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 167.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 54
m2m100_418M,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation.
It was introduced in this paper and first released in this repository.; The model that can directly translate between the 9,900 directions of 100 languages.
To translate into a target language, the target language id is forced as the first generated token.
To force the target language id as the first generated token, pass the forced_bos_token_id parameter to the generate method.; Note: M2M100Tokenizer depends on sentencepiece, so make sure to install it before running the example.; To install sentencepiece run pip install sentencepiece; See the model hub to look for more fine-tuned versions.",natural-language-processing,text2text-generation,https://huggingface.co/facebook/m2m100_418M,Paper: https://arxiv.org/pdf/2010.11125.pdf,License: mit,,101 languages,PyTorch; Rust; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 101,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 248327,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 23
tts_transformer-zh-cv7_css10,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9511aaa882b322dc3d8_Text-to-Speech.svg,,Transformer text-to-speech model from fairseq S^2 (paper/code):; See also fairseq S^2 example.,audio,text-to-speech,https://huggingface.co/facebook/tts_transformer-zh-cv7_css10,Paper: https://arxiv.org/pdf/1809.08895.pdf; https://arxiv.org/pdf/2109.06912.pdf,,Datasets: common_voice; css10,Chinese,Fairseq,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 67,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 1848,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 832.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 15
hubert-dementia-screening,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c89a13229be9d530ef79_Feature%20Extraction.svg,,No model card; New: Create and edit this model card directly on the website!,multimodel,feature-extraction,https://huggingface.co/flax-community/hubert-dementia-screening,,,,,JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 3,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 418.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
all_datasets_v4_MiniLM-L6,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised 
contrastive learning objective. We used the pretrained 'MiniLM-L6-H384-uncased' model and fine-tuned in on a 
1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.; We developped this model during the 
Community week using JAX/Flax for NLP & CV, 
organized by Hugging Face. We developped this model as part of the project:
Train the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well
as intervention from Google’s Flax, JAX, and Cloud team member about efficient deep learning frameworks.; Our model is intented to be used as a sentence encoder. Given an input sentence, it ouptuts a vector which captures 
the sentence semantic information. The sentence vector may be used for information retrieval, clustering or sentence 
similarity tasks.; Here is how to use this model to get the features of a given text using SentenceTransformers library:; We use the pretrained 'MiniLM-L6-H384-uncased' which is a 6 layer version of 
'microsoft/MiniLM-L12-H384-uncased' by keeping only every second layer. 
Please refer to the model card for more detailed information about the pre-training procedure.",natural-language-processing,sentence-similarity,https://huggingface.co/flax-sentence-embeddings/all_datasets_v4_MiniLM-L6,Paper: https://arxiv.org/pdf/2104.08727.pdf; https://arxiv.org/pdf/1810.09305.pdf; https://arxiv.org/pdf/2102.07033.pdf; https://arxiv.org/pdf/1904.06472.pdf,,,English,PyTorch; Sentence Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 22,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 13138,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 91.6MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 3
mt5-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Google's mT5; mT5 is pretrained on the mC4 corpus, covering 101 languages:; Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.; Note: mT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; Pretraining Dataset: mC4",natural-language-processing,text2text-generation,https://huggingface.co/google/mt5-base,Paper: https://arxiv.org/pdf/2010.11934.pdf,License: apache-2.0,Datasets: mc4,102 languages,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 86,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 42162,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 7.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
chinese-roberta-wwm-ext-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c063a54aa4f4832e8bcb11_Fill-Mask.svg,,"For further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking. ; Pre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu; This repository is developed based on：https://github.com/google-research/bert; You may also interested in,; More resources by HFL: https://github.com/ymcui/HFL-Anthology",natural-language-processing,fill-mask,https://huggingface.co/hfl/chinese-roberta-wwm-ext-large,Paper: https://arxiv.org/pdf/1906.08101.pdf; https://arxiv.org/pdf/2004.13922.pdf,License: apache-2.0,,Chinese,PyTorch; TensorFlow; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 47,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 36588,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 4.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 2
ko-sroberta-multitask,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c0640e9ca5a179810f728e_Sentence%20Similarity.svg,,"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; KorSTS, KorNLI ?? ?????? ?? ??? ??? ??? ? KorSTS ?? ?????? ??? ?????.",natural-language-processing,sentence-similarity,https://huggingface.co/jhgan/ko-sroberta-multitask,,,,Korean,PyTorch; TensorFlow; Sentence Transformers; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 26,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 60963,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 887.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
manga-ocr-base,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"Optical character recognition for Japanese text, with the main focus being Japanese manga.; It uses Vision Encoder Decoder framework.; Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality
text recognition, robust against various scenarios specific to manga:; Code is available here.",multimodel,image-to-text,https://huggingface.co/kha-white/manga-ocr-base,,License: apache-2.0,Datasets: manga109s,Japanese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 41,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 33169,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 444.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 5
t5-base-japanese-web,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"megagonlabs/t5-base-japanese-web is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.Training codes are available on GitHub.; The vocabulary size of this model is 32K.
8K version is also available.; We used following corpora for pre-training.; We used Japanese Wikipedia to train SentencePiece.; It took about 126 hours with TPU v3-8",natural-language-processing,text2text-generation,https://huggingface.co/megagonlabs/t5-base-japanese-web,Paper: https://arxiv.org/pdf/1910.10683.pdf,License: apache-2.0,Datasets: mc4; wiki40b,Japanese,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 13,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 751,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 991.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
trocr-large-stage1,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c67dee42f488ff915465_Image-to-Text.svg,,"TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.",multimodel,image-to-text,https://huggingface.co/microsoft/trocr-large-stage1,Paper: https://arxiv.org/pdf/2109.10282.pdf,,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 8,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2580,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 0
t5-base-finetuned-wikiSQL,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c45a4cdefda2b124a8f8_Text2Text%20Generation.svg,,"Google's T5 fine-tuned on WikiSQL for English to SQL translation.; The T5 model was presented in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu in Here the abstract:; Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.; ; Dataset ID: wikisql from  Huggingface/NLP",natural-language-processing,text2text-generation,https://huggingface.co/mrm8488/t5-base-finetuned-wikiSQL,Paper: https://arxiv.org/pdf/1910.10683.pdf,License: apache-2.0,Datasets: wikisql,English,PyTorch; JAX; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 35,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 3847,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 2.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 1
vit-age-classifier,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c062f02b5a28e26197f913_Image%20Classification.svg,,A vision transformer finetuned to classify the age of a given person's face. ,computer-vision,image-classification,https://huggingface.co/nateraw/vit-age-classifier,,,Datasets: fairface,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 27,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 2539259,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 343.0MB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 4
dpt-large,,,,,,,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c1c9ca967baeceb7975cf3_Depth_Estimation-2.svg,,"Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. 
It was introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. 
DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.
; The model card has been written in combination by the Hugging Face team and Intel.; Here is how to use this model for zero-shot depth estimation on an image:; For more code examples, we refer to the documentation.; Table 1. Comparison to the state of the art on monocular depth estimation. We evaluate zero-shot cross-dataset transfer according to the
protocol defined in [30]. Relative performance is computed with respect to the original MiDaS model [30]. Lower is better for all metrics. (Ranftl et al., 2021)",computer-vision,depth-estimation,https://huggingface.co/Intel/dpt-large,Paper: https://arxiv.org/pdf/2103.13413.pdf,License: apache-2.0,,,PyTorch; Transformers,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c04e110d6b76f9eed4a03b_heart-icon%20(1).svg,likes 79,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fbd8519be7a7b8243bb_download_icon.svg,downloads 166271,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fc92b95cbe3c8785fb1_size_icon.svg,model size 1.0GB,https://uploads-ssl.webflow.com/64bcb453e3df2ded5d02d93f/64c05fce61bda9e68fed4508_usage_icon.svg,usage 144
