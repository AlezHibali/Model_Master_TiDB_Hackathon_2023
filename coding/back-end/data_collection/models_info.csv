model_name,task,libraries,language,license,paper,likes,datasets,downloads_last_month,model_size,model_usage,huggingface_link,abstract
Llama-2-7b,Text Generation,PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,863,,0,0.0,2,https://huggingface.co/meta-llama/Llama-2-7b,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-70b-chat-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,https://arxiv.org/pdf/2307.09288.pdf,532,,"41,430",0.0,90,https://huggingface.co/meta-llama/Llama-2-70b-chat-hf,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-7b-chat-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,https://arxiv.org/pdf/2307.09288.pdf,308,,"77,918",0.0,53,https://huggingface.co/meta-llama/Llama-2-7b-chat-hf,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-70b-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,https://arxiv.org/pdf/2307.09288.pdf,291,,"155,399",0.0,44,https://huggingface.co/meta-llama/Llama-2-70b-hf,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
stable-diffusion-xl-base-0.9,Text-to-Image,Diffusers,,other,https://arxiv.org/pdf/2108.01073.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2307.01952.pdf,1.19k,,"237,736",0.0,49,https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9,"SDXL 0.9 Research License Agreement; Copyright (c) Stability AI Ltd.; This License Agreement (as may be amended in accordance with this License Agreement, “License”), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (“Licensee” or “you”) and Stability AI Ltd. (“Stability AI” or “we”) applies to your use of any computer program, algorithm, source code, object code, software, models, or model weights that is made available by Stability AI under this License (“Software”) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (“Documentation”). By using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the “Software Products”), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; LICENSE GRANTa. Subject to your compliance with the Documentation and Sections 2, 3, and 5, Stability AI grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty free and limited license under Stability AI’s copyright interests to use, reproduce, and create derivative works of the Software solely for your non-commercial research purposes. The foregoing license is personal to you, and you may not assign, sublicense, distribute, publish, host, or otherwise make available this Software, derivative works of the Software, models or model weights associated with the Software, this License, or any other rights or obligations under this License without Stability AI’s prior written consent; any such assignment or sublicense without Stability AI’s prior written consent will be void and will automatically and immediately terminate this License.  For sake of clarity, this License does not grant to you the right or ability to extend any license to the Software, derivative works of the Software, or associated models or model weights to a non-Licensee, nor does this License permit you to create a new Licensee, such as by making available a copy of this License.  If you would like rights not granted by this License, you may seek permission by sending an email to legal@stability.ai.b. You may make a reasonable number of copies of the Documentation solely for your use in connection with the license to the Software granted above.c. The grant of rights expressly set forth in this Section 1 (License Grant) are the complete grant of rights to you in the Software Products, and no other licenses are granted, whether by waiver, estoppel, implication, equity or otherwise. Stability AI and its licensors reserve all rights not expressly granted by this License.; RESTRICTIONSYou will not, and will not permit, assist or cause any third party to:a. use, modify, copy, reproduce, create derivative works of, or distribute the Software Products (or any derivative works thereof, works incorporating the Software Products, or any data produced by the Software), in whole or in part, for (i) any commercial or production purposes, (ii) military purposes or in the service of nuclear technology, (iii) purposes of surveillance, including any research or development relating to surveillance, (iv) biometric processing, (v) in any manner that infringes, misappropriates, or otherwise violates any third-party rights, or (vi) in any manner that violates any applicable law and violating any privacy or security laws, rules, regulations, directives, or governmental requirements (including the General Data Privacy Regulation (Regulation (EU) 2016/679), the California Consumer Privacy Act, and any and all laws governing the processing of biometric information), as well as all amendments and successor laws to any of the foregoing;b. alter or remove copyright and other proprietary notices which appear on or in the Software Products;c. utilize any equipment, device, software, or other means to circumvent or remove any security or protection used by Stability AI in connection with the Software, or to circumvent or remove any usage restrictions, or to enable functionality disabled by Stability AI; ord. offer or impose any terms on the Software Products that alter, restrict, or are inconsistent with the terms of this License.e. 1) violate any applicable U.S. and non-U.S. export control and trade sanctions laws (“Export Laws”); 2) directly or indirectly export, re-export, provide, or otherwise transfer Software Products: (a) to any individual, entity, or country prohibited by Export Laws; (b) to anyone on U.S. or non-U.S. government restricted parties lists; or (c) for any purpose prohibited by Export Laws, including nuclear, chemical or biological weapons, or missile technology applications; 3) use or download Software Products if you or they are: (a) located in a comprehensively sanctioned jurisdiction, (b) currently listed on any U.S. or non-U.S. restricted parties list, or (c) for any purpose prohibited by Export Laws; and (4) will not disguise your location through IP proxying or other methods."
Llama-2-13B-chat-GGML,Text Generation,Transformers; PyTorch,English,other,,246,,549,119316.51276142121,5,https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 13B-chat.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
Llama-2-13b-chat-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,https://arxiv.org/pdf/2307.09288.pdf,206,,"14,790",0.0,29,https://huggingface.co/meta-llama/Llama-2-13b-chat-hf,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-7b-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,https://arxiv.org/pdf/2307.09288.pdf,182,,"19,069",0.0,10,https://huggingface.co/meta-llama/Llama-2-7b-hf,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
FreeWilly2,Text Generation,PyTorch; Transformers,English,cc-by-nc-4.0,https://arxiv.org/pdf/2307.09288.pdf; https://arxiv.org/pdf/2306.02707.pdf,181,conceptofmind/cot_submix_original; conceptofmind/flan2021_submix_original; conceptofmind/t0_submix_original; conceptofmind/niv2_submix_original,10,282554.7216642381,2,https://huggingface.co/stabilityai/FreeWilly2,"FreeWilly2 is a Llama2 70B model finetuned on an Orca style Dataset; Start chatting with FreeWilly2 using the following code snippet:; FreeWilly should be used with this prompt format:; FreeWilly2 is trained on our internal Orca-style dataset; Models are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (BF16), and optimized with AdamW. We outline the following hyperparameters:"
chatglm2-6b,,PyTorch; Transformers,Chinese; English,,https://arxiv.org/pdf/2103.10360.pdf; https://arxiv.org/pdf/2210.02414.pdf; https://arxiv.org/pdf/1911.02150.pdf,1.24k,,"1,391,548",12801.130681915283,64,https://huggingface.co/THUDM/chatglm2-6b,"
  💻 Github Repo • 🐦 Twitter • 📃 [GLM@ACL 22] [GitHub] • 📃 [GLM-130B@ICLR 23] [GitHub] 
; 
    👋 Join our Slack and WeChat
; ChatGLM2-6B 是开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM2-6B 引入了如下新特性：; ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the following new features:; 可以通过如下代码调用 ChatGLM-6B 模型来生成对话："
Llama-2-13b-hf,Text Generation,PyTorch; Safetensors; Transformers,English,,https://arxiv.org/pdf/2307.09288.pdf,150,,"39,218",0.0,11,https://huggingface.co/meta-llama/Llama-2-13b-hf,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-70b,Text Generation,PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,146,,0,0.0,3,https://huggingface.co/meta-llama/Llama-2-70b,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-70b-chat,Text Generation,PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,146,,0,0.0,2,https://huggingface.co/meta-llama/Llama-2-70b-chat,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-7b-chat,Text Generation,PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,138,,0,0.0,8,https://huggingface.co/meta-llama/Llama-2-7b-chat,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-13B-chat-GPTQ,Text Generation,Transformers; PyTorch,English,other,,117,,"5,046",7436.603778953552,,https://huggingface.co/TheBloke/Llama-2-13B-chat-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 13B-chat.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; To continue a conversation:"
Baichuan-13B-Chat,Text Generation,PyTorch; Transformers,Chinese; English,,https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2009.03300.pdf,385,,"980,642",27168.33809402466,2,https://huggingface.co/baichuan-inc/Baichuan-13B-Chat,"Baichuan-13B-Chat为Baichuan-13B系列模型中对齐后的版本，预训练模型可见Baichuan-13B-Base。; Baichuan-13B 是由百川智能继 Baichuan-7B 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。本次发布包含有预训练 (Baichuan-13B-Base) 和对齐 (Baichuan-13B-Chat) 两个版本。Baichuan-13B 有如下几个特点：; Baichuan-13B-Chat is the aligned version in the Baichuan-13B series of models, and the pre-trained model can be found at Baichuan-13B-Base.; Baichuan-13B is an open-source, commercially usable large-scale language model developed by Baichuan Intelligence, following Baichuan-7B. With 13 billion parameters, it achieves the best performance in standard Chinese and English benchmarks among models of its size. This release includes two versions: pre-training (Baichuan-13B-Base) and alignment (Baichuan-13B-Chat). Baichuan-13B has the following features:; 如下是一个使用Baichuan-13B-Chat进行对话的示例，正确输出为""乔戈里峰。世界第二高峰———乔戈里峰西方登山者称其为k2峰，海拔高度是8611米，位于喀喇昆仑山脉的中巴边境上"""
Llama-2-13B-GGML,Text Generation,Transformers; PyTorch,English,other,,108,,125,119316.51207782746,3,https://huggingface.co/TheBloke/Llama-2-13B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
Llama-2-13b,Text Generation,PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,98,,0,0.0,,https://huggingface.co/meta-llama/Llama-2-13b,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
Llama-2-7B-GGML,Text Generation,Transformers; PyTorch,English,other,,93,,275,61870.11285907746,,https://huggingface.co/TheBloke/Llama-2-7B-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
chatglm-fitness-RLHF,,PyTorch; PEFT,Chinese; English,apache-2.0,,131,,0,12392.449543075563,1,https://huggingface.co/fb700/chatglm-fitness-RLHF,模型体验地址：https://huggingface.co/spaces/fb700/chatglm-fitness-RLHF; ChatGLM-6B 是开源中英双语对话模型，本次训练基于ChatGLM-6B 的第一代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上开展训练。通过训练我们对模型有了更深刻的认知，LLM在一直在进化，好的方法和数据可以挖掘出模型的更大潜能。; 首先，用40万条高质量数据进行强化训练，以提高模型的基础能力；; 第二，使用30万条人类反馈数据，构建一个表达方式规范优雅的语言模式（RM模型）；; 第三，在保留SFT阶段三分之一训练数据的同时，增加了30万条fitness数据，叠加RM模型，对ChatGLM-6B进行强化训练。
Llama-2-7B-Chat-GGML,Text Generation,Transformers; PyTorch,English,other,,91,,339,61870.1127614212,3,https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 7b Chat.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods."
ControlNet-v1-1,,,,openrail,,2.06k,,0,20787.228354110706,,https://huggingface.co/lllyasviel/ControlNet-v1-1,"This is the model files for ControlNet 1.1.
This model card will be filled in a more detailed way after 1.1 is officially merged into ControlNet.; Unable to determine this model’s pipeline type. Check the
								docs 
.
							"
stable-diffusion-v1-5,Text-to-Image,Diffusers,,creativeml-openrail-m,https://arxiv.org/pdf/2207.12598.pdf; https://arxiv.org/pdf/2112.10752.pdf; https://arxiv.org/pdf/2103.00020.pdf; https://arxiv.org/pdf/2205.11487.pdf; https://arxiv.org/pdf/1910.09700.pdf,8.74k,,"6,901,572",24514.578015937805,1545,https://huggingface.co/runwayml/stable-diffusion-v1-5,"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
For more information about how Stable Diffusion functions, please have a look at 🤗's Stable Diffusion blog.; The Stable-Diffusion-v1-5 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 
checkpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on ""laion-aesthetics v2 5+"" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.; You can use this both with the 🧨Diffusers library and the RunwayML GitHub repository.; For more detailed instructions, use-cases and examples in JAX follow the instructions here; Download the weights "
Llama-2-13b-chat,Text Generation,PyTorch,English,,https://arxiv.org/pdf/2307.09288.pdf,81,,0,0.0,3,https://huggingface.co/meta-llama/Llama-2-13b-chat,"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here."
ruGPT-3.5-13B,Text Generation,PyTorch; Transformers,English; Russian,mit,,72,,768,53885.92852474212,3,https://huggingface.co/ai-forever/ruGPT-3.5-13B,"Language model for Russian. Model has 13B parameters as you can guess from it's name. This is our biggest model so far and it was used for trainig GigaChat (read more about it in the article).; Model was pretrained on a 300Gb of various domains, than additionaly trained on the 100 Gb of code and legal documets. Here is the dataset structure:; ; Training data was deduplicated, the text deduplication includes 64-bit hashing of each text in the corpus for keeping texts with a unique hash. We also filter the documents based on their text compression rate using zlib4. The most strongly and weakly compressing deduplicated texts are discarded.; Model was trained using Deepspeed and Megatron libraries, on 300B tokens dataset for 3 epochs, around 45 days on 512 V100. After that model was finetuned 1 epoch with sequence length 2048 around 20 days on 200 GPU A100 on additional data (see above)."
falcon-40b,Text Generation,PyTorch; Transformers,4 languages,apache-2.0,https://arxiv.org/pdf/2205.14135.pdf; https://arxiv.org/pdf/1911.02150.pdf; https://arxiv.org/pdf/2101.00027.pdf; https://arxiv.org/pdf/2005.14165.pdf; https://arxiv.org/pdf/2104.09864.pdf; https://arxiv.org/pdf/2306.01116.pdf,2.13k,tiiuae/falcon-refinedweb,"155,172",85660.43191665648,40,https://huggingface.co/tiiuae/falcon-40b,"Falcon-40B is a 40B parameters causal decoder-only model built by TII and trained on 1,000B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.; Paper coming soon 😊.; We get it. AI is everywhere! Is it taking over? ; Before we debate the scant likelihood of a cyborg assassin from the future terminating humanity, let’s get to know the newbie that has soared to top-spot on the leaderboard – Falcon 40B.; Falcon 40B is the UAE’s and the Middle East’s first home-grown, open-source large language model (LLM) with 40 billion parameters trained on one trillion tokens. The brainchild of the Technology Innovation Institute (TII), Falcon 40B has generated a tremendous amount of global interest and intrigue, but what really sweetens the deal is its transparent, open-source feature. "
Llama-2-70B-chat-GPTQ,Text Generation,Transformers; PyTorch,English,other,,67,,"6,204",36149.566440467825,,https://huggingface.co/TheBloke/Llama-2-70B-chat-GPTQ,"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 70B Chat.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware for these quantisations!"
llama-30b-instruct-2048,Text Generation,PyTorch; Transformers,English,,,65,sciq; metaeval/ScienceQA_text_only; GAIR/lima; Open-Orca/OpenOrca; openbookqa,540,66634.06467716215,5,https://huggingface.co/upstage/llama-30b-instruct-2048,No other data was used except for the dataset mentioned above
LLongMA-2-7b,Text Generation,PyTorch; Transformers,,,https://arxiv.org/pdf/2108.12409.pdf; https://arxiv.org/pdf/2212.10554.pdf,63,,162,13824.554655342103,1,https://huggingface.co/conceptofmind/LLongMA-2-7b,"LLongMA-2, a suite of Llama-2 models, trained at 8k context length using linear positional interpolation scaling. The model was trained in collaboration with Emozilla of NousResearch and Kaiokendev.; We worked directly with Kaiokendev, to extend the context length of the Llama-2 7b model through fine-tuning. The models pass all our evaluations and maintain the same perplexity at 8k extrapolation surpassing the performance of other recent methodologies.; The model has identical performance to LLaMA 2 under 4k context length, performance scales directly to 8k, and works out-of-the-box with the new version of transformers (4.31) or with trust_remote_code for <= 4.30.; A Llama-2 13b model trained at 8k will release soon on huggingface here: https://huggingface.co/conceptofmind/LLongMA-2-13b; Applying the method to the rotary position embedding requires only slight changes to the model's code by dividing the positional index, t, by a scaling factor."
control_v1p_sd15_qrcode_monster,,Diffusers,English,openrail++,,213,,"2,805",5177.407102165223,4,https://huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster,"; This model is made to generate creative QR codes that still scan.
Keep in mind that not all generated codes might be readable, but you can try different parameters and prompts to get the desired results.; NEW VERSION; Introducing the upgraded version of our model - Controlnet QR code Monster v2.
V2 is a huge upgrade over v1, for scannability AND creativity.; QR codes can now seamlessly blend the image by using a gray-colored background (#808080)."
