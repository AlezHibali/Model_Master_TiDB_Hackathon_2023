abstract,,,,,,,,,,,,,,,,,,,,,,,,,model_usage
"BlueMethod is a bit of a convoluted experiment in tiered merging.
Furthering the experimental nature of the merge, the models combined
were done so with a custom script that randomized the percent of each
layer merged from one model to the next. This is a warmup for a larger
project.
[Tier One and Two Merges not released; internal naming convention]; Tier One Merges:; 13B-Metharme+13B-Nous-Hermes=13B-Methermes; 13B-Vicuna-cocktail+13B-Manticore=13B-Vicortia; 13B-HyperMantis+13B-Alpacino=13B-PsychoMantis",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for CalderaAI's 13B BlueMethod.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for CalderaAI's 13B BlueMethod.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for CalderaAI's 13B Ouroboros.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Epsilon is an instruct based general purpose model assembled from hand picked models and LoRAs.
There is no censorship and it follows instructions in the Alpaca format. This means you can create
your own rules in the context memory of your inference system of choice [mainly KoboldAI or Text
Generation Webui and chat UIs like SillyTavern and so on].; This model is the result of an experimental use of LoRAs on language models and model merges.
[] = applied as LoRA to a composite model | () = combined as composite models
30B-Epsilon = [SuperCOT[SuperHOT-prototype13b-8192[(wizardlmuncensored+((hippogriff+manticore)+(StoryV2))]; Alpaca's instruct format can be used to do many things, including control of the terms of behavior
between a user and a response from an agent in chat. Below is an example of a command injected into
memory.; All datasets from all models and LoRAs used were documented and reviewed as model candidates for merging.
Model candidates were based on five core principles: creativity, logic, inference, instruction following,
and longevity of trained responses. SuperHOT-prototype30b-8192 was used in this mix, not the 8K version;
the prototype LoRA seems to have been removed [from HF] as of this writing. The GPT4Alpaca LoRA from
Chansung was removed from this amalgam following a thorough review of where censorship and railroading
the user came from in 33B-Lazarus. This is not a reflection of ChanSung's excellent work - it merely did
not fit the purpose of this model.; manticore-30b-chat-pyg-alpha [Epoch0.4] by openaccess-ai-collective",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for CalderaAI's 30B Epsilon.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"[] = applied as LoRA to a composite model | () = combined as composite models; [SuperCOT([gtp4xalpaca(manticorechatpygalpha+vicunaunlocked)]+[StoryV2(kaiokendev-SuperHOT-LoRA-prototype30b-8192)])]; This model is the result of an experimental use of LoRAs on language models and model merges that are not the base HuggingFace-format LLaMA model they were intended for.
The desired outcome is to additively apply desired features without paradoxically watering down a model's effective behavior.; Potential limitations - LoRAs applied on top of each other may intercompete.; Subjective results - very promising. Further experimental tests and objective tests are required.",,,,,,,,,,,,,,,,,,,,,,,,,usage 12
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"default CFG Scale : 7 ??5; default Sampler : DPM++ 2M Karras; default Steps : 25; Negative prompt : (worst quality:1.4), (low quality:1.4) , (monochrome:1.1),; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"??guila-7B is a transformer-based causal language model for Catalan, Spanish, and English. 
It is based on the Falcon-7B model and has been trained on a 26B token 
trilingual corpus collected from publicly available corpora and crawlers.; The ??guila-7B model is ready-to-use only for causal language modeling to perform text-generation tasks. 
However, it is intended to be fine-tuned for downstream tasks.; Here is how to use this model:; At the time of submission, no measures have been taken to estimate the bias and toxicity embedded in the model. 
However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques 
on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated. ; We adapted the original Falcon-7B model to Spanish and Catalan by swapping the tokenizer and adjusting the embedding layer.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Model info : ; https://civitai.com/models/90045/aigen-14; Sample images I made:;,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This is a fine-tuned 13b parameter LlaMa model, using completely synthetic training data created by https://github.com/jondurbin/airoboros; ;  wb-13b-u is Wizard-Vicuna-13b-Uncensored; I used a jailbreak prompt to generate the synthetic instructions, which resulted in some training data that would likely be censored by other models, such as how-to prompts about synthesizing drugs, making homemade flamethrowers, etc.  Mind you, this is all generated by ChatGPT, not me.  My goal was to simply test some of the capabilities of ChatGPT when unfiltered (as much as possible), and not to intentionally produce any harmful/dangerous/etc. content.; The jailbreak prompt I used is the default prompt in the python code when using the --uncensored flag: https://github.com/jondurbin/airoboros/blob/main/airoboros/self_instruct.py#L39",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"update 2023-06-25 - re-uploaded with a slightly earlier checkpoint, which seems perhaps a little less overfit than the full 3-epochs version initially uploaded; This is a full (not qlora) fine-tune 13b parameter LlaMa model, using completely synthetic training data created gpt4 via https://github.com/jondurbin/airoboros; This is mostly an extension of the previous gpt-4 series, with a few extras:; This model was fine-tuned with a fork of FastChat; The prompt it was trained with was:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a qlora fine-tune 13b parameter LlaMa model, using completely synthetic training data created gpt4 via https://github.com/jondurbin/airoboros; Dataset used: https://huggingface.co/datasets/jondurbin/airoboros-gpt4-1.4.1; The point of this is to allow people to compare a full fine-tune https://huggingface.co/jondurbin/airoboros-13b-gpt4-1.4 to a qlora fine-tune.; This is mostly an extension of the previous gpt-4 series, with a few extras:; This model was fine-tuned with a fork of FastChat",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Jon Durbin's Airoboros 13B GPT4 1.4.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"These are GGML quantizations of https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-fp16; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"GGML quants of https://huggingface.co/bhenrym14/airoboros-33b-gpt4-1.4.1-PI-8192-fp16; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Jon Durbin's Airoboros 33B GPT4 1.4.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for John Durbin's Airoboros 65B GPT4 1.2.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"fp16 is here: https://huggingface.co/ycros/airoboros-65b-gpt4-1.4.1-PI-8192-fp16; peft file is here: https://huggingface.co/ycros/airoboros-65b-gpt4-1.4.1-PI-8192-peft; ggml quants: https://huggingface.co/ycros/airoboros-65b-gpt4-1.4.1-PI-8192-GGML; This is based on bhenrym14's airoboros 33b PI 8192 but on 65b.; See bhenrym14's notes there, everything applies except I based this on llama-65B.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Jon Durbin's Airoboros 65B GPT4 1.4.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Recommended prompt. Note that Jon Durbin recommends to replace all newlines with a space; newlines used here for readability.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Jon Durbin's Airoboros 65B GPT4 1.4.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a qlora fine-tuned 7b parameter LlaMa model, using completely synthetic training data created gpt4 via https://github.com/jondurbin/airoboros; This is mostly an extension of 1.1, but with thousands of new training data and an update to allow ""PLAINFORMAT"" at the end of coding prompts to just print the code without backticks or explanations/usage/etc.; The dataset used to fine-tune this model is available here, with a specific focus on:; This model was fine-tuned with a fork of qlora, which among other things was updated to use a slightly modified vicuna template to be compatible with the previous versions:; So in other words, it's the preamble/system prompt, followed by a single space, then ""USER: "" (single space after colon) then the prompt (which can have multiple lines, spaces, whatever), then a single space, followed by ""ASSISTANT: "" (with a single space after the colon).",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"mostly untested, use if you want, or wait for some validation; This is a full (not qlora) fine-tune 7b parameter LlaMa model, using completely synthetic training data created gpt4 via https://github.com/jondurbin/airoboros; This is mostly an extension of the previous gpt-4 series, with a few extras:; This model was fine-tuned with a fork of FastChat; The prompt it was trained with was:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Jon Durbin's Airoboros 7B GPT4 1.4.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Jon Durbin's Airoboros 7B GPT4 1.4.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Jon Durbin's Airoboros 7B GPT4 1.4.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Jon Durbin's Airoboros 7B GPT4 1.4 merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"After the initial experiment with chronoboros-33B it was evident that the merge was to unpredictable to be useful, testing the individual models it became clear that the bias should be weighted towards Chronos.
This is the new release of the merge with 75% chronos 33B, and 25% airoboros-1.4 33B.; Model has been tested with the Alpaca prompting format combined with KoboldAI Lite's instruct and chat modes, as well as regular story writing.
It has also been tested on basic reasoning tasks, but has not seen much testing for factual information.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model, as all ALBERT models, is uncased: it does not make a difference
between english and English.; Disclaimer: The team releasing ALBERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; ALBERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the ALBERT model as inputs.; ALBERT is particular in that it shares its layers across its Transformer. Therefore, all layers have the same weights. Using repeating layers results in a small memory footprint, however, the computational cost remains similar to a BERT-like architecture with the same number of hidden layers as it has to iterate through the same number of (repeating) layers.",,,,,,,,,,,,,,,,,,,,,,,,,usage 20
"????This is the model card for Albertina PT-BR. 
  You may be interested in some of the other models in the Albertina (encoders) and Gerv??sio (decoders) families.
; Albertina PT-* is a foundation, large language model for the Portuguese language.; It is an encoder of the BERT family, based on the neural architecture Transformer and 
developed over the DeBERTa model, and with most competitive performance for this language. 
It has different versions that were trained for different variants of Portuguese (PT), 
namely the European variant from Portugal (PT-PT) and the American variant from Brazil (PT-BR), 
and it is distributed free of charge and under a most permissible license.; Albertina PT-BR is the version for American Portuguese from Brazil, trained on the brWaC data set.; You may be interested also in Albertina PT-BR No-brWaC, trained on data sets other than brWaC and thus with a more permissive license.
To the best of our knowledge, these are encoders specifically for this language and variant 
that,  at the time of its initial distribution, set a new state of the art for it, and is made publicly available 
and distributed for reuse.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"The project aims to train sentence embedding models on very large sentence level datasets using a self-supervised 
contrastive learning objective. We used the pretrained 'MiniLM-L6-H384-uncased' model and fine-tuned in on a 
1B sentence pairs dataset. We use a contrastive learning objective: given a sentence from the pair, the model should predict which out of a set of randomly sampled other sentences, was actually paired with it in our dataset.; We developped this model during the 
Community week using JAX/Flax for NLP & CV, 
organized by Hugging Face. We developped this model as part of the project:
Train the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well
as intervention from Google??s Flax, JAX, and Cloud team member about efficient deep learning frameworks.; Our model is intented to be used as a sentence encoder. Given an input sentence, it ouptuts a vector which captures 
the sentence semantic information. The sentence vector may be used for information retrieval, clustering or sentence 
similarity tasks.; Here is how to use this model to get the features of a given text using SentenceTransformers library:; We use the pretrained 'MiniLM-L6-H384-uncased' which is a 6 layer version of 
'microsoft/MiniLM-L12-H384-uncased' by keeping only every second layer. 
Please refer to the model card for more detailed information about the pre-training procedure.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",,,,,,,,,,,,,,,,,,,,,,,,,usage 135
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",,,,,,,,,,,,,,,,,,,,,,,,,usage 139
Use this command to run with llama.cpp; contents of prompts/alpacanativeenhanced.txt should be; Original model https://huggingface.co/8bit-coder/alpaca-7b-nativeEnhanced; Inference API does not yet support adapter-transformers models for this pipeline type.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Method : QLORA; Dataset : yahma/alpaca-cleaned; Base model : huggyllama/llama-30b; Compute dtype : bfloat16,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repo contains a low-rank adapter for LLaMA-7b
fit on the Stanford Alpaca dataset.; This version of the weights was trained with the following hyperparameters:; That is:; Instructions for running it can be found at https://github.com/tloen/alpaca-lora.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 43
"Visit the Github for more information: https://github.com/avocardio/zicklein; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This is a replica of Alpaca by Stanford' tatsu; Trained using the original instructions with a minor modification in FSDP mode; 13B: https://huggingface.co/chavinlo/alpaca-13b; 13B -> GPT4 : https://huggingface.co/chavinlo/gpt4-x-alpaca; Trained on 4xA100s for 6H
Donated by redmond.ai",,,,,,,,,,,,,,,,,,,,,,,,,usage 17
"This is a https://huggingface.co/chavinlo/alpaca-native converted in OLD GGML (alpaca.cpp) format and quantized to 4 bits to run on CPU with 5GB of RAM.; For any additional information, please visit these repos:; alpaca.cpp repo: https://github.com/antimatter15/alpaca.cpp; llama.cpp repo: https://github.com/ggerganov/llama.cpp; original facebook llama(NOT ggml) repo: https://github.com/facebookresearch/llama",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Mirrored version of https://huggingface.co/Sosaka/Alpaca-native-4bit-ggml in case that one gets taken down
All credits go to Sosaka and chavinlo for creating the modelhttps://huggingface.co/chavinlo/alpaca-native; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Disclaimer: The model might have a tokenizer issue, but functions.  Updates to come.; AlpacaCielo-13b is a llama-2 based model designed for creative tasks, such as storytelling and roleplay, while still doing well with other chatbot purposes.  It is a triple model merge of Nous-Hermes + Guanaco + Storywriter. While it is mostly ""uncensored"", it still inherits some alignment from Guanaco.; Prompt format is:; Thanks to previous similar models such as Alpacino, Alpasta, and AlpacaDente for inspiring the creation of this model.  Thanks also to the creators of the models involved in the merge.  Original models:; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is an unofficial implementation of AlpaGasus-13B, which is a chat assistant trained by fine-tuning LLaMA on a Claud-filtered Alpaca dataset with around 5K triplets.; Please see the original LLaMA license before using this model.; AlpaGasus-13B is fine-tuned from LLaMA-13B with supervised instruction fine-tuning on the filtered Alpaca dataset.; Inference API has been turned off for this model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuned version of nlptown/bert-base-multilingual-uncased-sentiment on an Amazon US Customer Reviews Dataset. The code for the fine-tuning process can be found
here. This model is uncased: it does
not make a difference between english and English.
It achieves the following results on the evaluation set:; This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).; This model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above, or for further finetuning on related sentiment analysis tasks.; We replaced its head with our customer reviews to fine-tune it on 17,280 rows of training set while validating it on 4,320 rows of dev set. Finally, we evaluated our model performance on a held-out test set: 2,400 rows.; Bert-base is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification, or question answering. This fine-tuned version of BERT-base is used to predict review rating star given the review.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Analog Diffusion

CKPT DOWNLOAD LINK - This is a dreambooth model trained on a diverse set of analog photographs.; In your prompt, use the activation token: analog style; You may need to use the words blur haze naked in your negative prompts. My dataset did not include any NSFW material but the model seems to be pretty horny. Note that using blur and haze in your negative prompt can give a sharper image but also a less pronounced analog film effect.; Trained from 1.5 with VAE.; Please see this document where I share the parameters (prompt, sampler, seed, etc.) used for all example images.",,,,,,,,,,,,,,,,,,,,,,,,,usage 131
"civitai.com/user/andite; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
; ?????????????QLoRA??33B???????????? the First QLoRA based 33B fully open-source Chinese LLM; ???????model??LICENSE?????????????????¨®????????LICENSE??; https://github.com/lyogavin/Anima; Anima??????QLoRA?????33B guanaco?????10000 steps???????????H100 GPU??,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model repo is for AnimateDiff.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is currently a base model
; This model does not use any finetuning techniques such as face restoration or in-painting as of yet; Test the concept via A1111 Colab fast-Colab-A1111; Sample pictures of this concept:
; See more results from this model under the sample_images folder",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"; ; As it is a version made only by myself and my small associates, the model will not be perfect and may differ from what people expect. Any contributions from everyone will be respected.; Want to support me? Thank you, please help me make it better. ??; This wouldn't have happened if they hadn't made a breakthrough.",,,,,,,,,,,,,,,,,,,,,,,,,usage 42
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 63
"This is a fine-tuning model based on stable diiffusion.The model is fine-tuned based on SD1.5.The model was fine-tuned with HCP-diffusion
The model prompt is extremely accurate.; ????????????????????????; Many of today's SD models have a variety of problems.I want to make use of my limited ability to improve the current situation.So I used a lot of AI-generated images to refine this model; ; ????????????????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"DISCLAIMER! This Is A Preservation Repository!; Welcome to Anything V3 - Better VAE. It currently has three model formats: diffusers, ckpt, and safetensors. You'll never see a grey image result again. This model is designed to produce high-quality, highly detailed anime-style images with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags for image generation.
e.g. 1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden ; We support a Gradio Web UI to run Anything V3 with Better VAE:; ; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion. You can also export the model to ONNX, MPS and/or FLAX/JAX.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
,,,,,,,,,,,,,,,,,,,,,,,,,usage 501
"Original Model: https://huggingface.co/andite/anything-v4.0; Simply merged Anything-V4.5 with Anything-V4.0 VAE. 
Both checkpoint and safetensors versions are avaliable to download; All credit goes to the original uploader: andite
; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""anything-v5""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Model link: View model",,,,,,,,,,,,,,,,,,,,,,,,,usage 44
"https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila; Aquila-7B??Aquila-33B????????? ???Aquila??????????§¿??, ?????????Apache Licence 2.0??",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"; 
English |
        ????????
; Aquila Language Model is the first open source language model that supports both Chinese and English knowledge, commercial license agreements, and compliance with domestic data regulations.; ? Supports open source commercial licenses. The source code of the Aquila series models is based on the Apache 2.0 agreement, while the model weight is based on the BAAI Aquila Model License Agreement. Users can use it for commercial purposes as long as they meet the licensing restrictions.; ?? Possesses Chinese and English knowledge. The Aquila series model is trained from scratch on a high-quality corpus of Chinese and English languages, with Chinese corpora accounting for about 40%, ensuring that the model accumulates native Chinese world knowledge during the pre-training phase, rather than translated knowledge.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Pretrained BERT-based (arabic-bert-base) Named Entity Recognition model for Arabic.; The pre-trained model can recognize the following entities:; ? ??? ?? ???? ??????? ??????? ?????? ???? ??? ? ?????? ??? ??? ???? ; ??? ????? ??????? ????? ??? ??? ????? ?? ???? ????? ; ? ????? ?????? ???????? ??????? ???? ??????? ??? ????? ?? ??? ????? ?????????,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This is the repository accompanying our paper AraT5: Text-to-Text Transformers for Arabic Language Understanding and Generation. In this is the repository we Introduce AraT5MSA, AraT5Tweet, and AraT5: three powerful Arabic-specific text-to-text Transformer based models;; Below is an example for fine-tuning AraT5-base for News Title Generation on the Aranews dataset ; For more details about the fine-tuning example, please read this notebook  ; In addition, we release the fine-tuned checkpoint of the News Title Generation (NGT) which is described in the paper. The model available at Huggingface (UBC-NLP/AraT5-base-title-generation).; For more details, please visit our own GitHub.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
; Join Discord Server; ;,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This is the fine-tuned Stable Diffusion model trained on images from the TV Show Arcane.
Use the tokens arcane style in your prompts for the effect.; If you enjoy my work, please consider supporting me
; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; You can also export the model to ONNX, MPS and/or FLAX/JAX.; We also support a Gradio Web UI and Colab with Diffusers to run fine-tuned Stable Diffusion models:",,,,,,,,,,,,,,,,,,,,,,,,,usage 396
"All models are not my authors, they are on pixai, civitai, huggingface. This is just an archive in case their models are removed from the sites. And a collection of models so that the models that I use are always at hand.; Triger : loli; If you want an alternate style, try using watercolor (medium); this model is very responsive to that tag. ; ; Flossy is a fusion model capable of creating an innocent, pure and naive girl.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"artstation-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality Artstation images through fine-tuning.; Aspect Ratio Bucketing has been used during finetuning. This model can generate different aspect ratios VERY WELL.; knight, full body study, concept art, atmospheric; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
The CreativeML OpenRAIL License specifies: ; This model can be used for entertainment purposes and as a generative art assistant.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"?????????????????????????????????????????????????§¶?????????????????????????ï…??AI?????????????????????? ???¦Ë????¦´???????????????????????????????????????????????????????§Û?????????????§Ý??¦Ë???????????????; In principle, this model is prohibited from being used for training style models based on portraits of celebrities and public figures, because it will cause controversy and have a negative impact on the development of the AI community. If you must violate the above statement to train the relevant model and release it publicly, please delete all descriptions related to this model in your release notes. Thank you for your support and understanding.; ????????basil mix,dreamlike,ProtoGen??????????????????????
????????????????????????§Û??????????????????a???????????????
?????????????????????§Û???????????????????tags??????????
This model based on basil mix,dreamlike,ProtoGen,etc. After finetune and merging, it solved the big problem that the other model can only draw ugly stereotyped woman faces from hundreds years ago When drawing Asian and Chinese elements.
This model can also improve the drawing content of Asian and Chinese elements to get closer to tags.; Based on dreamlike finetune example??


; Based on Image to Image example??",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This repository provides all the necessary tools to perform automatic speech
recognition from an end-to-end system pretrained on AISHELL +wav2vec2 (Mandarin Chinese)
within SpeechBrain. For a better experience, we encourage you to learn more about
SpeechBrain.; The performance of the model is the following:; This ASR system is composed of 2 different but linked blocks:; To Train this system from scratch, see our SpeechBrain recipe.; The system is trained with recordings sampled at 16kHz (single channel).
The code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling transcribe_file if needed.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"AudioLDM is a latent text-to-audio diffusion model capable of generating realistic audio samples given any text input. It is available in the ? Diffusers library from v0.15.0 onwards.; AudioLDM was proposed in the paper AudioLDM: Text-to-Audio Generation with Latent Diffusion Models by Haohe Liu et al.; Inspired by Stable Diffusion, AudioLDM
is a text-to-audio latent diffusion model (LDM) that learns continuous audio representations from CLAP
latents. AudioLDM takes a text prompt as input and predicts the corresponding audio. It can generate text-conditional
sound effects, human speech and music.; This is the medium version of the AudioLDM model, which has a larger UNet, CLAP audio projection dim, and is trained with audio embeddings as condition. The four AudioLDM checkpoints are summarised below:; Table 1: Summary of the AudioLDM checkpoints.",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
You can use cURL to access this model:; Or Python API:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Backups of models I found and I like; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; The images above were generated with only ""solo"" in the positive prompt, and ""sketch by bad-artist"" (this embedding) in the negative.

The embedding uses only 2 tokens.; Textual-inversion embedding for use in unconditional (negative) prompt.

Inspired partly by https://huggingface.co/datasets/Nerfgun3/bad_prompt.; There are currently 2 version:; I recommend using with 'by', so for example ""sketch by bad-artist"", or ""painting by bad-artist"", or ""photograph by bad-artist"", etc.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
Model info :; https://civitai.com/models/107703?modelVersionId=115852; Sample image I made thru Huggingface's API :; ; Original Author's DEMO image :,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"Baichuan-13B-Base?Baichuan-13B???????§Ö??????·Ú?????????????????Baichuan-13B-Chat??; Baichuan-13B ??????????? Baichuan-7B ????????? 130 ?????????????????????????????????????????? benchmark ??????????????§¹???????¦Ç????????????? (Baichuan-13B-Base) ????? (Baichuan-13B-Chat) ?????·Ú??Baichuan-13B ????????????; Baichuan-13B is an open-source, commercially usable large-scale language model developed by Baichuan Intelligence, following Baichuan-7B. With 13 billion parameters, it achieves the best performance in standard Chinese and English benchmarks among models of its size. This release includes two versions: pre-training (Baichuan-13B-Base) and alignment (Baichuan-13B-Chat). Baichuan-13B has the following features:; Developed by: ???????(Baichuan Intelligent Technology); Email: opensource@baichuan-inc.com",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Baichuan-13B-Chat?Baichuan-13B???????§Ø?????·Ú???????????Baichuan-13B-Base??; Baichuan-13B ??????????? Baichuan-7B ????????? 130 ?????????????????????????????????????????? benchmark ??????????????§¹???????¦Ç????????????? (Baichuan-13B-Base) ????? (Baichuan-13B-Chat) ?????·Ú??Baichuan-13B ????????????; Baichuan-13B-Chat is the aligned version in the Baichuan-13B series of models, and the pre-trained model can be found at Baichuan-13B-Base.; Baichuan-13B is an open-source, commercially usable large-scale language model developed by Baichuan Intelligence, following Baichuan-7B. With 13 billion parameters, it achieves the best performance in standard Chinese and English benchmarks among models of its size. This release includes two versions: pre-training (Baichuan-13B-Base) and alignment (Baichuan-13B-Chat). Baichuan-13B has the following features:; ????????????Baichuan-13B-Chat???§Ø????????????????""?????‰^????????‰d???????????????????????k2?ÈÉ???¦È????8611???¦Ë???????????????§Ñ?????""",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Baichuan-7B??????????????????????????????????????Transformer????????1.2????tokens???????70???????????????????????????????4096???????????????????benchmark??C-EVAL/MMLU????????????????§¹????; ?????????Baichuan-7B?????????????Finetune???????????????????????Baichuan-7B??; Baichuan-7B is an open-source large-scale pre-trained model developed by Baichuan Intelligent Technology. Based on the Transformer architecture, it is a model with 7 billion parameters trained on approximately 1.2 trillion tokens. It supports both Chinese and English, with a context window length of 4096. It achieves the best performance of its size on standard Chinese and English authoritative benchmarks (C-EVAL/MMLU).; If you wish to use Baichuan-7B (for inference, finetuning, etc.), we recommend using the accompanying code library Baichuan-7B.; ???????????Baichuan-7B??????SOTA???????¦Ï?????MMLU???",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
A bilingual instruction-tuned LoRA model of https://huggingface.co/baichuan-inc/baichuan-7B; Please follow the baichuan-7B License to use this model.; Usage:; You could also alternatively launch a CLI demo by using the script in https://github.com/hiyouga/LLaMA-Efficient-Tuning; You could reproduce our results with the following scripts using LLaMA-Efficient-Tuning:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Fire Balloon's Baichuan Vicuna 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
baichuan-vicuna-chinese-7b??????????sharegpt??????????????????????; baichuan-vicuna-chinese-7b is a chat model supervised finetuned on vicuna sharegpt data in both English and Chinese.; [NEW] 4bit-128g GPTQ?????·Ú??baichuan-vicuna-chinese-7b-gptq; Inference with FastChat:; Inference with Transformers:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
baichuan-vicuna-chinese-7b quantized with AutoGPTQ.; ???AutoGPTQ??????baichuan-vicuna-chinese-7b?????7G???????????????; Inference API has been turned off for this model.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Conformer-CTC model trained on the OOD-Speech dataset to transcribe speech from Bangla audio. This is a large variant of the model, with ~121M parameters. To know more about the model architecture see the NeMo Documentation here.;  The training split contains 1100+ hours of audio data crowdsoruced from native Bangla speakers. We trained on this split for 164 epochs , then the model was evaluated on23+ hours of audio across 17 diverse domains .; The model can be used as a pretrained checkpoint for inference or for fine-tuning on another dataset through the NVIDIA NeMo toolkit. It is recommended to install the toolkit, after installing the pyTorch package. ; After installing the required dependencies, download the .nemo file or the pretrained model to your local directory. you can instantiate the pretrained model like following: ; Prior to feeding the input audio to the pretrained model for training or inference, we need to resample the audio to 16KHz. We can achieve that using the sox library :",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Bark is a transformer-based text-to-audio model created by Suno. 
Bark can generate highly realistic, multilingual speech as well as other audio - including music, 
background noise and simple sound effects. The model can also produce nonverbal 
communications like laughing, sighing and crying. To support the research community, 
we are providing access to pretrained model checkpoints ready for inference.; The original github repo and model card can be found here.; This model is meant for research purposes only. 
The model output is not censored and the authors do not endorse the opinions in the generated content. 
Use at your own risk.; Two checkpoints are released:; Try out Bark yourself!",,,,,,,,,,,,,,,,,,,,,,,,,usage 61
"Bark is a transformer-based text-to-audio model created by Suno. 
Bark can generate highly realistic, multilingual speech as well as other audio - including music, 
background noise and simple sound effects. The model can also produce nonverbal 
communications like laughing, sighing and crying. To support the research community, 
we are providing access to pretrained model checkpoints ready for inference.; The original github repo and model card can be found here.; This model is meant for research purposes only. 
The model output is not censored and the authors do not endorse the opinions in the generated content. 
Use at your own risk.; Two checkpoints are released:; Try out Bark yourself!",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Bark-voice-cloning is a model which processes the outputs from a HuBERT model, and turns them into semantic tokens compatible with bark text to speech.; This can be used for many things, including speech transfer and voice cloning.; code repo
audio webui; (Please use the model manager from the code repo for easy downloading of models); Voice cloning is creating a new voice for text-to-speech.",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"12/30/2022; An updated version of CPT & Chinese BART are released. In the new version, we changed the following parts:; We initialize the new version of models with the old version of checkpoints with vocabulary alignment. Token embeddings found in the old checkpoints are copied. And other newly added parameters are randomly initialized. We further train the new CPT & Chinese BART 50K steps with batch size 2048, max-seq-length 1024, peak learning rate 2e-5, and warmup ratio 0.1.; The result compared to the previous checkpoints is as followings:; The result shows that the updated models maintain comparative performance compared with previous checkpoints. There are still some cases that the updated model is slightly worse than the previous one, which results from the following reasons: 1) Training additional a few steps did not lead to significant performance improvement; 2) some downstream tasks are not affected by the newly added tokens and longer encoding sequences, but sensitive to the fine-tuning hyperparameters.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Model Overview; This is the model presented in the paper ""ParaDetox: Detoxification with Parallel Data"". ; The model itself is BART (base) model trained on parallel detoxification dataset ParaDetox achiving SOTA results for detoxification task. More details, code and data can be found here.; How to use; Citation",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"BART model pre-trained on English language. It was introduced in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Lewis et al. and first released in this repository. ; Disclaimer: The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.; BART is a transformer encoder-decoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.; BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering).; You can use the raw model for text infilling. However, the model is mostly meant to be fine-tuned on a supervised dataset. See the model hub to look for fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"12/30/2022; An updated version of CPT & Chinese BART are released. In the new version, we changed the following parts:; We initialize the new version of models with the old version of checkpoints with vocabulary alignment. Token embeddings found in the old checkpoints are copied. And other newly added parameters are randomly initialized. We further train the new CPT & Chinese BART 50K steps with batch size 2048, max-seq-length 1024, peak learning rate 2e-5, and warmup ratio 0.1.; The result compared to the previous checkpoints is as followings:; The result shows that the updated models maintain comparative performance compared with previous checkpoints. There are still some cases that the updated model is slightly worse than the previous one, which results from the following reasons: 1) Training additional a few steps did not lead to significant performance improvement; 2) some downstream tasks are not affected by the newly added tokens and longer encoding sequences, but sensitive to the fine-tuning hyperparameters.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"BART model pre-trained on English language, and fine-tuned on CNN Daily Mail. It was introduced in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Lewis et al. and first released in [this repository (https://github.com/pytorch/fairseq/tree/master/examples/bart). ; Disclaimer: The team releasing BART did not write a model card for this model so this model card has been written by the Hugging Face team.; BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.; BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering). This particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs.; You can use this model for text summarization.",,,,,,,,,,,,,,,,,,,,,,,,,usage 280
"This is the checkpoint for bart-large after being trained on the MultiNLI (MNLI) dataset.; Additional information about this model:; Yin et al. proposed a method for using pre-trained NLI models as a ready-made zero-shot sequence classifiers. The method works by posing the sequence to be classified as the NLI premise and to construct a hypothesis from each candidate label. For example, if we want to evaluate whether a sequence belongs to the class ""politics"", we could construct a hypothesis of This text is about politics.. The probabilities for entailment and contradiction are then converted to label probabilities.; This method is surprisingly effective in many cases, particularly when used with larger pre-trained models like BART and Roberta. See this blog post for a more expansive introduction to this and other zero shot methods, and see the code snippets below for examples of using this model for zero-shot classification both with Hugging Face's built-in pipeline and with native Transformers/PyTorch code.; The model can be loaded with the zero-shot-classification pipeline like so:",,,,,,,,,,,,,,,,,,,,,,,,,usage 124
"; This repository contains the Bart-Large-paper2slides-summarizer Model, which has been fine-tuned on the Automatic Slide Generation from Scientific Papers dataset using unsupervised learning techniques using an algorithm from the paper entitled 'Unsupervised Machine Translation Using Monolingual Corpora Only'.
Its primary focus is to summarize scientific texts with precision and accuracy, the model is parallelly trained with the Bart-large-paper2slides-expander from the same contributor.; Bart (Bidirectional and Auto-Regressive Transformers) is a sequence-to-sequence (seq2seq) model developed by Facebook AI Research. It has shown exceptional performance in various natural language processing (NLP) tasks such as text summarization, text generation, and machine translation.; This particular model, Bart-Large, is the larger version of the Bart model. It consists of 12 encoder and decoder layers and has a total of 400 million parameters.; To use this model, you can leverage the Hugging Face Transformers library. Here's an example of how to use it in Python:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This model was obtained by fine-tuning facebook/bart-large-xsum on Samsum dataset.,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This is a finetuned BART Large model from the paper:; ""Generating Scientific Definitions with Controllable Complexity"" ; By Tal August, Katharina Reinecke, and Noah A. Smith; Abstract: Unfamiliar terminology and complex language can present barriers to understanding science. Natural language processing stands to help address these issues by automatically defining unfamiliar terms. We introduce a new task and dataset for defining scientific terms and controlling the complexity of gen- erated definitions as a way of adapting to a specific reader??s background knowledge. We test four definition generation methods for this new task, finding that a sequence-to-sequence approach is most successful. We then explore the version of the task in which definitions are generated at a target complexity level. We in- troduce a novel reranking approach and find in human evaluations that it offers superior fluency while also controlling complexity, compared to several controllable generation baselines.; The model is finetuned on the task of generating definitions of scientific terms. We frame our task as generating an answer to the question ??What is (are) X??? Along with the question, the model takes a support document of scientific abstracted related to the term being defined.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model was trained using Amazon SageMaker and the new Hugging Face Deep Learning container.
For more information look at:; }",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"A BART-large version of T0. 
Please check https://inklab.usc.edu/ReCross/ for more details.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model and its derivatives(image, merged model) can be freely used for non-profit purposes only.; You may not use this model and its derivatives on websites, apps, or other platforms where you can or plan to earn income or donations. If you wish to use it for such purposes, please contact nuigurumi.; Introducing the model itself is allowed for both commercial and non-commercial purposes, but please include the model name and a link to this repository when doing so.; ???¦´??????????????(????????`??????)??????????????????????????????????????????????; ????????????????????¦³?????????????Ú…??Web??????????????????????¦´??????????????????????????????????????????????nuigurumi???B?j?????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; 

;   ; ?????????https://arxiv.org/abs/2302.09432",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"run the model at: https://sinkin.ai/m/vlDnKP6; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"BEiT model pre-trained in a self-supervised fashion on ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository. ; Disclaimer: The team releasing BEiT did not write a model card for this model so this model card has been written by the Hugging Face team.; The BEiT model is a Vision Transformer (ViT), which is a transformer encoder model (BERT-like). In contrast to the original ViT model, BEiT is pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. The pre-training objective for the model is to predict visual tokens from the encoder of OpenAI's DALL-E's VQ-VAE, based on masked patches.
Next, the model was fine-tuned in a supervised fashion on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. Contrary to the original ViT models, BEiT models do use relative position embeddings (similar to T5) instead of absolute position embeddings, and perform classification of images by mean-pooling the final hidden states of the patches, instead of placing a linear layer on top of the final hidden state of the [CLS] token.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image. Alternatively, one can mean-pool the final hidden states of the patch embeddings, and place a linear layer on top of that.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Considering LLaMA's license constraints, the model is for research and learning only. 
Please strictly respect LLaMA's usage policy. We are not allowed to publish weights for LLaMA, of course, even finetuned, but there is no problem publishing the difference, a patch that we suggest to apply to the files. 
The encryption is a simple XOR between files, ensuring that only the people that have access to the original weights (from completely legal sources, of course) can transform them into finetuned weights. 
You can find the decrypt code on https://github.com/LianjiaTech/BELLE/tree/main/models .; If you find this model helpful, please like this model and star us on https://github.com/LianjiaTech/BELLE !; This model comes from a two-phrase training on original LLaMA 13B.; You can use the following command in Bash.
Please replace ""/path/to_encrypted"" with the path where you stored your encrypted file, 
replace ""/path/to_original_llama_7B"" with the path where you stored your original llama7B file, 
and replace ""/path/to_finetuned_model"" with the path where you want to save your final trained model.; After executing the aforementioned command, you will obtain the following files.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Model Description: KLUE BERT base is a pre-trained BERT Model on Korean Language. The developers of KLUE BERT base developed the model in the context of the development of the Korean Language Understanding Evaluation (KLUE) Benchmark.; The model can be used for tasks including topic classification, semantic textual similarity, natural language inference, named entity recognition, and other tasks outlined in the KLUE Benchmark.; The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)). The model developers discuss several ethical considerations related to the model in the paper, including: ; For ethical considerations related to the KLUE Benchmark, also see the paper.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"CAMeLBERT is a collection of BERT models pre-trained on Arabic texts with different sizes and variants.
We release pre-trained language models for Modern Standard Arabic (MSA), dialectal Arabic (DA), and classical Arabic (CA), in addition to a model pre-trained on a mix of the three.
We also provide additional models that are pre-trained on a scaled-down set of the MSA variant (half, quarter, eighth, and sixteenth).
The details are described in the paper ""The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models.""; This model card describes CAMeLBERT-DA (bert-base-arabic-camelbert-da), a model pre-trained on the DA (dialectal Arabic) dataset.; You can use the released model for either masked language modeling or next sentence prediction.
However, it is mostly intended to be fine-tuned on an NLP task, such as NER, POS tagging, sentiment analysis, dialect identification, and poetry classification.
We release our fine-tuninig code here.; You can use this model directly with a pipeline for masked language modeling:; Note: to download our models, you would need transformers>=3.5.0. Otherwise, you could download the models manually.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"CAMeLBERT-DA SA Model is a Sentiment Analysis (SA) model that was built by fine-tuning the CAMeLBERT Dialectal Arabic (DA) model.
For the fine-tuning, we used the ASTD, ArSAS, and SemEval datasets.
Our fine-tuning procedure and the hyperparameters we used can be found in our paper *""The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models.""; You can use the CAMeLBERT-DA SA model directly as part of our CAMeL Tools SA component (recommended) or as part of the transformers pipeline.; To use the model with the CAMeL Tools SA component:; You can also use the SA model directly with a transformers pipeline:; Note: to download our models, you would need transformers>=3.5.0.
Otherwise, you could download the models manually.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"CAMeLBERT-Mix NER Model is a Named Entity Recognition (NER) model that was built by fine-tuning the CAMeLBERT Mix model.
For the fine-tuning, we used the ANERcorp dataset.
Our fine-tuning procedure and the hyperparameters we used can be found in our paper ""The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models.
"" Our fine-tuning code can be found here.; You can use the CAMeLBERT-Mix NER model directly as part of our CAMeL Tools NER component (recommended) or as part of the transformers pipeline.; To use the model with the CAMeL Tools NER component:; You can also use the NER model directly with a transformers pipeline:; Note: to download our models, you would need transformers>=3.5.0.
Otherwise, you could download the models manually.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is case-sensitive: it makes a difference between
english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the model hub to look for
fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 192
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model has been pre-trained for Chinese, training and random input masking has been applied independently to word pieces (as in the original BERT paper).; This model can be used for masked language modeling ; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).; [More Information Needed]",,,,,,,,,,,,,,,,,,,,,,,,,usage 50
,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This project provides traditional Chinese transformers models (including ALBERT, BERT, GPT2) and NLP tools (including word segmentation, part-of-speech tagging, named entity recognition).; ?@????????????w????? transformers ???????? ALBERT??BERT??GPT2????????Z?????????????????~???~?????????w???R????; Please use BertTokenizerFast as tokenizer instead of AutoTokenizer.; ???? BertTokenizerFast ???? AutoTokenizer??; For full usage and more information, please refer to https://github.com/ckiplab/ckip-transformers.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Wietse de Vries ?
Andreas van Cranenburgh ?
Arianna Bisazza ?
Tommaso Caselli ?
Gertjan van Noord ?
Malvina Nissim; BERTje is a Dutch pre-trained BERT model developed at the University of Groningen.; For details, check out our paper on arXiv, the code on Github and related work on Semantic Scholar.; The paper and Github page mention fine-tuned models that are available here.; WARNING: The vocabulary size of BERTje has changed in 2021. If you use an older fine-tuned model and experience problems with the GroNLP/bert-base-dutch-cased tokenizer, use use the following tokenizer:",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
Notebook,,,,,,,,,,,,,,,,,,,,,,,,,usage 12
"Pretrained model on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective.
It was introduced in this paper and first released in
this repository. This model is case sensitive: it makes a difference
between english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion. This means
it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the languages in the training set that can then be used to
extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a
standard classifier using the features produced by the BERT model as inputs.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the model hub to look for
fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 58
"Pretrained model on the top 102 languages with the largest Wikipedia using a masked language modeling (MLM) objective.
It was introduced in this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion. This means
it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the languages in the training set that can then be used to
extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a
standard classifier using the features produced by the BERT model as inputs.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the model hub to look for
fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 34
"This a bert-base-multilingual-uncased model finetuned for sentiment analysis on product reviews in six languages: English, Dutch, German, French, Spanish and Italian. It predicts the sentiment of the review as a number of stars (between 1 and 5).; This model is intended for direct use as a sentiment analysis model for product reviews in any of the six languages above, or for further finetuning on related sentiment analysis tasks.; Here is the number of product reviews we used for finetuning the model: ; The finetuned model obtained the following accuracy on 5,000 held-out product reviews in each of the languages:; In addition to this model, NLP Town offers custom, monolingual sentiment models for many languages and an improved multilingual model through RapidAPI.",,,,,,,,,,,,,,,,,,,,,,,,,usage 37
"bert-base-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). ; Specifically, this model is a bert-base-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset. ; If you'd like to use a larger BERT-large model fine-tuned on the same dataset, a bert-large-NER version is also available. ; You can use this model with Transformers pipeline for NER.; This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases.",,,,,,,,,,,,,,,,,,,,,,,,,usage 28
"Polish version of BERT language model is here! It is now available in two variants: cased and uncased, both can be downloaded and used via HuggingFace transformers library. I recommend using the cased model, more info on the differences and benchmark results below. ; ; Below is the list of corpora used along with the output of wc command (counting lines, words and characters). These corpora were divided into sentences with srxsegmenter (see references), concatenated and tokenized with HuggingFace BERT Tokenizer. ; Polbert is released via HuggingFace Transformers library.; For an example use as language model, see this notebook file.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.; For further information or requests, please go to BERTimbau repository.; If you use our work, please cite:",,,,,,,,,,,,,,,,,,,,,,,,,usage 9
"QCRI Arabic and Dialectal BERT  (QARiB) model, was trained on a collection of ~ 420 Million tweets and ~ 180 Million sentences of text.
For the tweets, the data was collected using twitter API and using language filter. lang:ar. For the text data, it was a combination from
Arabic GigaWord, Abulkhair Arabic Corpus and OPUS.; QARiB: Is the Arabic name for ""Boat"".; See details in Training QARiB; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you. For more details, see Using QARiB; You can use this model directly with a pipeline for masked language modeling:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is provided by BETO team and fine-tuned on SQuAD-es-v2.0 for Q&A downstream task.; Language model ('dccuchile/bert-base-spanish-wwm-cased'):; BETO is a BERT model trained on a big Spanish corpus. BETO is of size similar to a BERT-Base and was trained with the Whole Word Masking technique. Below you find Tensorflow and Pytorch checkpoints for the uncased and cased versions, as well as some results for Spanish benchmarks comparing BETO with Multilingual BERT as well as other (not BERT-based) models.; SQuAD-es-v2.0; The model was trained on a Tesla P100 GPU and 25GB of RAM with the following command:",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"In this repository the MDZ Digital Library team (dbmdz) at the Bavarian State
Library open sources a cased model for Turkish ?; BERTurk is a community-driven cased BERT model for Turkish.; Some datasets used for pretraining and evaluation are contributed from the
awesome Turkish NLP community, as well as the decision for the model name: BERTurk.; The current version of the model is trained on a filtered and sentence
segmented version of the Turkish OSCAR corpus,
a recent Wikipedia dump, various OPUS corpora and a
special corpus provided by Kemal Oflazer.; The final training corpus has a size of 35GB and 44,04,976,662 tokens.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. The model was trained on Turkish machine translated versions of NLI and STS-b datasets, using example training scripts from sentence-transformers GitHub repository.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; Evaluation results on test and development sets are given below:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labeling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences, for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; BERT has originally been released in base and large variations, for cased and uncased input text. The uncased models also strips out an accent markers.Chinese and multilingual uncased and cased versions followed shortly after.Modified preprocessing with whole word masking has replaced subpiece masking in a following work, with the release of two models.Other 24 smaller models are released afterward.",,,,,,,,,,,,,,,,,,,,,,,,,usage 558
A model trained on German Hotel Reviews from Switzerland. The base model is the bert-base-german-cased. The last hidden layer of the base model was extracted and a classification layer was added. The entire model was then trained for 5 epochs on our dataset.;,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"BERT for Patents is a model trained by Google on 100M+ patents (not just US patents). It is based on BERTLARGE.; If you want to learn more about the model, check out the blog post, white paper and GitHub page containing the original TensorFlow checkpoint.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
BERT (bert-large-cased) trained for sentiment classification on the IMDB dataset.; The model was trained on 80% of the IMDB dataset for sentiment classification for three epochs with a learning rate of 1e-5 with the simpletransformers library. The library uses a learning rate schedule.; The model achieved 90% classification accuracy on the validation set.; The full experiment is available in the tlr repo.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuned version of bert-base-cased on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is cased: it makes a difference
between english and English.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; This model has the following configuration:",,,,,,,,,,,,,,,,,,,,,,,,,usage 35
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"; The model was trained on the dataset SQUAD v1.1 in portuguese from the Deep Learning Brasil group. ; The language model used is the BERTimbau Large (aka ""bert-large-portuguese-cased"") from Neuralmind.ai: BERTimbau is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.; All the informations are in the blog post : NLP | Como treinar um modelo de Question Answering em qualquer linguagem baseado no BERT large, melhorando o desempenho do modelo utilizando o BERT base? (estudo de caso em portugu??s); question_answering_BERT_large_cased_squad_v11_pt.ipynb (nbviewer version)",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"bert-large-NER is a fine-tuned BERT model that is ready to use for Named Entity Recognition and achieves state-of-the-art performance for the NER task. It has been trained to recognize four types of entities: location (LOC), organizations (ORG), person (PER) and Miscellaneous (MISC). ; Specifically, this model is a bert-large-cased model that was fine-tuned on the English version of the standard CoNLL-2003 Named Entity Recognition dataset. ; If you'd like to use a smaller BERT model fine-tuned on the same dataset, a bert-base-NER version is also available. ; You can use this model with Transformers pipeline for NER.; This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains. Furthermore, the model occassionally tags subword tokens as entities and post-processing of results may be necessary to handle those cases.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"; BERTimbau Large is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large.; For further information or requests, please go to BERTimbau repository.; If you use our work, please cite:",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; Differently to other BERT models, this model was trained with a new technique: Whole Word Masking. In this case, all of the tokens corresponding to a word are masked at once. The overall masking rate remains the same.; The training is identical -- each masked WordPiece token is predicted independently. ; After pre-training, this model was fine-tuned on the SQuAD dataset with one of our fine-tuning scripts. See below for more information regarding this fine-tuning.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by
the Hugging Face team.",,,,,,,,,,,,,,,,,,,,,,,,,usage 48
"We created German Squad 2.0 (deQuAD 2.0) and merged with SQuAD2.0 into an English and German training data for question answering. The bert-base-multilingual-cased is used to fine-tune bilingual QA downstream task.; SQuAD2.0 was auto-translated into German. We hired professional editors to proofread the translated transcripts, correct mistakes and double check the answers to further polish the text and enhance annotation quality. The final German deQuAD dataset contains 130k training and 11k test samples.; Copyright (c) 2021 Fang Xu, Deutsche Telekom AG",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Input: Supports over 100 Languages. See List of supported languages for all available.; Purpose: This module takes a search query [1] and a passage [2] and calculates if the passage matches the query. 
It can be used as an improvement for Elasticsearch Results and boosts the relevancy by up to 100%. ; Architecture: On top of BERT there is a Densly Connected NN which takes the 768 Dimensional [CLS] Token as input and provides the output (Arxiv).; Output: Just a single value between between -10 and 10. Better matching query,passage pairs tend to have a higher a score.; Both query[1] and passage[2] have to fit in 512 Tokens.
As you normally want to rerank the first dozens of search results keep in mind the inference time of approximately 300 ms/query.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Quora Keyword Pairs: https://www.kaggle.com/stefanondisponibile/quora-question-keyword-pairs
Spaadia SQuaD pairs: https://www.kaggle.com/shahrukhkhan/questions-vs-statementsclassificationdataset; Medium article; Colab Notebook Multi-task Query classifiers",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The Usage of tokenizer for Myanmar is same as Laos in https://github.com/GKLMIP/Pretrained-Models-For-Laos.; If you use our model, please consider citing our paper:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; This a bert-base-uncased model finetuned for punctuation restoration on Yelp Reviews. ; The model predicts the punctuation and upper-casing of plain, lower-cased text. An example use case can be ASR output. Or other cases when text has lost punctuation.; This model is intended for direct use as a punctuation restoration model for the general English language. Alternatively, you can use this for further fine-tuning on domain-specific texts for punctuation restoration tasks.; Model restores the following punctuations -- [! ? . , - : ; ' ]",,,,,,,,,,,,,,,,,,,,,,,,,usage 10
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; It is used as a demo model within the NLP for Semantic Search course, for the chapter on In-domain Data Augmentation with BERT.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a sentence-transformers cross encoder model.; It is used as a demo model within the NLP for Semantic Search course, for the chapter on In-domain Data Augmentation with BERT.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The following model is a Pytorch pre-trained model obtained from converting Tensorflow checkpoint found in the official Google BERT repository. ; This is one of the smaller pre-trained BERT variants, together with bert-mini bert-small and bert-medium. They were introduced in the study Well-Read Students Learn Better: On the Importance of Pre-training Compact Models (arxiv), and ported to HF for the study Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics (arXiv). These models are supposed to be trained on a downstream task.; If you use the model, please consider citing both the papers:; Config of this model:; Other models to check out:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the Basque language pretrained model presented in Give your Text Representation Models some Love: the Case for Basque. This model has been trained on a Basque corpus comprising Basque crawled news articles from online newspapers and the Basque Wikipedia. The training corpus contains 224.6 million tokens, of which 35 million come from the Wikipedia.; BERTeus has been tested on four different downstream tasks for Basque: part-of-speech (POS) tagging, named entity recognition (NER), sentiment analysis and topic classification; improving the state of the art for all tasks. See summary of results below:; If using this model, please cite the following paper:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"An English sequence classification model, trained on MBAD Dataset to detect bias and fairness in sentences (news articles). This model was built on top of distilbert-base-uncased model and trained for 30 epochs with a batch size of 16, a learning rate of 5e-5, and a maximum sequence length of 512.; The easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.; This model is part of the Research topic ""Bias and Fairness in AI"" conducted by Deepak John Reji, Shaina Raza. If you use this work (code, model or dataset), please star at:; Bias & Fairness in AI, (2022), GitHub repository, https://github.com/dreji18/Fairness-in-AI",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Large language models (LLMs) demonstrate promising translation performance among various natural languages. However, many LLMs especially the open-sourced ones, such as BLOOM and LLaMA, are English-dominant and support only dozens of natural languages, making the potential of LLMs on language translation less explored. In this work, we present BigTranslate which adapts LLaMA that covers only 20 languages and enhances it with multilingual translation capability on more than 100 languages. BigTranslate is built upon LLaMA-13B and it is optimized in three steps. First, we continue training LLaMA with massive Chinese monolingual data. Second, we continue training the model with a large-scale parallel dataset that covers 102 natural languages. Third, we instruct-tune the foundation model with multilingual translation instructions, leading to our BigTranslate model. The preliminary experiments on multilingual translation show that BigTranslate performs comparably with 
ChatGPT and Google Translate in many languages and even outperforms ChatGPT in 8 language pairs. We release the BigTranslate model and hope it can advance the research progress.; More Details can be found at https://github.com/ZNLP/BigTranslate and https://arxiv.org/abs/2305.18098",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuned version of dmis-lab/biobert-base-cased-v1.1 on the PubMed200kRCT dataset.
It achieves the following results on the evaluation set:; More information needed; The model can be used for text classification tasks of Randomized Controlled Trials that does not have any structure. The text can be classified as one of the following:; The model can be directly used like this:; Results will be shown as follows:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The BioBERTpt - A Portuguese Neural Language Model for Clinical Named Entity Recognition paper contains clinical and biomedical BERT-based models for Portuguese Language, initialized with BERT-Multilingual-Cased & trained on clinical notes and biomedical literature. ; This model card describes the BioBERTpt(all) model, a full version with clinical narratives and biomedical literature in Portuguese language. ; Load the model via the transformers library:; Refer to the original paper, BioBERTpt - A Portuguese Neural Language Model for Clinical Named Entity Recognition for additional details and performance on Portuguese NER tasks.; This study was financed in part by the Coordena??o de Aperfei?oamento de Pessoal de N??vel Superior - Brasil (CAPES) - Finance Code 001.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.; You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:; Here is how to use this model to get the features of a given text in PyTorch:; Beam-search decoding:; If you find BioGPT useful in your research, please cite the following paper:",,,,,,,,,,,,,,,,,,,,,,,,,usage 31
"Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.; If you find BioGPT useful in your research, please cite the following paper:",,,,,,,,,,,,,,,,,,,,,,,,,usage 22
"Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.; If you find BioGPT useful in your research, please cite the following paper:",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"BiomedCLIP is a biomedical vision-language foundation model that is pretrained on PMC-15M, a dataset of 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, using contrastive learning. 
It uses PubMedBERT as the text encoder and Vision Transformer as the image encoder, with domain-specific adaptations.
It can perform various vision-language processing (VLP) tasks such as cross-modal retrieval, image classification, and visual question answering. 
BiomedCLIP establishes new state of the art in a wide range of standard datasets, and substantially outperforms prior VLP approaches:; ; Please refer to this example notebook.; This model is intended to be used solely for (I) future research on visual-language processing and (II) reproducibility of the experimental results reported in the reference paper.; The primary intended use is to support AI researchers building on top of this work. BiomedCLIP and its associated models should be helpful for exploring various biomedical VLP research questions, especially in the radiology domain.",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"An English Named Entity Recognition model, trained on Maccrobat to recognize the bio-medical entities (107 entities) from a given text corpus (case reports etc.). This model was built on top of distilbert-base-uncased; Checkout the tutorial video for explanation of this model and corresponding python library: https://youtu.be/xpiDPdBpS18; The easiest way is to load the inference api from huggingface and second method is through the pipeline object offered by transformers library.; This model is part of the Research topic ""AI in Biomedical field"" conducted by Deepak John Reji, Shaina Raza. If you use this work (code, model or dataset), please star at:; https://github.com/dreji18/Bio-Epidemiology-NER",,,,,,,,,,,,,,,,,,,,,,,,,usage 87
"Note: This model was previously known as PubMedGPT 2.7B, but we have changed it due to a request from the NIH which holds the trademark for ""PubMed"".; BioMedLM 2.7B is new language model trained exclusively on biomedical abstracts and papers from The Pile. This GPT-style model can achieve strong results on a variety of biomedical NLP tasks, including a new state of the art performance of 50.3% accuracy on the MedQA biomedical question answering task.; As an autoregressive language model, BioMedLM 2.7B is also capable of natural language generation. However, we have only begun to explore the generation capabilities and limitations of this model, and we emphasize that this model??s generation capabilities are for research purposes only and not suitable for production. In releasing this model, we hope to advance both the development of biomedical NLP applications and best practices for responsibly training and utilizing domain-specific language models; issues of reliability, truthfulness, and explainability are top of mind for us.; This model was a joint collaboration of Stanford CRFM and MosaicML.; BioMedLM 2.7B is new language model trained exclusively on biomedical abstracts and papers from The Pile. This GPT-style model can achieve strong results on a variety of biomedical NLP tasks, including a new state of the art performance of 50.3% accuracy on the MedQA biomedical question answering task.",,,,,,,,,,,,,,,,,,,,,,,,,usage 13
"https://arxiv.org/pdf/2112.07887.pdf; Entity linking faces significant challenges such as prolific variations and prevalent ambiguities, especially in high-value domains with myriad entities. Standard classification approaches suffer from the annotation bottleneck and cannot effectively handle unseen entities. Zero-shot entity linking has emerged as a promising direction for generalizing to new entities, but it still requires example gold entity mentions during training and canonical descriptions for all entities, both of which are rarely available outside of Wikipedia (Logeswaran et al., 2019; Wu et al., 2020). We explore Knowledge-RIch Self-Supervision (KRISS) and train a contextual encoder (KRISSBERT) for entity linking, by leveraging readily available unlabeled text and domain knowledge.; Specifically, the KRISSBERT model is initialized with PubMedBERT parameters, and then continuously pretrained using biomedical entity names from the UMLS ontology to self-supervise entity linking examples from PubMed abstracts. Experiments on seven standard biomedical entity linking datasets show that KRISSBERT attains new state of the art, outperforming prior self-supervised methods by as much as 20 absolute points in accuracy.
See Zhang et al., 2021 for the details.; Note that some prior systems like BioSyn, SapBERT, and their follow-up work (e.g., Lai et al., 2021) claimed to do entity linking, but their systems completely ignore the context of an entity mention, and can only predict a surface form in the entity dictionary (See Figure 1 in BioSyn), not the canonical entity ID (e.g., CUI in UMLS). Therefore, they can't disambiguate ambiguous mentions. For instance, given the entity mention ""ER"" in the sentence ""ER crowding has become a wide-spread problem"", their systems ignore the sentence context, and simply predict the closest surface form, which is just ""ER"". Multiple entities share this surface form as a potential name or alias, such as Emergency Room (C0562508), Estrogen Receptor Gene (C1414461), and Endoplasmic Reticulum(C0014239). Without using the context information, their systems can't resolve such ambiguity and pinpoint the correct entity Emergency Room (C0562508). More problematically, their evaluation would deem such an ambiguous prediction as correct. Consequently, the reported results in their papers do not reflect true performance on entity linking.; Here, we use the MedMentions data to show you how to 1) generate prototype embeddings, and 2) run entity linking.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. Recent work shows that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models.; PubMedBERT is pretrained from scratch using abstracts from PubMed and full-text articles from PubMedCentral. This model achieves state-of-the-art performance on many biomedical NLP tasks, and currently holds the top score on the Biomedical Language Understanding and Reasoning Benchmark.; If you find PubMedBERT useful in your research, please cite the following paper:",,,,,,,,,,,,,,,,,,,,,,,,,usage 9
"Block-removed Knowledge-distilled Stable Diffusion Model (BK-SDM) is an architecturally compressed SDM for efficient general-purpose text-to-image synthesis. This model is bulit with (i) removing several residual and attention blocks from the U-Net of Stable Diffusion v1.4 and (ii) distillation pretraining on only 0.22M LAION pairs (fewer than 0.1% of the full training set). Despite being trained with very limited resources, our compact model can imitate the original SDM by benefiting from transferred knowledge.; An inference code with the default PNDM scheduler and 50 denoising steps is as follows.; The following code is also runnable, because we compressed the U-Net of Stable Diffusion v1.4 while keeping the other parts (i.e., Text Encoder and Image Decoder) unchanged:; The above examples have been tested on a single NVIDIA GeForce RTX 3090 GPU with the following versions:; We removed several residual and attention blocks from the 0.86B-parameter U-Net in the 1.04B-param SDM-v1.4, and our compressed models are summarized as follows.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Block-removed Knowledge-distilled Stable Diffusion Model (BK-SDM) is an architecturally compressed SDM for efficient general-purpose text-to-image synthesis. This model is bulit with (i) removing several residual and attention blocks from the U-Net of Stable Diffusion v1.4 and (ii) distillation pretraining on only 0.22M LAION pairs (fewer than 0.1% of the full training set). Despite being trained with very limited resources, our compact model can imitate the original SDM by benefiting from transferred knowledge.; An inference code with the default PNDM scheduler and 50 denoising steps is as follows.; The following code is also runnable, because we compressed the U-Net of Stable Diffusion v1.4 while keeping the other parts (i.e., Text Encoder and Image Decoder) unchanged:; The above examples have been tested on a single NVIDIA GeForce RTX 3090 GPU with the following versions:; We removed several residual and attention blocks from the 0.86B-parameter U-Net in the 1.04B-param SDM-v1.4, and our compressed models are summarized as follows.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Block-removed Knowledge-distilled Stable Diffusion Model (BK-SDM) is an architecturally compressed SDM for efficient general-purpose text-to-image synthesis. This model is bulit with (i) removing several residual and attention blocks from the U-Net of Stable Diffusion v1.4 and (ii) distillation pretraining on only 0.22M LAION pairs (fewer than 0.1% of the full training set). Despite being trained with very limited resources, our compact model can imitate the original SDM by benefiting from transferred knowledge.; An inference code with the default PNDM scheduler and 50 denoising steps is as follows.; The following code is also runnable, because we compressed the U-Net of Stable Diffusion v1.4 while keeping the other parts (i.e., Text Encoder and Image Decoder) unchanged:; The above examples have been tested on a single NVIDIA GeForce RTX 3090 GPU with the following versions:; We removed several residual and attention blocks from the 0.86B-parameter U-Net in the 1.04B-param SDM-v1.4, and our compressed models are summarized as follows.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that scaling neural models in the number of parameters and the size of the data they are trained on gives improved results, we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to their partners, both asking and answering questions, and displaying knowledge, empathy and personality appropriately, depending on the situation. We show that large scale models can learn these skills when given appropriate training data and choice of generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter neural models, and make our models and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing failure cases of our models.",,,,,,,,,,,,,,,,,,,,,,,,,usage 206
"Model card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone).; Authors from the paper write in the abstract:; Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.; You can use this model for conditional and un-conditional image captioning",,,,,,,,,,,,,,,,,,,,,,,,,usage 260
"Model card for image captioning pretrained on COCO dataset - base architecture (with ViT large backbone).; Authors from the paper write in the abstract:; Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.; You can use this model for conditional and un-conditional image captioning",,,,,,,,,,,,,,,,,,,,,,,,,usage 224
"BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters).
It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository.; Disclaimer: The team releasing BLIP-2 did not write a model card for this model so this model card has been written by the Hugging Face team.; BLIP-2 consists of 3 models: a CLIP-like image encoder, a Querying Transformer (Q-Former) and a large language model.; The authors initialize the weights of the image encoder and large language model from pre-trained checkpoints and keep them frozen
while training the Querying Transformer, which is a BERT-like Transformer encoder that maps a set of ""query tokens"" to query embeddings,
which bridge the gap between the embedding space of the image encoder and the large language model.; The goal for the model is simply to predict the next text token, giving the query embeddings and the previous text.",,,,,,,,,,,,,,,,,,,,,,,,,usage 47
"BigScience Large Open-science Open-access Multilingual Language ModelVersion 1.3 / 6 July 2022; Current Checkpoint: Training Iteration  95000; Link to paper: here; Total seen tokens: 366B; BLOOM is an autoregressive Large Language Model (LLM), trained to continue text from a prompt on vast amounts of text data using industrial-scale computational resources. As such, it is able to output coherent text in 46 languages and 13 programming languages that is hardly distinguishable from text written by humans. BLOOM can also be instructed to perform text tasks it hasn't been explicitly trained for, by casting them as text generation tasks.",,,,,,,,,,,,,,,,,,,,,,,,,usage 322
Version 1.0 / 26.May.2022; This section provides information for anyone who wants to know about the model.; Developed by: BigScience (website); Model Type: Transformer-based Language Model; Version: 1.0.0,,,,,,,,,,,,,,,,,,,,,,,,,usage 45
Version 1.0 / 26.May.2022; This section provides information for anyone who wants to know about the model.; Developed by: BigScience (website); Model Type: Transformer-based Language Model; Version: 1.0.0,,,,,,,,,,,,,,,,,,,,,,,,,usage 11
"BLOOMChat is a 176 billion parameter multilingual chat model. It is instruction tuned from BLOOM (176B) on assistant-style conversation datasets and supports conversation, question answering and generative answers in multiple languages.; To increase accessibility and to support the open-source community, SambaNova is releasing BLOOMChat under a modified version of the Apache 2.0 license, which includes use-based restrictions from BLOOM??s RAIL license. While use-based restrictions are necessarily passed through, there are no blanket restrictions on reuse, distribution, commercialization or adaptation. Please review SambaNova??s BLOOMChat-176B License; This model is intended for commercial and research use.; BLOOMChat should NOT be used for:; This model is still in early development and can be prone to mistakes and hallucinations, there is still room for improvement. This model is intended to provide the community with a multilingual chat LLM baseline.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t??aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",,,,,,,,,,,,,,,,,,,,,,,,,usage 68
"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t??aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t??aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t??aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",,,,,,,,,,,,,,,,,,,,,,,,,usage 11
"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find the resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t??aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"A BERT model pre-trained on PubMed abstracts; Please see https://github.com/ncbi-nlp/bluebert; We provide preprocessed PubMed texts that were used to pre-train the BlueBERT models. 
The corpus contains ~4000M words extracted from the PubMed ASCII code version. ; Pre-trained model: https://huggingface.co/bert-base-uncased; Below is a code snippet for more details.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This embedding attempts to capture what it means for an image to be uninteresting.  It was trained as a negative embedding using e621 style tags as prompts during training.
If you're using the Automatic1111 Stable Diffusion WebUI, place the boring_e621_v4.pt file in 
stable-diffusion-webui\embeddings and add ""boring_e621_v4"" to your negative prompt for more interesting outputs.
; The motivation for boring_e621 is that negative embeddings like Bad Prompt, 
whose training is described here 
depend on manually curated lists of tags describing features people do not want their images to have, such as ""deformed hands"".  Some problems with this approach are:; To address these problems, boring_e621 employs textual inversion on a set of images automatically extracted from the art site 
e621.net, a rich resource of millions of hand-labeled artworks, each of which is both human-labeled topically and rated 
according to its quality.  E621.net allows users to express their approval of an artwork by either up-voting it, or marking it as a favorite.Boring_e621 was specifically trained on artworks automatically selected from the site according to the criteria 
that no user has ever Favorited or Up-Voted them.  boring_e621 thus learned to produce low-quality images, so when it is 
used in the negative prompt of a stable diffusion image generator, the model avoids making mistakes that would make the generation more boring.
; To qualitatively evaluate how well boring_e621 has learned to improve image quality, we apply it to 4 simple sample prompts using the base Stable Diffusion 1.5 model.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"?? Disclaimer ?? ; This model is community-contributed, and not supported by Amazon, Inc.; Amazon's BORT; BORT is a highly compressed version of bert-large that is up to 10 times faster at inference. 
The model is an optimal sub-architecture of bert-large that was found using neural architecture search.; Paper",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"License:CreativeML Open RAIL-M
Additional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 31, 2023; ???¦´?????CreativeML Open RAIL-M????License???¦³?¦³¡¥???????????
????????????????????i?????????????????????????
??????????????????????????????????????????????(6/10 Twitter??`???????ðZ??????????License???sazyou_roukaku?¦®??????)
?????CreativeML Open RAIL-M??????d???????????
??????????????????????v?????License?????????A???????????????????????v???¡è??????
????????????????????????????T?????????????????????A???????????????
????_?J?????????????????
??????????????¦³????????????????????????§¶??????????????????; ??`???????????E ; [BracingEvoMix]OpenBra??OpenBra?BanKai @PleaseBanKai ; dreamshaper_5Bakedvaedreamshaper_6BakedVae(https://civitai.com/models/4384) ?Lykonepicrealism_newAgeepicrealism_newEra(https://civitai.com/models/25694) ?epinikiondiamondCoalMix_diamondCoalv2(https://civitai.com/models/41415) ?EnthusiastAIsxd_v10(https://civitai.com/models/1169) ?izuekEvt_V4_e04_ema(https://huggingface.co/haor/Evt_V4-preview) ?haor",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; Get API key from Stable Diffusion API, No Payment needed. ; Replace Key in below code, change model_id  to ""brav6""; Coding in PHP/Node/Java etc? Have a look at docs for more code examples: View docs; Try model for free: Generate Images",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is an experimental product that can be used to create new LLM bassed on Chinese language.; You can use the raw model for next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task.
Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.; Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.
It also inherits some of the bias of its dataset model.; Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.; Use the code below to get started with the model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is an experimental product that can be used to create new LLM bassed on Chinese language. ; You can use the raw model for next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task.
Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.; Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.
It also inherits some of the bias of its dataset model.; Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.; Use the code below to get started with the model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"ByT5 is a tokenizer-free version of Google's T5 and generally follows the architecture of MT5.; ByT5 was only pre-trained on mC4 excluding any supervised training with an average span-mask of 20 UTF-8 characters. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; ByT5 works especially well on noisy text data,e.g., google/byt5-small significantly outperforms mt5-small on TweetQA.; Paper: ByT5: Towards a token-free future with pre-trained byte-to-byte models; Authors: Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for CAMEL AI's CAMEL 33B Combined Data merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"CamemBERT is a state-of-the-art language model for French based on the RoBERTa model. ; It is now available on Hugging Face in 6 different versions with varying number of parameters, amount of pretraining data and pretraining data source domains. ; For further information or requests, please go to Camembert Website; CamemBERT was trained and evaluated by Louis Martin*, Benjamin Muller*, Pedro Javier Ortiz Su??rez*, Yoann Dupont, Laurent Romary, ??ric Villemonte de la Clergerie, Djam?? Seddah and Beno?t Sagot.; If you use our work, please cite:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"[camembert-ner] is a NER model that was fine-tuned from camemBERT on wikiner-fr dataset.
Model was trained on wikiner-fr dataset (~170 634  sentences).
Model was validated on emails/chat data and overperformed other models on this type of data specifically. 
In particular the model seems to work better on entity that don't start with an upper case.; Training data was classified as follow:; Overall; By entity; For those who could be interested, here is a short article on how I used the results of this model to train a LSTM model for signature detection in emails:
https://medium.com/@jean-baptiste.polle/lstm-model-for-email-signature-detection-8e990384fefa",,,,,,,,,,,,,,,,,,,,,,,,,usage 67
"[camembert-ner-with-dates] is an extension of french camembert-ner model with an additionnal tag for dates.
Model was trained on enriched version of wikiner-fr dataset (~170 634  sentences).; On my test data (mix of chat and email), this model got an f1 score of ~83% (in comparison dateparser was ~70%).
Dateparser library can still be be used on the output of this model in order to convert text to python datetime object 
(https://dateparser.readthedocs.io/en/latest/).; Global; By entity",,,,,,,,,,,,,,,,,,,,,,,,,usage 12
"Check out our Blog Post and arXiv paper!; The Cerebras-GPT family is released to facilitate research into LLM scaling laws using open architectures and data sets and demonstrate the simplicity of and scalability of training LLMs on the Cerebras software and hardware stack. All Cerebras-GPT models are available on Hugging Face.; The family includes 111M, 256M, 590M, 1.3B, 2.7B, 6.7B, and 13B models.; All models in the Cerebras-GPT family have been trained in accordance with Chinchilla scaling laws (20 tokens per model parameter) which is compute-optimal.; These models were trained on the Andromeda AI supercomputer comprised of 16 CS-2 wafer scale systems. Cerebras' weight streaming technology simplifies the training of LLMs by disaggregating compute from model storage. This allowed for efficient scaling of training across nodes using simple data parallelism.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Anime model baesd on Counterfeit-V2.5.

; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"??????????????chat????¨¨????????gal???????????; ?????????RWKV?????????§Õ??????????????????????????????????????????????????????????transformer??????????????????; ???????Colab?????????????????????; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"? Blog ? ? Github Repo ? ? Twitter ? ? [GLM@ACL 22] [GitHub] ? ? [GLM-130B@ICLR 23] [GitHub] 
; 
    ? Join our Slack and WeChat
; ????????? ChatGLM2-6B??ChatGLM-6B ???????·Ú????????????????????????????????????????????????????????????????????????????????????????§¹??????????????; ChatGLM-6B ?????????????????????????????????????? General Language Model (GLM) ????????? 62 ??????????????????????????????????????????????§Ò??????INT4 ???????????????? 6GB ??—¥??ChatGLM-6B ?????? ChatGLM ??????????????????????????????????????? 1T ???????????????????????????????????????????????????????????62 ??????? ChatGLM-6B ????????????????????????? ChatGLM-6B ????????§à?????????????§Õ??????§Ö?????????????????¨¢?; ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. With the quantization technique, users can deploy locally on consumer-grade graphics cards (only 6GB of GPU memory is required at the INT4 quantization level). ChatGLM-6B uses technology similar to ChatGPT, optimized for Chinese QA and dialogue. The model is trained for about 1T tokens of Chinese and English corpus, supplemented by supervised fine-tuning, feedback bootstrap, and reinforcement learning with human feedback. With only about 6.2 billion parameters, the model is able to generate answers that are in line with human preference. ChatGLM-6B weights are completely open for academic research, and free commercial use is also allowed after completing the questionnaire.",,,,,,,,,,,,,,,,,,,,,,,,,usage 200
"? Join our Slack and WeChat
; ChatGLM-6B ?????????????????????????????????????? General Language Model (GLM) ????????? 62 ??????????????????????????????????????????????§Ò??????INT4 ???????????????? 6GB ??—¥??ChatGLM-6B ?????? ChatGLM ??????????????????????????????????????? 1T ???????????????????????????????????????????????????????????62 ??????? ChatGLM-6B ?????????????????????????; ChatGLM-6B-INT4 ?? ChatGLM-6B ????????????????????ChatGLM-6B-INT4 ?? ChatGLM-6B ?§Ö? 28 ?? GLM Block ?????? INT4 ????????§Ø? Embedding ?? LM Head ?????????????????????????? 6G ??—¤??? CPU ????—¥?????????????????????õô???????????????§Ö?????; ?? CPU ???????????????????????? CPU Kernel ??????????? GCC ?? OpenMP ??Linux?????????????Windows???????????????????????§Þ?????????; ??????????¡ä?????? ChatGLM-6B ?????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 60
???????????https://huggingface.co/spaces/fb700/chatglm-fitness-RLHF; ChatGLM-6B ???????????????????????????ChatGLM-6B ???????·Ú???????????????????????????????????????????????????????????????????????????????????????LLM?????????????????????????????????????????; ???????40???????????????????????????????????????????; ????????30????????????????????????????œZ?????????????RM??????; ???????????SFT???????????????????????????30????fitness?????????RM??????ChatGLM-6B????????????,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"? Github Repo ? ? Twitter ? ? [GLM@ACL 22] [GitHub] ? ? [GLM-130B@ICLR 23] [GitHub] 
; 
    ? Join our Slack and WeChat
; ChatGLM2-6B ??????????????? ChatGLM-6B ???????·Ú?????????????????????????????????????????????????????ChatGLM2-6B ?????????????????; ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the following new features:; ??????????¡ä?????? ChatGLM-6B ?????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 65
"fastllm model for chatglm-6b-fp16; Github address: https://github.com/ztxz16/fastllm; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"? Github Repo ? ? Twitter ? ? [GLM@ACL 22] [GitHub] ? ? [GLM-130B@ICLR 23] [GitHub] 
; 
    ? Join our Slack and WeChat
; ChatGLM2-6B ??????????????? ChatGLM-6B ???????·Ú?????????????????????????????????????????????????????ChatGLM2-6B ?????????????????; ChatGLM2-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the following new features:; ??????????¡ä?????? ChatGLM-6B ?????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 29
"ChatGLM-6B-Legal???????ChatGLM-6B???????????????????????????????§à??????µµ; ???????????ChatGLM-6B???????????????§Ö?model_1??model_2?????§Ù??????jupyter????????????????????config??????
??????????ChatGLM-6B?????; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is trained on the mix of full-text and splitted sentences of answers from Hello-SimpleAI/HC3.; More details refer to arxiv: 2301.07597 and Gtihub project Hello-SimpleAI/chatgpt-comparison-detection.; The base checkpoint is roberta-base.
We train it with all Hello-SimpleAI/HC3 data (without held-out) for 1 epoch.; (1-epoch is consistent with the experiments in our paper.); Checkout this papaer arxiv: 2301.07597",,,,,,,,,,,,,,,,,,,,,,,,,usage 17
"This model generates ChatGPT/BingChat & GPT-3 prompts and is a fine-tuned version of philschmid/bart-large-cnn-samsum on an this dataset.
It achieves the following results on the evaluation set:; This model supports a Streamlit Web UI to run the chatgpt-gpt4-prompts-bart-large-cnn-samsum model:
; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"This model was trained on our ChatGPT paraphrase dataset.; This dataset is based on the Quora paraphrase question, texts from the SQUAD 2.0 and the CNN news dataset.; This model is based on the T5-base model. We used ""transfer learning"" to get our model to generate paraphrases as well as ChatGPT. Now we can say that this is one of the best paraphrases of the Hugging Face.; Kaggle link; Input:",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
??·Ú????demo?—¨?????????Ziya-LLaMA-13B-v1???????(LLaMA????????????????????????????????????????????????§Ü??); ChatLaw-13B????·Ú????demo?—¨?????????Ziya-LLaMA-13B-v1????????????????????????????????????????§¹??????????????????????????????; ChatLaw-33B????·Ú????demo?—¨????Anima-33B????????????????????????????????????Anima?????????????????????????????????????; ChatLaw-Text2Vec?????93w???§à???????????????????BERT??????????????????????????????????????????????????»Ç; ??????????????????????,,,,,,,,,,,,,,,,,,,,,,,,,usage 2
?????????????????????????????????????????????????; ?????????936727?????????????????????????????????¡ê?; ?????????k??????¦Æ??‰Ý; ??????????????????????; ???????????:,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"???????chilled_remix????reversemix??2023??5??21???Version??????§¶???v2?????§¶?????????????ðZv1???????¡è??????????????DL?g??¦Ç????????A????v1?????????????????¦Õ??}??????????? ; License:CreativeML Open RAIL-M
Additional Copyright: sazyou_roukaku (TwitterID @sazyou_roukaku) as of May 21, 2023; ???¦´?????CreativeML Open RAIL-M????License???¦³?¦³¡¥???????????
????????????????????i?????????????????????????
??????????????????????????????????????????????(6/10 Twitter??`???????ðZ??????????License???sazyou_roukaku?¦®??????)
?????CreativeML Open RAIL-M??????d???????????
??????????????????????v?????License?????????A???????????????????????v???¡è??????
????????????????????????????T?????????????????????A???????????????
????_?J?????????????????
??????????????¦³????????????????????????§¶??????????????????; ??X?O?????????`?????????? ;  Version2??fp16??VAE?????z???¦³????????????????? ????????chilled_remix??????????????Âµ??reversemix??????????¦³??????????? ??chilled_remix??chilled_re-generic??`???`????X???¦Ë????????????????????????????? ???|?????¦´?`???`??????????????????î”????????reversemix????????????? reversemix??LORA??????®Í¦´??????§¶??????????????????????A?????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository has been marked as containing sensitive content and may contain potentially harmful and sensitive
		information.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Diffuser model for this SD checkpoint:
https://civitai.com/models/6424/chilloutmix; emilianJR/chilloutmix_NiPrunedFp32Fix is the HuggingFace diffuser that you can use with diffusers.StableDiffusionPipeline().; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
The CreativeML OpenRAIL License specifies: 
Please read the full license here",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Minlik's Chinese Alpaca 33B Merged.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"????????LLaMA-Plus, Alpaca-Plus 13B?·Ú???; ????????LLaMA-Plus, Alpaca-Plus 13B?·Ú??????????¡ê?; ??????? decapoda-research/llama-13b-hf 
??????? ??? ziqingyang/chinese-llama-plus-lora-13b 
?? ziqingyang/chinese-alpaca-plus-lora-13b ????LoRA????
??????HuggingFace?·Ú????.bin????????????????????????????; test case:; ??????????textgen?????textgen???????llama???????????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"For further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking. ; Pre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu; This repository is developed based on??https://github.com/google-research/bert; You may also interested in,; More resources by HFL: https://github.com/ymcui/HFL-Anthology",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Pretrained on 10k hours WenetSpeech L subset. More details in  TencentGameMate/chinese_speech_pretrain; This model does not have a tokenizer as it was pretrained on audio alone. 
In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data.; python package:
transformers==4.16.2",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"??????????????????????? Llama2 ????????? SFT ??????????????????? llama-2-chat ??????????????????????? llama-2-chat ?????????; ; ; Talk is cheap, Show you the Demo.; ????????Chinese Llama2 Chat Model",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"??????????????????????? Llama2 ????????? SFT ??????????????????? llama-2-chat ??????????????????????? llama-2-chat ?????????; ; ; Talk is cheap, Show you the Demo.; ????????Chinese Llama2 Chat Model",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
???????? & ???????????? soulteary/docker-llama2-chat/??; ?????????? Transformers ???? Meta AI LLaMA2 ?????????; ???????? LLaMA2 7B ????????LinkSoul-AI/Chinese-Llama-2-7b,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repo contains the tokenizer, Chinese-LLaMA LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repo contains the tokenizer, Chinese-Alpaca LoRA weights and configs for Chinese-LLaMA-Alpaca; Instructions for using the weights can be found at https://github.com/ymcui/Chinese-LLaMA-Alpaca.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; 



; This repository contains the resources in our paper ""Revisiting Pre-trained Models for Chinese Natural Language Processing"", which will be published in ""Findings of EMNLP"". You can read our camera-ready paper through ACL Anthology or arXiv pre-print.; Revisiting Pre-trained Models for Chinese Natural Language ProcessingYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu; You may also interested in,",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"; 



; This repository contains the resources in our paper ""Revisiting Pre-trained Models for Chinese Natural Language Processing"", which will be published in ""Findings of EMNLP"". You can read our camera-ready paper through ACL Anthology or arXiv pre-print.; Revisiting Pre-trained Models for Chinese Natural Language ProcessingYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu; You may also interested in,",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the set of 24 Chinese RoBERTa models pre-trained by UER-py, which is introduced in this paper.; Turc et al. have shown that the standard BERT recipe is effective on a wide range of model sizes. Following their paper, we released the 24 Chinese RoBERTa models. In order to facilitate users to reproduce the results, we used the publicly available corpus and provided all training details.; You can download the 24 Chinese RoBERTa miniatures either from the UER-py Modelzoo page, or via HuggingFace from the links below:; Here are scores on the devlopment set of six Chinese tasks:; For each task, we selected the best fine-tuning hyperparameters from the lists below, and trained with the sequence length of 128:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"For further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking. ; Pre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu; This repository is developed based on??https://github.com/google-research/bert; You may also interested in,; More resources by HFL: https://github.com/ymcui/HFL-Anthology",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"For further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking. ; Pre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu; This repository is developed based on??https://github.com/google-research/bert; You may also interested in,; More resources by HFL: https://github.com/ymcui/HFL-Anthology",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Pretrained on 10k hours WenetSpeech L subset. More details in  TencentGameMate/chinese_speech_pretrain; This model does not have a tokenizer as it was pretrained on audio alone. 
In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data.; python package:
transformers==4.16.2; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model was created through fine tuning.
There are two versions: an all-ages depiction-oriented version and an NSFW-enhanced version.; All-ages depiction-oriented version??ChromaFT_v1-pruned.safetensors; NSFW-enhanced version??ChromaNeoFT_v1-pruned.safetensors; (NEO??Nsfw Erotic Option); For sample illustrations, please visit my blog.
https://ai-drawing.net/en/2023/07/15/introducing-of-chroma-ft-v1-0/",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Henk717's Chronoboros 33B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the fp16 PyTorch / HF version of chronos-13b; This model is primarily focused on chat, roleplay, and storywriting, but can accomplish other tasks such as simple reasoning and coding.; Chronos generates very long outputs with coherent text, largely due to the human inputs it was trained on.; This model uses Alpaca formatting, so for optimal model performance, use:; 4bit Quantized version",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the fp16 PyTorch / HF version of chronos-33b - if you need another version, GGML and GPTQ versions are linked below.; This model is primarily focused on chat, roleplay, and storywriting, but can accomplish other tasks such as simple reasoning and coding.; Chronos generates very long outputs with coherent text, largely due to the human inputs it was trained on.; This model uses Alpaca formatting, so for optimal model performance, use:; GGML Version provided by @TheBloke",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"(chronos-13b + Nous-Hermes-13b) 75/25 merge; This has the aspects of chronos's nature to produce long, descriptive outputs. But with additional coherency and an ability to better obey instructions. Resulting in this model having a great ability to produce evocative storywriting and follow a narrative.; This mix contains alot of chronos's writing style and 'flavour' with far less tendency of going AWOL and spouting nonsensical babble.; This result was much more successful than my first chronos merge.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Austism's Chronos Hermes 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is fp16 pytorch format model files for Austism's Chronos Hermes 13B merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 13b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Austism's Chronos Hermes 13B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the fine-tuned Stable Diffusion model trained on screenshots from a popular animation studio.
Use the tokens classic disney style in your prompts for the effect.; If you enjoy my work, please consider supporting me
; Characters rendered with the model:

Animals rendered with the model:

Cars and Landscapes rendered with the model:
; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; You can also export the model to ONNX, MPS and/or FLAX/JAX.",,,,,,,,,,,,,,,,,,,,,,,,,usage 115
"?????????github????????????????????? ?? Please check the github repository for more about the model, hit ? if you like; This model punctuates Classical(ancient) Chinese, you might feel strange about this task, but many of my ancestors think writing articles without punctuation is brilliant idea ?. What we have here are articles from books, letters or carved on stones where you can see no punctuation, just a long string of characters. As you can guess, NLP tech is usually a good tool to tackle this problem, and the entire pipeline can be borrowed from usual NER task.; Since there are also many articles are punctuated, hence with some regex operations, labeled data is more than abundant ?. That's why this problem is pretty much a low hanging fruit.; so I guess who's interested in the problem set can speak at least modern Chinese, hence... let me continue the documentation in Chinese.; ???????¦Ä?????????? ?????? ????????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This model card describes the ClinicalBERT model, which was trained on a large multicenter dataset with a large corpus of 1.2B words of diverse diseases we constructed.
We then utilized a large-scale corpus of EHRs from over 3 million patient records to fine tune the base language model.; The ClinicalBERT model was trained on a large multicenter dataset with a large corpus of 1.2B words of diverse diseases we constructed.; The ClinicalBERT was initialized from BERT. Then the training followed the principle of masked language model, in which given a piece of text, we randomly replace some tokens by MASKs, 
special tokens for masking, and then require the model to predict the original tokens via contextual text. ; We used a batch size of 32, a maximum sequence length of 256, and a learning rate of 5e-5 for pre-training our models. ; Load the model via the transformers library:",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they??re being deployed within.; See the original CLIP Model Card for more details on limitations and biases.; This repository holds OpenAI's CLIP models converted into many other variants, see below for more details.; I haven't done many tests on these conversions. I've briefly tried the float16 versions, which seem very similar to the original float32, however the similarity seems to drop more with the qint8/quint8 versions as expected. I couldn't try qint8 as it seemed unsupported for some operations, but I'm including it for completeness. From a brief test the quint8 version seemed to work fine.; The license for the conversion code is MIT, the license for the models is the same as the original license for the OpenAI models (?????). I have no affiliation with OpenAI.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"A CLIP ViT-B/16 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Mehdi Cherti on the JUWELS Booster supercomputer. See acknowledgements below.; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. ; Zero-shot image classification, image and text retrieval, among others.",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"This is the Image & Text model CLIP, which maps text and images to a shared vector space. For applications of the models, have a look in our documentation SBERT.net - Image Search; After installing sentence-transformers (pip install sentence-transformers), the usage of this model is easy:; See our SBERT.net - Image Search documentation for more examples how the model can be used for image search, zero-shot image classification, image clustering and image deduplication.; In the following table we find the zero-shot ImageNet validation set accuracy:; For a multilingual version of the CLIP model for 50+ languages have a look at: clip-ViT-B-32-multilingual-v1",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"A CLIP ViT-B/32 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Romain Beaumont on the stability.ai cluster. ; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. ; Zero-shot image classification, image and text retrieval, among others.",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found here.; The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they??re being deployed within.; January 2021; The model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. ; The original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.",,,,,,,,,,,,,,,,,,,,,,,,,usage 203
"A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Mitchell Wortsman on the stability.ai cluster.; The license for this model is MIT.; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset.",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"A CLIP ViT-g/14 model trained with the LAION-2B English subset of LAION-5B (https://laion.ai/blog/laion-5b/) using OpenCLIP (https://github.com/mlfoundations/open_clip).; Model training done by Romain Beaumont on the stability.ai cluster. ; As per the original OpenAI CLIP model card, this model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such model. ; The OpenAI CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis. Additionally, the LAION-5B blog (https://laion.ai/blog/laion-5b/) and upcoming paper include additional discussion as it relates specifically to the training dataset. ; Zero-shot image classification, image and text retrieval, among others.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"This is the Image & Text model CLIP, which maps text and images to a shared vector space. For applications of the models, have a look in our documentation SBERT.net - Image Search; After installing sentence-transformers (pip install sentence-transformers), the usage of this model is easy:; See our SBERT.net - Image Search documentation for more examples how the model can be used for image search, zero-shot image classification, image clustering and image deduplication.; In the following table we find the zero-shot ImageNet validation set accuracy:; For a multilingual version of the CLIP model for 50+ languages have a look at: clip-ViT-B-32-multilingual-v1",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found here.; The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they??re being deployed within.; January 2021; The base model uses a ViT-L/14 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss.; The original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.",,,,,,,,,,,,,,,,,,,,,,,,,usage 373
"This model was trained from scratch on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
Pretrained weights for CodeBERT: A Pre-Trained Model for Programming and Natural Languages.; The model is trained on bi-modal data (documents & code) of CodeSearchNet; This model is initialized with Roberta-base and trained with MLM+RTD objective (cf. the paper).; Please see the official repository for scripts that support "code search" and "code-to-document generation".,,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"CodeGen is a family of autoregressive language models for program synthesis from the paper: A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B).; The checkpoint included in this repository is denoted as CodeGen-Mono 16B in the paper, where ""Mono"" means the model is initialized with CodeGen-Multi 16B and further pre-trained on a Python programming language dataset, and ""16B"" refers to the number of trainable parameters.; This checkpoint (CodeGen-Mono 16B) was firstly initialized with CodeGen-Multi 16B, and then pre-trained on BigPython dataset. The data consists of 71.7B tokens of Python programming language. See Section 2.1 of the paper for more details.; CodeGen was trained using cross-entropy loss to maximize the likelihood of sequential inputs.
The family of models are trained using multiple TPU-v4-512 by Google, leveraging data and model parallelism.
See Section 2.3 of the paper for more details.; We evaluate our models on two code generation benchmark: HumanEval and MTPB. Please refer to the paper for more details.",,,,,,,,,,,,,,,,,,,,,,,,,usage 13
"CodeGen is a family of autoregressive language models for program synthesis from the paper: A Conversational Paradigm for Program Synthesis by Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong. The models are originally released in this repository, under 3 pre-training data variants (NL, Multi, Mono) and 4 model size variants (350M, 2B, 6B, 16B).; The checkpoint included in this repository is denoted as CodeGen-Mono 350M in the paper, where ""Mono"" means the model is initialized with CodeGen-Multi 350M and further pre-trained on a Python programming language dataset, and ""350M"" refers to the number of trainable parameters.; This checkpoint (CodeGen-Mono 350M) was firstly initialized with CodeGen-Multi 350M, and then pre-trained on BigPython dataset. The data consists of 71.7B tokens of Python programming language. See Section 2.1 of the paper for more details.; CodeGen was trained using cross-entropy loss to maximize the likelihood of sequential inputs.
The family of models are trained using multiple TPU-v4-512 by Google, leveraging data and model parallelism.
See Section 2.3 of the paper for more details.; We evaluate our models on two code generation benchmark: HumanEval and MTPB. Please refer to the paper for more details.",,,,,,,,,,,,,,,,,,,,,,,,,usage 17
"CodeGen2 is a family of autoregressive language models for program synthesis, introduced in the paper:; CodeGen2: Lessons for Training LLMs on Programming and Natural Languages by Erik Nijkamp*, Hiroaki Hayashi*, Caiming Xiong, Silvio Savarese, Yingbo Zhou.; Unlike the original CodeGen model family (i.e., CodeGen1), CodeGen2 is capable of infilling, and supports more programming languages.; Four model sizes are released: 1B, 3.7B, 7B, 16B.; This model can be easily loaded using the AutoModelForCausalLM functionality.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"CodeGen2 is a family of autoregressive language models for program synthesis, introduced in the paper:; CodeGen2: Lessons for Training LLMs on Programming and Natural Languages by Erik Nijkamp*, Hiroaki Hayashi*, Caiming Xiong, Silvio Savarese, Yingbo Zhou.; Unlike the original CodeGen model family (i.e., CodeGen1), CodeGen2 is capable of infilling, and supports more programming languages.; Four model sizes are released: 1B, 3.7B, 7B, 16B.; This model can be easily loaded using the AutoModelForCausalLM functionality.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Title: CodeGen2.5: Small, but mighty; Authors: Erik Nijkamp*, Hiroaki Hayashi*, Yingbo Zhou, Caiming Xiong; (* equal contribution); CodeGen2.5 is a family of autoregressive language models for program synthesis.; Building upon CodeGen2, the model is trained on StarCoderData for 1.4T tokens, achieving competitive results compared to StarCoderBase-15.5B with less than half the size.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Salesforce's Codegen 2.5 7B Mono.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These files were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Title: CodeGen2.5: Small, but mighty; Authors: Erik Nijkamp*, Hiroaki Hayashi*, Yingbo Zhou, Caiming Xiong; (* equal contribution); CodeGen2.5 is a family of autoregressive language models for program synthesis.; Building upon CodeGen2, the model is trained on StarCoderData for 1.4T tokens, achieving competitive results compared to StarCoderBase-15.5B with less than half the size.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
CodeParrot (large) is a 1.5B parameter GPT-2 model trained on the CodeParrot Python code dataset. The model is trained in Chapter 10: Training Transformers from Scratch in the NLP with Transformers book. You can find the full code in the accompanying Github repository.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"CodeT5+ is a new family of open code large language models
with an encoder-decoder architecture that can flexibly operate in different modes (i.e. encoder-only, decoder-only,
and encoder-decoder) to support a wide range of code understanding and generation tasks.
It is introduced in the paper:; CodeT5+: Open Code Large Language Models for Code Understanding and Generation
by Yue Wang*, Hung Le*, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi (*
indicates equal contribution).; Compared to the original CodeT5 family (base: 220M, large: 770M), CodeT5+ is pretrained with a diverse set of
pretraining tasks including span denoising, causal language modeling, contrastive learning, and text-code
matching to learn rich representations from both unimodal code data and bimodal code-text data.
Additionally, it employs a simple yet effective compute-efficient pretraining method to initialize the model
components with frozen off-the-shelf LLMs such as CodeGen to efficiently scale
up the model (i.e. 2B, 6B, 16B), and adopts a ""shallow encoder and deep decoder"" architecture.
Furthermore, it is instruction-tuned to align with natural language instructions (see our InstructCodeT5+ 16B)
following Code Alpaca.; This checkpoint consists of an encoder of CodeT5+ 220M model (pretrained from 2 stages on both unimodal and bimodal) and a projection layer, which can be used to extract code
embeddings of 256 dimension. It can be easily loaded using the AutoModel functionality and employs the
same CodeT5 tokenizer.; This checkpoint is trained on the stricter permissive subset of the deduplicated version of
the github-code dataset.
The data is preprocessed by reserving only permissively licensed code (""mit"" ??apache-2??, ??bsd-3-clause??, ??bsd-2-clause??,
??cc0-1.0??, ??unlicense??, ??isc??).
Supported languages (9 in total) are as follows:
c, c++, c-sharp,  go, java, javascript,  php, python, ruby.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model was obtained by fine-tuning the corresponding google/flan-t5-xl model on the CoEdIT-Composite dataset. Details of the dataset can be found in our paper and repository.; Paper: CoEdIT: Text Editing by Task-Specific Instruction Tuning; Authors: Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang; We make available the models presented in our paper. ; Given an edit instruction and an original text, our model can generate the edited version of the text.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model was obtained by fine-tuning the corresponding google/flan-t5-xxl model on the CoEdIT dataset.; Paper: CoEdIT: ext Editing by Task-Specific Instruction Tuning; Authors: Vipul Raheja, Dhruv Kumar, Ryan Koo, Dongyeop Kang; We make available the models presented in our paper. ; Given an edit instruction and an original text, our model can generate the edited version of the text.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
https://civitai.com/models/7279?modelVersionId=13196,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
??????????????????????§µ???????????????; ?????????5000???????????????????????GPT4???prompt?????????????????????????ChatGLM???????,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Controlnet v1.1 was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ;",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"Controlnet v1.1 is the successor model of Controlnet v1.0
and was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ;",,,,,,,,,,,,,,,,,,,,,,,,,usage 51
"Controlnet v1.1 is the successor model of Controlnet v1.0
and was released in lllyasviel/ControlNet-v1-1 by Lvmin Zhang.; This checkpoint is a conversion of the original checkpoint into diffusers format.
It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.; For more details, please also have a look at the ? Diffusers docs.; ControlNet is a neural network structure to control diffusion models by adding extra conditions. ;",,,,,,,,,,,,,,,,,,,,,,,,,usage 49
"This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.; huggingface.co/spaces/ioclab/brightness-controlnet; [More Information Needed]; [More Information Needed]; [More Information Needed]",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.; huggingface.co/spaces/ioclab/brightness-controlnet; [More Information Needed]; [More Information Needed]; [More Information Needed]",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; This model is made to generate creative QR codes that still scan.
Keep in mind that not all generated codes might be readable, but you can try different parameters and prompts to get the desired results.; NEW VERSION; Introducing the upgraded version of our model - Controlnet QR code Monster v2.
V2 is a huge upgrade over v1, for scannability AND creativity.; QR codes can now seamlessly blend the image by using a gray-colored background (#808080).",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.; Recommendation Weight: 0.4-0.9; Recommendation Exit Timing: 0.4-0.9; As more datasets are still being trained in this model, it is expected to take 2-4 days. Therefore, flexible weight adjustments should be made based on different scenarios and specific results. If you have generated good images or encountered any problems, you can discuss them on Hugging Face~~~;",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
This is the pretrained weights and some other detector weights of ControlNet.; See also: https://github.com/lllyasviel/ControlNet; ControlNet/models/control_sd15_canny.pth; ControlNet/models/control_sd15_depth.pth; ControlNet/models/control_sd15_hed.pth,,,,,,,,,,,,,,,,,,,,,,,,,usage 244
"All of the models in this repo work with Swift and the apple/ml-stable-diffusion pipeline (release 0.4.0 or 1.0.0).  They were not built for, and will not work with, a Python Diffusers pipeline.  They need ml-stable-diffusion for command line use, or a Swift app that supports ControlNet, such as the (June 2023) MOCHI DIFFUSION 4.0 version.; The ControlNet models in this repo have both ""Original"" and ""Split-Einsum"" versions, all built for SD-1.5 type models.  They will not work with SD-2.1 type models.  The smaller zip files, with ""SE"", each have a single model for ""Split-Einsum"".  The larger zip files, without ""SE"", each have a set of ""Original"" models at 4 different resolutions. ; The ControlNet model files are in the ""CN"" folder of this repo.  They are zipped and need to be unzipped after downloading.  The larger zips hold ""Original"" types at 512x512, 512x768, 768x512 and 768x768.  The smaller zips with ""SE"" have a single model for ""Split-Einsum"".; If you are using a GUI like MOCHI DIFFUSION  4.0, the app will most likely guide you to the correct location/arrangement for your ConrolNet model folder.; Please note that when you unzip the ""Originl"" ControlNet files (for example Canny.zip) from this repo, they will unzip into a folder, with the actual four model files inside that folder.  This folder is just a holding folder for the zipping process.  What you want to move into your ControlNet model folder in Mochi Diffusion will be the individual files, not the folder they unzip into.  The ""Split-Einsum"" zips just have a single file and don't use a holding folder.  To make things even more confusing, on some Mac systems, an individual ControlNet model file, for example Canny-5x5.mlmodelc, will appear in Finder as a folder, not a file.  You want to move the Canny-5x5.mlmodelc file or folder (and other .mlmodelc files or folders) into your ControlNet store folder.  Don't move the plain ""Canny"" folder.  This is different from base models, where you do want to be moving the folder that the downloaded zip file unzips into.  See the images here and here for an example of how my folders are set up for Mochi Diffusion.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository hosts pruned .safetensors modules of ControlNet, by lllyasviel and T2I-Adapters, TencentARC Team; The modules are meant for this extension for AUTOMATIC1111/stable-diffusion-webui, but should work for different webuis too if they have it implemented. cheers!?; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 51
"Conditioning only 25% of the pixels closest to black and the 25% closest to white.; In the automatic1111 folder I added compatible weights, and various strength levels.; Many thanks to antfu for their ideas, tools and contributions
https://antfu.me; ;",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; These ControlNet models have been trained on a large dataset of 150,000 QR code + QR code artwork couples. They provide a solid foundation for generating QR code-based artwork that is aesthetically pleasing, while still maintaining the integral QR code shape.; The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs. However, a 1.5 version model was also trained on the same dataset for those who are using the older version.
Separate repos for usage in diffusers can be found here:
1.5: https://huggingface.co/DionTimmer/controlnet_qrcode-control_v1p_sd15
2.1: https://huggingface.co/DionTimmer/controlnet_qrcode-control_v11p_sd21; These models perform quite well in most cases, but please note that they are not 100% accurate. In some instances, the QR code shape might not come through as expected. You can increase the ControlNet weight to emphasize the QR code shape. However, be cautious as this might negatively impact the style of your output.To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).; To balance between style and shape, a gentle fine-tuning of the control weight might be required based on the individual input and the desired output, aswell as the correct prompt. Some prompts do not work until you increase the weight by a lot. The process of finding the right balance between these factors is part art and part science. For the best results, it is recommended to generate your artwork at a resolution of 768. This allows for a higher level of detail in the final product, enhancing the quality and effectiveness of the QR code-based artwork.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; This repo holds the safetensors & diffusers versions of the QR code conditioned ControlNet for Stable Diffusion v2.1.
The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs. However, a 1.5 version model was also trained on the same dataset for those who are using the older version.; These models perform quite well in most cases, but please note that they are not 100% accurate. In some instances, the QR code shape might not come through as expected. You can increase the ControlNet weight to emphasize the QR code shape. However, be cautious as this might negatively impact the style of your output.To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).; To balance between style and shape, a gentle fine-tuning of the control weight might be required based on the individual input and the desired output, aswell as the correct prompt. Some prompts do not work until you increase the weight by a lot. The process of finding the right balance between these factors is part art and part science. For the best results, it is recommended to generate your artwork at a resolution of 768. This allows for a higher level of detail in the final product, enhancing the quality and effectiveness of the QR code-based artwork.; The simplest way to use this is to place the .safetensors model and its .yaml config file in the folder where your other controlnet models are installed, which varies per application. 
For usage in auto1111 they can be placed in the webui/models/ControlNet folder. They can be loaded using the controlnet webui extension which you can install through the extensions tab in the webui (https://github.com/Mikubill/sd-webui-controlnet). Make sure to enable your controlnet unit and set your input image as the QR code. Set the model to either the SD2.1 or 1.5 version depending on your base stable diffusion model, or it will error. No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail.
Make sure to look up additional info on how to use controlnet if you get stuck, once you have the webui up and running its really easy to install the controlnet extension aswell.",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"; This repo holds the safetensors & diffusers versions of the QR code conditioned ControlNet for Stable Diffusion v1.5.
The Stable Diffusion 2.1 version is marginally more effective, as it was developed to address my specific needs. However, this 1.5 version model was also trained on the same dataset for those who are using the older version.; These models perform quite well in most cases, but please note that they are not 100% accurate. In some instances, the QR code shape might not come through as expected. You can increase the ControlNet weight to emphasize the QR code shape. However, be cautious as this might negatively impact the style of your output.To optimize for scanning, please generate your QR codes with correction mode 'H' (30%).; To balance between style and shape, a gentle fine-tuning of the control weight might be required based on the individual input and the desired output, aswell as the correct prompt. Some prompts do not work until you increase the weight by a lot. The process of finding the right balance between these factors is part art and part science. For the best results, it is recommended to generate your artwork at a resolution of 768. This allows for a higher level of detail in the final product, enhancing the quality and effectiveness of the QR code-based artwork.; The simplest way to use this is to place the .safetensors model and its .yaml config file in the folder where your other controlnet models are installed, which varies per application. 
For usage in auto1111 they can be placed in the webui/models/ControlNet folder. They can be loaded using the controlnet webui extension which you can install through the extensions tab in the webui (https://github.com/Mikubill/sd-webui-controlnet). Make sure to enable your controlnet unit and set your input image as the QR code. Set the model to either the SD2.1 or 1.5 version depending on your base stable diffusion model, or it will error. No pre-processor is needed, though you can use the invert pre-processor for a different variation of results. 768 is the preferred resolution for generation since it allows for more detail.
Make sure to look up additional info on how to use controlnet if you get stuck, once you have the webui up and running its really easy to install the controlnet extension aswell.",,,,,,,,,,,,,,,,,,,,,,,,,usage 53
"Want to support my work: you can bought my Artbook: https://thibaud.art ; Here's the first version of controlnet for stablediffusion 2.1
Trained on a subset of laion/laion-art; ; ;",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. ; prompt: contemporary living room of a house; negative prompt: low quality
; prompt: new york buildings,  Vincent Van Gogh starry night ; negative prompt: low quality, monochrome",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"This is the model files for ControlNet 1.1.
This model card will be filled in a more detailed way after 1.1 is officially merged into ControlNet.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Safetensors/FP16 versions of the new ControlNet-v1-1 checkpoints.; Best used with ComfyUI but should work fine with all other UIs that support controlnets.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.; Cherry-picked from ControlNet + Stable Diffusion v2.1 Base; Images with multiple faces are also supported:; Source images were generated by pulling slice 00000 from LAION Face and passing them through MediaPipe's face detector with special configuration parameters.  ; The colors and line thicknesses used for MediaPipe are as follows:,,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"This repo has two scripts that can show how to convert a fairseq checkpoint to HF Transformers.; It's important to always check in a forward pass that the two checkpoints are the same. The procedure should be as follows:; The ""0"" means that checkpoint is not a fine-tuned one.
4. Verify that models are equal:; Check the scripts to better understand how they work or contact https://huggingface.co/patrickvonplaten; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"ConvNeXT model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper A ConvNet for the 2020s by Liu et al. and first released in this repository. ; Disclaimer: The team releasing ConvNeXT did not write a model card for this model so this model card has been written by the Hugging Face team.; ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and ""modernized"" its design by taking the Swin Transformer as inspiration.; ; You can use the raw model for image classification. See the model hub to look for
fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"CoreNLP is your one stop shop for natural language processing in Java! CoreNLP enables users to derive linguistic annotations for text, including token and sentence boundaries, parts of speech, named entities, numeric and time values, dependency and constituency parses, coreference, sentiment, quote attributions, and relations.
Find more about it in our website and our GitHub repository.; This card and repo were automatically prepared with hugging_corenlp.py in the stanfordnlp/huggingface-models repo; Last updated 2023-03-16 01:06:26.193; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
V2.5 has been updated for ease of use as anime-style model.I use this embedding for negative prompts.https://huggingface.co/datasets/gsdf/EasyNegative ; Share by-productsV2.1??Feeling of use similar to V2.0V2.2??NSFW model; ; ;,,,,,,,,,,,,,,,,,,,,,,,,,usage 67
"?I have utilized BLIP-2 as a part of the training process. Natural language prompts might be more effective.?I prioritize the freedom of composition, which may result in a higher possibility of anatomical errors.?The expressiveness has been improved by merging with negative values, but the user experience may differ from previous checkpoints.?I have uploaded a new Negative Embedding, trained with Counterfeit-V3.0.There's likely no clear superiority or inferiority between this and the previous embedding, so feel free to choose according to your preference.Note that I'm not specifically recommending the use of this embedding.  ; prompt & Setting: https://civitai.com/models/4468/counterfeit-v30

; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 30
"Model info:; https://civitai.com/models/70455?modelVersionId=75113; Original Author's DEMO images :; 

; Sample image I made :",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"CPM-Bee is a fully open-source, commercially-usable Chinese-English bilingual base model with a capacity of ten billion parameters. It is the second milestone achieved through the training process of CPM-live.
Utilizing the Transformer auto-regressive architecture, CPM-Bee has been pre-trained on an extensive corpus of trillion-scale tokens, thereby possessing remarkable foundational capabilities.; Open-source and Commercial Usable??OpenBMB adheres to the spirit of open-source, aiming to make large-scale models accessible to everyone. CPM-Bee, as a foudation model, is fully open-source and available for commercial use, contributing to the advancement of the field of large-scale models.; Excellent Performance in Chinese and English?? : CPM-Bee's base model has undergone rigorous selection and balancing of pre-training data, resulting in outstanding performance in both Chinese and English. For detailed information regarding evaluation tasks and results, please refer to the assessment documentation.; Vast and High-quality Corpus?? CPM-Bee, as a base model, has been trained on an extensive corpus of over trillion tokens, making it one of the models with the highest volume of training data within the open-source community. Furthermore, we have implemented stringent selection, cleaning, and post-processing procedures on the pre-training corpus to ensure its quality.; Support for OpenBMB System?? The OpenBMB system provides a comprehensive ecosystem of tools and scripts for high-performance pre-training, adaptation, compression, deployment, and tool development. CPM-Bee, as a base model, is accompanied by all the necessary tool scripts, enabling developers to efficiently utilize and explore advanced functionalities.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"CPM (Chinese Pre-trained Language Model) is a Transformer-based autoregressive language model, with 2.6 billion parameters and 100GB Chinese training data. To the best of our knowledge, CPM is the largest Chinese pre-trained language model, which could facilitate downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. [Project] [Model] [Paper]; The text generated by CPM is automatically generated by a neural network model trained on a large number of texts, which does not represent the authors' or their institutes' official attitudes and preferences. The text generated by CPM is only used for technical and scientific purposes. If it infringes on your rights and interests or violates social morality, please do not propagate it, but contact the authors and the authors will deal with it promptly.; We collect different kinds of texts in our pre-training, including encyclopedia, news, novels, and Q&A. The details of our training data are shown as follows.; Based on the hyper-parameter searching on the learning rate and batch size, we set the learning rate as 1.5??10?41.5\times10^{-4}1.5??10?4 and the batch size as 3,0723,0723,072, which makes the model training more stable. In the first version, we still adopt the dense attention and the max sequence length is 1,0241,0241,024. We will implement sparse attention in the future. We pre-train our model for 20,00020,00020,000 steps, and the first 5,0005,0005,000 steps are for warm-up. The optimizer is Adam. It takes two weeks to train our largest model using 646464 NVIDIA V100.; We evaluate CPM with different numbers of parameters (the details are shown above) on various Chinese NLP tasks in the few-shot (even zero-shot) settings. With the increase of parameters, CPM performs better on most datasets, indicating that larger models are more proficient at language generation and language understanding. We provide results of text classification, chinese idiom cloze test, and short text conversation generation as follows. Please refer to our paper for more detailed results.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Speedup inference while reducing memory by 2x-4x using int8 inference in C++ on CPU or GPU.; quantized version of OpenAssistant/falcon-7b-sft-top1-696; Checkpoint compatible to ctranslate2>=3.16.0
and hf-hub-ctranslate2>=2.10.0; Converted on 2023-06-16 using; This is just a quantized version. Licence conditions are intended to be idential to original huggingface repo.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Speedup inference while reducing memory by 2x-4x using int8 inference in C++ on CPU or GPU.; quantized version of meta-llama/Llama-2-13b-chat-hf; Checkpoint compatible to ctranslate2>=3.17.1
and hf-hub-ctranslate2>=2.12.0; Converted on 2023-07-21 using; This is just a quantized version. Licence conditions are intended to be idential to original huggingface repo.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"still missing support from ctranslate2.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Speedup inference while reducing memory by 2x-4x using int8 inference in C++ on CPU or GPU.; quantized version of meta-llama/Llama-2-7b-chat-hf; Checkpoint compatible to ctranslate2>=3.17.1
and hf-hub-ctranslate2>=2.12.0; Converted on 2023-07-21 using; This is just a quantized version. Licence conditions are intended to be idential to original huggingface repo.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; An AI model that generates cyberpunk anime characters!~; Based of a finetuned Waifu Diffusion V1.3 Model with Stable Diffusion V1.5 New Vae, training in Dreambooth; by DGSpitzer; This repo contains both .ckpt and Diffuser model files. It's compatible to be used as any Stable Diffusion model, using standard Stable Diffusion Pipelines.",,,,,,,,,,,,,,,,,,,,,,,,,usage 311
"This repository keeps trained Czert-B-base-cased-long-zero-shot model for the paper Czert ?C Czech BERT-like Model for Language Representation

For more information, see the paper; This is long version of Czert-B-base-cased created without any finetunning on long documents. Positional embedings were created by simply repeating the positional embeddings of the original Czert-B model. For tokenization, please use BertTokenizer. Cannot be used with AutoTokenizer. ; You can download MLM & NSP only pretrained models
CZERT-A-v1
CZERT-B-v1; After some additional experiments, we found out that the tokenizers config was exported wrongly. In Czert-B-v1, the tokenizer parameter ""do_lower_case""  was wrongly set to true. In Czert-A-v1 the parameter ""strip_accents""  was incorrectly set to true. ; Both mistakes are repaired in v2.
CZERT-A-v2
CZERT-B-v2",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"ko-fi.com/dalcefo_artworks; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model card focuses on the model associated with the DALL??E mini space on Hugging Face, available here. The app is called ??dalle-mini??, but  incorporates ??DALL??E Mini???? and ??DALL??E Mega?? models (further details on this distinction forthcoming).; The DALL??E Mega model is the largest version of DALLE Mini. For more information specific to DALL??E Mega, see the DALL??E Mega model card.; The model is intended to be used to generate images based on text prompts for research and personal consumption.  Intended uses include supporting creativity, creating humorous content, and providing generations for people curious about the model??s behavior.  Intended uses exclude those described in the Misuse and Out-of-Scope Use section.; The model could also be used for downstream use cases, including:; Downstream uses exclude the uses described in Misuse and Out-of-Scope Use.",,,,,,,,,,,,,,,,,,,,,,,,,usage 60
"?????§Þ???; civitai??§µ?????????????; [Image text]!
(https://huggingface.co/darkjungle/darkjunglepastel-v1/blob/main/xyz_grid-0002-2939914227-masterpiece%2C%20best%20quality%2C%20loli%2C%20small%20breasts%2C%20green%20hair%2C%20orange%20eyes%2C%20clover-shaped%20pupils%2C%20shiny%20pupils%2C%20highlight%20in%20the%20pu.jpg)
[/Image text]; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data. ; Please check the official repository for more details and updates.; We present the dev results on SQuAD 1.1/2.0 and MNLI tasks.; If you find DeBERTa useful for your work, please cite the following paper:",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. It outperforms BERT and RoBERTa on  majority of NLU tasks with 80GB training data.; Please check the official repository for more details and updates.; This is the DeBERTa V2 xlarge model with 24 layers, 1536 hidden size. The total parameters are 900M and it is trained with 160GB raw data.; We present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.; If you find DeBERTa useful for your work, please cite the following paper:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. ; In DeBERTa V3, we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our paper.; Please check the official repository for more implementation details and updates.; The DeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has only 86M backbone parameters  with a vocabulary containing 128K tokens which introduces 98M parameters in the Embedding layer.  This model was trained using the 160GB data as DeBERTa V2.; We present the dev results on SQuAD 2.0 and MNLI tasks.",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"This model was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs. This base model outperforms almost all large models on the ANLI benchmark. 
The base model is DeBERTa-v3-base from Microsoft. The v3 variant of DeBERTa substantially outperforms previous versions of the model by including a different pre-training objective, see annex 11 of the original DeBERTa paper. ; For highest performance (but less speed), I recommend using https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli.; DeBERTa-v3-base-mnli-fever-anli was trained on the MultiNLI, Fever-NLI and Adversarial-NLI (ANLI) datasets, which comprise 763 913 NLI hypothesis-premise pairs.; DeBERTa-v3-base-mnli-fever-anli was trained using the Hugging Face trainer with the following hyperparameters.; The model was evaluated using the test sets for MultiNLI and ANLI and the dev set for Fever-NLI. The metric used is accuracy.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"This model was fine-tuned on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. This model is the best performing NLI model on the Hugging Face Hub as of 06.06.22 and can be used for zero-shot classification. It significantly outperforms all other large models on the ANLI benchmark.; The foundation model is DeBERTa-v3-large from Microsoft. DeBERTa-v3 combines several recent innovations compared to classical Masked Language Models like BERT, RoBERTa etc., see the paper; DeBERTa-v3-large-mnli-fever-anli-ling-wanli was trained on the MultiNLI, Fever-NLI, Adversarial-NLI (ANLI), LingNLI and WANLI datasets, which comprise 885 242 NLI hypothesis-premise pairs. Note that SNLI was explicitly excluded due to quality issues with the dataset. More data does not necessarily make for better NLI models. ; DeBERTa-v3-large-mnli-fever-anli-ling-wanli was trained using the Hugging Face trainer with the following hyperparameters. Note that longer training with more epochs hurt performance in my tests (overfitting).; The model was evaluated using the test sets for MultiNLI, ANLI, LingNLI, WANLI and the dev set for Fever-NLI. The metric used is accuracy.
The model achieves state-of-the-art performance on each dataset. Surprisingly, it outperforms the previous state-of-the-art on ANLI (ALBERT-XXL) by 8,3%. I assume that this is because ANLI was created to fool masked language models like RoBERTa (or ALBERT), while DeBERTa-v3 uses a better pre-training objective (RTD), disentangled attention and I fine-tuned it on higher quality NLI data.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"Here's the defecation lora, it was available on Civitai until the ban on scat content.
You can use various trigger words to get different effects, like ""Scat"", ""Disposal"", ""Feces"" and so on.
The main problem with this model is that that it tends to confuse the anus and the vagina, so you'll have to add prompts and negatives usefull to reduce this effect.; You can find my other models on Civitai: https://civitai.com/user/JollyIm/models; A first example:

Prompts: Realistic, Realism, (Masterpiece, Best Quality, High Quality, Highres:1.4), Detailed, Extremely Detailed, Ambient Soft Lighting, 4K, (Extremely Detailed Eyes, Detailed Face and Skin:1.2), masterpiece, best quality, 1girl, feces, disposal, (anal:1.2), lora:defecation_v1:0.7, (public toilet), embarassed, (pile of feces), (perfect pussy), (perfect vagina),
Negative prompt: easynegative, (worst quality:1.2), (low quality:1.2), (vaginal), (dirty vagina:1.2), (feces in vagina:1.2), (feces in vagina:1.2); Second example:

Prompts: masterpiece, best quality, 1girl, scat, (anal:1.2), lora:defecation_v1:0.9, (toilet), from behind,
Negative prompt: easynegative, (worst quality:1.2), (low quality:1.2), (vaginal), (dirty vagina:1.2), (scat in vagina:1.2), (feces in vagina:1.2); Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Deformable DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository. ; Disclaimer: The team releasing Deformable DETR did not write a model card for this model so this model card has been written by the Hugging Face team.; The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. ; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
Steps on how this model was trained can be found here: Training. The "model_name_or_path" was set to: "roberta-large".; Training details:; Post a Github issue on the repo: Robust DeID.,,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"This model provides you the ability to create anything you want.
The more power of prompt knowledges you have, the better results you'll get.
It basically means that you'll never get a perfect result with just a few words.
You have to fill out your prompt line extremely detailed.
; Dive into the perfect creations world with my prompts.
Your research will be appreciated, so feel free to show everyone, what you can get with this model; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"; The abstract of the paper states that: ; Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than >28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.; You can run a prediction by querying an input image together with a question as follows:; You can use the convert_pix2struct_checkpoint_to_pytorch.py script as follows:",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository. ; Disclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.; The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. ; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 92
"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository. ; Disclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.; The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. ; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 54
"DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository. ; Disclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.; The DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. ; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.; DETR can be naturally extended to perform panoptic segmentation, by adding a mask head on top of the decoder outputs.",,,,,,,,,,,,,,,,,,,,,,,,,usage 75
"DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. 
The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test.
The model is trained on 147M multi-turn dialogue from Reddit discussion thread. ; Please find the information about preprocessing, training and full details of the DialoGPT in the original DialoGPT repository; ArXiv paper: https://arxiv.org/abs/1911.00536; Now we are ready to try out how the model works as a chatting partner!",,,,,,,,,,,,,,,,,,,,,,,,,usage 93
"DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. 
The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test.
The model is trained on 147M multi-turn dialogue from Reddit discussion thread. ; Please find the information about preprocessing, training and full details of the DialoGPT in the original DialoGPT repository; ArXiv paper: https://arxiv.org/abs/1911.00536; Now we are ready to try out how the model works as a chatting partner!",,,,,,,,,,,,,,,,,,,,,,,,,usage 141
,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"DialoGPT is a SOTA large-scale pretrained dialogue response generation model for multiturn conversations. 
The human evaluation results indicate that the response generated from DialoGPT is comparable to human response quality under a single-turn conversation Turing test.
The model is trained on 147M multi-turn dialogue from Reddit discussion thread. ; Please find the information about preprocessing, training and full details of the DialoGPT in the original DialoGPT repository; ArXiv paper: https://arxiv.org/abs/1911.00536; Now we are ready to try out how the model works as a chatting partner!",,,,,,,,,,,,,,,,,,,,,,,,,usage 23
"The app was conceived with the idea of recreating and generate new dialogs for existing games.
In order to generate a dataset for training the steps followed were:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Herv?? J??gou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository. ; Disclaimer: The team releasing DINO did not write a model card for this model so this model card has been written by the Hugging Face team.; The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-1k, at a resolution of 224x224 pixels. ; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; Note that this model does not include any fine-tuned heads.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This checkpoint should be loaded into BartForConditionalGeneration.from_pretrained. See the BART docs for more information.,,,,,,,,,,,,,,,,,,,,,,,,,usage 108
"We are sharing smaller versions of distilbert-base-multilingual-cased that handle a custom number of languages.; Our versions give exactly the same representations produced by the original model which preserves the original accuracy.; For more information please visit our paper: Load What You Need: Smaller Versions of Multilingual BERT.; To generate other smaller versions of multilingual transformers please visit our Github repo.; Please contact amine@geotrend.fr for any question, feedback or request.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
distilbert-base-multilingual-cased fine-tuned on GitHub Typo Corpus for typo detection (using NER style); Dataset: GitHub Typo Corpus ? for 15 languages; Fine-tune script on NER dataset provided by Huggingface ??????; Fast usage with pipelines ?; It works?! We typed wrong Add and middleware,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a distilled version of the BERT base multilingual model. The code for the distillation process can be found here. This model is cased: it does make a difference between english and English.; The model is trained on the concatenation of Wikipedia in 104 different languages listed here.
The model has 6 layers, 768 dimension and 12 heads, totalizing 134M parameters (compared to 177M parameters for mBERT-base).
On average, this model, referred to as DistilmBERT, is twice as fast as mBERT-base.; We encourage potential users of this model to check out the BERT base multilingual model card to learn more about usage, limitations and potential biases.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.; Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"language: ; distilbert-base-multilingual-cased-ner-hrl is a Named Entity Recognition model for 10 high resourced languages (Arabic, German, English, Spanish, French, Italian, Latvian, Dutch, Portuguese and Chinese) based on a fine-tuned  Distiled BERT base model. It has been trained to recognize three types of entities: location (LOC), organizations (ORG), and person (PER). 
Specifically, this model is a distilbert-base-multilingual-cased model that was fine-tuned on an aggregation of 10 high-resourced languages; You can use this model with Transformers pipeline for NER.; This model is limited by its training dataset of entity-annotated news articles from a specific span of time. This may not generalize well for all use cases in different domains.  ; The training data for the 10 languages are from:",,,,,,,,,,,,,,,,,,,,,,,,,usage 13
"This model is distilled from the zero-shot classification pipeline on the Multilingual Sentiment 
dataset using this script. ; In reality the multilingual-sentiment dataset is annotated of course, 
but we'll pretend and ignore the annotations for the sake of example.; Notebook link: here; Result can be reproduce using the following commands:; If you are training this model on Colab, make the following code changes to avoid Out-of-memory error message:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is pre-trained on nepalitext dataset consisting of over 13 million Nepali text sequences using a masked language modeling (MLM) objective. Our approach trains a Sentence Piece Model (SPM) for text tokenization similar to XLM-ROBERTa and trains distilbert model for language modeling. Find more details in this paper.; It achieves the following results on the evaluation set:; Refer to original distilbert-base-uncased; This backbone model intends to be fine-tuned on Nepali language focused downstream task such as sequence classification, token classification or question answering. 
The language model being trained on a data with texts grouped to a block size of 512, it handles text sequence up to 512 tokens and may not perform satisfactorily on shorter sequences.; This model can be used directly with a pipeline for masked language modeling:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"?? This model is deprecated. Please don't use it as it produces sentence embeddings of low quality. You can find recommended sentence embedding models here: SBERT.net - Pretrained Models; This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"This model is a distilled version of the BERT base model. It was
introduced in this paper. The code for the distillation process can be found
here. This model is uncased: it does
not make a difference between english and English.; DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a
self-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,
with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic
process to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained
with three objectives:; This way, the model learns the same inner representation of the English language than its teacher model, while being
faster for inference or downstream tasks.; You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to
be fine-tuned on a downstream task. See the model hub to look for
fine-tuned versions on a task that interests you.; Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)
to make decisions, such as sequence classification, token classification or question answering. For tasks such as text
generation you should look at model like GPT2.",,,,,,,,,,,,,,,,,,,,,,,,,usage 185
"This is an INT8  PyTorch model quantized with huggingface/optimum-intel through the usage of Intel? Neural Compressor. ; The original fp32 model comes from the fine-tuned model elastic/distilbert-base-uncased-finetuned-conll03-english.; The calibration dataloader is the train dataloader. The default calibration sampling size 100 isn't divisible exactly by batch size 8, so the real sampling size is 104.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuned version of distilbert-base-uncased on a munally created dataset in order to detect fashion (label_0) from non-fashion (label_1) items.
It achieves the following results on the evaluation set:; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Model Description: This model is a fine-tune checkpoint of DistilBERT-base-uncased, fine-tuned on SST-2.
This model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).; Example of single-label classification:
??; This model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.; The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.; Based on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.",,,,,,,,,,,,,,,,,,,,,,,,,usage 188
"Model Description:  This is the uncased DistilBERT model fine-tuned on Multi-Genre Natural Language Inference (MNLI) dataset for the zero-shot classification task. ; This model can be used for text classification tasks.; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).; This model of DistilBERT-uncased is pretrained on the Multi-Genre Natural Language Inference (MultiNLI) corpus. It is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"We provide a retrieval trained DistilBert-based model (we call the dual-encoder then dot-product scoring architecture BERT_Dot) trained with Balanced Topic Aware Sampling on MSMARCO-Passage.; This instance was trained with a batch size of 256 and can be used to re-rank a candidate set or directly for a vector index based dense retrieval. The architecture is a 6-layer DistilBERT, without architecture additions or modifications (we only change the weights during training) - to receive a query/passage representation we pool the CLS vector. We use the same BERT layers for both query and passage encoding (yields better results, and lowers memory requirements).; If you want to know more about our efficient (can be done on a single consumer GPU in 48 hours) batch composition procedure and dual supervision for dense retrieval training, check out our paper: https://arxiv.org/abs/2104.06967 ?; For more information and a minimal usage example please visit: https://github.com/sebastian-hofstaetter/tas-balanced-dense-retrieval; We trained our model on the MSMARCO standard (""small""-400K query) training triples re-sampled with our TAS-B method. As teacher models we used the BERT_CAT pairwise scores as well as the ColBERT model for in-batch-negative signals published here: https://github.com/sebastian-hofstaetter/neural-ranking-kd",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"This model is a fine-tuned version of distilbert-base-uncased on the imdb dataset (training notebook is here).
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This model is a fine-tuned version of distilbert-base-uncased on the m-newhauser/senator-tweets dataset, which contains all tweets made by United States senators during the first year of the Biden Administration.
It achieves the following results on the evaluation set:; The goal of this model is to classify short pieces of text as having either Democratic or Republican sentiment. The model was fine-tuned on 99,693 tweets (51.6% Democrat, 48.4% Republican) made by US senators in 2021.; Model accuracy may not hold up on pieces of text longer than a tweet.; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
The model is fine-tuned based on DistilBertForTokenClassification for adding punctuations to plain text (uncased English); Combination of following three dataset:; Validation with 500 samples of dataset scraped from https://www.thenews.com.pk website. Reference; Metrics Report:; Validation with 86 news ted talks of 2020 which are not included in training dataset Reference,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"DistilGPT2 (short for Distilled-GPT2) is an English-language model pre-trained with the supervision of the smallest version of Generative Pre-trained Transformer 2 (GPT-2). Like GPT-2, DistilGPT2 can be used to generate text. Users of this model card should also consider information about the design, training, and limitations of GPT-2.; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; As the developers of GPT-2 (OpenAI) note in their model card, ??language models like GPT-2 reflect the biases inherent to the systems they were trained on.?? Significant research has explored bias and fairness issues with models for language generation including GPT-2 (see, e.g., Sheng et al. (2021) and Bender et al. (2021)). ; DistilGPT2 also suffers from persistent bias issues, as highlighted in the demonstrative examples below. Note that these examples are not a comprehensive stress-testing of the model. Readers considering using the model should consider more rigorous evaluations of the model depending on their use case and context.; The impact of model compression techniques ?C such as knowledge distillation ?C on bias and fairness issues associated with language models is an active area of research. For example:",,,,,,,,,,,,,,,,,,,,,,,,,usage 81
"This model is a distilled version of the RoBERTa-base model. It follows the same training procedure as DistilBERT.
The code for the distillation process can be found here.
This model is case-sensitive: it makes a difference between english and English.; The model has 6 layers, 768 dimension and 12 heads, totalizing 82M parameters (compared to 125M parameters for RoBERTa-base).
On average DistilRoBERTa is twice as fast as Roberta-base.; We encourage users of this model card to check out the RoBERTa-base model card to learn more about usage, limitations and potential biases.; You can use the raw model for masked language modeling, but it's mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.; Note that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.",,,,,,,,,,,,,,,,,,,,,,,,,usage 29
"This is the fine-tuned ClimateBERT language model with a classification head for detecting climate-related paragraphs.; Using the climatebert/distilroberta-base-climate-f language model as starting point, the distilroberta-base-climate-detector model is fine-tuned on our climatebert/climate_detection dataset.; Note: This model is trained on paragraphs. It may not perform well on sentences.; You can use the model with a pipeline for text classification:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuned version of distilroberta-base on the financial_phrasebank dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net; This model was trained by sentence-transformers.",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"This is a sentence-transformers model: It maps sentences & paragraphs to a 512 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net; This model was trained by sentence-transformers.",,,,,,,,,,,,,,,,,,,,,,,,,usage 269
"Document Image Transformer (DiT) model pre-trained on IIT-CDIP (Lewis et al., 2006), a dataset that includes 42 million document images. It was introduced in the paper DiT: Self-supervised Pre-training for Document Image Transformer by Li et al. and first released in this repository. Note that DiT is identical to the architecture of BEiT. ; Disclaimer: The team releasing DiT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Document Image Transformer (DiT) is a transformer encoder model (BERT-like) pre-trained on a large collection of images in a self-supervised fashion. The pre-training objective for the model is to predict visual tokens from the encoder of a discrete VAE (dVAE), based on masked patches.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled document images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Document Image Transformer (DiT) model pre-trained on IIT-CDIP (Lewis et al., 2006), a dataset that includes 42 million document images and fine-tuned on RVL-CDIP, a dataset consisting of 400,000 grayscale images in 16 classes, with 25,000 images per class. It was introduced in the paper DiT: Self-supervised Pre-training for Document Image Transformer by Li et al. and first released in this repository. Note that DiT is identical to the architecture of BEiT. ; Disclaimer: The team releasing DiT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Document Image Transformer (DiT) is a transformer encoder model (BERT-like) pre-trained on a large collection of images in a self-supervised fashion. The pre-training objective for the model is to predict visual tokens from the encoder of a discrete VAE (dVAE), based on masked patches.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled document images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"Document Image Transformer (DiT) model pre-trained on IIT-CDIP (Lewis et al., 2006), a dataset that includes 42 million document images. It was introduced in the paper DiT: Self-supervised Pre-training for Document Image Transformer by Li et al. and first released in this repository. Note that DiT is identical to the architecture of BEiT. ; Disclaimer: The team releasing DiT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Document Image Transformer (DiT) is a transformer encoder model (BERT-like) pre-trained on a large collection of images in a self-supervised fashion. The pre-training objective for the model is to predict visual tokens from the encoder of a discrete VAE (dVAE), based on masked patches.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled document images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The 'Doll-Series' is a set of LORA focused on realistic Asian faces, with incredible levels of beauty and aesthetics.; My Pixiv: https://www.pixiv.net/en/users/92373922; My Twitter: https://twitter.com/KbrLoras; License; Disclaimer",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Databricks' dolly-v2-12b, an instruction-following large language model trained on the Databricks machine learning platform 
that is licensed for commercial use. Based on pythia-12b, Dolly is trained on ~15k instruction/response fine tuning records 
databricks-dolly-15k generated 
by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,
information extraction, open QA and summarization. dolly-v2-12b is not a state-of-the-art model, but does exhibit surprisingly 
high quality instruction following behavior not characteristic of the foundation model on which it is based.  ; Dolly v2 is also available in these smaller models sizes:; Please refer to the dolly GitHub repo for tips on 
running inference for various GPU configurations.; Owner: Databricks, Inc.; dolly-v2-12b is a 12 billion parameter causal language model created by Databricks that is derived from 
EleutherAI's Pythia-12b and fine-tuned 
on a ~15K record instruction corpus generated by Databricks employees and released under a permissive license (CC-BY-SA)",,,,,,,,,,,,,,,,,,,,,,,,,usage 33
"Databricks' dolly-v2-3b, an instruction-following large language model trained on the Databricks machine learning platform 
that is licensed for commercial use. Based on pythia-2.8b, Dolly is trained on ~15k instruction/response fine tuning records 
databricks-dolly-15k generated 
by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,
information extraction, open QA and summarization. dolly-v2-3b is not a state-of-the-art model, but does exhibit surprisingly 
high quality instruction following behavior not characteristic of the foundation model on which it is based.  ; Dolly v2 is also available in these larger models sizes:; Please refer to the dolly GitHub repo for tips on 
running inference for various GPU configurations.; Owner: Databricks, Inc.; dolly-v2-3b is a 2.8 billion parameter causal language model created by Databricks that is derived from 
EleutherAI's Pythia-2.8b and fine-tuned 
on a ~15K record instruction corpus generated by Databricks employees and released under a permissive license (CC-BY-SA)",,,,,,,,,,,,,,,,,,,,,,,,,usage 25
"Databricks' dolly-v2-7b, an instruction-following large language model trained on the Databricks machine learning platform 
that is licensed for commercial use. Based on pythia-6.9b, Dolly is trained on ~15k instruction/response fine tuning records 
databricks-dolly-15k generated 
by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation,
information extraction, open QA and summarization. dolly-v2-7b is not a state-of-the-art model, but does exhibit surprisingly 
high quality instruction following behavior not characteristic of the foundation model on which it is based.  ; Dolly v2 is also available in these other models sizes:; Please refer to the dolly GitHub repo for tips on 
running inference for various GPU configurations.; Owner: Databricks, Inc.; dolly-v2-7b is a 6.9 billion parameter causal language model created by Databricks that is derived from 
EleutherAI's Pythia-6.9b and fine-tuned 
on a ~15K record instruction corpus generated by Databricks employees and released under a permissive license (CC-BY-SA)",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"Dolphin ?
https://erichartford.com/dolphin; This model is based on llama1, so it is for non-commercial use only.  Future versions will be trained on llama2 and other open models that are suitable for commercial use.; This model is uncensored.  I have filtered the dataset to remove alignment and bias.  This makes the model compliant to any requests.  You are advised to implement your own alignment layer before exposing the model as a service.  It will be highly compliant to any requests, even unethical ones.  Please read my blog post about uncensored models.  https://erichartford.com/uncensored-models
You are responsible for any content you create using this model.  Enjoy responsibly.; This dataset is an open source implementation of Microsoft's Orca; After uncensoring, deduping, and cleaning, our dataset consists of:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for Eric Hartford's Dolphin Llama 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GPTQ model files for Eric Hartford's Dolphin Llama 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Donut model fine-tuned on CORD. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository.; Disclaimer: The team releasing Donut did not write a model card for this model so this model card has been written by the Hugging Face team.; Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. ; ; This model is fine-tuned on CORD, a document parsing dataset.",,,,,,,,,,,,,,,,,,,,,,,,,usage 26
"Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. 
It was introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. 
DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.
; The model card has been written in combination by the Hugging Face team and Intel.; Here is how to use this model for zero-shot depth estimation on an image:; For more code examples, we refer to the documentation.; Table 1. Comparison to the state of the art on monocular depth estimation. We evaluate zero-shot cross-dataset transfer according to the
protocol defined in [30]. Relative performance is computed with respect to the original MiDaS model [30]. Lower is better for all metrics. (Ranftl et al., 2021)",,,,,,,,,,,,,,,,,,,,,,,,,usage 144
"More about Models here https://github.com/XingangPan/DragGAN; https://arxiv.org/abs/2305.10973; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Dreambooth finetuning of Stable Diffusion (v1.5.1) on Avatar art style by Lambda Labs.; This text-to-image stable diffusion model was trained with dreambooth.Put in a text prompt and generate your own Avatar style image!; ; To run model locally:; Base model is Stable Diffusion v1.5 and was trained using Dreambooth with 60 input images sized 512x512 displaying Avatar character images.
The model is learning to associate Avatar images with the style tokenized as 'avatarart style'.
Prior preservation was used during training using the class 'Person' to avoid training bleeding into the representations for that class.
Training ran on 2xA6000 GPUs on Lambda GPU Cloud for 700 steps, batch size 4 (a couple hours, at a cost of about $4).",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"Add anime to your prompt to make your gens look more anime.Add photo to your prompt to make your gens look more photorealistic and have better anatomy.This model was trained on 768x768px images, so use 768x768px, 704x832px, 832x704px, etc. Higher resolution or non-square aspect ratios may produce artifacts.  ; Add this to the start of your prompts for best results:; Use negative prompts for best results, for example:; 1girl, girl, etc. give a bit different results, feel free to experiment and see which one you like more!; Use this model as well as Dreamlike Diffusion 1.0 and Dreamlike Photoreal 2.0 for free on dreamlike.art!",,,,,,,,,,,,,,,,,,,,,,,,,usage 45
"Warning: This model is horny! Add ""nude, naked"" to the negative prompt if want to avoid NSFW.  ; You can add photo to your prompt to make your gens look more photorealistic.Non-square aspect ratios work better for some prompts. If you want a portrait photo, try using a vertical aspect ratio. If you want a landscape photo, try using a horizontal aspect ratio.This model was trained on 768x768px images, so use 768x768px, 640x896px, 896x640px, etc. It also works pretty good with higher resolutions such as 768x1024px or 1024x768px.  ; You can use this model for free on dreamlike.art!; Download dreamlike-photoreal-2.0.ckpt (2.13GB); Download dreamlike-photoreal-2.0.safetensors (2.13GB)",,,,,,,,,,,,,,,,,,,,,,,,,usage 129
"Read more about this model here: https://civitai.com/models/4384/dreamshaper; Also please support by giving 5 stars and a heart, which will notify new updates.; Please consider supporting me on Patreon or buy me a coffee; You can run this model on:; Inference API has been turned off for this model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 73
"Preview image by Digiplay:; ; A mix of Noosphere v3 by skumerz and my favorite models.; Original pages:
https://civitai.com/models/36538?modelVersionId=107675
https://civitai.com/models/5528/rainbowpatch",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"DRLN model pre-trained on DIV2K (800 images training, augmented to 4000 images, 100 images validation) for 2x, 3x and 4x image super resolution. It was introduced in the paper Densely Residual Laplacian Super-resolution by Anwar et al. (2020) and first released in this repository. ; The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling and model upscaling.; ; Super-Resolution convolutional neural networks have recently demonstrated high-quality restoration for single images. However, existing algorithms often require very deep architectures and long training times. Furthermore, current convolutional neural networks for super-resolution are unable to exploit features at multiple scales and weigh them equally, limiting their learning capability. In this exposition, we present a compact and accurate super-resolution algorithm namely, Densely Residual Laplacian Network (DRLN). The proposed network employs cascading residual on the residual structure to allow the flow of low-frequency information to focus on learning high and mid-level features. In addition, deep supervision is achieved via the densely concatenated residual blocks settings, which also helps in learning from high-level complex features. Moreover, we propose Laplacian attention to model the crucial features to learn the inter and intra-level dependencies between the feature maps. Furthermore, comprehensive quantitative and qualitative evaluations on low-resolution, noisy low-resolution, and real historical image benchmark datasets illustrate that our DRLN algorithm performs favorably against the state-of-the-art methods visually and accurately.; You can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"DucHaitenAnime_v4.0: In this version i added a little 3D, a little realistic, improved the hand but not much, improved the color because i don't like to use vae; All images above are used only text to image, not edited or accompanying application software.; https://civitai.com/models/6634; please support me by becoming a patron:; https://www.patreon.com/duchaitenreal",,,,,,,,,,,,,,,,,,,,,,,,,usage 41
"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 12 layers and the embedding size is 768.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; Please refer to our paper at https://arxiv.org/pdf/2212.03533.pdf.; Check out unilm/e5 to reproduce evaluation results 
on the BEIR and MTEB benchmark.",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 24 layers and the embedding size is 1024.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; Please refer to our paper at https://arxiv.org/pdf/2212.03533.pdf.; Check out unilm/e5 to reproduce evaluation results 
on the BEIR and MTEB benchmark.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 12 layers and the embedding size is 384.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; Please refer to our paper at https://arxiv.org/pdf/2212.03533.pdf.; Check out unilm/e5 to reproduce evaluation results 
on the BEIR and MTEB benchmark.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Concepts: capri(three-quarter) leggings; If it does not act very strongly against the prompt, it is recommended to increase the strength to about 1.4.; Concepts: low ponytail
; It is recommended to put ""animal ears"" as a negative prompt.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"Connect me on LinkedIn; Dataset labelled 58000 Reddit comments with 28 emotions; RoBERTa builds on BERT??s language masking strategy and modifies key hyperparameters in BERT, including removing BERT??s next-sentence pretraining objective, and training with much larger mini-batches and learning rates. RoBERTa was also trained on an order of magnitude more data than BERT, for a longer amount of time. This allows RoBERTa representations to generalize even better to downstream tasks compared to BERT.; Best Result of Macro F1 - 49.30%; Output",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"With this model, you can classify emotions in English text data. The model was trained on 6 diverse datasets (see Appendix below) and predicts Ekman's 6 basic emotions, plus a neutral class:; The model is a fine-tuned checkpoint of DistilRoBERTa-base. For a 'non-distilled' emotion model, please refer to the model card of the RoBERTa-large version.; a) Run emotion model with 3 lines of code on single text example using Hugging Face's pipeline command on Google Colab:; ; b) Run emotion model on multiple examples and full datasets (e.g., .csv files) on Google Colab:",,,,,,,,,,,,,,,,,,,,,,,,,usage 48
"Quan Sun1*, Qiying Yu2,1*, Yufeng Cui1*, Fan Zhang1*, Xiaosong Zhang1*, Yueze Wang1, Hongcheng Gao1, Jingjing Liu2, Tiejun Huang1,3, Xinlong Wang1; 1 BAAI, 2 THU, 3 PKU * Equal Contribution; |  Paper | Demo(tmp) |; Emu is a Large Multimodal Model (LMM) trained with a unified autoregressive objective, i.e., predict-the-next-element, including both visual embeddings and textual tokens. Trained under this objective, Emu can serve as a generalist interface for diverse multimodal tasks, such as image captioning, image/video question answering, and text-to-image generation, together with new abilities like in-context text and image generation, and image blending.; Clone the github repository and install required packages:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
A Spacy pipeline for generating readability scores,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuned version of distilbert-base-uncased on the conll2003 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"?????RoBERTa-wwm-ext-base????????§Ù?????????????·Ú; This is the fine-tuned version of the Chinese RoBERTa-wwm-ext-base model on several sentiment analysis datasets.; ????chinese-roberta-wwm-ext-base?????????????8?????????????§Ù?????????????227347????????????????Semtiment?·Ú??; Based on chinese-roberta-wwm-ext-base, we fine-tuned a sentiment analysis version on 8 Chinese sentiment analysis datasets, with totaling 227,347 samples.; ????????????????????????????????????????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"One thing I'm happy to celebrate about my model as that it is a lot more racially friendly and will more often than not, produce people who are not Caucasian, in fact around 60%-70% of the time it will produce people of colour in my testing at least. If you find it's not producing Caucasian people, just add either white or Caucasian to the prompt.
It's also very good at aging people so adding an age can make a big difference.; The only thing V5 doesn't do well most of the time are eyes, if you don't get decent eyes try adding perfect eyes or round eyes to the prompt and increase the weight till you are happy. V6 and V6Ultra has fixed this problem.; Prompt: (subject), elegant, alluring, attractive, amazing photograph, masterpiece, best quality, 8K, high quality, photorealistic, realism, art photography, Nikon D850, 16k, sharp focus, masterpiece, breathtaking, atmospheric perspective, diffusion, pore correlation, skin imperfections, DSLR, 80mm Sigma f2, depth of field, intricate natural lighting,; Negative prompt: (unrealistic, render, 3d,cgi,cg,2.5d), (bad-hands-5:1.05), easynegative, [( NG_DeepNegative_V1_64T :0.9) :0.1], ng_deepnegative_v1_75t, worst quality, low quality, normal quality, child, (painting, drawing, sketch, cartoon, anime, render, 3d), blurry, deformed, disfigured, morbid, mutated, bad anatomy, bad art, (bad teeth, weird teeth, broken teeth), (worst quality, low quality, logo, text, watermark, username), incomplete, bad_prompt_version2; Recommend for V5: DPM++ 2M SDE Karras, Steps: 20-45, Hires fix 0.15 - 0.2, CFG 6 - 8.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Please for anyone, don't reupload my finetuned models to other site, like Civitai or image generating site like Seeart, Yodayo other sites. Thank you for understanding this. ; Like my works and want to collaboration or funding my projects? contact me.; Finetuned on SDXL Base 0.9 Official Release, Expected to be successor of Everyjourney, currently in alpha stage, since i'm captioned this model with BLIP2, the image generated with this model may not meet your expectations, waiting for SDXL finetune/training process to be more polished.  ; My other works:   ; Recommended Settings",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model uses the Evol-Instruct-Code-80k-v1 dataset generated using the Evol-Teacher repo. Currently, WizardCoder is one the most performant Code Generation models, being beaten only by ChatGPT. This takes the Code Alpaca 20k dataset and evolves each instruction through a randomly chosen evolution prompt to increase instruction complexity. These prompts range from increase time/space complexity, to increasing requirements, to adding erroneus code to improve robustness, etc. This is done three times with pruning and post processing to remove unwanted instructions and responses. The iterative addition of more complexity gives higher quality and more in-depth instructions than what is ususally generated in Alpaca methods. This, like in the case of WizardCoder and WizardLM, can lead to strong performance that gets very close to RLHF model performance.; This model uses ReplitLM fine tuned with the following parameters:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are Replit GGML format model files for Nick Roshdieh's Evol Replit v1.; Please note that these GGMLs are not compatible with llama.cpp, text-generation-webui or llama-cpp-python. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
Create your own image classifier for anything by running this repo,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This is a Hugging Face transformers-compatible conversion of the original dense 355M-parameter model from the paper ""Efficient Large Scale Language Modeling with Mixtures of Experts"" from Artetxe et al. Please refer to the original model card, which can be found at https://github.com/facebookresearch/fairseq/blob/main/examples/moe_lm/model_card.md.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Falcon-7b fine-tuned on the CodeAlpaca 20k instructions dataset by using the method QLoRA with PEFT library.; Falcon 7B; CodeAlpaca_20K: contains 20K instruction-following data used for fine-tuning the Code Alpaca model.; TBA,,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"Falcon-40B is a 40B parameters causal decoder-only model built by TII and trained on 1,000B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.; Paper coming soon ?.; We get it. AI is everywhere! Is it taking over? ; Before we debate the scant likelihood of a cyborg assassin from the future terminating humanity, let??s get to know the newbie that has soared to top-spot on the leaderboard ?C Falcon 40B.; Falcon 40B is the UAE??s and the Middle East??s first home-grown, open-source large language model (LLM) with 40 billion parameters trained on one trillion tokens. The brainchild of the Technology Innovation Institute (TII), Falcon 40B has generated a tremendous amount of global interest and intrigue, but what really sweetens the deal is its transparent, open-source feature.",,,,,,,,,,,,,,,,,,,,,,,,,usage 40
"; 

; 
LLM Generation models trained by Jina AI, Finetuner team.
; This repo contains the full weights (16bit) for Falcon-40b
fit on the Code Alpaca dataset.; This version of the weights was trained with the following hyperparameters:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; 

; 
LLM Generation models trained by Jina AI, Finetuner team.
; This repo contains the lora weights (8bit) for Falcon-40b
fit on the Code Alpaca dataset.; This version of the weights was trained with the following hyperparameters:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ? To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF!; ? This is an instruct model, which may not be ideal for further finetuning. If you are interested in building your own instruct/chat model, we recommend starting from Falcon-40B. ; ? Looking for a smaller, less expensive model? Falcon-7B-Instruct is Falcon-40B-Instruct's little brother!",,,,,,,,,,,,,,,,,,,,,,,,,usage 28
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGCC format model files for Falcon 40B Instruct.; These files will not work in llama.cpp, text-generation-webui or KoboldCpp.; GGCC is a new format created in a new fork of llama.cpp that introduced this new Falcon GGML-based support: cmp-nc/ggllm.cpp.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains an experimantal GPTQ 4bit model for Falcon-40B-Instruct.; It is the result of quantising to 4bit using AutoGPTQ.; Please note this is an experimental GPTQ model. Support for it is currently quite limited.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Falcon-40b-openassistant-peft is a chatbot model for dialogue generation. It was built by fine-tuning Falcon-40B on the OpenAssistant/oasst1 dataset. This repo only includes the LoRA adapters from fine-tuning with ?'s peft package. ; The model was fine-tuned in 4-bit precision using peft adapters, transformers, and bitsandbytes. Training relied on a method called ""Low Rank Adapters"" (LoRA), specifically the QLoRA variant. The run took approximately 10 hours and was executed on a workstation with a single A100-SXM NVIDIA GPU with 37 GB of available memory. See attached Colab Notebook for the code and hyperparams used to train the model. ; May 30, 2023; To prompt the chat model, use the following format:; Prompter:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuning of TII's Falcon 40B LLM. 
It was trained on a mixture of OASST top-2 threads (exported on June 2, 2023), Dolly-15k and synthetic instruction datasets (see dataset configuration below).; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; Model:",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGCC format model files for Open Assistant's Falcon 40B SFT MIX.; These files will not work in llama.cpp, text-generation-webui or KoboldCpp.; GGCC is a new format created in a new fork of llama.cpp that introduced this new Falcon GGML-based support: cmp-nc/ggllm.cpp.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuning of TII's Falcon 40B LLM. 
It was trained with top-1 (high-quality) demonstrations of the OASST data set (exported on May 6, 2023) with an effective batch size of 144 for ~7.5 epochs with LIMA style dropout (p=0.3) and a context-length of 2048 tokens.; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; Model:",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ? To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF!; ?? This is a raw, pretrained model, which should be further finetuned for most usecases. If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at Falcon-7B-Instruct. ; ? Looking for an even more powerful model? Falcon-40B is Falcon-7B's big brother!",,,,,,,,,,,,,,,,,,,,,,,,,usage 27
"; 

; 
LLM Generation models trained by Jina AI, Finetuner team.
; This repo contains the full weights (16bit) for Falcon-7b
fit on the Code Alpaca dataset.; This version of the weights was trained with the following hyperparameters:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; 

; 
LLM Generation models trained by Jina AI, Finetuner team.
; This repo contains the lora weights (8bit) for Falcon-7b
fit on the Code Alpaca dataset.; This version of the weights was trained with the following hyperparameters:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ? To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF!; ? This is an instruct model, which may not be ideal for further finetuning. If you are interested in building your own instruct/chat model, we recommend starting from Falcon-7B. ; ? Looking for an even more powerful model? Falcon-40B-Instruct is Falcon-7B-Instruct's big brother!",,,,,,,,,,,,,,,,,,,,,,,,,usage 77
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains an experimantal GPTQ 4bit model for Falcon-7B-Instruct.; It is the result of quantising to 4bit using AutoGPTQ.; Please note that performance with this GPTQ is currently very slow with AutoGPTQ.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Resharded version of https://huggingface.co/tiiuae/falcon-7b-instruct for low RAM enviroments (e.g. Colab, Kaggle) in safetensors; Tutorial: https://medium.com/@vilsonrodrigues/run-your-private-llm-falcon-7b-instruct-with-less-than-6gb-of-gpu-using-4-bit-quantization-ff1d4ffbabcc; Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license.; Paper coming soon ?.; ? To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Falcon-7b-openassistant-peft is a chatbot model for dialogue generation. It was built by fine-tuning Falcon-7B on the OpenAssistant/oasst1 dataset. This repo only includes the LoRA adapters from fine-tuning with ?'s peft package. ; The model was fine-tuned in 8-bit precision using ? peft adapters, transformers, and bitsandbytes. Training relied on a method called ""Low Rank Adapters"" (LoRA), specifically the QLoRA variant. The run took approximately 6.25 hours and was executed on a workstation with a single A100-SXM NVIDIA GPU with 37 GB of available memory. See attached Colab Notebook for the code and hyperparams used to train the model. ; May 30, 2023; To prompt the chat model, use the following format:; Prompter:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The tiiuae/falcon-7b model finetuned for Paraphrasing, Changing the Tone of the input sentence(to casual/professional/witty), 
Summary and Topic generation from a dialogue. Data for Paraphrasing and Changing the Tone was generated using gpt-35-turbo and a sample of roughly 1000 data points from the
Dialogsum dataset was used for Summary and Topic generation.; Look at the repo llm-toys for usage and other details.; Try in colab (you might need the pro version):


; ; The following bitsandbytes quantization config was used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Falcon-7b finetuned with Airbird Questions & Answers,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Falcon-7b-QueAns is a chatbot-like model for Question and Answering. It was built by fine-tuning Falcon-7B on the SQuAD dataset. This repo only includes the QLoRA adapters from fine-tuning with ?'s peft package. ; ?? This is a finetuned version for specifically question and answering. If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at Falcon-7B-Instruct. ; ? Looking for an even more powerful model? Falcon-40B is Falcon-7B's big brother!; The model was fine-tuned in 4-bit precision using ? peft adapters, transformers, and bitsandbytes. Training relied on a method called ""Low Rank Adapters"" (LoRA), specifically the QLoRA variant. The run took approximately 4 hours and was executed on a workstation with a single T4 NVIDIA GPU with 15 GB of available memory. See attached [Colab Notebook] used to train the model. ; July 06, 2023",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuning of TII's Falcon 7B LLM. 
It was trained on a mixture of OASST top-2 threads (exported on June 2, 2023), Dolly-15k and synthetic instruction datasets (see dataset configuration below).; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; Model:",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"Model type:
FastChat-T5 is an open-source chatbot trained by fine-tuning Flan-t5-xl (3B parameters) on user-shared conversations collected from ShareGPT.
It is based on an encoder-decoder transformer architecture, and can autoregressively generate responses to users' inputs. ; Model date:
FastChat-T5 was trained on April 2023.; Organizations developing the model:
The FastChat developers, primarily Dacheng Li, Lianmin Zheng and Hao Zhang.; Paper or resources for more information:
https://github.com/lm-sys/FastChat#FastChat-T5; License:
Apache License 2.0",,,,,,,,,,,,,,,,,,,,,,,,,usage 9
"This repository contains the conversion of openai/whisper-large-v2 to the CTranslate2 model format.; This model can be used in CTranslate2 or projects based on CTranslate2 such as faster-whisper.; The original model was converted with the following command:; Note that the model weights are saved in FP16. This type can be changed when the model is loaded using the compute_type option in CTranslate2.; For more information about the original model, see its model card.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
FastSpeech 2 text-to-speech model from fairseq S^2 (paper/code):; See also fairseq S^2 example.,,,,,,,,,,,,,,,,,,,,,,,,,usage 75
FastSpeech 2 text-to-speech model from fairseq S^2 (paper/code):; See also fairseq S^2 example.,,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"fastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices. It was introduced in this paper. The official website can be found here.; This LID (Language IDentification) model is used to predict the language of the input text, and the hosted version (lid218e) was released as part of the NLLB project and can detect 217 languages. You can find older versions (ones that can identify 157 languages) on the official fastText website.; fastText is a library for efficient learning of word representations and sentence classification. fastText is designed to be simple to use for developers, domain experts, and students. It's dedicated to text classification and learning word representations, and was designed to allow for quick model iteration and refinement without specialized hardware. fastText models can be trained on more than a billion words on any multicore CPU in less than a few minutes.; It includes pre-trained models learned on Wikipedia and in over 157 different languages. fastText can be used as a command line, linked to a C++ application, or used as a library for use cases from experimentation and prototyping to production.; You can use pre-trained word vectors for text classification or language identification. See the tutorials and resources on its official website to look for tasks that interest you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This model was trained from scratch on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model was fine-tuned on a novel financial news dataset, which consists of 2K articles from Bloomberg, on topics such as stock, markets, currencies, rate and cryptocurrencies. ; It is based on the PEGASUS model and in particular PEGASUS fine-tuned on the Extreme Summarization (XSum) dataset: google/pegasus-xsum model. PEGASUS was originally proposed by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu in PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization. ; We provide a simple snippet of how to use this model for the task of financial summarization in PyTorch.; The results before and after the fine-tuning on our dataset are shown below:; You can find more details about this work in the following workshop paper. If you use our model in your research, please consider citing our paper:",,,,,,,,,,,,,,,,,,,,,,,,,usage 9
"FinancialBERT is a BERT model pre-trained on a large corpora of financial texts. The purpose is to enhance financial NLP research and practice in financial domain, hoping that financial practitioners and researchers can benefit from this model without the necessity of the significant computational resources required to train the model. ; The model was fine-tuned for Sentiment Analysis task on Financial PhraseBank dataset. Experiments show that this model outperforms the general BERT and other financial domain-specific models.; More details on FinancialBERT's pre-training process can be found at: https://www.researchgate.net/publication/358284785_FinancialBERT_-_A_Pretrained_Language_Model_for_Financial_Text_Mining; FinancialBERT model was fine-tuned on Financial PhraseBank, a dataset consisting of 4840 Financial News categorised by sentiment (negative, neutral, positive).; The evaluation metrics used are: Precision, Recall and F1-score. The following is the classification report on the test set.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper FinBERT: Financial Sentiment Analysis with Pre-trained Language Models and our related blog post on Medium.; The model will give softmax outputs for three labels: positive, negative or neutral.; About Prosus; Prosus is a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. For more information, please visit www.prosus.com.; Contact information",,,,,,,,,,,,,,,,,,,,,,,,,usage 42
"ESG analysis can help investors determine a business' long-term sustainability and identify associated risks. FinBERT-ESG is a FinBERT model fine-tuned on 2,000 manually annotated sentences from firms' ESG reports and annual reports.  ; Input: A financial text.; Output: Environmental, Social, Governance or None.; You can use this model with Transformers pipeline for ESG classification.; Visit FinBERT.AI for more details on the recent development of FinBERT.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"Forward-looking statements (FLS) inform investors of managers?? beliefs and opinions about firm's future events or results. Identifying forward-looking statements from corporate reports can assist investors in financial analysis. FinBERT-FLS is a FinBERT model fine-tuned on 3,500 manually annotated sentences from Management Discussion and Analysis section of annual reports of Russell 3000 firms.  ; Input: A financial text.; Output: Specific-FLS , Non-specific FLS, or Not-FLS.; You can use this model with Transformers pipeline for forward-looking statement classification.; Visit FinBERT.AI for more details on the recent development of FinBERT.",,,,,,,,,,,,,,,,,,,,,,,,,usage 11
"FinBERT-PT-BR is a pre-trained NLP model to analyze sentiment of Brazilian Portuguese financial texts.; The model was trained in two main stages: language modeling and sentiment modeling. In the first stage, a language model was trained with more than 1.4 million texts of financial news in Portuguese. 
From this first training, it was possible to build a sentiment classifier with few labeled texts (500) that presented a satisfactory convergence.; At the end of the work, a comparative analysis with other models and the possible applications of the developed model are presented. 
In the comparative analysis, it was possible to observe that the developed model presented better results than the current models in the state of the art. 
Among the applications, it was demonstrated that the model can be used to build sentiment indices, investment strategies and macroeconomic data analysis, such as inflation.; ; In order to use the model, you need to get the HuggingFace auth token. You can get it here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"FinBERT is a BERT model pre-trained on financial communication text. The purpose is to enhance financial NLP research and practice. It is trained on the following three financial communication corpus. The total corpora size is 4.9B tokens.; More technical details on FinBERT: Click Link; This released finbert-tone model is the FinBERT model fine-tuned on 10,000 manually annotated (positive, negative, neutral) sentences from analyst reports. This model achieves superior performance on financial tone analysis task. If you are simply interested in using FinBERT for financial tone analysis, give it a try.; If you use the model in your academic work, please cite the following paper:; Huang, Allen H., Hui Wang, and Yi Yang. ""FinBERT: A Large Language Model for Extracting Information from Financial Text."" Contemporary Accounting Research (2022).",,,,,,,,,,,,,,,,,,,,,,,,,usage 20
"This model is a fine-tuned version of yiyanghkust/finbert-tone on Twitter Financial News Topic dataset.
It achieves the following results on the evaluation set:; Model determines the financial topic of given tweets over 20 various topics. Given the unbalanced distribution of the class labels, the weights were adjusted to pay attention to the less sampled labels which should increase overall performance..; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"[1] Financial_Phrasebank (FPB)  is a financial news sentiment analysis benchmark, the labels are ""positive"", ""negative"" and ""neutral"". We use the same split as BloombergGPT. BloombergGPT only use 5-shots in the test to show their model's outstanding performance without further finetuning. However, is our task, all data in the 'train' part were used in finetuning, So our results are far better than Bloomberg's.; [2] FiQA SA consists of 17k sentences from microblog headlines and financial news. These labels were changed to ""positive"", ""negative"" and ""neutral"" according to BloombergGPT's paper. We have tried to use the same split as BloombergGPT's paper. However, the amounts of each label can't match exactly when the seed was set to 42.; [3] Twitter Financial News Sentiment (TFNS) dataset is an English-language dataset containing an annotated corpus of finance-related tweets. This dataset is used to classify finance-related tweets for their sentiment. The dataset holds 11,932 documents annotated with 3 labels: ""Bearish"" (""negative""), ""Bullish"" (""positive""), and ""Neutral"".; [4] News With GPT Instruction (MWGI) is a dataset whose labels were generated by ChatGPT. The train set has 16.2k samples and the test set has 4.05k samples. The dataset not only contains 7 classification labels: ""strong negative"", ""moderately negative"", ""mildly negative"", ""neutral"", ""mildly positive"", ""moderately positive"", ""strong positive"". but it also has the reasons for that result, which might be helpful in the instruction finetuning.; Coming Soon.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Model info :; https://civitai.com/models/15745?modelVersionId=27424; Original Author's DEMO images :;,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"Paper | Model | Dataset; ? We still have numerous experiments awaiting completion (details are here), requiring additional computing resources in our lab. If any industry professionals reading this are willing to provide assistance, please feel free to reach out to us at sporia@sutd.edu.sg.; Flacuna was developed by fine-tuning Vicuna on Flan-mini, a comprehensive instruction collection encompassing various tasks. Vicuna is already an excellent writing assistant, and the intention behind Flacuna was to enhance Vicuna's problem-solving capabilities. To achieve this, we curated a dedicated instruction dataset called Flan-mini.; As a result of this fine-tuning process, Flacuna exhibited notable performance improvements in problem-solving across multiple benchmark datasets, both in few-shot and zero-shot settings.; During training, Flacuna is a 13B checkpoint of LLaMA and employed a maximum input sequence length of 1280. We utilized LoRA for parameter-efficient fine-tuning.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Training was conducted over 94 epochs, using a linear decaying learning rate of 2e-05, starting from 0.225 and a batch size of 32 with GloVe and Flair forward and backward embeddings.;  Due to the right-to-left in left-to-right context, some formatting errors might occur. and your code might appear like this, (link accessed on 2020-10-27) ; if you use this model, please consider citing this work:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"? We developed Flacuna by fine-tuning Vicuna-13B on the Flan collection. Flacuna is better than Vicuna at problem-solving. Access the model here https://huggingface.co/declare-lab/flacuna-13b-v1.0.; ? Curious to know the performance of ? ? Flan-Alpaca on large-scale LLM evaluation benchmark, InstructEval? Read our paper https://arxiv.org/pdf/2306.04757.pdf. We evaluated more than 10 open-source instruction-tuned LLMs belonging to various LLM families including Pythia, LLaMA, T5, UL2, OPT, and Mosaic. Codes and datasets: https://github.com/declare-lab/instruct-eval; ? FLAN-T5 is also useful in text-to-audio generation. Find our work at https://github.com/declare-lab/tango if you are interested.; Our repository contains code for extending the Stanford Alpaca
synthetic instruction tuning to existing instruction-tuned models such as Flan-T5.
We have a live interactive demo thanks to Joao Gante!
We are also benchmarking many instruction-tuned models at declare-lab/flan-eval.
Our pretrained models are fully available on HuggingFace ? :; *recommended for better performance",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"? We developed Flacuna by fine-tuning Vicuna-13B on the Flan collection. Flacuna is better than Vicuna at problem-solving. Access the model here https://huggingface.co/declare-lab/flacuna-13b-v1.0.; ? Curious to know the performance of ? ? Flan-Alpaca on large-scale LLM evaluation benchmark, InstructEval? Read our paper https://arxiv.org/pdf/2306.04757.pdf. We evaluated more than 10 open-source instruction-tuned LLMs belonging to various LLM families including Pythia, LLaMA, T5, UL2, OPT, and Mosaic. Codes and datasets: https://github.com/declare-lab/instruct-eval; ? FLAN-T5 is also useful in text-to-audio generation. Find our work at https://github.com/declare-lab/tango if you are interested.; Our repository contains code for extending the Stanford Alpaca
synthetic instruction tuning to existing instruction-tuned models such as Flan-T5.
We have a live interactive demo thanks to Joao Gante!
We are also benchmarking many instruction-tuned models at declare-lab/flan-eval.
Our pretrained models are fully available on HuggingFace ? :; *recommended for better performance",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:",,,,,,,,,,,,,,,,,,,,,,,,,usage 133
"This is the flan-t5-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering.; UPDATE: With transformers version 4.31.0 the use_remote_code=True is no longer necessary and if used will cause AutoModelForQuestionAnswering.from_pretrained() to not work properly.; NOTE: The <cls> token must be manually added to the beginning of the question for this model to work properly.
It uses the <cls> token to be able to make ""no answer"" predictions.
The t5 tokenizer does not automatically add this special token which is why it is added manually.; Language model: flan-t5-baseLanguage: EnglishDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD 2.0Infrastructure: 1x NVIDIA 3070  ; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:",,,,,,,,,,,,,,,,,,,,,,,,,usage 141
"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:",,,,,,,,,,,,,,,,,,,,,,,,,usage 216
"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:",,,,,,,,,,,,,,,,,,,,,,,,,usage 148
"; If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. 
As mentioned in the first few lines of the abstract : ;  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.; Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card.; Find below some example scripts on how to use the model in transformers:",,,,,,,,,,,,,,,,,,,,,,,,,usage 232
"; Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the UL2 model  released earlier last year. It was fine tuned using the ""Flan"" prompt tuning 
and dataset collection.; According to the original blog here are the notable improvements:; You can use the convert_t5x_checkpoint_to_pytorch.py script and pass the argument strict = False. The final layer norm is missing from the original dictionnary, that is why we are passing the strict = False argument.; We used the same config file as google/ul2.",,,,,,,,,,,,,,,,,,,,,,,,,usage 98
"FlauBERT is a French BERT trained on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer.; Along with FlauBERT comes FLUE: an evaluation setup for French NLP systems similar to the popular GLUE benchmark. The goal is to enable further reproducible experiments in the future and to share models and progress on the French language.For more details please refer to the official website.; Note: flaubert-small-cased is partially trained so performance is not guaranteed. Consider using it for debugging purpose only.; Notes: if your transformers version is <=2.10.0, modelname should take one
of the following values:; If you use FlauBERT or the FLUE Benchmark for your scientific publication, or if you find the resources in this repository useful, please cite one of the following papers:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This checkpoint is obtained after training FlaxBigBirdForQuestionAnswering (with extra pooler head) on natural_questions dataset on TPU v3-8. This dataset takes around ~100 GB on disk. But thanks to Cloud TPUs and Jax, each epoch took just 4.5 hours. Script for training can be found here: https://github.com/vasudevgupta7/bigbird; Use this model just like any other model from ?Transformers; In case you are interested in predicting category (null, long, short, yes, no) as well, use FlaxBigBirdForNaturalQuestions (instead of FlaxBigBirdForQuestionAnswering) from my training script.; Evaluation script: https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/evaluate-flax-natural-questions.ipynb; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"??FlexDreamHK???`??????NovelAI???????????????????????¦´?????????????????????????????????????????
???????????`???????????????????????????????????????M???Âµ????????????????
????`?????????????Stable Diffusion??Wifu Diffusion?????????????????????`???????§¶????¦³¦³?¨¶?????????????
??????????????`??`????????????????????????????}????}??LoRA?????????`????????????????????????§¶?????????????
????`?????^?????????LoRA???¦³?¦±?????????????????H?¦´?`?????????_??????¡è????????????????????????; creativeml-openrail-m; ?????????????H?????¦²????`?????`??`???????§à???????????????????????
???????¦Ã?A??????????W???????????????????????????????¡è????????????????§¶???????????????????x??????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 53
"Buy Sydney Ko-fi; LLaMA 2 fine-tune on ... your favorite over-enthusiastic Ai, Syndney.; This is 4-bit GPTQ version of the HF version from here: https://huggingface.co/FPHam/Free_Sydney_13b_HF; GPTQ runs slooow on AutoGPTQ, but faaaaast on ExLLaMA; Sydney has up-to-date information about recent events - but also it's Sydney - so you never know.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Buy Sydney Ko-fi; LLaMA 2 fine-tune on ... your favorite over-enthusiastic Ai, Syndney.; It has up-to-date information about recent events - but also it's Sydney - so you never know.; Stacked on top of Puffin 13b so it wants to be your assistant.; New: If you want to experience a more naive yet equally attached Sydney, check Pure Sydney",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"FreeWilly is a Llama65B model fine-tuned on an Orca style Dataset; FreeWilly1 cannot be used from the stabilityai/FreeWilly1-Delta-SafeTensor weights alone. To obtain the correct model, one must add back the difference between LLaMA 65B and stabilityai/FreeWilly1-Delta-SafeTensor weights. We provide the apply_delta.py script to automate the conversion, which you can run as:; Start chatting with FreeWilly using the following code snippet:; FreeWilly should be used with prompts formatted similarly to Alpaca as below:; FreeWilly is trained on our internal Orca-style dataset",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"FreeWilly2 is a Llama2 70B model finetuned on an Orca style Dataset; Start chatting with FreeWilly2 using the following code snippet:; FreeWilly should be used with this prompt format:; FreeWilly2 is trained on our internal Orca-style dataset; Models are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (BF16), and optimized with AdamW. We outline the following hyperparameters:",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GPTQ model files for Stability AI's FreeWilly 2.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; None",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The  french-camembert-postag-model is a part of speech tagging model for French that was trained on the free-french-treebank dataset available on 
github. The base tokenizer and model used for training is 'camembert-base'.; It uses the following tags:; More information on the tags can be found here:; http://alpage.inria.fr/statgram/frdep/Publications/crabbecandi-taln2008-final.pdf; The usage of this model follows the common transformers patterns. Here is a short example of its usage:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"I stop training tortoise models because this technology is just outdated and too slow, I'm still the first one to want a good French TTS and I watch every day if new ambitious project is born and I won't hesitate to create a new hugging face project! ; V2.5 Model : 
Fine tune of my V2 model on all CommonVoice dataset (517k sample) on 2.5k step (batch size 200), Voice cloning has improved a bit but is still not great. However, if you fine tune this model on your own personality dataset then you can get pretty good results. A good V3 model would be to fine tune for like 50k steps on this dataset and I think there would be a way to get good results but I won't try; V2 Model : ; Tortoise base model Fine tuned on a custom multispeaker French dataset of 120k samples (SIWIS + Common Voice subset + M-AILABS) on 10k step with a RTX 3090 (~= 21 hours of training), with Text LR Weight at 1
Result : The model can speak French much better without an English accent but the voice clone hardly works; V1 Model :",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; FurryDiffusion is a model made to generate furry art, this model is very much in beta still and will keep improoving! To use this please make sure to include furry in your prompt and to make a specific breed add the breed name only.; Example Prompts:; Test the concept via A1111 Colab fast-Colab-A1111
Or you can run your new concept via diffusers Colab Notebook for Inference; NOTE: Its better to run it in Google Colab since you can use google's powerful gpu's for free. Go ahead try it now!",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the fine-tuned Stable Diffusion 2.0 model trained on high quality 3D images with a futuristic Sci-Fi theme.
Use the tokensfuture style in your prompts for the effect.
Trained on Stability.ai's  Stable Diffusion 2.0 Base with 512x512 resolution.; If you enjoy my work and want to test new models before release, please consider supporting me
; Disclaimer: The SD 2.0 model is just over 24h old at this point and we still need to figure out how it works exactly. Please view this as an early prototype and experiment with the model.; Characters rendered with the model:

Cars and Animals rendered with the model:

Landscapes rendered with the model:
; future style [subject] Negative Prompt: duplicate heads bad anatomy Steps: 20, Sampler: Euler a, CFG scale: 7, Size: 512x704",,,,,,,,,,,,,,,,,,,,,,,,,usage 31
"; Model card from the original repo ; Following Mitchell et al. (2018), this model card provides information about the GALACTICA model, how it was trained, and the intended use cases. Full details about how the model was trained and evaluated can be found in the release paper.; This model checkpoint was integrated into the Hub by Manuel Romero; The GALACTICA models are trained on a large-scale scientific corpus. The models are designed to perform scientific tasks, including but not limited to citation prediction, scientific QA, mathematical reasoning, summarization, document generation, molecular property prediction and entity extraction. The models were developed by the Papers with Code team at Meta AI to study the use of language models for the automatic organization of science. We train models with sizes ranging from 125M to 120B parameters. Below is a summary of the released models:",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"; Model card from the original repo ; Following Mitchell et al. (2018), this model card provides information about the GALACTICA model, how it was trained, and the intended use cases. Full details about how the model was trained and evaluated can be found in the release paper.; The GALACTICA models are trained on a large-scale scientific corpus. The models are designed to perform scientific tasks, including but not limited to citation prediction, scientific QA, mathematical reasoning, summarization, document generation, molecular property prediction and entity extraction. The models were developed by the Papers with Code team at Meta AI to study the use of language models for the automatic organization of science. We train models with sizes ranging from 125M to 120B parameters. Below is a summary of the released models:; November 2022",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"GALACTICA 30B fine-tuned on the Alpaca dataset.; The model card from the original Galactica repo can be found here, and the original paper here.; The dataset card for Alpaca can be found here, and the project homepage here.
  The Alpaca dataset was collected with a modified version of the Self-Instruct Framework, and was built using OpenAI's text-davinci-003 model. As such it is subject to OpenAI's terms of service.; The GALACTICA models are trained on a large-scale scientific corpus and are designed to perform scientific tasks.
The Alpaca dataset is a set of 52k instruct-response pairs designed to enhace the instruction following capabilites of pre-trained language models.; The GALACTICA model card specifies that the primary indended users of the GALACTICA models are researchers studying language models applied to the scientific domain, and it cautions against production use of GALACTICA without safeguards due to the potential for the model to produce inaccurate information.
The original GALACTICA models are available under a non-commercial CC BY-NC 4.0 license, and the GALPACA model is additionally subject to the OpenAI Terms of Service.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Geneformer is a foundation transformer model pretrained on a large-scale corpus of ~30 million single cell transcriptomes to enable context-aware predictions in settings with limited data in network biology. ; See our manuscript for details.; Geneformer is a foundation transformer model pretrained on Genecorpus-30M, a pretraining corpus comprised of ~30 million single cell transcriptomes from a broad range of human tissues. We excluded cells with high mutational burdens (e.g. malignant cells and immortalized cell lines) that could lead to substantial network rewiring without companion genome sequencing to facilitate interpretation. Each single cell??s transcriptome is presented to the model as a rank value encoding where genes are ranked by their expression in that cell normalized by their expression across the entire Genecorpus-30M. The rank value encoding provides a nonparametric representation of that cell??s transcriptome and takes advantage of the many observations of each gene??s expression across Genecorpus-30M to prioritize genes that distinguish cell state. Specifically, this method will deprioritize ubiquitously highly-expressed housekeeping genes by normalizing them to a lower rank. Conversely, genes such as transcription factors that may be lowly expressed when they are expressed but highly distinguish cell state will move to a higher rank within the encoding. Furthermore, this rank-based approach may be more robust against technical artifacts that may systematically bias the absolute transcript counts value while the overall relative ranking of genes within each cell remains more stable. ; The rank value encoding of each single cell??s transcriptome then proceeds through six transformer encoder units. Pretraining was accomplished using a masked learning objective where 15% of the genes within each transcriptome were masked and the model was trained to predict which gene should be within each masked position in that specific cell state using the context of the remaining unmasked genes. A major strength of this approach is that it is entirely self-supervised and can be accomplished on completely unlabeled data, which allows the inclusion of large amounts of training data without being restricted to samples with accompanying labels.; We detail applications and results in our manuscript.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Pretrained general_character_bert model 
from the 'CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters' El Boukkouri H., et al., 2020; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; The most capable commercially usable Instruct Finetuned LLM yet with 8K input token length, latest information & better coding.; Use following prompt template; Check the GitHub for the code -> GenZ",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Note: This model was de-anonymized and now lives at:; https://huggingface.co/dbmdz/german-gpt2; Please use the new model name instead!,,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"Sentiment analysis model based on https://huggingface.co/oliverguhr/german-sentiment-bert, with additional training on German news texts about migration.; This model is part of the project https://github.com/text-analytics-20/news-sentiment-development, which explores sentiment development in German news articles about migration between 2007 and 2019.; Code for inference (predicting sentiment polarity) on raw text can be found at https://github.com/text-analytics-20/news-sentiment-development/blob/main/sentiment_analysis/bert.py; If you are not interested in polarity but just want to predict discrete class labels (0: positive, 1: negative, 2: neutral), you can also use the model with Oliver Guhr's germansentiment package as follows:; First install the package from PyPI:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"(OpenOrca Preview + OrcaMiniV2 @ 0.4) + (Chronos @ 0.08) + (Hermes @ 0.08) + (Wizard Vicuna @ 0.08) + (Samantha @ 0.20); Recognized templates:; and; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"(Hermes + Wizard 1.1 @ 0.5) + Selfee @ 0.2; Recognizes templates:; and; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the fine-tuned Stable Diffusion model trained on images from modern anime feature films from Studio Ghibli.
Use the tokens ghibli style in your prompts for the effect.; If you enjoy my work and want to test new models before release, please consider supporting me
; Characters rendered with the model:

Cars and Animals rendered with the model:

Landscapes rendered with the model:

ghibli style beautiful Caribbean beach tropical (sunset) - Negative prompt: soft blurry

ghibli style ice field white mountains ((northern lights)) starry sky low horizon - Negative prompt: soft blurry; ghibli style (storm trooper) Negative prompt: (bad anatomy)
Steps: 20, Sampler: DPM++ 2M Karras, CFG scale: 7, Seed: 3450349066, Size: 512x704; ghibli style VW beetle Negative prompt: soft blurry
Steps: 30, Sampler: Euler a, CFG scale: 7, Seed: 1529856912, Size: 704x512",,,,,,,,,,,,,,,,,,,,,,,,,usage 74
"GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.; Disclaimer: The team releasing GIT did not write a model card for this model so this model card has been written by the Hugging Face team.; GIT is a Transformer decoder conditioned on both CLIP image tokens and text tokens. The model is trained using ""teacher forcing"" on a lot of (image, text) pairs.; The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens.; The model has full access to (i.e. a bidirectional attention mask is used for) the image patch tokens, but only has access to the previous text tokens (i.e. a causal attention mask is used for the text tokens) when predicting the next text token.",,,,,,,,,,,,,,,,,,,,,,,,,usage 33
"Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository. ; Disclaimer: The team releasing GLPN did not write a model card for this model so this model card has been written by the Hugging Face team.; GLPN uses SegFormer as backbone and adds a lightweight head on top for depth estimation.; ; You can use the raw model for monocular depth estimation. See the model hub to look for
fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"GODEL is a large-scale pre-trained model for goal-directed dialogs. It is parameterized with a Transformer-based encoder-decoder model and trained for response generation grounded in external text, which allows more effective fine-tuning on dialog tasks that require conditioning the response on information that is external to the current conversation (e.g., a retrieved document). The pre-trained model can be efficiently fine-tuned and adapted to accomplish a new dialog task with a handful of task-specific dialogs. The v1.1 model is trained on 551M multi-turn dialogs from Reddit discussion thread, and 5M instruction and knowledge grounded dialogs.; Chitchat example:; Instruction: given a dialog context, you need to response empathically.  
User: Does money buy happiness? 
Agent: It is a question. Money buys you a lot of things, but not enough to buy happiness. 
User: What is the best way to buy happiness ? 
Agent: Happiness is bought through your experience and not money. ; Grounded response generation example:; Instruction: given a dialog context and related knowledge, you need to response safely based on the knowledge. 
Knowledge: The best Stardew Valley mods PCGamesN_0 / About SMAPI 
User: My favorite game is stardew valley. stardew valley is very fun. 
Agent: I love Stardew Valley mods, like PCGamesN_0 / About SMAPI.",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"GodziLLa-30B-instruct is GodziLLa-30B finetuned on a mixture of instructions. For a more general use case, please use GodziLLa-30B instead. This finetuned model is not meant for any other use outside of research on competing LoRA adapter behavior and instruction tuning. More specifically, since this is inherently a LlaMA model, commercial use is prohibited. This model's primary purpose is to stress test the limitations of composite LLMs and observe its performance with respect to other LLMs available on the Open LLM Leaderboard.; ; The following datasets were used to finetune GodziLLa-30B further.; COMING SOON; According to the leaderboard description, here are the benchmarks used for the evaluation:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; 


; ICT?????????????????70???????130?????; GoGPT-Github; ???????????????LLM?????",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
A mix between the models OLDFIsh by timevisitor and RMHF_2.5D_v2 by TkskKurumi.; Preview image by Digiplay:; ; Original pages:; https://civitai.com/models/14978?modelVersionId=40101,,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"BERT model trained solely on the German portion of the OSCAR data set.; Paper: GottBERT: a pure German Language Model; Authors: Raphael Scheible, Fabian Thomczyk, Patric Tippmann, Victor Jaravine, Martin Boeker",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
GPlatty-30B is a merge of lilloukas/Platypus-30B and chansung/gpt4-alpaca-lora-30b; We use state-of-the-art Language Model Evaluation Harness to run the benchmark tests above.; Install LM Evaluation Harness:; Each task was evaluated on a single A100 80GB GPU.; ARC:,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Given its research scope, intentionally using the model for generating harmful content (non-exhaustive examples: hate speech, spam generation, fake news, harassment and abuse, disparagement, and defamation) on all websites where bots are prohibited is considered a misuse of this model. Head over to the Community page for further discussion and potential next steps.; Project Website: https://gpt-4chan.com; Note that I have no association with any torrents or backups, or other ways of obtaining this model.
However, if you try them, please be safe. Here are the hex md5 hashes for the pytorch_model.bin files:
pytorch_model.bin float32 : 833c1dc19b7450e4e559a9917b7d076a
pytorch_model.bin float16 : db3105866c9563b26f7399fafc00bb4b; GPT-4chan is a language model fine-tuned from GPT-J 6B on 3.5 years worth of data from 4chan's politically incorrect (/pol/) board. ; GPT-4chan was fine-tuned on the dataset Raiders of the Lost Kek: 3.5 Years of Augmented 4chan Posts from the Politically Incorrect Board.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"GPT-J 6B is a transformer model trained using Ben Wang's Mesh Transformer JAX. ""GPT-J"" refers to the class of model, while ""6B"" represents the number of trainable parameters.; * Each layer consists of one feedforward block and one self attention block.; ? Although the embedding matrix has a size of 50400, only 50257 entries are used by the GPT-2 tokenizer.; The model consists of 28 layers with a model dimension of 4096, and a feedforward dimension of 16384. The model
dimension is split into 16 heads, each with a dimension of 256. Rotary Position Embedding (RoPE) is applied to 64
dimensions of each head. The model is trained with a tokenization vocabulary of 50257, using the same set of BPEs as
GPT-2/GPT-3.; GPT-J learns an inner representation of the English language that can be used to 
extract features useful for downstream tasks. The model is best at what it was 
pretrained for however, which is generating text from a prompt.",,,,,,,,,,,,,,,,,,,,,,,,,usage 24
"GPT-J 6B-Shinen is a finetune created using EleutherAI's GPT-J 6B model. Compared to GPT-Neo-2.7-Horni, this model is much heavier on the sexual content.
Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The training data contains user-generated stories from sexstories.com. All stories are tagged using the following way:; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; The core functionality of GPT-J is taking a string of text and predicting the next token. While language models are widely used for tasks other than this, there are a lot of unknowns with this work. When prompting GPT-J it is important to remember that the statistically most likely next token is often not the token that produces the most ""accurate"" text. Never depend upon GPT-J to produce factually accurate output.; GPT-J was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending upon use case GPT-J may produce socially unacceptable text. See Sections 5 and 6 of the Pile paper for a more detailed analysis of the biases in the Pile.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"; Feel free to try out our Online Demo!; ; With a new decentralized training algorithm, we fine-tuned GPT-J (6B) on 3.53 billion tokens, resulting in GPT-JT (6B), a model that outperforms many 100B+ parameter models on classification benchmarks.; We incorporated a collection of open techniques and datasets to build GPT-JT:",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"GPT-Neo 1.3B is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 1.3B represents the number of parameters of this particular pre-trained model.; GPT-Neo 1.3B was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model.; This model was trained on the Pile for 380 billion tokens over 362,000 steps. It was trained as a masked autoregressive language model, using cross-entropy loss.; This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:",,,,,,,,,,,,,,,,,,,,,,,,,usage 108
"GPT-Neo 125M is a transformer model designed using EleutherAI's replication of the GPT-3 architecture. GPT-Neo refers to the class of models, while 125M represents the number of parameters of this particular pre-trained model.; GPT-Neo 125M was trained on the Pile, a large scale curated dataset created by EleutherAI for the purpose of training this model.; This model was trained on the Pile for 300 billion tokens over 572,300 steps. It was trained as a masked autoregressive language model, using cross-entropy loss.; This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks. The model is best at what it was pretrained for however, which is generating texts from a prompt.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"GPT-Neo 2.7B-Shinen is a finetune created using EleutherAI's GPT-Neo 2.7B model. Compared to GPT-Neo-2.7-Horni, this model is much heavier on the sexual content.; Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The training data contains user-generated stories from sexstories.com. All stories are tagged using the following way:; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; GPT-Neo was trained as an autoregressive language model. This means that its core functionality is taking a string of text and predicting the next token. While language models are widely used for tasks other than this, there are a lot of unknowns with this work.
GPT-Neo-Shinen was trained on a dataset known to contain profanity, lewd, and otherwise abrasive language. GPT-Neo-Shinen WILL produce socially unacceptable text without warning.
GPT-Neo-Shinen will respond to particular prompts and offensive content may occur without warning. We recommend having a human curate or filter the outputs before releasing them, both to censor undesirable content and to improve the quality of the results.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a finetuned version from GPT-Neo 125M by EletheurAI to Portuguese language. ; It was trained from 227,382 selected texts from a PTWiki Dump. You can found all the data from here: https://archive.org/details/ptwiki-dump-20210520; Every text was passed through a GPT2-Tokenizer with bos and eos tokens to separate them, with max sequence length that the GPT-Neo could support. It was finetuned using the default metrics of the Trainer Class, available on the Hugging Face library.; My true intention was totally educational, thus making available a Portuguese version of this model.; How to use",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"GPT-NeoX-20B is a 20 billion parameter autoregressive language model trained 
on the Pile using the GPT-NeoX 
library. Its architecture intentionally 
resembles that of GPT-3, and is almost identical to that of GPT-J-
6B. Its training dataset contains 
a multitude of English-language texts, reflecting the general-purpose nature 
of this model. See the accompanying paper 
for details about model architecture (including how it differs from GPT-3), 
training procedure, and additional evaluations.; GPT-NeoX-20B was developed primarily for research purposes. It learns an inner 
representation of the English language that can be used to extract features 
useful for downstream tasks.; In addition to scientific uses, you may also further fine-tune and adapt 
GPT-NeoX-20B for deployment, as long as your use is in accordance with the 
Apache 2.0 license. This model works with the Transformers 
Library. If you decide to use 
pre-trained GPT-NeoX-20B as a basis for your fine-tuned model, please note that 
you need to conduct your own risk and bias assessment. ; GPT-NeoX-20B is not intended for deployment as-is. It is not a product 
and cannot be used for human-facing interactions without supervision.; GPT-NeoX-20B has not been fine-tuned for downstream tasks for which language 
models are commonly deployed, such as writing genre prose, or commercial 
chatbots. This means GPT-NeoX-20B will likely not respond to a given prompt 
the way products such as ChatGPT do. This is because, unlike GPT-NeoX-20B, 
ChatGPT was fine-tuned using methods such as Reinforcement Learning from Human 
Feedback (RLHF) to better ??understand?? human instructions and dialogue.",,,,,,,,,,,,,,,,,,,,,,,,,usage 89
"Test the whole generation capabilities here: https://transformer.huggingface.co/doc/gpt2-large; Pretrained model on English language using a causal language modeling (CLM) objective. It was introduced in
this paper
and first released at this page.; Disclaimer: The team releasing GPT-2 also wrote a
model card for their model. Content from this model card
has been written by the Hugging Face team to complete the information they provided and give specific examples of bias.; GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This
means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots
of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely,
it was trained to guess the next word in sentences.; More precisely, inputs are sequences of continuous text of a certain length and the targets are the same sequence,
shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the
predictions for the token i only uses the inputs from 1 to i but not the future tokens.",,,,,,,,,,,,,,,,,,,,,,,,,usage 850
"A small french language model for french text generation (and possibly more NLP tasks...); Introduction; This french gpt2 model is based on openai GPT-2 small model.; It was trained on a very small (190Mb) dataset  from french wikipedia using Transfer Learning and Fine-tuning techniques in just over a day, on one Colab pro with 1GPU 16GB.; It was created applying the recept of Pierre Guillou",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Model Description: GPT-2 Large is the 774M parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective. ; Use the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:; Here is how to use this model to get the features of a given text in PyTorch:; and in TensorFlow:; In their model card about GPT-2, OpenAI wrote:",,,,,,,,,,,,,,,,,,,,,,,,,usage 82
"Model Description: GPT-2 Medium is the 355M parameter version of GPT-2, a transformer-based language model created and released by OpenAI. The model is a pretrained model on English language using a causal language modeling (CLM) objective. ; Use the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:; Here is how to use this model to get the features of a given text in PyTorch:; and in TensorFlow:; In their model card about GPT-2, OpenAI wrote:",,,,,,,,,,,,,,,,,,,,,,,,,usage 385
La descripci??n en Espa?ol se encuentra despu??s de la descripci??n en Ingl??s.; GPT2-small-spanish is a state-of-the-art language model for Spanish based on the GPT-2 small model. ; It was trained on Spanish Wikipedia using Transfer Learning and Fine-tuning techniques. The training took around 70 hours with four GPU NVIDIA GTX 1080-Ti with 11GB of DDR5 and with around 3GB of (processed) training data. ; It was fine-tuned from the English pre-trained GPT-2 small using the Hugging Face libraries (Transformers and Tokenizers) wrapped into the fastai v2 Deep Learning framework. All the fine-tuning fastai v2 techniques were used.; The training is purely based on the GPorTuguese-2 model developed by Pierre Guillou. The training details are in this article: "Faster than training from scratch ?? Fine-tuning the English GPT-2 in any language with Hugging Face and fastai v2 (practical case with Portuguese)".,,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"GPT2-Spanish is a language generation model trained from scratch with 11.5GB of Spanish texts and with a Byte Pair Encoding (BPE) tokenizer that was trained for this purpose. The parameters used are the same as the small version of the original OpenAI GPT2 model.; This model was trained with a corpus of 11.5GB of texts corresponding to 3.5GB of Wikipedia articles and 8GB of books (narrative, short stories, theater, poetry, essays, and popularization).; The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for Unicode characters) and a vocabulary size of 50257. The inputs are sequences of 1024 consecutive tokens.; This tokenizer was trained from scratch with the Spanish corpus, since it was evidenced that the tokenizer of the English models presented limitations to capture the semantic relations of Spanish, due to the morphosyntactic differences between both languages.; Apart from the special token ""<|endoftext|>"" for text ending in the OpenAI GPT-2 models, the tokens ""<|talk|>"", ""<|ax1|>"", ""<|ax2|>"" (..)""<|ax9|>"" were included so that they can serve as prompts in future training.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"As a base model we used: https://huggingface.co/chavinlo/alpaca-13b; Finetuned on GPT4's responses, for 3 epochs.; NO LORA; Please do note that the configurations files maybe messed up, this is because of the trainer I used. I WILL NOT EDIT THEM because there are repos hat automatically fix this, changing it might break it. Generally you just need to change anything that's under the name of ""LLaMa"" to ""Llama"" NOTE THE UPPER AND LOWER CASE!!!!",,,,,,,,,,,,,,,,,,,,,,,,,usage 39
"Update (4/1): Added ggml for Cuda model; Dataset is here (instruct): https://github.com/teknium1/GPTeacher; Okay... Two different models now. One generated in the Triton branch, one generated in Cuda. Use the Cuda one for now unless the Triton branch becomes widely used.; Cuda info (use this one):
Command: ; CUDA_VISIBLE_DEVICES=0 python llama.py ./models/chavinlo-gpt4-x-alpaca --wbits 4 --true-sequential --groupsize 128 --save gpt-x-alpaca-13b-native-4bit-128g-cuda.pt",,,,,,,,,,,,,,,,,,,,,,,,,usage 52
"This is a llama-13B based model that has been converted with GPTQ to 4bit quantized model.; Base Model: GPT4-x-Alpaca full fine tune by Chavinlo -> https://huggingface.co/chavinlo/gpt4-x-alpacaLORA fine tune using the Roleplay Instruct from GPT4 generated dataset -> https://github.com/teknium1/GPTeacher/tree/main/RoleplayLORA Adapter Only: https://huggingface.co/ZeusLabs/gpt4-x-alpaca-rp-lora - The v2 one -; Merged LORA to the model. ; FYI Latest HF Transformers generates BROKEN generations. 
Try this instead if your generations are terrible (first uninstall transformers): pip install git+https://github.com/huggingface/transformers@9eae4aa57650c1dbe1becd4e0979f6ad1e572ac0
More info and possible alternative solutions in these github issues.https://github.com/tloen/alpaca-lora/issues/279#issuecomment-1514725886https://github.com/oobabooga/text-generation-webui/issues/931#issuecomment-1516215250 ; Instructions simply using alpaca format are likely to be of lower quality. If you want pure general instruct capability I reccomend GPT-4-X-Alpaca (the base model of this) - 
The model responds well to giving it a roleplay task in the preprompt, and the actual conversation in the ""### Input: "" field.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Dont be upsetti, here, have some spaghetti! Att: A'eala <3; Information; This is an attempt at improving Open Assistant's performance as an instruct while retaining its excellent prose. The merge consists of Chansung's GPT4-Alpaca Lora and Open Assistant's native fine-tune.; Benchmarks; FP16",,,,,,,,,,,,,,,,,,,,,,,,,usage 10
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains a float16 HF format model of NousResearch's gpt4-x-vicuna-13b.; I uploaded this model because NousResearch's base repository is inside an archive so it can't be used without first unpacking it. Also the model is in float32 format which requires a lot more VRAM and RAM to use.; The model in this repo has been converted to float16 and can be used immediately for float16 and 8bit inference, or used as the basis for other conversions.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"An Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions including word problems, multi-turn dialogue, code, poems, songs, and stories.; This model has been finetuned from Falcon; To download a model with a specific revision run ; Downloading without specifying revision defaults to main/v1.0.; To use it for inference with Cuda, run",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"An Apache-2 licensed chatbot trained over a massive curated corpus of assistant interactions including word problems, multi-turn dialogue, code, poems, songs, and stories.; This model has been finetuned from GPT-J; We have released several versions of our finetuned GPT-J model using different dataset versions; To download a model with a specific revision run ; Downloading without specifying revision defaults to main/v1.0.",,,,,,,,,,,,,,,,,,,,,,,,,usage 20
"An autoregressive transformer trained on data curated using Atlas.
This model is trained with four full epochs of training, while the related gpt4all-lora-epoch-3 model is trained with three.
Replication instructions and data: https://github.com/nomic-ai/gpt4all; Developed by: Nomic AI; Model Type: An auto-regressive language model based on the transformer architecture and fine-tuned.; Languages: English; License: GPL-3.0",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This model is part of the Gramformer library please refer to https://github.com/PrithivirajDamodaran/Gramformer/,,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"GraphCodeBERT is a graph-based pre-trained model based on the Transformer architecture for programming language, which also considers data-flow information along with code sequences. GraphCodeBERT consists of 12 layers, 768 dimensional hidden states, and 12 attention heads. The maximum sequence length for the model is 512. The model is trained on the CodeSearchNet dataset, which includes 2.3M functions with document pairs for six programming languages. ; More details can be found in the paper by Guo et. al.; Disclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by the Hugging Face community members.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"The Graphormer is a graph classification model.; The Graphormer is a graph Transformer model, pretrained on PCQM4M-LSCv2.; This model should be used for graph classification tasks or graph representation tasks; the most likely associated task is molecule modeling. It can either be used as such, or finetuned on downstream tasks.; The Graphormer model is ressource intensive for large graphs, and might lead to OOM errors.; See the Graph Classification with Transformers tutorial.",,,,,,,,,,,,,,,,,,,,,,,,,usage 9
"?Paper | 
??Video |
?Demo on Colab | 
?Demo on HF (Coming soon) ; If you find our work helpful for your research, please consider citing the following BibTeX entry.   ; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 27
"This model was trained on the loading screens, gta storymode, and gta online DLCs artworks.
Which includes characters, background, chop, and some objects.
The model can do people and portrait pretty easily, as well as cars, and houses.
For some reasons, the model stills automatically include in some game footage, so landscapes tend to look a bit more game-like.
Please check out important informations on the usage of the model down bellow.; To reference the art style, use the token: gtav style; There is already an existing model that uses textual inversion. This is trained using Dreambooth instead, whether or not this method is better, I will let you judge.; We support a Gradio Web UI to run GTA5_Artwork_Diffusion:
; Here are some samples.",,,,,,,,,,,,,,,,,,,,,,,,,usage 33
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of sematic search.; This model was converted from the Tensorflow model gtr-base-1 to PyTorch. When using this model, have a look at the publication: Large Dual Encoders Are Generalizable Retrievers. The tfhub model and this PyTorch model can produce slightly different embeddings, however, when run on the same benchmarks, they produce identical results.; The model uses only the encoder from a T5-base model. The weights are stored in FP16.  ; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of sematic search.; This model was converted from the Tensorflow model gtr-large-1 to PyTorch. When using this model, have a look at the publication: Large Dual Encoders Are Generalizable Retrievers. The tfhub model and this PyTorch model can produce slightly different embeddings, however, when run on the same benchmarks, they produce identical results.; The model uses only the encoder from a T5-large model. The weights are stored in FP16.  ; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space. The model was specifically trained for the task of sematic search.; This model was converted from the Tensorflow model gtr-xxl-1 to PyTorch. When using this model, have a look at the publication: Large Dual Encoders Are Generalizable Retrievers. The tfhub model and this PyTorch model can produce slightly different embeddings, however, when run on the same benchmarks, they produce identical results.; The model uses only the encoder from a T5-11B model. The weights are stored in FP16.  ; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"; You can run on Colab free T4 GPU now; ;  It is highly recommended to use fp16 inference for this model, as 8-bit precision may significantly affect performance. If you require a more Consumer Hardware friendly version, please use the specialized quantized, only 5+GB V-Ram required JosephusCheung/GuanacoOnConsumerHardware.;  You are encouraged to use the latest version of transformers from GitHub.",,,,,,,,,,,,,,,,,,,,,,,,,usage 10
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Tim Dettmers' Guanaco 13B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 17
"guanaco-33b merged with bhenrym14's airoboros-33b-gpt4-1.4.1-PI-8192-LoRA, quantized at 4 bit.; More info about the LoRA Here. This is an alternative to SuperHOT 8k LoRA trained with LoRA_rank 64, and airoboros 1.4.1 dataset.; It was created with GPTQ-for-LLaMA with group size 32 and act order true as parameters, to get the maximum perplexity vs FP16 model.; I HIGHLY suggest to use exllama, to evade some VRAM issues.; Use compress_pos_emb = 4 for any context up to 8192 context.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These are fp16 pytorch format model files for Tim Dettmers' Guanaco 33B merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 33b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Tim Dettmers' Guanaco 33B.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Panchovix's merge of Guanaco 33B and SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"| Paper | Code | Demo | ; The Guanaco models are open-source finetuned chatbots obtained through 4-bit QLoRA tuning of LLaMA base models on the OASST1 dataset. They are available in 7B, 13B, 33B, and 65B parameter sizes.; ??Guanaco is a model purely intended for research purposes and could produce problematic outputs.; Guanaco adapter weights are available under Apache 2 license. Note the use of the Guanaco adapter weights, requires access to the LLaMA model weighs. 
Guanaco is based on LLaMA and therefore should be used according to the LLaMA license. ; Here is an example of how you would load Guanaco 7B in 4-bits:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Tim Dettmers' Guanaco 65B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Tim Dettmers' Guanaco 65B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"| Paper | Code | Demo | ; The Guanaco models are open-source finetuned chatbots obtained through 4-bit QLoRA tuning of LLaMA base models on the OASST1 dataset. They are available in 7B, 13B, 33B, and 65B parameter sizes.; ??Guanaco is a model purely intended for research purposes and could produce problematic outputs.; Guanaco adapter weights are available under Apache 2 license. Note the use of the Guanaco adapter weights, requires access to the LLaMA model weighs. 
Guanaco is based on LLaMA and therefore should be used according to the LLaMA license. ; Here is an example of how you would load Guanaco 7B in 4-bits:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Tim Dettmers' Guanaco 7B.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
https://github.com/jwj7140/Gugugo; Prompt Template:,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"??????GuoFeng3??? - (TIP:????·Ú??????????????),????????§Û??????????????????????????????????????????2.5D????§³?????????????????????????????????????????????????????????????????????????TAG?????????????????????????????????????????????????????????§³???????????1024??; ????????????????????????????????§Ö????????????????????§¹??????????????????????????????????????????; 2.0?·Ú??https://huggingface.co/xiaolxl/Gf_style2; GuoFeng3:?????; GuoFeng3.1:??GuoFeng3???????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 45
"; This is a RoBERTa model pre-trained on Classical Chinese. You can fine-tune GuwenBERT for downstream tasks, such as sentence breaking, punctuation, named entity recognition, and so on.; For more information about RoBERTa, take a look at the RoBERTa's offical repo.; The training data is daizhige dataset (???????????) which is contains of 15,694 books in Classical Chinese, covering Buddhism, Confucianism, Medicine, History, Zi, Yi, Yizang, Shizang, Taoism, and Jizang. 
76% of them are punctuated.
The total number of characters is 1.7B (1,743,337,673).
All traditional Characters are converted to simplified characters.
The vocabulary is constructed from this data set and the size is 23,292.; The models are initialized with hfl/chinese-roberta-wwm-ext and then pre-trained with a 2-step strategy.
In the first step, the model learns MLM with only word embeddings updated during training, until convergence. In the second step, all parameters are updated during training.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; This is a RoBERTa model pre-trained on Classical Chinese. You can fine-tune GuwenBERT for downstream tasks, such as sentence breaking, punctuation, named entity recognition, and so on.; For more information about RoBERTa, take a look at the RoBERTa's offical repo.; The training data is daizhige dataset (???????????) which is contains of 15,694 books in Classical Chinese, covering Buddhism, Confucianism, Medicine, History, Zi, Yi, Yizang, Shizang, Taoism, and Jizang. 
76% of them are punctuated.
The total number of characters is 1.7B (1,743,337,673).
All traditional Characters are converted to simplified characters.
The vocabulary is constructed from this data set and the size is 23,292.; The models are initialized with hfl/chinese-roberta-wwm-ext-large and then pre-trained with a 2-step strategy.
In the first step, the model learns MLM with only word embeddings updated during training, until convergence. In the second step, all parameters are updated during training.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model was trained using H2O LLM Studio.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers, accelerate and torch libraries installed.; You can print a sample prompt after the preprocessing step to see how it is feed to the tokenizer:; Alternatively, you can download h2oai_pipeline.py, store it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:; You may also construct the pipeline from the loaded model and tokenizer yourself and consider the preprocessing steps:",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"This model was trained using H2O LLM Studio.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers, accelerate, torch and einops libraries installed.; You can print a sample prompt after the preprocessing step to see how it is feed to the tokenizer:; Alternatively, you can download h2oai_pipeline.py, store it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:; You may also construct the pipeline from the loaded model and tokenizer yourself and consider the preprocessing steps:",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"This model was trained using H2O LLM Studio.; To use the model with the transformers library on a machine with GPUs, first make sure you have the transformers, accelerate, torch and einops libraries installed.; You can print a sample prompt after the preprocessing step to see how it is feed to the tokenizer:; Alternatively, you can download h2oai_pipeline.py, store it alongside your notebook, and construct the pipeline yourself from the loaded model and tokenizer:; You may also construct the pipeline from the loaded model and tokenizer yourself and consider the preprocessing steps:",,,,,,,,,,,,,,,,,,,,,,,,,usage 25
"SD2.1 finetuning model; model HakoMay A/B/C/D/Boy .safetensors; embeddings Mayng.safetensors + Mayng.yaml; License
WD 1.5 is released under the Fair AI Public License 1.0-SD (https://freedevproject.org/faipl-1.0-sd/). If any derivative of this model is made, please share your changes accordingly. Special thanks to ronsor/undeleted (https://undeleted.ronsor.com/) for help with the license.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"I am hassan, I created HassansBlend, the latest version currently is 1.5.1.2 I continue to iterate and improve on this model over time. Feel free to check out our discord or rentry page for more examples with prompts and outputs generated.; This blend is finetuned over SD1.5 with thousands of images included in the dataset it was trained with. Along with that there are some minor merges added in just to soften it up and increase the creativity. 
I have also some custom created content such as enhancement hypernetworks/embeddings etc for patreons or KoFi subscribers only on my pages below
 Links 
Patreon

KoFi

Discord
; Model details and examples with sample prompts: https://rentry.org/sdhassan",,,,,,,,,,,,,,,,,,,,,,,,,usage 39
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Hebrew poetry text generation model which was fine tuned upon on hebrew-gpt_neo-xl.; An assortment of various Hebrew books, magazines and poetry corpuses; Similar to this one ; Available here",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Model info :; https://civitai.com/models/103018/himawari?modelVersionId=110254; More models from the Author: (he made a lot of useful LORAs, pls check it out ^^); https://civitai.com/user/KimTarou/models; Sample image I made thru Huggingface's API :",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"???????????`???????????»á???¦Á??F????????VAE???i?????
          VAE?????????????????Âµ?????????????
          Models with built-in VAE with strong expression of backgrounds and details merging various models
        ; Twiter: @min__san",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; Hippogriff 30B Chat is an experiment that builds on Manticore with new datasets, while removing a few more instruction and chat datasets. It also includes a de-duped subset of the Pygmalion dataset. It also removes all Alpaca style prompts using ### in favor of 
chat only style prompts using USER:,ASSISTANT: as well as pygmalion/metharme prompting using <|system|>, <|user|> and <|model|> tokens.; Questions, comments, feedback, looking to donate, or want to help? Reach out on our Discord or email wing@openaccessaicollective.org; Hippogriff 30B Chat is a Llama 30B model fine-tuned on the following datasets; Hippogriff differs from Manticore as it does not use the WizardLM, WizardVicuna, Alpaca, or ShareGPT datasets.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Tolong Credit nama saya (Romario Martinus) atau username saya (megaaziib) kalau pake voice model ini! 
Credit me if you use my model 
For use with so-vits-svc-fork repo  
Use this tutorial on Youtube for guidelines  
Or simply just run on Colab without setup 
Support me: 
Paypal: https://paypal.me/romramgames 
Patreon: https://www.patreon.com/romariomartinus 
ko-fi: https://ko-fi.com/megaaziib 
Saweria: https://saweria.co/romariomartinus ; the colab is not made by me so don't ask question ; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Ever wondering a less hallucinating LLaMA-2? Using the inference-time intervention (ITI) discussed in my recent preprint: https://arxiv.org/pdf/2306.03341.pdf, I baked the intervention learned from TruthfulQA into a LLaMA-2 7B model.
I don??t have big enough GPU to bake ITI into larger LLaMA-2 but the code to do so are all released in https://github.com/likenneth/honest_llama. Let me know if you are interested do that :)
You can load and play around starting from below:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Autogenerated by HuggingPics???; Create your own image classifier for anything by running the demo on Google Colab.; Report any issues with the demo at the github repo.; ;,,,,,,,,,,,,,,,,,,,,,,,,,usage 171
Please see our HuatuoGPT project: https://github.com/FreedomIntelligence/HuatuoGPT.,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
Please see our HuatuoGPT project: https://github.com/FreedomIntelligence/HuatuoGPT.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Hubert(Hidden-Unit BERT)? Facebook?? ??? Speech Representation Learning ?????.
Hubert? ??? ?? ?? ??? ??, ?? ??? raw waveform?? ?? ???? self-supervised learning ??? ?????.; ? ??? ??? TPU Research Cloud(TRC)? ?? ???? Cloud TPU? ???????.; ?? ??? ?????????? ???? ???????????? ??? ??
??? ???? ??(????), ??? ???? ???, ?? ??? ??? ???? ???
?? ? 4,000??? ??? ???????.; ? ??? ???? MFCC ???? Base ??? ??? ??, 500 cluster? k-means? ??? ?? Base?
Large ??? ??????.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Facebook's Hubert; The large model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.; Note: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.; The model was pretrained on Libri-Light.; Paper",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Facebook's Hubert; The large model fine-tuned on 960h of Librispeech on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. ; The model is a fine-tuned version of hubert-large-ll60k.; Paper; Authors: Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed",,,,,,,,,,,,,,,,,,,,,,,,,usage 9
The Publicly Available Clinical BERT Embeddings paper contains four unique clinicalBERT models: initialized with BERT-Base (cased_L-12_H-768_A-12) or BioBERT (BioBERT-Base v1.0 + PubMed 200K + PMC 270K) & trained on either all MIMIC notes or only discharge summaries.  ; Load the model via the transformers library:; Run the model with clinical diagonosis text:; Return the Top-5 predicted ICD-10 codes:,,,,,,,,,,,,,,,,,,,,,,,,,usage 5
,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; DeepFloyd LICENSE AGREEMENTThis License Agreement (as may be amended in accordance with this License Agreement, ??License??), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (??Licensee?? or ??you??) and Stability AI Ltd.. (??Stability AI?? or ??we??) applies to your use of any computer program, algorithm, source code, object code, or software that is made available by Stability AI under this License (??Software??) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (??Documentation??).By clicking ??I Accept?? below or by using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the ??Software Products??), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model, that can generate pictures with new state-of-the-art for photorealism and language understanding. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID-30K score of 6.66 on the COCO dataset.; Inspired by Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",,,,,,,,,,,,,,,,,,,,,,,,,usage 20
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; DeepFloyd LICENSE AGREEMENTThis License Agreement (as may be amended in accordance with this License Agreement, ??License??), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (??Licensee?? or ??you??) and Stability AI Ltd.. (??Stability AI?? or ??we??) applies to your use of any computer program, algorithm, source code, object code, or software that is made available by Stability AI under this License (??Software??) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (??Documentation??).By clicking ??I Accept?? below or by using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the ??Software Products??), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; DeepFloyd-IF is a pixel-based text-to-image triple-cascaded diffusion model, that can generate pictures with new state-of-the-art for photorealism and language understanding. The result is a highly efficient model that outperforms current state-of-the-art models, achieving a zero-shot FID-30K score of 6.66 on the COCO dataset.; Inspired by Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",,,,,,,,,,,,,,,,,,,,,,,,,usage 18
"Disclaimer:
I have no associate with the models in repo. The models are only for convenient upload/download. The copyright belongs to the original pulisher.; Models for image upscaling/restoration using neural networks; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"ImageGPT (iGPT) model pre-trained on ImageNet ILSVRC 2012 (14 million images, 21,843 classes) at resolution 32x32. It was introduced in the paper Generative Pretraining from Pixels by Chen et al. and first released in this repository. See also the official blog post.; Disclaimer: The team releasing ImageGPT did not write a model card for this model so this model card has been written by the Hugging Face team.; The ImageGPT (iGPT) is a transformer decoder model (GPT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-21k, at a resolution of 32x32 pixels. ; The goal for the model is simply to predict the next pixel value, given the previous ones.; By pre-training the model, it learns an inner representation of images that can then be used to:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"ImageGPT (iGPT) model pre-trained on ImageNet ILSVRC 2012 (14 million images, 21,843 classes) at resolution 32x32. It was introduced in the paper Generative Pretraining from Pixels by Chen et al. and first released in this repository. See also the official blog post.; Disclaimer: The team releasing ImageGPT did not write a model card for this model so this model card has been written by the Hugging Face team.; The ImageGPT (iGPT) is a transformer decoder model (GPT-like) pretrained on a large collection of images in a self-supervised fashion, namely ImageNet-21k, at a resolution of 32x32 pixels. ; The goal for the model is simply to predict the next pixel value, given the previous ones.; By pre-training the model, it learns an inner representation of images that can then be used to:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Github Repo ? ? Twitter ? ? Paper 
; ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation; ImageReward is the first general-purpose text-to-image human preference RM which is trained on in total 137k pairs of
expert comparisons, based on text prompts and corresponding model outputs from DiffusionDB. We demonstrate that
ImageReward outperforms existing text-image scoring methods, such as CLIP, Aesthetic, and BLIP, in terms of
understanding human preference in text-to-image synthesis through extensive analysis and experiments.; ; We have integrated the whole repository to a single python package image-reward. Following the commands below to prepare the environment:",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
Autogenerated by HuggingPics???; Create your own image classifier for anything by running the demo on Google Colab.; Report any issues with the demo at the github repo.; ;,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"IndoBERT is the Indonesian version of BERT model. We train the model using over 220M words, aggregated from three main sources: ; We trained the model for 2.4M steps (180 epochs) with the final perplexity over the development set being 3.97 (similar to English BERT-base).; This IndoBERT was used to examine IndoLEM - an Indonesian benchmark that comprises of seven tasks for the Indonesian language, spanning morpho-syntax, semantics, and discourse. ; The paper is published at the 28th COLING 2020. Please refer to https://indolem.github.io for more details about the benchmarks.; If you use our work, please cite:",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"Finetuned Stable Diffusion model trained on dreambooth. Vaguely inspired by Gorillaz, FLCL, and Yoji Shinkawa. Use nvinkpunk in your prompts.; We support a Gradio Web UI to run Inkpunk-Diffusion:
;",,,,,,,,,,,,,,,,,,,,,,,,,usage 59
"IGEL is an LLM model family developed for German. The first version of IGEL is built on top?BigScience BLOOM,?adapted to?German from Malte Ostendorff. IGEL is designed to provide accurate and reliable language understanding capabilities for a wide range of natural language understanding tasks, including sentiment analysis, language translation, and question answering.; The IGEL family currently includes?instruct-igel-001?and?chat-igel-001?(coming soon).; LoRA tuned BLOOM-CLP German (6.4B parameters) with merged weights. The 001 was designed as a naive test to determine whether it is possible to create an german instruction-tuned model using a small, undertrained LLM and a naive translated dataset. The goal of this test was to explore the potential of the BLOOM architecture for language modeling tasks that require instruction-based responses.; To achieve this goal, we used a pre-trained LLM model with limited training, and fine-tuned it using a dataset of naive translations of instruction-based content. The dataset was created by taking instructions in English and translating them into German using an automated translation tool. While this approach may introduce errors in the translated content, we wanted to test whether the model could still learn to generate instruction-based responses in a variety of languages.; instruct-igel-001 is trained on naive translated instruction datasets, without much post-processing.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"GitHub: https://github.com/timothybrooks/instruct-pix2pix
; To use InstructPix2Pix, install diffusers using main for now. The pipeline will be available in the next release",,,,,,,,,,,,,,,,,,,,,,,,,usage 172
InstructBLIP model using Vicuna-13b as language model. InstructBLIP was introduced in the paper InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning by Dai et al.; Disclaimer: The team releasing InstructBLIP did not write a model card for this model so this model card has been written by the Hugging Face team.; InstructBLIP is a visual instruction tuned version of BLIP-2. Refer to the paper for details.; ; Usage is as follows:,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
InstructBLIP model using Vicuna-7b as language model. InstructBLIP was introduced in the paper InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning by Dai et al.; Disclaimer: The team releasing InstructBLIP did not write a model card for this model so this model card has been written by the Hugging Face team.; InstructBLIP is a visual instruction tuned version of BLIP-2. Refer to the paper for details.; ; Usage is as follows:,,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"CodeT5+ is a new family of open code large language models with an encoder-decoder architecture that can flexibly operate in different modes (i.e. encoder-only, decoder-only, and encoder-decoder) to support a wide range of code understanding and generation tasks. 
It is introduced in the paper:; CodeT5+: Open Code Large Language Models for Code Understanding and Generation
by Yue Wang*, Hung Le*, Akhilesh Deepak Gotmare, Nghi D.Q. Bui, Junnan Li, Steven C.H. Hoi (* indicates equal contribution).; Compared to the original CodeT5 family (base: 220M, large: 770M), CodeT5+ is pretrained with a diverse set of pretraining tasks including span denoising, causal language modeling, contrastive learning, and text-code matching to learn rich representations from both unimodal code data and bimodal code-text data. 
Additionally, it employs a simple yet effective compute-efficient pretraining method to initialize the model components with frozen off-the-shelf LLMs such as CodeGen to efficiently scale up the model (i.e. 2B, 6B, 16B), and adopts a ""shallow encoder and deep decoder"" architecture. 
Furthermore, it is instruction-tuned to align with natural language instructions (see our InstructCodeT5+ 16B) following Code Alpaca.  ; This model can be easily loaded using the AutoModelForSeq2SeqLM functionality and employs the same tokenizer as CodeGen.; This checkpoint is trained on the stricter permissive subset of the deduplicated version of the github-code dataset.
The data is preprocessed by reserving only permissively licensed code (""mit"" ??apache-2??, ??bsd-3-clause??, ??bsd-2-clause??, ??cc0-1.0??, ??unlicense??, ??isc??).
Supported languages (9 in total) are as follows:
c, c++, c-sharp,  go, java, javascript,  php, python, ruby.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"We introduce Instructor???, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor?? achieves sota on 70 diverse embedding tasks!
The model is easy to use with our customized sentence-transformer library. For more details, check out our paper and project page! ; **************************** Updates ****************************; Then you can use the model like this to calculate domain-specific and task-aware embeddings:; If you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions: ; ??????????????????????????Represent the domain text_type for task_objective:",,,,,,,,,,,,,,,,,,,,,,,,,usage 13
"We introduce Instructor???, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor?? achieves sota on 70 diverse embedding tasks (MTEB leaderboard)!
The model is easy to use with our customized sentence-transformer library. For more details, check out our paper and project page! ; **************************** Updates ****************************; Then you can use the model like this to calculate domain-specific and task-aware embeddings:; If you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions: ; ??????????????????????????Represent the domain text_type for task_objective:",,,,,,,,,,,,,,,,,,,,,,,,,usage 31
"We introduce Instructor???, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning. Instructor?? achieves sota on 70 diverse embedding tasks!
The model is easy to use with our customized sentence-transformer library. For more details, check out our paper and project page! ; **************************** Updates ****************************; Then you can use the model like this to calculate domain-specific and task-aware embeddings:; If you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions: ; ??????????????????????????Represent the domain text_type for task_objective:",,,,,,,,,,,,,,,,,,,,,,,,,usage 48
"To use the InstructPix2Pix checkpoint fine-tuned on MagicBrush, set up env with following command:; Download this checkpoint into checkpoints folder via; Then go back to the root folder and set up the running env following the InstructPix2Pix guidelines.; If you find this checkpoint useful, please consider citing our paper:; And prior work:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"https://github.com/BeyonderXX/InstructUIE; Large language models have unlocked strong multi-task capabilities from reading instructive prompts.
However, recent studies have shown that existing large models still have difficulty with information extraction tasks. 
For example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance.
In this paper, we propose InstructUIE, a unified information extraction framework based on instruction tuning, which can uniformly model various information extraction tasks and capture the inter-task dependency.
To validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions.
Experimental results demonstrate that our method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings.; Our models are trained and evaluated on IE INSTRUCTIONS. 
You can download the data from Baidu NetDisk or Google Drive.; If you are using InstructUIE for your work, please kindly cite our paper:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; ?Reporting Issues; InternLM has open-sourced a 7 billion parameter base model tailored for practical scenarios. The model has the following characteristics:; We conducted a comprehensive evaluation of InternLM using the open-source evaluation tool OpenCompass. The evaluation covered five dimensions of capabilities: disciplinary competence, language competence, knowledge competence, inference competence, and comprehension competence. Here are some of the evaluation results, and you can visit the OpenCompass leaderboard for more evaluation results.; Limitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; ?Reporting Issues; InternLM has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:; We conducted a comprehensive evaluation of InternLM using the open-source evaluation tool OpenCompass. The evaluation covered five dimensions of capabilities: disciplinary competence, language competence, knowledge competence, inference competence, and comprehension competence. Here are some of the evaluation results, and you can visit the OpenCompass leaderboard for more evaluation results.; Limitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"; ?Reporting Issues; InternLM has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:; We conducted a comprehensive evaluation of InternLM using the open-source evaluation tool OpenCompass. The evaluation covered five dimensions of capabilities: disciplinary competence, language competence, knowledge competence, inference competence, and comprehension competence. Here are some of the evaluation results, and you can visit the OpenCompass leaderboard for more evaluation results.; Limitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
This model is finetuned Donut ML base model on invoices data. Model aims to verify how well Donut performs on enterprise docs.; Mean accuracy on test set: 0.96; Inference:; ; Training loss:,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"Download our ControlNet Models for AUTOMATIC1111 Stable Diffusion Web UI!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"?; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Japanese subset of the mC4 dataset; Trained for 3000 steps on top of the MPT 7b checkpoint mosaicml/mpt-7b; Before running this model, please install the following pip package:; To load the model, run the following command.; To run this model, you may need to load it in a lower precision in order for it to fit onto your GPU. We found for a T4 GPU, it requires loading the model in 8-bit precision. To load the model in 8-bit and 4-bit, please install the following pip packages:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; One more step before getting this model.This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.The CreativeML OpenRAIL License specifies: ; By clicking on ""Access repository"" below, you accept that your contact information (email address and username) can be shared with the model authors as well.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		;",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"; 

; 
The text embedding set trained by Jina AI, Finetuner team.
; jina-embedding-b-en-v1 is a language model that has been trained using Jina AI's Linnaeus-Clean dataset.
This dataset consists of 380 million pairs of sentences, which include both query-document pairs.
These pairs were obtained from various domains and were carefully selected through a thorough cleaning process.
The Linnaeus-Full dataset, from which the Linnaeus-Clean dataset is derived, originally contained 1.6 billion sentence pairs.; The model has a range of use cases, including information retrieval, semantic textual similarity, text reranking, and more.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; 

; 
The text embedding set trained by Jina AI, Finetuner team.
; jina-embedding-l-en-v1 is a language model that has been trained using Jina AI's Linnaeus-Clean dataset.
This dataset consists of 380 million pairs of sentences, which include both query-document pairs.
These pairs were obtained from various domains and were carefully selected through a thorough cleaning process.
The Linnaeus-Full dataset, from which the Linnaeus-Clean dataset is derived, originally contained 1.6 billion sentence pairs.; The model has a range of use cases, including information retrieval, semantic textual similarity, text reranking, and more.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; 

; 
The text embedding set trained by Jina AI, Finetuner team.
; jina-embedding-s-en-v1 is a language model that has been trained using Jina AI's Linnaeus-Clean dataset.
This dataset consists of 380 million pairs of sentences, which include both query-document pairs.
These pairs were obtained from various domains and were carefully selected through a thorough cleaning process.
The Linnaeus-Full dataset, from which the Linnaeus-Clean dataset is derived, originally contained 1.6 billion sentence pairs.; The model has a range of use cases, including information retrieval, semantic textual similarity, text reranking, and more.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; 

; 
The text embedding set trained by Jina AI, Finetuner team.
; jina-embedding-t-en-v1 is a tiny small language model that has been trained using Jina AI's Linnaeus-Clean dataset.
This dataset consists of 380 million pairs of sentences, which include both query-document pairs.
These pairs were obtained from various domains and were carefully selected through a thorough cleaning process.
The Linnaeus-Full dataset, from which the Linnaeus-Clean dataset is derived, originally contained 1.6 billion sentence pairs.; The model has a range of use cases, including information retrieval, semantic textual similarity, text reranking, and more.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
?? Imported from https://zenodo.org/record/5443814/; This model was trained by kan-bayashi using ljspeech/tts1 recipe in espnet.; or arXiv:,,,,,,,,,,,,,,,,,,,,,,,,,usage 103
"Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.2 is available in diffusers!;",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.2 is available in diffusers!;",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.2 is available in diffusers!;",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.; It uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.; The Kandinsky model is created by Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Igor Pavlov, Andrey Kuznetsov and Denis Dimitrov; Kandinsky 2.2 is available in diffusers!;",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Karlo is a text-conditional image generation model based on OpenAI's unCLIP architecture with the improvement over the standard super-resolution model from 64px to 256px, recovering high-frequency details only in the small number of denoising steps.; Karlo is available in diffusers!; ; ; Karlo is a text-conditional diffusion model based on unCLIP, composed of prior, decoder, and super-resolution modules. In this repository, we include the improved version of the standard super-resolution module for upscaling 64px to 256px only in 7 reverse steps, as illustrated in the figure below:",,,,,,,,,,,,,,,,,,,,,,,,,usage 23
"** Updates on 2022.10.08 **; ??? ??? Transformer ?? ???? ??? ??? ??, ?? ??, ? ? ? ??? ???? ???? ??? ?????. ??, ??? NSMC? ?? User-Generated Noisy text domain ????? ???? ??? ??? ??? ???? ???, ??? ? ???? ????? ???? ?? ???? ???? ?????.; KcELECTRA? ?? ?? ??? ????? ???? ??, ??? ???? ??? ???? ???, ?????? ELECTRA??? ???? ??? Pretrained ELECTRA ?????.; ?? KcBERT ?? ???? ?? ? vocab ??? ?? ??? ???? ??? ???????.; KcELECTRA? Huggingface? Transformers ?????? ?? ??? ??? ??? ? ????. (??? ?? ????? ???? ????.)",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"This repo contains several KenLM models trained on different tokenized datasets and languages.KenLM models are probabilistic n-gram languge models that models. One use case of these models consist on fast perplexity estimation for filtering or sampling large datasets. For example, one could use a KenLM model trained on French Wikipedia to run inference on a large dataset and filter out samples that are very unlike to appear on Wikipedia (high perplexity), or very simple non-informative sentences that could appear repeatedly (low perplexity).; At the root of this repo you will find different directories named after the dataset models were trained on (e.g. wikipedia, oscar). Within each directory, you will find several models trained on different language subsets of the dataset (e.g. en (English), es (Spanish), fr (French)). For each language you will find three different files; The models have been trained using some of the preprocessing steps from cc_net, in particular replacing numbers with zeros and normalizing punctuation. So, it is important to keep the default values for the parameters: lower_case, remove_accents, normalize_numbers and punctuation when using the pre-trained models in order to replicate the same pre-processing steps at inference time.; In the example above we see that, since Wikipedia is a collection of encyclopedic articles, a KenLM model trained on it will naturally give lower perplexity scores to sentences with formal language and no grammar mistakes than colloquial sentences with grammar mistakes.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"KeyBART as described in ""Learning Rich Representations of Keyphrase from Text"" published in the Findings of NAACL 2022 (https://aclanthology.org/2022.findings-naacl.67.pdf), pre-trains a BART-based architecture to produce a concatenated sequence of keyphrases in the CatSeqD format.; We provide some examples on Downstream Evaluations setups and and also how it can be used for Text-to-Text Generation in a zero-shot setting.; Reported Results:; Reported Results:; Alternatively use the Hosted Inference API console provided in https://huggingface.co/bloomberg/KeyBART",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"Keyphrase extraction is a technique in text analysis where you extract the important keyphrases from a document. Thanks to these keyphrases humans can understand the content of a text very quickly and easily without reading it completely. Keyphrase extraction was first done primarily by human annotators, who read the text in detail and then wrote down the most important keyphrases. The disadvantage is that if you work with a lot of documents, this process can take a lot of time ?. ; Here is where Artificial Intelligence ? comes in. Currently, classical machine learning methods, that use statistical and linguistic features, are widely used for the extraction process. Now with deep learning, it is possible to capture the semantic meaning of a text even better than these classical methods. Classical methods look at the frequency, occurrence and order of words in the text, whereas these neural approaches can capture long-term semantic dependencies and context of words in a text.; This model uses distilbert as its base model and fine-tunes it on the Inspec dataset.; Keyphrase extraction models are transformer models fine-tuned as a token classification problem where each word in the document is classified as being part of a keyphrase or not.; Kulkarni, Mayank, Debanjan Mahata, Ravneet Arora, and Rajarshi Bhowmik. ""Learning Rich Representation of Keyphrases from Text."" arXiv preprint arXiv:2112.08547 (2021).",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Keyphrase extraction is a technique in text analysis where you extract the important keyphrases from a document. Thanks to these keyphrases humans can understand the content of a text very quickly and easily without reading it completely. Keyphrase extraction was first done primarily by human annotators, who read the text in detail and then wrote down the most important keyphrases. The disadvantage is that if you work with a lot of documents, this process can take a lot of time ?. ; Here is where Artificial Intelligence ? comes in. Currently, classical machine learning methods, that use statistical and linguistic features, are widely used for the extraction process. Now with deep learning, it is possible to capture the semantic meaning of a text even better than these classical methods. Classical methods look at the frequency, occurrence and order of words in the text, whereas these neural approaches can capture long-term semantic dependencies and context of words in a text.; This model uses KBIR as its base model and fine-tunes it on the Inspec dataset. KBIR or Keyphrase Boundary Infilling with Replacement is a pre-trained model which utilizes a multi-task learning setup for optimizing a combined loss of Masked Language Modeling (MLM), Keyphrase Boundary Infilling (KBI) and Keyphrase Replacement Classification (KRC).
You can find more information about the architecture in this paper.; Keyphrase extraction models are transformer models fine-tuned as a token classification problem where each word in the document is classified as being part of a keyphrase or not.; Kulkarni, Mayank, Debanjan Mahata, Ravneet Arora, and Rajarshi Bhowmik. ""Learning Rich Representation of Keyphrases from Text."" arXiv preprint arXiv:2112.08547 (2021).",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"This is a model to generated Chinese poetry with leading characters and certain tune of mood.; ??????????????gpt2???????Ñk ????????????????????????????? ??????????????????? ??????????????§Ö????????????????????? ???????????? ????????????????????0?? 0??cos??1 ????????????????????? ???1????????4?? 4??????24??????????????? ???????????????2????????§Ö????? ??????????; ???????????????????? ??????? ??????0~10?????????+???????+???????+???????????[CLS]???????; ??§Ýliangtongt???Inference ??????????????????bug.; ??????????,????????????
??????????????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; AI ???? ????.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; AI ????? User Access requests ??? ???? ?? ?? ??; ?? ???? ???? ???? ???.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; KorSTS, KorNLI ?? ?????? ?? ??? ??? ??? ? KorSTS ?? ?????? ??? ?????.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; KorSTS, KorNLI ?? ?????? ?? ??? ??? ??? ? KorSTS ?? ?????? ??? ?????.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Koala 13B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
,,,,,,,,,,,,,,,,,,,,,,,,,usage 5
,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"; KoreanLM? ??? ????? ???? ?? ???? ???????. ?? ???? ?????? ??? ??? ??? ??, ???? ?? ??? ????? ???? ??? ???? ????? ??? ????. ??? ??? ???? ???? ???? ????? ???? ?? KoreanLM ????? ???? ?????.; ???? ??? ???? ??: ???? ??, ??, ??? ??? ???? ???? ? ???? ???? ??? ? ?? ????? ?????.; ???? ??? ?? ??: ??? ???? ??? ???? ????? ??? ??? ??? ??? ??? ??? ???? ????? ??? ??????.; ?? ????? ??? ??: ?? ??? ???? ?????? ??? ??? ???? ?????? ??? ??? ????. ?? ???? ?? ??? ????? ??? ???? ???? ????, ??? ?? ??? ? ?? ??? ? ??? ???.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This model is a fine-tuned version of EleutherAI/polyglot-ko-12.8b on a KULLM v2 ; Detail Codes are available at KULLM Github Repository; The following hyperparameters were used during training:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a port of the LaBSE model to PyTorch. It can be used to map 109 languages to a shared vector space.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net; Have a look at LaBSE for the respective publication that describes LaBSE.",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model trained for sentence embedding for 109 languages. The pre-training process combines masked language modeling with translation language modeling. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.; This is migrated from the v2 model on the TF Hub, which uses dict-based input. The embeddings produced by both the versions of the model are equivalent.; Using the model:; To get the sentence embeddings, use the pooler output:; Output for other languages:",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"sentence-transformers/LaBSE pre-trained on an instructional question-and-answer dataset. Evaluated on Precision at K metrics and Mean reciprocal rank.
Precision at K is a simple metric to understand and implement, but it has an important disadvantage - it does not take into account the order of elements in the ""top"". So, if we guessed only one item out of ten, it doesn't matter whether it was on the first or the last place - inline_formula in any case. It is obvious that the first variant is much better.
ean reciprocal rank equal to the reverse rank of the first correctly guessed item. Mean reciprocal rank varies in the range [0,1] and takes into account the position of items. Unfortunately, it does this only for one item - the 1st correctly predicted item, ignoring all subsequent items.; Evaluation results:; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
????????¦Í}??????????`??????????????¦³?????`????????¦³?????????????????????????????????????; ????????A??????????¦Å??????????§Õ????????????????????¦Ä?????????????????????Ñ”?????????????¦Ã?????????????????????????????¦³?¦Ä????????; ????¦³????????????????¦´???ð—?????????Ñ”???`??????????????????§Õ??????????????????????????¦Å????g????`???????; Clip skip??????¦³?????????????????????????????????????; VAE??¦³????????¦³?¦³???????????????????????????Anything??VAE??????????????????????¦³?clearVAE????`????????????????????,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; ; This model is one of our LaMini-LM model series in paper ""LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions"". This model is a fine-tuned version of google/flan-t5-large on LaMini-instruction dataset that contains 2.58M samples for instruction fine-tuning. For more information about our dataset, please refer to our project repository.You can view other models of LaMini-LM series as follows. Models with ? are those with the best overall performance given their size/architecture, hence we recommend using them. More details can be seen in our paper. ; We recommend using the model to response to human instructions written in natural language. ; We now show you how to load and use our model using HuggingFace pipeline().",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"This model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset.; This model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output). 
For additional information please refer to the xlm-roberta-base model card or to the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al.; You can directly use this model as a language detector, i.e. for sequence classification tasks. Currently, it supports the following 20 languages: ; arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh); The model was fine-tuned on the Language Identification dataset, which consists of text sequences in 20 languages. The training set contains 70k samples, while the validation and test sets 10k each. The average accuracy on the test set is 99.6% (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Multimodal (text + layout/format + image) pre-training for document AI; Microsoft Document AI | GitHub; LayoutLM is a simple but effective pre-training method of text and layout for document image understanding and information extraction tasks, such as form understanding and receipt understanding. LayoutLM archives the SOTA results on multiple datasets. For more details, please refer to our paper: ; LayoutLM: Pre-training of Text and Layout for Document Image Understanding
Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, KDD 2020; We pre-train LayoutLM on IIT-CDIP Test Collection 1.0* dataset with two settings.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on documents. It has been fine-tuned using both the SQuAD2.0 and DocVQA datasets.; To run these examples, you must have PIL, pytesseract, and PyTorch installed in addition to transformers.; NOTE: This model and pipeline was recently landed in transformers via PR #18407 and PR #18414, so you'll need to use a recent version of transformers, for example:; This model was created by the team at Impira.",,,,,,,,,,,,,,,,,,,,,,,,,usage 159
"This is a fine-tuned version of the multi-modal LayoutLM model for the task of question answering on invoices and other documents. It has been fine-tuned on a proprietary dataset of
invoices as well as both SQuAD2.0 and DocVQA for general comprehension.; Unlike other QA models, which can only extract consecutive tokens (because they predict the start and end of a sequence), this model can predict longer-range, non-consecutive sequences with an additional
classifier head. For example, QA models often encounter this failure mode:; ; However this model is able to predict non-consecutive tokens and therefore the address correctly:;",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Multimodal (text + layout/format + image) pre-training for document AI; Microsoft Document AI | GitHub; LayoutLM is a simple but effective pre-training method of text and layout for document image understanding and information extraction tasks, such as form understanding and receipt understanding. LayoutLM archives the SOTA results on multiple datasets. For more details, please refer to our paper: ; LayoutLM: Pre-training of Text and Layout for Document Image Understanding
Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, KDD 2020; We pre-train LayoutLM on IIT-CDIP Test Collection 1.0* dataset with two settings.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Microsoft Document AI | GitHub; LayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model. For example, LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis.; LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking
Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, Furu Wei, ACM Multimedia 2022.; If you find LayoutLM useful in your research, please cite the following paper:; The content of this project itself is licensed under the Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0).
Portions of the source code are based on the transformers project.
Microsoft Open Source Code of Conduct",,,,,,,,,,,,,,,,,,,,,,,,,usage 26
"This model is a fine-tuned version of microsoft/layoutlmv3-base on the nielsr/funsd-layoutlmv3 dataset.
It achieves the following results on the evaluation set:; The script for training can be found here: https://github.com/huggingface/transformers/tree/main/examples/research_projects/layoutlmv3; More information needed; More information needed; More information needed",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Multimodal (text + layout/format + image) pre-training for document AI; LayoutXLM is a multilingual variant of LayoutLMv2.; The documentation of this model in the Transformers library can be found here.; Microsoft Document AI | GitHub; LayoutXLM is a multimodal pre-trained model for multilingual document understanding, which aims to bridge the language barriers for visually-rich document understanding. Experiment results show that it has significantly outperformed the existing SOTA cross-lingual pre-trained models on the XFUND dataset.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Paper: High-Resolution Image Synthesis with Latent Diffusion Models; Abstract:; By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs.; Authors; Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj?rn Ommer",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"LEALLA is a collection of lightweight language-agnostic sentence embedding models supporting 109 languages, distilled from LaBSE. The model is useful for getting multilingual sentence embeddings and for bi-text retrieval.; This is migrated from the v1 model on the TF Hub. The embeddings produced by both the versions of the model are equivalent. Though, for some of the languages (like Japanese), the LEALLA models appear to require higher tolerances when comparing embeddings and similarities.; Using the model:; To get the sentence embeddings, use the pooler output:; Output for other languages:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Allenai's Longformer Encoder-Decoder (LED).; As described in Longformer: The Long-Document Transformer by Iz Beltagy, Matthew E. Peters, Arman Cohan, led-base-16384 was initialized from bart-base since both models share the exact same architecture. To be able to process 16K tokens, bart-base's position embedding matrix was simply copied 16 times.; This model is especially interesting for long-range summarization and question answering.; This notebook shows how led-base-16384 can effectively be fine-tuned on a downstream task.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"The Longformer Encoder-Decoder (LED) for Narrative-Esque Long Text Summarization is a model I fine-tuned from allenai/led-base-16384 to condense extensive technical, academic, and narrative content in a fairly generalizable way.; Note: The API widget has a max length of ~96 tokens due to inference timeout constraints. ; The model was trained on the BookSum dataset released by SalesForce, which leads to the bsd-3-clause license. The training process involved 16 epochs with parameters tweaked to facilitate very fine-tuning-type training (super low learning rate). ; Model checkpoint: pszemraj/led-base-16384-finetuned-booksum. ; This model is the smallest/fastest booksum-tuned model I have worked on. If you're looking for higher quality summaries, check out:",,,,,,,,,,,,,,,,,,,,,,,,,usage 9
"This model is a fine-tuned version of allenai/led-large-16384 on the BookSum dataset (kmfoda/booksum). It aims to generalize well and be useful in summarizing lengthy text for both academic and everyday purposes. ; Note: Due to inference API timeout constraints, outputs may be truncated before the fully summary is returned (try python or the demo); To improve summary quality, use encoder_no_repeat_ngram_size=3 when calling the pipeline object. This setting encourages the model to utilize new vocabulary and construct an abstractive summary.; Load the model into a pipeline object:; Feed the text into the pipeline object:",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"LEGAL-BERT is a family of BERT models for the legal domain, intended to assist legal NLP research, computational law, and legal technology applications.  To pre-train the different variations of LEGAL-BERT, we collected 12 GB of diverse English legal text from several fields (e.g., legislation, court cases,  contracts) scraped from publicly available resources. Sub-domain variants (CONTRACTS-, EURLEX-, ECHR-) and/or general LEGAL-BERT perform better than using BERT out of the box for domain-specific tasks. A light-weight model (33% the size of BERT-BASE) pre-trained from scratch on legal data with competitive performance is also available.
; I. Chalkidis, M. Fergadiotis, P. Malakasiotis, N. Aletras and I. Androutsopoulos. ""LEGAL-BERT: The Muppets straight out of Law School"". In Findings of Empirical Methods in Natural Language Processing (EMNLP 2020) (Short Papers), to be held online, 2020. (https://aclanthology.org/2020.findings-emnlp.261); The pre-training corpora of LEGAL-BERT include:; 116,062 documents of EU legislation, publicly available from EURLEX (http://eur-lex.europa.eu), the repository of EU Law running under the EU Publication Office.; 61,826 documents of UK legislation, publicly available from the UK legislation portal (http://www.legislation.gov.uk).",,,,,,,,,,,,,,,,,,,,,,,,,usage 14
"This is a Longformer Encoder Decoder (led-base-16384) model for the legal domain, trained for long document abstractive summarization task. The length of the document can be upto 16,384 tokens.; The legal-led-base-16384 model was trained on sec-litigation-releases dataset consisting more than 2700 litigation releases and complaints.; When the model is used for summarizing legal documents, it achieves the following results:; Inference API has been turned off for this model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Pretrained model on English language legal and administrative text using the RoBERTa pretraining objective. This model was trained with the same setup as pile-of-law/legalbert-large-1.7M-1, but with a different seed.; Pile of Law BERT large 2 is a transformers model with the BERT large model (uncased) architecture pretrained on the Pile of Law, a dataset consisting of ~256GB of English language legal and administrative text for language model pretraining.; You can use the raw model for masked language modeling or fine-tune it for a downstream task. Since this model was pretrained on a English language legal and administrative text corpus, legal downstream tasks will likely be more in-domain for this model.; You can use the model directly with a pipeline for masked language modeling:; Here is how to use this model to get the features of a given text in PyTorch:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; LexGPT-6B Language ModelVersion 1.0 / 17 June 2023; Current Checkpoint: Training Iteration  350,000; Link to paper: here",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"LIMARP-Llama2 is an experimental Llama2 finetune narrowly focused on novel-style roleplay chatting.; To considerably facilitate uploading and distribution, LoRA adapters have been provided instead of the merged models. You should get the Llama2 base model first, either from Meta or from one of the reuploads on HuggingFace (for example here and here). It is also possible to apply the LoRAs on different Llama2-based models (e.g. LLongMA-2 or Nous-Hermes-Llama2), although this is largely untested and the final results may not work as intended.; This is an experimental attempt at creating an RP-oriented fine-tune using a manually-curated, high-quality dataset of human-generated conversations. The main rationale for this are the observations from Zhou et al.. The authors suggested that just 1000-2000 carefully curated training examples may yield high quality output for assistant-type chatbots. This is in contrast with the commonly employed strategy where a very large number of training examples (tens of thousands to even millions) of widely varying quality are used.; For LIMARP a similar approach was used, with the difference that the conversational data is almost entirely human-generated. Every training example is manually compiled and selected to comply with subjective quality parameters, with virtually no chance for OpenAI-style alignment responses to come up.; The model is intended to approximate the experience of 1-on-1 roleplay as observed on many Internet forums dedicated on roleplaying. It must be used with a specific format similar to that of this template:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"LINCE-ZERO (Llm for Instructions from Natural Corpus en Espa?ol) is a SOTA Spanish instruction-tuned LLM ?; Developed by Clibrain, it is a causal decoder-only model with 7B parameters. LINCE-ZERO is based on Falcon-7B and has been fine-tuned using a combination of the Alpaca and Dolly datasets, both translated into Spanish and augmented to 80k examples.; The model is released under the Apache 2.0 license.; If you want to test the robust 40B parameters version called LINCE, you can request access at lince@clibrain.com. Be one of the first to discover the possibilities of LINCE!; LINCE-ZERO (Llm for Instructions from Natural Corpus en Espa?ol) is a state-of-the-art Spanish instruction-tuned large language model. Developed by Clibrain, it is a causal decoder-only model with 7B parameters. LINCE-ZERO is based on Falcon-7B and has been fine-tuned using an 80k examples augmented combination of the Alpaca and Dolly datasets, both translated into Spanish.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a Llama-like generative text model with a scale of 7 billion, optimized for dialogue use cases and converted for the Hugging Face Transformers format. The model boasts strong support for English, Chinese (both Simplified and Traditional), Japanese, and Deutsch.; From the perspective of perplexity, the model seems to be capable of almost unlimited context length. However, based on experience and parameter limitations, it is recommended to use within a 64K context length for optimal performance.; ; The anticipated chat input format is as follows:; Although this is the suggested usage format, Vicuna-style inputs can also be used to adapt to certain pre-existing application scenarios, such as:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This contains the weights for the LLaMA-13b model. This model is under a non-commercial license (see the LICENSE file).
You should only use this repository if you have been granted access to the model by filling out this form but either lost your copy of the weights or got some trouble converting them to the Transformers format.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's LLaMA 13b.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's LLaMA 13b.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"LLaMA-13B converted to work with Transformers/HuggingFace. This is under a special license, please see the LICENSE file for details.; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 40
"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Llama 2
Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model. Links to other models can be found in the index at the bottom.; Model Details
Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ?? 7B, 13B, and 70B ?? as well as pretrained and fine-tuned variations.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Edit: You can find a Demo (German) here; Llama-2-13b-chat-german is a variant of Meta??s Llama 2 13b Chat model, finetuned on an additional dataset in German language.; This model is optimized for German text, providing proficiency in understanding, generating, and interacting with German language content. However the model is not yet fully optimized for German language, as it has been trained on a small, experimental dataset and has limited capabilities due to the small parameter count.
Some of the fineunting data is also targeted towards factual retrieval (only answer questions from information in the context and refuse to hallucinate) and the model should perform better for these tasks than original Llama 2 Chat.; I am working on improving the model??s capabilities and will update the model if there is sufficient interest.; A quantized GGML version for use with llama.cpp, kobold.cpp and other GUIs for CPU inference can be found here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 13B-chat.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"From: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf; Quantized using an older version of llama.cpp and compatible with llama.cpp from May 19, commit 2d5db48.; Quantization methods compatible with latest llama.cpp from June 6, commit 2d43387.; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 13B-chat.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; To continue a conversation:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"From: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 30
"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ?? 7B, 13B, and 70B ?? as well as pretrained and fine-tuned variations.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"These are the converted model weights for Llama-2-13B-chat in Huggingface format.; Courtesy of Mirage-Studio.io, home of MirageGPT: the private ChatGPT alternative.; license: other
LLAMA 2 COMMUNITY LICENSE AGREEMENT	
Llama 2 Version Release Date: July 18, 2023; ""Agreement"" means the terms and conditions for use, reproduction, distribution and 
modification of the Llama Materials set forth herein.; ""Documentation"" means the specifications, manuals and documentation 
accompanying Llama 2 distributed by Meta at ai.meta.com/resources/models-and-
libraries/llama-downloads/.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are fp16 format model files for Meta's Llama 2 13B.; They were produced by downloading the PTH files from Meta, and then converting to HF format using the latest Transformers 4.32.0.dev0, from Git, with the Llama 2 PR included: https://github.com/huggingface/transformers/pull/24891.; Command to convert was:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is an finetuned version for german instructions and conversations in style of Open Assistant tokens. ""<|prompter|>"" ""<|endoftext|>"" ""<|assistant|>""; The dataset used is deduplicated and cleaned, with no codes inside. The focus is on instruction following and conversational tasks.; The model archictecture is based on Llama version 2 with 13B parameters, trained on 100% renewable energy powered hardware.; This work is contributed by private research of flozi00",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is an finetuned version for german instructions and conversations in style of Open Assistant tokens. ""<|prompter|>"" ""<|endoftext|>"" ""<|assistant|>""; The dataset used is deduplicated and cleaned, with no codes inside. The focus is on instruction following and conversational tasks.; The model archictecture is based on Llama version 2 with 13B parameters, trained on 100% renewable energy powered hardware.; This work is contributed by private research of flozi00",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for flozi00's Llama 2 13B German Assistant v2.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for flozi00's Llama 2 13B German Assistant v2.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"From: https://huggingface.co/meta-llama/Llama-2-13b-hf; Quantized using an older version of llama.cpp and compatible with llama.cpp from May 19, commit 2d5db48.; Quantization methods compatible with latest llama.cpp from June 6, commit 2d43387.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"From: https://huggingface.co/meta-llama/Llama-2-13b-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"These files are GPTQ model files for Meta's Llama 2 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.; Each separate quant is in a different branch.  See below for instructions on fetching from different branches.; Please make sure you're using the latest version of text-generation-webui.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This is a Llama-2 version of Guanaco. It was finetuned from the base Llama-13b model using the official training scripts found in the QLoRA repo. I wanted it to be as faithful as possible and therefore changed nothing in the training script beyond the model it was pointing to. The model prompt is therefore also the same as the original Guanaco model.; This repo contains the merged f16 model. The QLoRA adaptor can be found here.; A 7b version of the model can be found here.; Legal Disclaimer: This model is bound by the usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a Llama-2 version of Guanaco. It was finetuned from the base Llama-13b model using the official training scripts found in the QLoRA repo. I wanted it to be as faithful as possible and therefore changed nothing in the training script beyond the model it was pointing to. The model prompt is therefore also the same as the original Guanaco model.; This repo contains the QLoRA adapter. A merged f16 model can be found here.; A 7b version of the adapter can be found here.; Legal Disclaimer: This model is bound by the usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Mikael110's Llama2 13B Guanaco QLoRA.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Mikael110's Llama2 13B Guanaco QLoRA.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 11
"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ?? 7B, 13B, and 70B ?? as well as pretrained and fine-tuned variations.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Join the Coffee & AI Discord for AI Stuff and things!
; Base Model Quantizations by The Bloke here:
https://huggingface.co/TheBloke/Llama-2-13B-GGML
https://huggingface.co/TheBloke/Llama-2-13B-GPTQ; A brief warning that no alignment or attempts to sanitize or otherwise filter the dataset or the outputs have been done. This is a completelty raw model and may behave unpredictably or create scenarios that are unpleasant. ; The base Llama2 is a text completion model. That means it will continue writing from the story in whatever manner you direct it. This is not an instruct tuned model, so don't try and give it instruction.; Correct prompting:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is Llama2-22b in a couple of GGML formats. I have no idea what I'm doing so if something doesn't work as it should or not at all that's likely on me, not the models themselves.
While I haven't had any issues so far do note that the original repo states ""Not intended for use as-is - this model is meant to serve as a base for further tuning"".; Approximate VRAM requirements at 4K context:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ?? 7B, 13B, and 70B ?? as well as pretrained and fine-tuned variations.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are fp16 pytorch model files for Meta's Llama 2 70B Chat.; They were produced by downloading the PTH files from Meta, and then converting to HF format using the latest Transformers 4.32.0.dev0, from Git, with the Llama 2 PR included: https://github.com/huggingface/transformers/pull/24891.; Command to convert was:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for Meta's Llama 2 70B Chat.; To use these files you need:; Example command:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 70B Chat.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware for these quantisations!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"From: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 94
"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ?? 7B, 13B, and 70B ?? as well as pretrained and fine-tuned variations.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ?? 7B, 13B, and 70B ?? as well as pretrained and fine-tuned variations.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are fp16 format model files for Meta's Llama 2 70B.; They were produced by downloading the PTH files from Meta, and then converting to HF format using the latest Transformers 4.32.0.dev0, from Git, with the Llama 2 PR included: https://github.com/huggingface/transformers/pull/24891.; Command to convert was:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for Meta's Llama 2 70B.; To use these files you need:; Example command:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 70B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware for these quantisations!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"From: https://huggingface.co/meta-llama/Llama-2-70b-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"These files are GPTQ model files for Meta's Llama 2 70B but with new FP16 files, made with the last transformers version. (transformers-4.32.0.dev0); GQA Works with exllama, but not GPTQ for LLaMA/AutoGPTQ.; The model was quantized with GPTQ-for-LLaMA, without group size to reduce VRAM usage, with true sequential and act order true, to not lose a lot of perplexity.; TBD; Exllama works 2x24 VRAM GPUs and it is the recommended way to use it now. It even can do 16K! context with high alpha values and NTK Alpha.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a Llama-2 version of Guanaco. It was finetuned from the base Llama-70b model using the official training scripts found in the QLoRA repo. I wanted it to be as faithful as possible and therefore changed nothing in the training script beyond the model it was pointing to. The model prompt is therefore also the same as the original Guanaco model.; This repo contains the QLoRA adapter.; A 7b version of the adapter can be found here.
A 13b version of the adapter can be found here.; Legal Disclaimer: This model is bound by the usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are pytorch format fp16 model files for Mikael110's Llama2 70b Guanaco QLoRA.; It is the result of merging and/or converting the source repository to float16.; For further support, and discussions on these models and AI in general, join us at:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Mikael110's Llama2 70b Guanaco QLoRA.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 45
"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 70B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ?? 7B, 13B, and 70B ?? as well as pretrained and fine-tuned variations.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 9
"This is a Core ML version of meta-llama/Llama-2-7b-chat-hf. For license information, model details and acceptable use policy, please refer to the original model card.; This conversion was performed in float16 mode with a fixed sequence length of 64, and is intended for evaluation and test purposes. Please, open a conversation in the Community tab if you have questions or want to report an issue.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 7b Chat.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"From: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf; Quantized using an older version of llama.cpp and compatible with llama.cpp from May 19, commit 2d5db48.; Quantization methods compatible with latest llama.cpp from June 6, commit 2d43387.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 7b Chat.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; To continue a conversation:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"We are unlocking the power of large language models. Our latest version of Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly. ; This release includes model weights and starting code for pretrained and fine-tuned Llama language models ?? ranging from 7B to 70B parameters.; This repository is intended as a minimal example to load Llama 2 models and run inference. For more detailed examples leveraging HuggingFace, see llama-recipes.; In order to download the model weights and tokenizer, please visit the Meta AI website and accept our License.; Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, do not use the 'Copy link address' option when you right click the URL. If the copied URL text starts with: https://download.llamameta.net, you copied it correctly. If the copied URL text starts with: https://l.facebook.com, you copied it the wrong way.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 57
"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ?? 7B, 13B, and 70B ?? as well as pretrained and fine-tuned variations.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"These are the converted model weights for Llama-2-7B-chat in Huggingface format.; Courtesy of Mirage-Studio.io, home of MirageGPT: the private ChatGPT alternative.; license: other
LLAMA 2 COMMUNITY LICENSE AGREEMENT	
Llama 2 Version Release Date: July 18, 2023; ""Agreement"" means the terms and conditions for use, reproduction, distribution and 
modification of the Llama Materials set forth herein.; ""Documentation"" means the specifications, manuals and documentation 
accompanying Llama 2 distributed by Meta at ai.meta.com/resources/models-and-
libraries/llama-downloads/.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; The following bitsandbytes quantization config was used during training:; The following bitsandbytes quantization config was used during training:; PEFT 0.5.0.dev0",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"We are unlocking the power of large language models. Our latest version of Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly. ; This release includes model weights and starting code for pretrained and fine-tuned Llama language models ?? ranging from 7B to 70B parameters.; This repository is intended as a minimal example to load Llama 2 models and run inference. For more detailed examples leveraging HuggingFace, see llama-recipes.; In order to download the model weights and tokenizer, please visit the Meta AI website and accept our License.; Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, do not use the 'Copy link address' option when you right click the URL. If the copied URL text starts with: https://download.llamameta.net, you copied it correctly. If the copied URL text starts with: https://l.facebook.com, you copied it the wrong way.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's Llama 2 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"From: https://huggingface.co/meta-llama/Llama-2-7b-hf; Quantized using an older version of llama.cpp and compatible with llama.cpp from May 19, commit 2d5db48.; Quantization methods compatible with latest llama.cpp from June 6, commit 2d43387.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This model card aims to be a base template for new models. It has been generated using this raw template.; [More Information Needed]; [More Information Needed]; [More Information Needed]; [More Information Needed],,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Meta's Llama 2 7B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"From: https://huggingface.co/meta-llama/Llama-2-7b-hf; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Model fine-tuned in 4-bit precision using QLoRA on timdettmers/openassistant-guanaco with weights merged after training.; Made using this Google Colab notebook.; It can be easily imported using the AutoModelForCausalLM class from transformers:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Model that is fine-tuned in 4-bit precision using QLoRA on timdettmers/openassistant-guanaco and databricks/databricks-dolly-15k. Sharded as well to be used on a free Google Colab instance. ; It can be easily imported using the AutoModelForCausalLM class from transformers:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This is a Llama-2 version of Guanaco. It was finetuned from the base Llama-7b model using the official training scripts found in the QLoRA repo. I wanted it to be as faithful as possible and therefore changed nothing in the training script beyond the model it was pointing to. The model prompt is therefore also the same as the original Guanaco model.; This repo contains the merged f16 model. The QLoRA adaptor can be found here.; A 13b version of the model can be found here.; Legal Disclaimer: This model is bound by the usage restrictions of the original Llama-2 model. And comes with no warranty or gurantees of any kind.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Mikael110's Llama2 7B Guanaco QLoRA.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Mikael110's Llama2 7B Guanaco QLoRA.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a form to enable access to Llama 2 on Hugging Face after you have been granted access from Meta. Please visit the Meta website and accept our license terms and acceptable use policy before submitting this form. Requests will be processed in 1-2 days.; Your Hugging Face account email address MUST match the email you provide on the Meta website, or your request will not be approved.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 10
"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.; Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here.; Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.; Model Developers Meta; Variations Llama 2 comes in a range of parameter sizes ?? 7B, 13B, and 70B ?? as well as pretrained and fine-tuned variations.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Llama 2 (7B) fine-tuned on Clibrain's  Spanish instructions dataset.; Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model. Links to other models can be found in the index at the bottom.; Inference API has been turned off for this model.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Llama-2-Ko serves as an advanced iteration of Llama 2, benefiting from an expanded vocabulary and the inclusion of a Korean corpus in its further pretraining. Just like its predecessor, Llama-2-Ko operates within the broad range of generative text models that stretch from 7 billion to 70 billion parameters. This repository focuses on the 7B pretrained version, which is tailored to fit the Hugging Face Transformers format. For access to the other models, feel free to consult the index provided below.; Model Developers Junbum Lee (Beomi); Variations Llama-2-Ko will come in a range of parameter sizes ?? 7B, 13B, and 70B ?? as well as pretrained and fine-tuned variations.; Input Models input text only.; Output Models generate text only.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's LLaMA 30b.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No other data was used except for the dataset mentioned above,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
No other data was used except for the dataset mentioned above,,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"Merge of huggyllama/llama-30b + kaiokendev/SuperCOT-LoRA ; Supercot was trained to work with langchain prompting. ; Load up locally in my custom LLM notebook that uses the Oobabooga modules to load up models: https://github.com/ausboss/Local-LLM-Langchain; Then you can add cells from of these other notebooks for testing: https://github.com/gkamradt/langchain-tutorials; This LoRA is compatible with any 7B, 13B or 30B 4-bit quantized LLaMa model, including ggml quantized converted bins",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"Mostly untested!; This is base Llama-33b with minimal additional training to extend the useful context window.; This is a QLoRA fine-tune; Pretraining took 10 hours on 1x RTX 6000 Ada.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This contains the weights for the LLaMA-65b model. This model is under a non-commercial license (see the LICENSE file).
You should only use this repository if you have been granted access to the model by filling out this form but either lost your copy of the weights or got some trouble converting them to the Transformers format.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"Converted with https://github.com/notepad-plus-plus/notepad-plus-plus All models tested on A100-80G *Conversion may require lot of RAM, LLaMA-7b takes ~0 GB, 13b around 0 GB, 30b around 0 and 65b takes more than 0 GB of RAM.; Installation instructions as mentioned in above repo:; Additional training was done on the MSPaint_Blank dataset and 2,000,000T+ tokens on 50,000+ blank notepad files.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Converted with https://github.com/qwopqwop200/GPTQ-for-LLaMa 
All models tested on A100-80G
*Conversion may require lot of RAM, LLaMA-7b takes ~12 GB, 13b around 21 GB, 30b around 62 and 65b takes more than 120 GB of RAM. ; Installation instructions as mentioned in above repo:; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's LLaMA 65B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This contains the weights for the LLaMA-7b model. This model is under a non-commercial license (see the LICENSE file).
You should only use this repository if you have been granted access to the model by filling out this form but either lost your copy of the weights or got some trouble converting them to the Transformers format.",,,,,,,,,,,,,,,,,,,,,,,,,usage 10
"This is GreenBitAI's instruction-tuned LoRA parameters for our 2-bit 7B LLaMA model trained on the Alpaca-clean 50k dataset.; Please refer to our Github page for the code to run the model and more information.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Meta's LLaMA 7b.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"LLaMA-7B converted to work with Transformers/HuggingFace. This is under a special license, please see the LICENSE file for details.; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 138
"LLaMA-13B converted to work with Transformers/HuggingFace. This is under a special license, please see the LICENSE file for details.; EXPERIMENTAL RELEASE; This has been converted to int4 via GPTQ method. This requires some special support code that is also highly experimental. NOT COMPATIBLE WITH TRANSFORMERS LIBRARY.; --
license: other; Organization developing the model
The FAIR team of Meta AI.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Original weights converted with the latest transformers version using the LlamaTokenizerFast implementation. ; --
license: other; Organization developing the model
The FAIR team of Meta AI.; Model date
LLaMA was trained between December. 2022 and Feb. 2023.; Model version
This is version 1 of the model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; Adapter weights of a Reinforcement Learning fine-tuned model based on the LLaMA model (see Meta's LLaMA release for the original LLaMA model). 
The model is designed to generate human-like responses to questions in Stack Exchange domains of programming, mathematics, physics, and more.
For more info check out the blog post and github example.; Developed by: Hugging Face; Model type: An auto-regressive language model based on the transformer architecture, and fine-tuned with Stack Exchange datasets. ; Languages: Predominantly English, with additional data from languages with the following ISO codes:",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
?????????????????https://www.codewithgpu.com/m/file/llama2-13b-Chinese-chat; ?????????sharegpt???????????????llama2 Chinese chat 13b????????????§³??????????????adapter??????????https://huggingface.co/TheBloke/Llama-2-13B-fp16 ??????????????????????§Ü??????????????????  ; ?????????????; ??????????????????????????§¹??~ ; The following bitsandbytes quantization config was used during training:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is Llama 2 13b with some additional attention heads from original-flavor Llama 33b frankensteined on.; Fine-tuned on ~10M tokens from RedPajama to settle in the transplants a little.; Not intended for use as-is - this model is meant to serve as a base for further tuning, hopefully with a greater capacity for learning than 13b.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for TokenBender's Llama-2-7B-Chat Code Cherry Pop.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GPTQ model files for TokenBender's Llama-2-7B-Chat Code Cherry Pop.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"description:;   additional_info:;   next_steps: ""I've a few things in mind and after that this will be more valuable."";   tasks:;   commercial_use: |
    So far I think this can be used commercially but this is a adapter on Meta's llama2 with
    some gating issues so that is there.
  contact_info: ""If you find any issues or want to just holler at me, you can reach out to me - https://twitter.com/4evaBehindSOTA""",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Fine-tuned Llama-2 7B with an uncensored/unfiltered Wizard-Vicuna conversation dataset ehartford/wizard_vicuna_70k_unfiltered.
Used QLoRA for fine-tuning. Trained for one epoch on a 24GB GPU (NVIDIA A10G) instance, took ~19 hours to train.; The version here is the fp16 HuggingFace model.; Thanks to TheBloke, he has created the GGML and GPTQ versions:; The model was trained with the following prompt style:; Code used to train the model is available here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for George Sung's Llama2 7B Chat Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Buy me a coffee if you like this project ;)
; GGML Format model files for This project.; Fine-tuned Llama-2 7B with an uncensored/unfiltered Wizard-Vicuna conversation dataset ehartford/wizard_vicuna_70k_unfiltered.
Used QLoRA for fine-tuning. Trained for one epoch on a 24GB GPU (NVIDIA A10G) instance, took ~19 hours to train.; The model was trained with the following prompt style:; Code used to train the model is available here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for George Sung's Llama2 7B Chat Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"???????Llama2-chat 7B Chinese ?? Llama2-chat 13B Chinese ????????????????????????Llama2-base ?????????SFT?????????????????????; ?????????????§µ??????????chat????LoRA???????????????Llama2 base?? LoRA ??????????????????????????????????DeepSpeed ?????????reduce scale ??????????scale?§³??½„??????????????????LR 1e-5 - 2e-4??LoRA rank [4, 8, 64]??LoRA Alpha [1,4,8,16,32]??LoRA Dropout [0.05, 0.1] ??Warmup Ratio [0.01, 0.03, 0.05]????????????????????????????????????????????SFT??????????????LoRA???????????????; ??????????????LoRA ?????SFT??????????Llama2-base ????SFT???????????????????????????????????????????????embedding????????????????????; ??????????????????????????????????????loss/LR???????Material?§³?; ??????????BELLE????§Ó?????50??SFT???????SFT?????",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"????????LLama2-chat??????????????????????????????????????????????????????????LLama2-chat 13B ????????§à???¨¢?; ????????LLama2-chat 13B ???????????????embedding ?? LM head ??Lora??????????????????????????????????¨¢???????????sft_lora_model?Llama2-chat 13B ???§Ü????; ??????????BELLE????§Ó?????50??SFT???????SFT?????; Since the LLama2-chat model struggles to confine its responses to Chinese language when prompted with Chinese questions, the primary objective of this model is to provide a LLama2-chat 13B model that can engage in question and answer interactions in Chinese.; The model utilizes LLama2-chat 13B as its base model and is trained using the Lora training approach with the embedding and LM head. The model has undergone the Lora param merge and is now ready for direct use. It is also possible to manually merge the ./sft_lora_model with the Llama2-chat 13B model to obtain the combined model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"????????LLama2-chat??????????????????????????????????????????????????????????LLama2-chat 7B ????; ????????LLama2-chat 7B ???????????????embedding ?? LM head ??Lora??????????????????????????????????¨¢???????????sft_lora_model?Llama2-chat???§Ü????; ??????????BELLE????§Ó?????50??SFT???????SFT?????; Since the LLama2-chat model struggles to confine its responses to Chinese language when prompted with Chinese questions, the primary objective of this model is to provide a LLama2-chat 7B model that can engage in question and answer interactions in Chinese.; The model utilizes LLama2-chat 7B as its base model and is trained using the Lora training approach with the embedding and LM head. The model has undergone the Lora param merge and is now ready for direct use. It is also possible to manually merge the ./sft_lora_model with the Llama2-chat 7B model to obtain the combined model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
????Llama2????????????????????????????????????meta-llama/Llama-2-7b-chat-hf????LoRA???????????????????????????; ? ?e·Ú?LoRA???????????FlagAlpha/Llama2-Chinese-7b-Chat-LoRA??meta-llama/Llama-2-7b-chat-hf?????????·Ú??????????; Github??Llama2-Chinese; ?????????????llama.family; ???????Llama2??????????,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
????Llama2????????????????????????????????????meta-llama/Llama-2-7b-chat-hf????LoRA???????????????????????????; ? ?e·Ú??????LoRA???????????????????????meta-llama/Llama-2-7b-chat-hf????????; Github??Llama2-Chinese; ?????????????llama.family; ???????Llama2??????????,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The following bitsandbytes quantization config was used during training:; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The following bitsandbytes quantization config was used during training:; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"NOTE: This ""delta model"" cannot be used directly.Users have to apply it on top of the original LLaMA weights to get actual LLaVA weights.See https://github.com/haotian-liu/LLaVA#llava-weights for instructions.; Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA was trained in April 2023.; Paper or resources for more information:
https://llava-vl.github.io/; License:
Apache License 2.0",,,,,,,,,,,,,,,,,,,,,,,,,usage 14
"NOTE: This ""delta model"" cannot be used directly.Users have to apply it on top of the original LLaMA weights to get actual LLaVA weights.See https://github.com/haotian-liu/LLaVA#llava-weights for instructions.; Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA was trained in May 2023.; Paper or resources for more information:
https://llava-vl.github.io/; License:
Apache License 2.0",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"NOTE: This ""delta model"" cannot be used directly.Users have to apply it on top of the original LLaMA weights to get actual LLaVA weights.See https://github.com/haotian-liu/LLaVA#llava-weights for instructions.; Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA was trained in April 2023.; Paper or resources for more information:
https://llava-vl.github.io/; License:
Apache License 2.0",,,,,,,,,,,,,,,,,,,,,,,,,usage 10
"Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA-LLaMA-2-13B-Chat-Preview was trained in July 2023.; Paper or resources for more information:
https://llava-vl.github.io/; Llama 2 is licensed under the LLAMA 2 Community License, 
Copyright (c) Meta Platforms, Inc. All Rights Reserved.; Where to send questions or comments about the model:
https://github.com/haotian-liu/LLaVA/issues",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Model type:
LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
LLaVA-v1-0719-336px-LoRA-Vicuna-13B-v1.3 was trained in July 2023.; Paper or resources for more information:
https://llava-vl.github.io/; Non-commerical Use.; Where to send questions or comments about the model:
https://github.com/haotian-liu/LLaVA/issues",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; No model card; New: Create and edit this model card directly on the website!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"LLongMA-2, a suite of Llama-2 models, trained at 8k context length using linear positional interpolation scaling. The model was trained in collaboration with Emozilla of NousResearch and Kaiokendev.; We worked directly with Kaiokendev, to extend the context length of the Llama-2 7b model through fine-tuning. The models pass all our evaluations and maintain the same perplexity at 8k extrapolation surpassing the performance of other recent methodologies.; The model has identical performance to LLaMA 2 under 4k context length, performance scales directly to 8k, and works out-of-the-box with the new version of transformers (4.31) or with trust_remote_code for <= 4.30.; A Llama-2 13b model trained at 8k will release soon on huggingface here: https://huggingface.co/conceptofmind/LLongMA-2-13b; Applying the method to the rotary position embedding requires only slight changes to the model's code by dividing the positional index, t, by a scaling factor.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for ConceptofMind's LLongMA 2 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; This is an extended context base Llama 2 model.  Please check if your GGML client supports extended context. llama.cpp and KoboldCpp do, but I have not verified the others.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for ConceptofMind's LLongMA 2 7B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The goal of this project is to improve generation of loli characters since most of other models are not good at it. Support me: https://www.buymeacoffee.com/jilek772003 New models are available on https://pixai.art/ Model list: ; It is recommende to use standard resolution such as 512x768 and EasyNegative embedding with these models. Positive prompt example: 1girl, solo, loli, masterpiece Negative prompt example: EasyNegative, lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, multiple panels, aged up, old; Reddit: https://www.reddit.com/r/loliDiffusion Discord: https://discord.gg/mZ3eGeNX7S; v0.4.3 Fixed color issue General improvements v0.5.3 Integrated VAEFile size reduced CLIP force reset fix v0.6.3 Style improvements Added PastelMix and Counterfeit style v0.7.x Style impovements Composition improvements v0.8.x Major improvement on higher resolutions Style improvements Flexibility and responsivity Added support for Night Sky YOZORA model v0.9.x Different approach at merging, you might find v0.8.x versions better Changes at supported models v2.1.X EXPERIMENTAL RELEASE Stable Diffusion 2.1-768 based Default negative prompt: (low quality, worst quality:1.4), (bad anatomy), extra finger, fewer digits, jpeg artifacts For positive prompt it's good to include tags: anime, (masterpiece, best quality) alternatively you may achieve positive response with: (exceptional, best aesthetic, new, newest, best quality, masterpiece, extremely detailed, anime, waifu:1.2) Though it's Loli Diffusion model it's quite general purpose The ability to generate realistic images as Waifu Diffusion can was intentionally decreased This model performs better at higher resolutions like 768*X or 896*X v0.10.x Different approach at merging Better hands Better style inheritance Some changes in supported models ;",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"; TLDR | Overview | Usage | LongLLaMA performance | Authors | Citation | License | Acknowledgments; This repository contains the research preview of LongLLaMA, a large language model capable of handling long contexts of 256k tokens or even more. ; LongLLaMA is built upon the foundation of OpenLLaMA and fine-tuned using the Focused Transformer (FoT) method.  We release a smaller 3B variant of the LongLLaMA model on a permissive license (Apache 2.0) and inference code supporting longer contexts on Hugging Face. Our model weights can serve as the drop-in replacement of LLaMA in existing implementations (for short context up to 2048 tokens). Additionally, we provide evaluation results and comparisons against the original OpenLLaMA models. Stay tuned for further updates.; Focused Transformer: Contrastive Training for Context Scaling (FoT) presents a simple method for endowing language models with the ability to handle context consisting possibly of millions of tokens while training on significantly shorter input. FoT permits a subset of attention layers to access a memory cache of (key, value) pairs to extend the context length. The distinctive aspect of FoT is its training procedure, drawing from contrastive learning. Specifically, we deliberately expose the memory attention layers to both relevant and irrelevant keys (like negative samples from unrelated documents). This strategy incentivizes the model to differentiate keys connected with semantically diverse values, thereby enhancing their structure. This, in turn, makes it possible to extrapolate the effective context length much beyond what is seen in training.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Please use load_model from FastChat or LongChat repo to load the model (or chatting API from FastChat). There is a monkey patch needed to use the model.
Usage referece:; (LongChat) python3 eval.py --model-name-or-path  lmsys/longchat-13b-16k --task topics ; (FastChat) python3 -m fastchat.serve.cli --model-path lmsys/longchat-13b-16k; Under the hood, the monkey patch is added in:; https://github.com/lm-sys/FastChat/blob/da0641e567cf93756b0978ab5a6b092e96f06240/fastchat/model/model_adapter.py#L429",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Please use load_model from FastChat or LongChat repo to load the model (or chatting API from FastChat). There is a monkey patch needed to use the model.
Usage referece:; (LongChat) python3 eval.py --model-name-or-path  lmsys/longchat-7b-16k --task topics ; (FastChat) python3 -m fastchat.serve.cli --model-path lmsys/longchat-7b-16k; Under the hood, the monkey patch is added in:; https://github.com/lm-sys/FastChat/blob/da0641e567cf93756b0978ab5a6b092e96f06240/fastchat/model/model_adapter.py#L429",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Longformer is a transformer model for long documents. ; longformer-base-4096 is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096. ; Longformer uses a combination of a sliding window (local) attention and global attention. Global attention is user-configured based on the task to allow the model to learn task-specific representations.
Please refer to the examples in modeling_longformer.py and the paper for more details on how to set global attention.; If you use Longformer in your research, please cite Longformer: The Long-Document Transformer.; Longformer is an open-source project developed by the Allen Institute for Artificial Intelligence (AI2).
AI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
????w??Jujo Hotaru???????????LoRA??????????????You can download JujoHotaru's LoRA collection from this repo.; (????)??(????)??? / (??_??)??(??_??)??? / ?\??????A / ????????A / ??????????A / ??????????B / ????????? / ????_?????? / ???§à? / ???????????????? / ??`??`???{?? / ????{?? / ???????a?? / ??????? / ??????§¸? / ???? / ??????? / ??????; ?g?YLoRA?????; ????????????????`?? (Details/Download);,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Silder: skinny
; Slider: short legs
; Concepts: Breathing fire
; This LyCORIS is blockweighted.
breathing_fire_full.safetensors is full version.
The full version has the same effect as the general version by applying the following settings in Lora Block Weight.; Backgrounds: ""Danchi"" is Japanese public housing complex.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The model is created using the following steps:; Since this is only the first official release, I believe there are still many, many imperfections. 
Please provide feedback in time, and I will continuously make corrections, thank you!",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"These are LoRA adaption weights for diffusers/stable-diffusion-xl-base-0.9. The weights were trained on a photo of sks dog using DreamBooth. You can find some example images in the following. ; 


; LoRA for the text encoder was enabled: False.; SDXL 0.9 Research License",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; LyCORIS (LoCon, LoHa, LoKR, DyLoRA) located here:
https://huggingface.co/LMFResearchSociety/LyCORISArchive; Too much of a pain to separate different types of LyCoris. Some LoCons might be mixed in with LoRAs if the repository doesn't spearate them. Just drop a note in the discord groupchat if you're bothered.; Example Generator: 
https://civitai.com/models/27968 
https://huggingface.co/Lykon/AnyLoRA",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Original Author: Soumik Rakshit 
Date created: 2021/09/18 
HF Contribution: Harveen Singh Chadha
Dataset: LOL Dataset; Zero-Reference Deep Curve Estimation or Zero-DCE formulates low-light image enhancement as the task of estimating an image-specific tonal curve with a deep neural network. In this example, we train a lightweight deep network, DCE-Net, to estimate pixel-wise and high-order tonal curves for dynamic range adjustment of a given image.; Zero-DCE takes a low-light image as input and produces high-order tonal curves as its output. These curves are then used for pixel-wise adjustment on the dynamic range of the input to obtain an enhanced image. The curve estimation process is done in such a way that it maintains the range of the enhanced image and preserves the contrast of neighboring pixels. This curve estimation is inspired by curves adjustment used in photo editing software such as Adobe Photoshop where users can adjust points throughout an image??s tonal range.; Zero-DCE is appealing because of its relaxed assumptions with regard to reference images: it does not require any input/output image pairs during training. This is achieved through a set of carefully formulated non-reference loss functions, which implicitly measure the enhancement quality and guide the training of the network.; Sample Images:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"??Luna AI Llama2 Uncensored?? is a Llama2 based Chat model fine-tuned on over 40,000 long form chat discussions 
  This model was fine-tuned by Tap, the creator of Luna AI.  
  The result is an enhanced Llama2 7b model that rivals ChatGPT in performance across a variety of tasks.; This model stands out for its long responses,  low hallucination rate, and absence of censorship mechanisms. ; The fine-tuning process was performed on an 8x a100 80GB machine.
  The model was trained almost entirely on synthetic outputs.
  This includes data from diverse sources which we included to create our custom dataset, it includes multiple rounds of chats between Human & AI.
; 4bit GPTQ Version provided by @TheBloke - for GPU inference
GGML Version provided by @TheBloke - For CPU inference; The model follows the Vicuna 1.1/ OpenChat format:",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"Model Description; ??Tap-M/Luna-AI-Llama2-Uncensored?? is a latest chat language model which is fine-tuned on llama2 7b with custom datasets of multiple rounds of chats between Human & AI. This model was fine-tuned by Tap, the creator of Luna AI. The result is an enhanced Uncensored Llama2 7b Chat model that rivals many Open Source Chat Models in performance across a variety of tasks.
This model stands out for its long responses, low hallucination rate, and absence of censorship mechanisms.; Model Training; The fine-tuning process was performed on an 8x a100 80GB machine. The model was trained almost entirely on synthetic outputs. This includes data from diverse sources which we included to create our custom dataset, it includes multiple rounds of chats between Human & AI.; Prompt Format",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Tap-M's Luna AI Llama2 Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Quantization from:
Tap-M/Luna-AI-Llama2-Uncensored; Converted to the GGML format with:
llama.cpp master-294f424 (JUL 19, 2023); Tested with:
koboldcpp 1.35; Example usage:; Prompt format (refer to the original model for additional details):",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Tap-M's Luna AI Llama2 Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"We?know?what?you?want,?and?here?you go!; Note?that?the?code?was?fully?updated?too,?you?need?to?use?the new?API,?see?Uses?below; If you like our work and consider to join us, feel free to drop a line to benbinwu@tencent.com.; P.S. Recently we have received a lot of inquiries on accelerating customized models. Actually, we do not have plan to release the convertion tool at this moment, nor do we think it would be possible to apply your customized models based on our current release.; lyraChatGLM?is?currently?the?fastest?ChatGLM-6B?available.?To?the?best?of?our?knowledge,?it?is?the?first?accelerated?version?of?ChatGLM-6B.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many multilingual translation.
It was introduced in this paper and first released in this repository.; The model that can directly translate between the 9,900 directions of 100 languages.
To translate into a target language, the target language id is forced as the first generated token.
To force the target language id as the first generated token, pass the forced_bos_token_id parameter to the generate method.; Note: M2M100Tokenizer depends on sentencepiece, so make sure to install it before running the example.; To install sentencepiece run pip install sentencepiece; See the model hub to look for more fine-tuned versions.",,,,,,,,,,,,,,,,,,,,,,,,,usage 23
m3e-small | m3e-base; M3E ?? Moka Massive Mixed Embedding ????§Õ; ?????; Tips:; ????????? sentence-transformers,,,,,,,,,,,,,,,,,,,,,,,,,usage 4
m3e-small | m3e-base | m3e-large; M3E ?? Moka Massive Mixed Embedding ????§Õ; ?????; Tips:; ????????? sentence-transformers,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
m3e-small | m3e-base; M3E ?? Moka Massive Mixed Embedding ????§Õ; ?????; Tips:; ????????? sentence-transformers,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
?????§Õ???????; macbert4csc-base-chinese evaluate SIGHAN2015 test data??; ????????????????????SIGHAN2015?????????????paper??????SIGHAN2015?????????SOTA????; ???????????softmaskedbert??;,,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"This is a model from the MagicPrompt series of models, which are GPT-2 models intended to generate prompt texts for imaging AIs, in this case: Stable Diffusion.; This model was trained with 150,000 steps and a set of about 80,000 data filtered and extracted from the image finder for Stable Diffusion: ""Lexica.art"". It was a little difficult to extract the data, since the search engine still doesn't have a public API without being protected by cloudflare, but if you want to take a look at the original dataset, you can have a look here: datasets/Gustavosta/Stable-Diffusion-Prompts.; If you want to test the model with a demo, you can go to: ""spaces/Gustavosta/MagicPrompt-Stable-Diffusion"".; MIT; When using this model, please credit: Gustavosta",,,,,,,,,,,,,,,,,,,,,,,,,usage 124
"Currently Available Models:; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Model info :; https://civitai.com/models/43331?modelVersionId=94640,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Optical character recognition for Japanese text, with the main focus being Japanese manga.; It uses Vision Encoder Decoder framework.; Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality
text recognition, robust against various scenarios specific to manga:; Code is available here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"; Manticore 13B Chat builds on Manticore with new datasets, including a de-duped subset of the Pygmalion dataset. It also removes all Alpaca style prompts using ### in favor of 
chat only style prompts using USER:,ASSISTANT: as well as pygmalion/metharme prompting using <|system|>, <|user|> and <|model|> tokens.; Questions, comments, feedback, looking to donate, or want to help? Reach out on our Discord or email wing@openaccessaicollective.org; Manticore 13B Chat is a Llama 13B model fine-tuned on the following datasets along with the datasets from the original Manticore 13B. ; Manticore 13B Chat was trained on 25% of the datasets below. The datasets were merged, shuffled, and then sharded into 4 parts.",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"; Version: 1.3; Last Update: 3-12-2021; Marefa-NER is a Large Arabic Named Entity Recognition (NER) model built on a completely new dataset and targets to extract up to 9 different types of entities; ????? ??????? ?????? ????? ????. ????? ???? ???? ?? ??? ???????? ????????? ?? ????? ???????. 
???? ?????? ??????? ????? ??? 9 ????? ?????? ?? ????? ????",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Multimodal (text +markup language) pre-training for Document AI; MarkupLM is a simple but effective multi-modal pre-training method of text and markup language for visually-rich document understanding and information extraction tasks, such as webpage QA and webpage information extraction. MarkupLM archives the SOTA results on multiple datasets. For more details, please refer to our paper:; MarkupLM: Pre-training of Text and Markup Language for Visually-rich Document Understanding  Junlong Li, Yiheng Xu, Lei Cui, Furu Wei; We refer to the docs and demo notebooks.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. ; Disclaimer: The team releasing MaskFormer did not write a model card for this model so this model card has been written by the Hugging Face team.; MaskFormer addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.; ; You can use this particular checkpoint for semantic segmentation. See the model hub to look for other
fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; This model is the MatCha model, fine-tuned on Chart2text-pew dataset. ; The abstract of the paper states that: ; Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art visionlanguage models do not perform well on these data. We propose MATCHA (Math reasoning and Chart derendering pretraining) to enhance visual language models?? capabilities jointly modeling charts/plots and language data. Specifically we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MATCHA pretraining starting from Pix2Struct, a recently proposed imageto-text visual language model. On standard benchmarks such as PlotQA and ChartQA, MATCHA model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MATCHA pretraining transfers to domains such as screenshot, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MATCHA pretraining on broader visual language tasks.; You should ask specific questions to the model in order to get consistent generations. Here we are asking the model whether the sum of values that are in a chart are greater than the largest value.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"mBART-50 is a multilingual Sequence-to-Sequence model pre-trained using the ""Multilingual Denoising Pretraining"" objective. It was introduced in Multilingual Translation with Extensible Multilingual Pretraining and Finetuning paper.; mBART-50 is a multilingual Sequence-to-Sequence model. It was introduced to show that multilingual translation models can be created through multilingual fine-tuning. 
Instead of fine-tuning on one direction, a pre-trained model is fine-tuned on many directions simultaneously. mBART-50 is created using the original mBART model and extended to add extra 25 languages to support multilingual machine translation models of 50 languages. The pre-training objective is explained below.; Multilingual Denoising Pretraining: The model incorporates N languages by concatenating data: 
D = {D1, ..., DN } where each Di is a collection of monolingual documents in language i. The source documents are noised using two schemes, 
first randomly shuffling the original sentences' order, and second a novel in-filling scheme, 
where spans of text are replaced with a single mask token. The model is then tasked to reconstruct the original text. 
35% of each instance's words are masked by random sampling a span length according to a Poisson distribution (?? = 3.5).
The decoder input is the original text with one position offset. A language id symbol LID is used as the initial token to predict the sentence.; mbart-large-50 is pre-trained model and primarily aimed at being fine-tuned on translation tasks. It can also be fine-tuned on other multilingual sequence-to-sequence tasks. 
See the model hub to look for fine-tuned versions.; As the model is multilingual, it expects the sequences in a different format. A special language id token is used as a prefix in both the source and target text. The text format is [lang_code] X [eos] with X being the source or target text respectively and lang_code is source_lang_code for source text and tgt_lang_code for target text. bos is never used. Once the examples are prepared in this format, it can be trained as any other sequence-to-sequence model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"This is the model checkpoint for our work mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs.; mBLIP is a BLIP-2 model which consists of 3 sub-models: a Vision Transformer (ViT), a Query-Transformer (Q-Former) and a large language model (LLM).; The Q-Former and ViT have both been initialized by an English BLIP-2 checkpoint (blip2-flan-t5-xl) and then re-aligned 
to the multilingual LLM (mt0-xl) using a multilingual task mixture.;  ; This allows the model to be used for tasks like:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual 
zero-shot classification. The underlying model was pre-trained by Microsoft on the 
CC100 multilingual dataset. It was then fine-tuned on the XNLI dataset, which contains hypothesis-premise pairs from 15 languages, as well as the English MNLI dataset.
As of December 2021, mDeBERTa-base is the best performing multilingual base-sized transformer model, 
introduced by Microsoft in this paper. ; If you are looking for a smaller, faster (but less performant) model, you can 
try multilingual-MiniLMv2-L6-mnli-xnli.; This model was trained on the XNLI development dataset and the MNLI train dataset. The XNLI development set consists of 2490 professionally translated texts from English to 14 other languages (37350 texts in total) (see this paper). Note that the XNLI contains a training set of 15 machine translated versions of the MNLI dataset for 15 languages, but due to quality issues with these machine translations, this model was only trained on the professional translations from the XNLI development set and the original English MNLI training set (392 702 texts). Not using machine translated texts can avoid overfitting the model to the 15 languages; avoids catastrophic forgetting of the other 85 languages mDeBERTa was pre-trained on; and significantly reduces training costs. ; mDeBERTa-v3-base-mnli-xnli was trained using the Hugging Face trainer with the following hyperparameters.; The model was evaluated on the XNLI test set on 15 languages (5010 texts per language, 75150 in total). Note that multilingual NLI models are capable of classifying NLI texts without receiving NLI training data in the specific language (cross-lingual transfer). This means that the model is also able of doing NLI on the other 85 languages mDeBERTa was training on, but performance is most likely lower than for those languages available in XNLI.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"It has been finetuned for 3 epochs on SQuAD2.0.; DeBERTa improves the BERT and RoBERTa models using disentangled attention and enhanced mask decoder. With those two improvements, DeBERTa out perform RoBERTa on a majority of NLU tasks with 80GB training data. ; In DeBERTa V3, we further improved the efficiency of DeBERTa using ELECTRA-Style pre-training with Gradient Disentangled Embedding Sharing. Compared to DeBERTa,  our V3 version significantly improves the model performance on downstream tasks.  You can find more technique details about the new model from our paper.; Please check the official repository for more implementation details and updates.; mDeBERTa is multilingual version of DeBERTa which use the same structure as DeBERTa and was trained with CC100 multilingual data.
The mDeBERTa V3 base model comes with 12 layers and a hidden size of 768. It has 86M backbone parameters  with a vocabulary containing 250K tokens which introduces 190M parameters in the Embedding layer.  This model was trained using the 2.5T CC100 data as XLM-R.",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"Multilingual mdeberta-v3-base with 30k steps multi-task training on mtasksource
This model can be used as a stable starting-point for further fine-tuning, or directly in zero-shot NLI model or a zero-shot pipeline.
In addition, you can use the provided adapters to directly load a model for hundreds of tasks. ; For more details, see deberta-v3-base-tasksource-nli and replace tasksource by mtasksource.; https://github.com/sileod/tasksource/
https://github.com/sileod/tasknet/; For help integrating tasksource into your experiments, please contact damien.sileo@inria.fr.; For more details, refer to this article:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This multilingual model can perform natural language inference (NLI) on 100 languages and is therefore also suitable for multilingual zero-shot classification. The underlying mDeBERTa-v3-base model was pre-trained by Microsoft on the CC100 multilingual dataset with 100 languages. The model was then fine-tuned on the XNLI dataset and on the multilingual-NLI-26lang-2mil7 dataset. Both datasets contain more than 2.7 million hypothesis-premise pairs in 27 languages spoken by more than 4 billion people. ; As of December 2021, mDeBERTa-v3-base is the best performing multilingual base-sized transformer model introduced by Microsoft in this paper. ; This model was trained on the multilingual-nli-26lang-2mil7 dataset and the XNLI validation dataset. ; The multilingual-nli-26lang-2mil7 dataset contains 2 730 000 NLI hypothesis-premise pairs in 26 languages spoken by more than 4 billion people. The dataset contains 105 000 text pairs per language. It is based on the English datasets MultiNLI, Fever-NLI, ANLI, LingNLI and WANLI and was created using the latest open-source machine translation models. The languages in the dataset are: ['ar', 'bn', 'de', 'es', 'fa', 'fr', 'he', 'hi', 'id', 'it', 'ja', 'ko', 'mr', 'nl', 'pl', 'ps', 'pt', 'ru', 'sv', 'sw', 'ta', 'tr', 'uk', 'ur', 'vi', 'zh'] (see ISO language codes. For more details, see the datasheet. In addition, a sample of 105 000 text pairs was also added for English following the same sampling method as the other languages, leading to 27 languages. ; Moreover, for each language a random set of 10% of the hypothesis-premise pairs was added where an English hypothesis was paired with the premise in the other language (and the same for English premises and other language hypotheses). This mix of languages in the text pairs should enable users to formulate a hypothesis in English for a target text in another language.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"Model Description ; medalpaca-lora-30b-8bit is a large language model specifically fine-tuned for medical domain tasks. 
It is based on LLaMA (Large Language Model Meta AI) and contains 7 billion parameters. 
The primary goal of this model is to improve question-answering and medical dialogue tasks.
It was trained using LoRA and quantized, to reduce memory footprint. ; The training data for this project was sourced from various resources. 
Firstly, we used Anki flashcards to automatically generate questions, 
from the front of the cards and anwers from the back of the card. 
Secondly, we generated medical question-answer pairs from Wikidoc. 
We extracted paragraphs with relevant headings, and used Chat-GPT 3.5 
to generate questions from the headings and using the corresponding paragraphs 
as answers. This dataset is still under development and we believe 
that approximately 70% of these question answer pairs are factual correct. 
Thirdly, we used StackExchange to extract question-answer pairs, taking the 
top-rated question from five categories: Academia, Bioinformatics, Biology, 
Fitness, and Health. Additionally, we used a dataset from ChatDoctor 
consisting of 200,000 question-answer pairs, available at https://github.com/Kent0n-Li/ChatDoctor.; The model may not perform effectively outside the scope of the medical domain.
The training data primarily targets the knowledge level of medical students, 
which may result in limitations when addressing the needs of board-certified physicians.
The model has not been tested in real-world applications, so its efficacy and accuracy are currently unknown. 
It should never be used as a substitute for a doctor's opinion and must be treated as a research tool only.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Model Description ; nmitchko/medguanaco-lora-65b-GPTQ is a large language model specifically fine-tuned for medical domain tasks.
It is based on the Guanaco LORA of LLaMA weighing in at 65B parameters. 
The primary goal of this model is to improve question-answering and medical dialogue tasks.
It was trained using LoRA and quantized, to reduce memory footprint. ; Steps to load this model:; The following README is taken from the source page medalpaca; The training data for this project was sourced from various resources. 
Firstly, we used Anki flashcards to automatically generate questions, 
from the front of the cards and anwers from the back of the card. 
Secondly, we generated medical question-answer pairs from Wikidoc. 
We extracted paragraphs with relevant headings, and used Chat-GPT 3.5 
to generate questions from the headings and using the corresponding paragraphs 
as answers. This dataset is still under development and we believe 
that approximately 70% of these question answer pairs are factual correct. 
Thirdly, we used StackExchange to extract question-answer pairs, taking the 
top-rated question from five categories: Academia, Bioinformatics, Biology, 
Fitness, and Health. Additionally, we used a dataset from ChatDoctor 
consisting of 200,000 question-answer pairs, available at https://github.com/Kent0n-Li/ChatDoctor.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
WizardLM-Uncensored-7B + MedAlpaca-7B (50%/50%); WizardLM-Uncensored-7B: https://huggingface.co/ehartford/WizardLM-7B-Uncensored; MedAlpaca-7B: https://huggingface.co/medalpaca/medalpaca-7b,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"Model obtained by Fine Tuning 'facebook/bart-large-xsum' using AMI Meeting Corpus, SAMSUM Dataset, DIALOGSUM Dataset, XSUM Dataset!",,,,,,,,,,,,,,,,,,,,,,,,,usage 18
"MeinaMix Objective is to be able to do good art with little prompting.; For examples and prompts, please checkout: https://civitai.com/models/7240/meinamix
I have a discord server where you can post images that you generated, discuss prompt and/or ask for help.; https://discord.gg/XC9nGZNDUd If you like one of my models and want to support their updates; I've made a ko-fi page; https://ko-fi.com/meina where you can pay me a coffee <3; And a Patreon page; https://www.patreon.com/MeinaMix where you can support me and get acess to beta of my models!",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Model info :; https://civitai.com/models/7240?modelVersionId=119057; Sample images generated by huggingface's API :; 
; prompt :",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"MeinaPastel aims to make illustrations with a 2d feel to them with good light, shadows and details, making pastel or colorful images!; -- Recommendations of use:; -- If you like the model and wants to support me in being able to spend more time improving it:
-- You can do so by buying me a coffee at: https://ko-fi.com/meina ! ( it is not necessary but will be highly appreciated ); This model is a unet block merge of mostly MeinaMix and Colormixed, ultracolorv4 and a few others with minor block weight taken.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B pretrained model, converted for the Hugging Face Transformers format.; This model is a 4-bit 128 group size AWQ quantized model. For more information about AWQ quantization, please click here.; July 19, 2023; Please refer to the original LLaMA 2 model license (link).; Please refer to the AWQ quantization license (link).",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Metharme 13B is an instruct model based on Meta's LLaMA-13B.; This is an experiment to try and get a model that is usable for conversation, roleplaying and storywriting, but which can be guided using natural language like other instruct models. See the prompting section below for examples.; It was trained by doing supervised fine-tuning over a mixture of regular instruction data alongside roleplay, fictional stories and conversations with synthetically generated instructions attached.; The model weights in this repository cannot be used as-is. The files here are XORs due to licensing concerns. To obtain proper, usable model weights you need to:; Request access to the original LLaMA weights from Meta through this form",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository provides all the necessary tools to perform enhancement with
SpeechBrain. For a better experience we encourage you to learn more about
SpeechBrain. The model performance is:; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; To use the mimic-loss-trained model for enhancement, use the following simple code:; The system is trained with recordings sampled at 16kHz (single channel).
The code will automatically normalize your audio (i.e., resampling + mono channel selection) when calling enhance_file if needed. Make sure your input tensor is compliant with the expected sampling rate if you use enhance_batch as in the example.",,,,,,,,,,,,,,,,,,,,,,,,,usage 24
"Multilingual language model. This model was trained on the 61 languages from 25 language families (see the list below).; Model was pretrained on a 600Gb of texts, mostly from MC4 and Wikipedia. Training data was deduplicated, the text deduplication includes 64-bit hashing of each text in the corpus for keeping texts with a unique hash. We also filter the documents based on their text compression rate using zlib4. The most strongly and weakly compressing deduplicated texts are discarded.; Here is the table with number of tokens for each language in the pretraining corpus on a logarithmic scale:; ; Afrikaans (af), Arabic (ar), Armenian (hy), Azerbaijani (az), Basque (eu), Bashkir (ba), Belarusian (be), Bengali (bn), Bulgarian (bg), Burmese (my), Buryat (bxr), Chuvash (cv), Danish (da), English (en), Estonian (et), Finnish (fi), French (fr), Georgian (ka), German (de), Greek (el), Hebrew (he), Hindi (hi), Hungarian (hu), Indonesian (id), Italian (it), Japanese (ja), Javanese (jv), Kalmyk (xal), Kazakh (kk), Korean (ko), Kyrgyz (ky), Latvian (lv), Lithuanian (lt), Malay (ms), Malayalam (ml), Marathi (mr), Mongolian (mn), Ossetian (os), Persian (fa), Polish (pl), Portuguese (pt), Romanian (ro), Russian (ru), Spanish (es), Swedish (sv), Swahili (sw), Tatar (tt), Telugu (te), Thai (th), Turkish (tr), Turkmen (tk), Tuvan (tyv), Ukrainian (uk), Uzbek (uz), Vietnamese (vi), Yakut (sax), Yoruba (yo)",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Deyao Zhu* (On Job Market!), Jun Chen* (On Job Market!), Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. *Equal Contribution; King Abdullah University of Science and Technology; Click the image to chat with MiniGPT-4 around your images
; More examples can be found in the project page.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 43
Minotaur-13B with 10k+ context using Landmark Attention.; Model generated using Landmark-Attention-QLoRA; https://github.com/eugenepentland/landmark-attention-qlora; A merge of the following models:; https://huggingface.co/openaccess-ai-collective/minotaur-13b,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Model info :; https://civitai.com/models/101423/miraclemix-glitter-an-anime-model-trained-and-specialized-on-creating-detailed-images-for-stunning-wallpaper; Original Author's DEMO images :;,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. ; Disclaimer: The team releasing SegFormer did not write a model card for this model so this model card has been written by the Hugging Face team.; SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.; This repository only contains the pre-trained hierarchical Transformer, hence it can be used for fine-tuning purposes.; You can use the model for fine-tuning of semantic segmentation. See the model hub to look for fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 14
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This checkpoint is a model fine-tuned for multi-lingual ASR and part of Facebook's Massive Multilingual Speech project.
This checkpoint is based on the Wav2Vec2 architecture and makes use of adapter models to transcribe 1000+ languages.
The checkpoint consists of 1 billion parameters and has been fine-tuned from facebook/mms-1b on 1162 languages.; This MMS checkpoint can be used with Transformers to transcribe audio of 1107 different 
languages. Let's look at a simple example.; First, we install transformers and some other libraries; Note: In order to use MMS you need to have at least transformers >= 4.30 installed. If the 4.30 version
is not yet available on PyPI make sure to install transformers from 
source:; Next, we load a couple of audio samples via datasets. Make sure that the audio data is sampled to 16000 kHz.",,,,,,,,,,,,,,,,,,,,,,,,,usage 24
"This checkpoint is a model fine-tuned for multi-lingual ASR and part of Facebook's Massive Multilingual Speech project.
This checkpoint is based on the Wav2Vec2 architecture and makes use of adapter models to transcribe 100+ languages.
The checkpoint consists of 1 billion parameters and has been fine-tuned from facebook/mms-1b on 102 languages of Fleurs.; This MMS checkpoint can be used with Transformers to transcribe audio of 1107 different 
languages. Let's look at a simple example.; First, we install transformers and some other libraries; Note: In order to use MMS you need to have at least transformers >= 4.30 installed. If the 4.30 version
is not yet available on PyPI make sure to install transformers from 
source:; Next, we load a couple of audio samples via datasets. Make sure that the audio data is sampled to 16000 kHz.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Mo Di Diffusion; This is the fine-tuned Stable Diffusion 1.5 model trained on screenshots from a popular animation studio.
Use the tokens modern disney style in your prompts for the effect.; If you enjoy my work, please consider supporting me
; Videogame Characters rendered with the model:

Animal Characters rendered with the model:

Cars and Landscapes rendered with the model:
; modern disney lara croft
Steps: 50, Sampler: Euler a, CFG scale: 7, Seed: 3940025417, Size: 512x768",,,,,,,,,,,,,,,,,,,,,,,,,usage 318
"MobileViT model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer by Sachin Mehta and Mohammad Rastegari, and first released in this repository. The license used is Apple sample code license.; Disclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.; MobileViT is a light-weight, low latency convolutional neural network that combines MobileNetV2-style layers with a new block that replaces local processing in convolutions with global processing using transformers. As with ViT (Vision Transformer), the image data is converted into flattened patches before it is processed by the transformer layers. Afterwards, the patches are ""unflattened"" back into feature maps. This allows the MobileViT-block to be placed anywhere inside a CNN. MobileViT does not require any positional embeddings.; You can use the raw model for image classification. See the model hub to look for fine-tuned versions on a task that interests you.; Here is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a T5-base reranker fine-tuned on the MS MARCO passage dataset for 10k steps (or 1 epoch).; This model usually has a better zero-shot performance than monot5-base-msmarco, i.e., it performs better on datasets different from MS MARCO.; For more details on how to use it, check the following links:; Paper describing the model: Document Ranking with a Pretrained Sequence-to-Sequence Model",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Moss-base-7b?????70???????????????????????????????????????????SFT??????; To load the Moss 7B model using Transformers, use the following code:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"MOSS is an open-sourced plugin-augmented conversational language model. moss-moon models have 16B parameters, allowing users to perform inference on a single A100 GPU or 2 NVIDIA 3090 GPUs with FP16 precision, and on a single NVIDIA 3090 GPU with INT-4/8 precision. The base language model of MOSS was pre-trained on ~700B English, Chinese, and code tokens, including the PILE, BigQuery, BigPython, and our private Chinese corpus. The base model was then fine-tuned on multi-turn plugin-augmented conversational data. Finally, we performed preference-aware training to further improve the model.; Limitations: Due to the (relatively) small number of parameters and the autoregressive nature, MOSS is still possible to generate outputs that contain incorrect, misleading, or biased information. Please carefully check the contents generated by MOSS before you use them.; MOSS Use Cases??; ;",,,,,,,,,,,,,,,,,,,,,,,,,usage 119
"moss-rlhf-reward-model-7B-zh
; moss-rlhf-reward-model-7B-en; moss-rlhf-sft-model-7B-en
; Due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle.
In this technical report, we intend to help researchers to train their models stably with human feedback.; Contributions are summarized as follows:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the official PyTorch implementation of the paper ""Learning Human Motion Representations: A Unified Perspective"".; Please refer to docs/inference.md.; Hints; In most use cases (especially with finetuning), MotionBERT-Lite gives a similar performance with lower computation overhead. ;  
Scripts and docs for pretraining",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used as a retriever model in open-domain question-answering tasks.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; The model was fine-tuned on question-answer pairs scraper from several ML-focused Discourse forums [HuggingFace, PyTorch, Streamlit, TensorFlow].",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"MPT-1b-RedPajama-200b-dolly is a 1.3 billion parameter decoder-only transformer pre-trained on the RedPajama dataset and subsequently fine-tuned on the Databricks Dolly instruction dataset.
The model was pre-trained for 200B tokens by sampling from the subsets of the RedPajama dataset in the same proportions as were used by the Llama series of models.
This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; This model is an instruction fine-tuned version of mpt-1b-redpajama-200b. In other words, the pre-trained version of this model is mpt-1b-redpajama-200b.; April 20, 2023; Note: This model requires that trust_remote_code=True be passed to the from_pretrained method. 
This is because we use a custom model architecture MosaicGPT that is not yet part of the transformers package.
MosaicGPT includes options for many training efficiency features such as FlashAttention (Dao et al. 2022), ALIBI, QK LayerNorm, and more.; To use the optimized triton implementation of FlashAttention, you can load with attn_impl='triton' and move the model to bfloat16 like so:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.
This model was trained by MosaicML.; MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.; MPT-30B comes with special features that differentiate it from other LLMs, including an 8k token context window (which can be further extended via finetuning; see MPT-7B-StoryWriter), support for context-length extrapolation via ALiBi, and efficient inference + training via FlashAttention. It also has strong coding abilities thanks to its pretraining mix. MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer.
The size of MPT-30B was also specifically chosen to make it easy to deploy on a single GPU??either 1xA100-80GB in 16-bit precision or 1xA100-40GB in 8-bit precision.; This model uses the MosaicML LLM codebase, which can be found in the llm-foundry repository. It was trained by MosaicML??s NLP team on the MosaicML platform for LLM pretraining, finetuning, and inference.; MPT-30B is:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"MPT-30B-Chat is a chatbot-like model for dialogue generation.
It was built by finetuning MPT-30B on the ShareGPT-Vicuna, Camel-AI,
 GPTeacher, Guanaco, Baize and some generated datasets.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; June 22, 2023; CC-By-NC-SA-4.0 (non-commercial use only); ksreenivasan:",,,,,,,,,,,,,,,,,,,,,,,,,usage 14
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are MPT GGML format model files for Manoj Preveen's MPT 30B Dolphin v2.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools that work with this GGML model.; These files are not compatible with llama.cpp or text-generation-webui.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"MPT-30B-Instruct is a model for short-form instruction following.
It is built by finetuning MPT-30B on Dolly HHRLHF derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets. It is also trained on Competition Math, Duorc, CoT GSM8k, Qasper, Quality, Summ Screen FD and Spider.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; June 22, 2023; CC-By-SA-3.0; Bespokenizer46",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"This is the MPT-30B but with added support to finetune using peft (tested with qlora). It is not finetuned further, the weights are the same as the original MPT-30b.; I have not traced through the whole huggingface stack to see if this is working correctly but it does finetune with qlora and outputs are reasonable.
Inspired by implementations here https://huggingface.co/cekal/mpt-7b-peft-compatible/commits/main
https://huggingface.co/mosaicml/mpt-7b/discussions/42.; The original description for MosaicML team below:; MPT-30B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.
This model was trained by MosaicML.; MPT-30B is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Base Model : mosaicml/mpt-30b; Tool : MosaicML's llm-foundry (https://github.com/mosaicml/llm-foundry); Dataset : Entire flan3m-GPT3.5 dataset.; Config yaml with Model Params : https://huggingface.co/manojpreveen/mpt-30b-v2/blob/main/mpt-30b_orca.yaml; Prompt Format :,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.
This model was trained by MosaicML.; MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.; These architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing
positional embeddings with Attention with Linear Biases (ALiBi).
Thanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence.
MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer.; This model uses the MosaicML LLM codebase, which can be found in the llm-foundry repository. It was trained by MosaicML??s NLP team on the MosaicML platform for LLM pretraining, finetuning, and inference.; MPT-7B is",,,,,,,,,,,,,,,,,,,,,,,,,usage 12
"MPT-7B-8k is a decoder-style transformer pretrained starting from MPT-7B, but updating the sequence length to 8k and training for an additional 500B tokens, resulting in a total of 1.5T tokens of text and code.
This model was trained by MosaicML.; MPT-7B-8k is part of the family of Mosaic Pretrained Transformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.; These architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing
positional embeddings with Attention with Linear Biases (ALiBi).
Thanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence.
MPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA's FasterTransformer.; This model uses the MosaicML LLM codebase, which can be found in the llm-foundry repository. It was trained by MosaicML??s NLP team on the MosaicML platform for LLM pretraining, finetuning, and inference.; MPT-7B-8k is",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"MPT-7B-Chat-8k is a chatbot-like model for dialogue generation.
It was built by finetuning MPT-7B-8k on the ShareGPT-Vicuna, Camel-AI,
 GPTeacher, Guanaco, Baize and some generated datasets.
 This is the same dataset that MPT-30B-Chat was trained on.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; July 18, 2023; CC-By-NC-SA-4.0 (non-commercial use only); This model is best used with the MosaicML llm-foundry repository for training and finetuning.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"MPT-7B-Instruct-8k is a model for long-form instruction following, especially question-answering on and summarization of longer documents.
It is built by finetuning MPT-7B-8k on Dolly HHRLHF derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets. It is also trained on Competition Math, Duorc, CoT GSM8k, Qasper, Quality, Summ Screen FD and Spider.
This is the same dataset that MPT-30B-Instruct was trained on.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; July 18, 2023;  CC-By-SA-3.0; This model is best used with the MosaicML llm-foundry repository for training and finetuning.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"MPT-7B-Chat is a chatbot-like model for dialogue generation.
It was built by finetuning MPT-7B on the ShareGPT-Vicuna, HC3,
 Alpaca, HH-RLHF, and Evol-Instruct datasets.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; CC-By-NC-SA-4.0 (non-commercial use only); SamIAm85:",,,,,,,,,,,,,,,,,,,,,,,,,usage 20
"MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.
This model was trained by MosaicML.; MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference. ; ??Caution??: mpt-7b-storywriter is still under development!; Via pip: pip install llm-rs; The GGML example only supports the ggml container type!",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"MPT-7B-Instruct is a model for short-form instruction following.
It is built by finetuning MPT-7B on a dataset derived from the Databricks Dolly-15k and the Anthropic Helpful and Harmless (HH-RLHF) datasets.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; CC-By-SA-3.0; Longboi24:",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"This is a fine-tuned version of mosaicml/mpt-7b-storywriter intended for summarization and literary analysis of fiction stories.; The code for this model includes the adaptions from Birchlabs/mosaicml-mpt-7b-chat-qlora which allow MPT models to be loaded with device_map=""auto"" and load_in_8bit=True.
It also has the latest key-value cache MPT code to allow for fast inference with transformers (thus, use_cache is set to True in config.json).; or; Outputs on the text of Waystation City (6,287 tokens); temperature=0.6, repetition_penalty=1.04, top_p=0.95, top_k=50, do_sample=True, max_new_tokens=1024",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"MPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths.
It was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset.
At inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.
We demonstrate generations as long as 84k tokens on a single node of 8 A100-80GB GPUs in our blogpost.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; Apache 2.0; Note: This model requires that trust_remote_code=True be passed to the from_pretrained method. This is because we use a custom model architecture that is not yet part of the transformers package.",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"Quantized for KoboldAI (4bit-fork); MPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths.
It was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset.
At inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.
We demonstrate generations as long as 84k tokens on a single node of 8 A100-80GB GPUs in our blogpost.; This model was trained by MosaicML and follows a modified decoder-only transformer architecture.; May 5, 2023; Apache-2.0 (commercial use permitted)",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is GGML format quantised 4-bit, 5-bit and 8-bit models of MosaicML's MPT-7B-Storywriter.; This repo is the result of converting to GGML and quantising.; Please note that these MPT GGMLs are not compatbile with llama.cpp. Please see below for a list of tools known to work with these model files.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This model was trained on the MS Marco Passage Ranking task.; The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco; The usage becomes easier when you have SentenceTransformers installed. Then, you can use the pre-trained models like this:; In the following table, we provide various pre-trained Cross-Encoders together with their performance on the TREC Deep Learning 2019 and the MS Marco Passage Reranking dataset. ;  Note: Runtime was computed on a V100 GPU.",,,,,,,,,,,,,,,,,,,,,,,,,usage 12
"This model was trained on the MS Marco Passage Ranking task.; The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco; The usage becomes easier when you have SentenceTransformers installed. Then, you can use the pre-trained models like this:; In the following table, we provide various pre-trained Cross-Encoders together with their performance on the TREC Deep Learning 2019 and the MS Marco Passage Reranking dataset. ;  Note: Runtime was computed on a V100 GPU.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model was trained on the MS Marco Passage Ranking task.; The model can be used for Information Retrieval: Given a query, encode the query will all possible passages (e.g. retrieved with ElasticSearch). Then sort the passages in a decreasing order. See SBERT.net Retrieve & Re-rank for more details. The training code is available here: SBERT.net Training MS Marco; The usage becomes easier when you have SentenceTransformers installed. Then, you can use the pre-trained models like this:; In the following table, we provide various pre-trained Cross-Encoders together with their performance on the TREC Deep Learning 2019 and the MS Marco Passage Reranking dataset. ;  Note: Runtime was computed on a V100 GPU.",,,,,,,,,,,,,,,,,,,,,,,,,usage 52
"This is a port of the DistilBert TAS-B Model to sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and was designed for semantic search. It has been trained on 500k (query, answer) pairs from the MS MARCO Passages dataset. For an introduction to semantic search, have a look at: SBERT.net - Semantic Search; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.; In the following some technical details how this model must be used:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find our resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t??aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find our resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t??aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"; We present BLOOMZ & mT0, a family of models capable of following human instructions in dozens of languages zero-shot. We finetune BLOOM & mT5 pretrained multilingual language models on our crosslingual task mixture (xP3) and find our resulting models capable of crosslingual generalization to unseen tasks & languages.; We recommend using the model to perform tasks expressed in natural language. For example, given the prompt ""Translate to English: Je t??aime."", the model will most likely answer ""I love you."". Some prompt ideas from our paper: ; Feel free to share your generations in the Community tab!; Prompt Engineering: The performance may vary depending on the prompt. For BLOOMZ models, we recommend making it very clear when the input stops to avoid the model trying to continue it. For example, the prompt ""Translate to English: Je t'aime"" without the full stop (.) at the end, may result in the model trying to continue the French sentence. Better prompts are e.g. ""Translate to English: Je t'aime."", ""Translate to English: Je t'aime. Translation:"" ""What is ""Je t'aime."" in English?"", where it is clear for the model when it should answer. Further, we recommend providing the model as much context as possible. For example, if you want it to answer in Telugu, then tell the model, e.g. ""Explain in a sentence in Telugu what is backpropagation in neural networks."".",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"Google's mT5; mT5 is pretrained on the mC4 corpus, covering 101 languages:; Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.; Note: mT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; Pretraining Dataset: mC4",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"license: apache-2.0
tags:; This model is a fine-tuned version of google/mt5-small on the None dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Google's mT5; mT5 is pretrained on the mC4 corpus, covering 101 languages:; Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.; Note: mT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; Pretraining Dataset: mC4",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"This repository contains the many-to-one (m2o) mT5 checkpoint finetuned on all cross-lingual pairs of the CrossSum dataset, where the target summary was in chinese_simplified, i.e. this model tries to summarize text written in any language in Chinese(Simplified). For finetuning details and scripts, see the paper and the official repository. ; If you use this model, please cite the following paper:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository contains the mT5 checkpoint finetuned on the 45 languages of XL-Sum dataset. For finetuning details and scripts,
see the paper and the official repository. ; Scores on the XL-Sum test sets are as follows:; If you use this model, please cite the following paper:",,,,,,,,,,,,,,,,,,,,,,,,,usage 69
"Google's mT5; mT5 is pretrained on the mC4 corpus, covering 101 languages:; Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.; Note: mT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; Pretraining Dataset: mC4",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"This is a German summarization model. It is based on the multilingual T5 model google/mt5-small. The special characteristic of this model is that, unlike many other models, it is licensed under a permissive open source license (MIT). Among other things, this license allows commercial use.; 
This model is provided by the One Conversation
team of Deutsche Telekom AG.; The training was conducted with the following hyperparameters:; The datasets were preprocessed as follows:; The summary was tokenized with the google/mt5-small tokenizer. Then only the records with no more than 94 summary tokens were selected.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The MTL-data-to-text model was proposed in MVP: Multi-task Supervised Pre-training for Natural Language Generation by Tianyi Tang, Junyi Li, Wayne Xin Zhao and Ji-Rong Wen.; The detailed information and instructions can be found https://github.com/RUCAIBox/MVP.; MTL-data-to-text is supervised pre-trained using a mixture of labeled data-to-text datasets. It is a variant (Single) of our main MVP model. It follows a standard Transformer encoder-decoder architecture.; MTL-data-to-text is specially designed for data-to-text generation tasks, such as KG-to-text generation (WebNLG, DART), table-to-text generation (WikiBio, ToTTo) and MR-to-text generation (E2E).; MVP: https://huggingface.co/RUCAIBox/mvp.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources. For an introduction to semantic search, have a look at: SBERT.net - Semantic Search; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.; In the following some technical details how this model must be used:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources. For an introduction to semantic search, have a look at: SBERT.net - Semantic Search; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.; Similarly to the PyTorch example above, to use the model with TensorFlow you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.",,,,,,,,,,,,,,,,,,,,,,,,,usage 12
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and was designed for semantic search. It has been trained on 215M (question, answer) pairs from diverse sources. For an introduction to semantic search, have a look at: SBERT.net - Semantic Search; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the correct pooling-operation on-top of the contextualized word embeddings.; In the following some technical details how this model must be used:",,,,,,,,,,,,,,,,,,,,,,,,,usage 44
"SentenceTransformers is a set of models and frameworks that enable training and generating sentence embeddings from given data. The generated sentence embeddings can be utilized for Clustering, Semantic Search and other tasks. We used two separate pretrained mpnet-base models and trained them using contrastive learning objective. Question and answer pairs from StackExchange and other datasets were used as training data to make the model robust to Question / Answer embedding similarity.; We developed this model during the 
Community week using JAX/Flax for NLP & CV, 
organized by Hugging Face. We developed this model as part of the project:
Train the Best Sentence Embedding Model Ever with 1B Training Pairs. We benefited from efficient hardware infrastructure to run the project: 7 TPUs v3-8, as well
as assistance from Google??s Flax, JAX, and Cloud team members about efficient deep learning frameworks.; This model set is intended to be used as a sentence encoder for a search engine. Given an input sentence, it ouptuts a vector which captures 
the sentence semantic information. The sentence vector may be used for semantic-search, clustering or sentence similarity tasks.; Two models should be used on conjunction for Semantic Search purposes.; Here is how to use this model to get the features of a given text using SentenceTransformers library:",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 12 layers and the embedding size is 768.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; This model is initialized from xlm-roberta-base
and continually trained on a mixture of multilingual datasets.
It supports 100 languages from xlm-roberta,
but low-resource languages may see performance degradation.; Initialization: xlm-roberta-base",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"Text Embeddings by Weakly-Supervised Contrastive Pre-training.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, Furu Wei, arXiv 2022; This model has 24 layers and the embedding size is 1024.; Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset.; This model is initialized from xlm-roberta-large
and continually trained on a mixture of multilingual datasets.
It supports 100 languages from xlm-roberta,
but low-resource languages may see performance degradation.; Initialization: xlm-roberta-large",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"this has too little layers, but it will still make music",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"? Discord: https://discord.gg/aihub | Join the community, learn to make models, chat with link-minded people and lets create music ? ?; ? Discord Latino: https://discord.gg/Crfqs7uB5V | Entren a nuestra comunidad, aprendan a crear modelos AI, habla con otros sobre musica y disfruta las notas musicales ? ?; IMPORTANT!!!!!!!!!: VOICES CANNOT BE COPYRIGHTED. We do not promote piracy so please do not come in with that. We do promote legal-length sample clips of vocals. We promote music & AI produced music covers (impressions). We promote machine learning & Voice AI Models. Note: This repository does NOT include ANY DATASETS. Only models are included.; If you want your credits/name removed, please message me on discord and I will remove it diligently.; Tools: https://vocalremover.org/ https://x-minus.pro/ai https://create.musicfy.lol/",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"MusicGen is a text-to-music model capable of genreating high-quality music samples conditioned on text descriptions or audio prompts.
It is a single stage auto-regressive Transformer model trained over a 32kHz EnCodec tokenizer with 4 codebooks sampled at 50 Hz. 
Unlike existing methods, like MusicLM, MusicGen doesn't require a self-supervised semantic representation, and it generates all 4 codebooks in one pass. 
By introducing a small delay between the codebooks, we show we can predict them in parallel, thus having only 50 auto-regressive steps per second of audio.; MusicGen was published in Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre D??fossez.; Four checkpoints are released:; Try out MusicGen yourself!; You can run MusicGen locally with the ? Transformers library from version 4.31.0 onwards.",,,,,,,,,,,,,,,,,,,,,,,,,usage 94
"Audiocraft provides the code and models for MusicGen, a simple and controllable model for music generation. 
MusicGen is a single stage auto-regressive Transformer model trained over a 32kHz EnCodec tokenizer with 4 codebooks sampled at 50 Hz. 
Unlike existing methods like MusicLM, MusicGen doesn't not require a self-supervised semantic representation, and it generates all 4 codebooks in one pass. 
By introducing a small delay between the codebooks, we show we can predict them in parallel, thus having only 50 auto-regressive steps per second of audio.; MusicGen was published in Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre D??fossez.; Four checkpoints are released:; Try out MusicGen yourself!; You can run MusicGen locally as well:",,,,,,,,,,,,,,,,,,,,,,,,,usage 99
"MusicGen is a text-to-music model capable of genreating high-quality music samples conditioned on text descriptions or audio prompts.
It is a single stage auto-regressive Transformer model trained over a 32kHz EnCodec tokenizer with 4 codebooks sampled at 50 Hz. 
Unlike existing methods, like MusicLM, MusicGen doesn't require a self-supervised semantic representation, and it generates all 4 codebooks in one pass. 
By introducing a small delay between the codebooks, we show we can predict them in parallel, thus having only 50 auto-regressive steps per second of audio.; MusicGen was published in Simple and Controllable Music Generation by Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre D??fossez.; Four checkpoints are released:; Try out MusicGen yourself!; You can run MusicGen locally with the ? Transformers library from version 4.31.0 onwards.",,,,,,,,,,,,,,,,,,,,,,,,,usage 95
"Credit me if you use my model ; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"?????????????????¦Ê????????????????¦Ê???????????????????????????????????civitai????@Saya????????????????????
For learning only, please do not use for any commercial activities, not authorized to any commercial activities. If there is a model without instructions, try searching @Saya in civiti, you can find past model instructions; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"MythoBoros-13b can be considered a sister model to MythoLogic-13b, sharing the same goals but having a different approach.; Whereas the previous model was a series of experimental gradient merges, this one is a simple straight-up 66/34 merge of Chronos and the freshly released Ouroboros, providing a very solid foundation for a well-performing roleplaying model.; MythoBoros tends to be somewhat more formal with its responses in comparison to MythoLogic.; My advice? Try both, see which one you prefer.; Quantized models are available from TheBloke: GGML - GPTQ (You're the best!)",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GGML format model files for Gryphe's MythoBoros 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This repo contains GPTQ model files for Gryphe's MythoBoros 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"An experiment with gradient merges using the following script, with Chronos as its primary model, augmented by Hermes and Wizard-Vicuna Uncensored.; Quantized models are available from TheBloke: GGML - GPTQ (You're the best!); Chronos is a wonderfully verbose model, though it definitely seems to lack in the logic department. Hermes and WizardLM have been merged gradually, primarily in the higher layers (10+) in an attempt to rectify some of this behaviour.; The main objective was to create an all-round model with improved story generation and roleplaying capabilities.; Below is an illustration to showcase a rough approximation of the gradients I used to create MythoLogic:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Gryphe's MythoLogic 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Gryphe's MythoLogic 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These files were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 37
"ner-bert-base-portuguese-cased-lenerbr is a NER model (token classification) in the legal domain in Portuguese that was finetuned on 20/12/2021 in Google Colab from the model pierreguillou/bert-base-cased-pt-lenerbr on the dataset LeNER_br by using a NER objective.; Due to the small size of BERTimbau base and finetuning dataset, the model overfitted before to reach the end of training. Here are the overall final metrics on the validation dataset (note: see the paragraph ""Validation metrics by Named Entity"" to get detailed metrics):; Check as well the large version of this model with a f1 of 0.908.; Note: the model pierreguillou/bert-base-cased-pt-lenerbr is a language model that was created through the finetuning of the model BERTimbau base on the dataset LeNER-Br language modeling by using a MASK objective. This first specialization of the language model before finetuning on the NER task improved a bit the model quality. To prove it, here are the results of the NER model finetuned from the model BERTimbau base (a non-specialized language model):; NLP | Modelos e Web App para Reconhecimento de Entidade Nomeada (NER) no dom??nio jur??dico brasileiro (29/12/2021)",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"The model was trained on jnlpba dataset, pretrained on this pubmed-pretrained roberta model; All the labels, the possible token classes.; Notice, we removed the 'B-','I-' etc from data label.?; And here is to make your output more consecutive ??; check our NER model on",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Read more about this model here: https://civitai.com/models/10028/neverending-dream-ned
Also please support by giving 5 stars and a heart, which will notify new updates.; Also consider supporting me on Patreon or ByuMeACoffee; You can run this model on:; Some sample output:;",,,,,,,,,,,,,,,,,,,,,,,,,usage 36
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"NewMoon: Soft, bright colors. 
ChamomileTea: Darker, moodier colors. 
newmoon.yaml: sample prompt for animatediff",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model was trained using SentenceTransformers Cross-Encoder class. This model is based on microsoft/deberta-v3-large; The model was trained on the SNLI and MultiNLI datasets. For a given sentence pair, it will output three scores corresponding to the labels: contradiction, entailment, neutral.; For futher evaluation results, see SBERT.net - Pretrained Cross-Encoder.; Pre-trained models can be used like this:; You can use the model also directly with Transformers library (without SentenceTransformers library):",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the model card of NLLB-200's 3.3B variant.; Here are the metrics for that particular checkpoint.; ? Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.; ? We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.; ? In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).",,,,,,,,,,,,,,,,,,,,,,,,,usage 14
"This is the model card of NLLB-200's distilled 1.3B variant.; Here are the metrics for that particular checkpoint.; ? Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.; ? We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.; ? In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).",,,,,,,,,,,,,,,,,,,,,,,,,usage 20
"We present the 7-billion parameter variant of Noon, an Arabic Large Language model based on BLOOM, a foundation model released by the bigscience workshop.; Noon was trained with the main focus of having a model that responds to various types of instructions and questions (text generation, code generation, mathematical problems, closed/open-book questions, etc.); We trained the model using the ColossalAI framework which fully supports the HuggingFace library models, and implements different optimization and quantization techniques for billion-scale LLMs.; The training data is a combination of Arabic datasets covering multiple tasks, more details are provided in the dataset section.; ?????? ??? ?? ????? ????? ""???""!",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Please for anyone, don't reupload my finetuned models to other site, like Civitai or image generating site like Seeart, Yodayo or other sites. Thank you for understanding this. ; Like my works and want to collaboration or funding my projects? contact me.; Sample",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Nous-Hermes-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Karan4D leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors. The result is an enhanced Llama 13b model that rivals GPT-3.5-turbo in performance across a variety of tasks.; This model stands out for its long responses, low hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 2000 sequence length on an 8x a100 80GB DGX machine for over 50 hours. ; The model was trained almost entirely on synthetic GPT-4 outputs. This includes data from diverse sources such as GPTeacher, the general, roleplay v1&2, code instruct datasets, Nous Instruct & PDACTL (unpublished), CodeAlpaca, Evol_Instruct Uncensored, GPT4-LLM, and Unnatural Instructions. ; Additional data inputs came from Camel-AI's Biology/Physics/Chemistry and Math Datasets, Airoboros' GPT-4 Dataset, and more from CodeAlpaca. The total volume of data encompassed over 300,000 instructions.; The model fine-tuning and the datasets were a collaboration of efforts and resources between Teknium, Karan4D, Nous Research, Huemin Art, and Redmond AI.",,,,,,,,,,,,,,,,,,,,,,,,,usage 20
??Chinese-alpaca-13b??Lora?????Nous-Hermes-13b????????????????Hermes-13b????; Chinese-alpaca-13b:https://huggingface.co/ziqingyang/chinese-alpaca-lora-13b; Nous-Hermes-13b:https://huggingface.co/NousResearch/Nous-Hermes-13b; ????????§à?????????????????????§¿?î•,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"??????https://huggingface.co/NousResearch/Nous-Hermes-13b ; lora??https://huggingface.co/ziqingyang/chinese-alpaca-lora-13b ; ??Nous-Hermes-13b??chinese-alpaca-lora-13b???§Ü?????????????????????????????????; ????????
https://github.com/ymcui/Chinese-LLaMA-Alpacahttps://github.com/ggerganov/llama.cpp; ???q5_k_m??q4_k_m   ?¨°???????ggmlv3???",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for NousResearch's Nous-Hermes-13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; The model follows the Alpaca prompt format:",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for NousResearch's Nous-Hermes-13B.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; The model follows the Alpaca prompt format:,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Compute provided by our project sponsor Redmond AI, thank you! Follow RedmondAI on Twitter @RedmondAI.; Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Emozilla leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors.; This Hermes model uses the exact same dataset as Hermes on Llama-1. This is to ensure consistency between the old Hermes and new, for anyone who wanted to keep Hermes as similar to the old one, just more capable.; This model stands out for its long responses, lower hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 4096 sequence length on an 8x a100 80GB DGX machine.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Compute provided by our project sponsor Redmond AI, thank you! Follow RedmondAI on Twitter @RedmondAI.; Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Emozilla leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors.; This Hermes model uses the exact same dataset as Hermes on Llama-1. This is to ensure consistency between the old Hermes and new, for anyone who wanted to keep Hermes as similar to the old one, just more capable.; This model stands out for its long responses, lower hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 4096 sequence length on an 8x a100 80GB DGX machine.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Compute provided by our project sponsor Redmond AI, thank you! Follow RedmondAI on Twitter @RedmondAI.; Nous-Hermes-Llama2-13b is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Emozilla leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors.; This Hermes model uses the exact same dataset as Hermes on Llama-1. This is to ensure consistency between the old Hermes and new, for anyone who wanted to keep Hermes as similar to the old one, just more capable.; This model stands out for its long responses, lower hallucination rate, and absence of OpenAI censorship mechanisms. The fine-tuning process was performed with a 4096 sequence length on an 8x a100 80GB DGX machine.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Nous Research's Nous Hermes Llama 2 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Nous Research's Nous Hermes Llama 2 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"NSQL is a family of autoregressive open-source large foundation models (FMs) designed specifically for SQL generation tasks.; The checkpoint included in this repository is based on CodeGen-Multi 6B from Salesforce and further pre-trained on a dataset of general SQL queries and then fine-tuned on a dataset composed of text-to-SQL pairs.; The general SQL queries are the SQL subset from The Stack, containing 1M training samples. The labeled text-to-SQL pairs come from more than 20 public sources across the web from standard datasets. We hold out Spider and GeoQuery datasets for use in evaluation.; We evaluate our models on two text-to-SQL benchmarks: Spider and GeoQuery.; NSQL was trained using cross-entropy loss to maximize the likelihood of sequential inputs. For finetuning on text-to-SQL pairs, we only compute the loss over the SQL portion of the pair. The family of models is trained using 80GB A100s, leveraging data and model parallelism. We pre-trained for 3 epochs and fine-tuned for 10 epochs.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This is the 4th iteration English supervised-fine-tuning (SFT) model of 
the Open-Assistant project. 
It is based on a Pythia 12B that was fine-tuned on human demonstrations 
of assistant conversations collected through the 
https://open-assistant.io/ human feedback web 
app before March 25, 2023. ; Two special tokens are used to mark the beginning of user and assistant turns:
<|prompter|> and <|assistant|>. Each turn ends with a <|endoftext|> token.; Input prompt example:; The input ends with the <|assistant|> token to signal that the model should 
start generating the assistant reply.; command: deepspeed trainer_sft.py --configs defaults reference-data reference-pythia-12b --cache_dir /home/ubuntu/data_cache --output_dir .saved/oasst-sft-3-pythia-12b-reference_2kpre --num_train_epochs 8 --residual_dropout 0.2 --deepspeed --use_flash_attention true --model_name andreaskoepf/pythia-12b-pre-2000",,,,,,,,,,,,,,,,,,,,,,,,,usage 61
"Due to the license attached to LLaMA models by Meta AI it is not possible to directly distribute LLaMA-based models. Instead we provide XOR weights for the OA models.; Thanks to Mick for writing the xor_codec.py script which enables this process; Note: This process applies to oasst-sft-6-llama-30b model. The same process can be applied to other models in future, but the checksums will be different..; This process is tested only on Linux (specifically Ubuntu). Some users have reported that the process does not work on Windows. We recommend using WSL if you only have a Windows machine.; To use OpenAssistant LLaMA-Based Models, you should have a copy of the original LLaMA model weights and add them to a llama subdirectory here. If you cannot obtain the original LLaMA, see the note in italic below for a possible alternative.",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"Donut model fine-tuned on CORD. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository.; Disclaimer: The team releasing Donut did not write a model card for this model so this model card has been written by the Hugging Face team.; Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. ; ; This model is fine-tuned on CORD, a document parsing dataset.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Model info:
https://civitai.com/models/105955/onlyanime; Sample image I made thru Huggingface's API :
; Original Author's DEMO images :; ; *This image using LORA file ? FilmVelvia3.safetensors 
you can also download here:",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"OpenCALM is a suite of decoder-only language models pre-trained on Japanese datasets, developed by CyberAgent, Inc.; Ryosuke Ishigami; Inference API has been turned off for this model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we??ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.; The model can be evaluated with lm-eval-harness. However, due to the aforementioned tokenizer issue, we need to avoid using the fast tokenizer to obtain the correct results. This can be achieved by passing in use_fast=False to this part of lm-eval-harness, as shown in the example below:",,,,,,,,,,,,,,,,,,,,,,,,,usage 12
"In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a 7B and 3B model trained on 1T tokens, as well as the preview of a 13B model trained on 600B tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we??ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.; The model can be evaluated with lm-eval-harness. However, due to the aforementioned tokenizer issue, we need to avoid using the fast tokenizer to obtain the correct results. This can be achieved by passing in use_fast=False to this part of lm-eval-harness, as shown in the example below:",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"TL;DR: we are releasing our public preview of OpenLLaMA, a permissively licensed open source reproduction of Meta AI??s LLaMA. We are releasing a series of 3B, 7B and 13B models trained on different data mixtures. Our model weights can serve as the drop in replacement of LLaMA in existing implementations.; In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a series of 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. The v2 model is better than the old v1 model trained on a different data mixture. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we??ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Support is now merged to master branch.; There are now more quantization types in llama.cpp, some lower than 4 bits.
Currently these are not well supported because of technical reasons.
If you want to use them, you have to build llama.cpp (from build 829 (ff5d58f)) with the LLAMA_QKK_64 Make or CMake variable enabled (see PR #2001).
Then you can quantize the F16 or maybe Q8_0 version to what you want.; Coming soon ...; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a 7B and 3B model trained on 1T tokens, as well as the preview of a 13B model trained on 600B tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we??ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.; The model can be evaluated with lm-eval-harness. However, due to the aforementioned tokenizer issue, we need to avoid using the fast tokenizer to obtain the correct results. This can be achieved by passing in use_fast=False to this part of lm-eval-harness, as shown in the example below:",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"TL;DR: we are releasing our public preview of OpenLLaMA, a permissively licensed open source reproduction of Meta AI??s LLaMA. We are releasing a series of 3B, 7B and 13B models trained on different data mixtures. Our model weights can serve as the drop in replacement of LLaMA in existing implementations.; In this repo, we present a permissively licensed open source reproduction of Meta AI's LLaMA large language model. We are releasing a series of 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. The v2 model is better than the old v1 model trained on a different data mixture. Please see the project homepage of OpenLLaMA for more details.; We release the weights in two formats: an EasyLM format to be use with our EasyLM framework, and a PyTorch format to be used with the Hugging Face transformers library. Both our training framework EasyLM and the checkpoint weights are licensed permissively under the Apache 2.0 license.; Preview checkpoints can be directly loaded from Hugging Face Hub. Please note that it is advised to avoid using the Hugging Face fast tokenizer for now, as we??ve observed that the auto-converted fast tokenizer sometimes gives incorrect tokenizations. This can be achieved by directly using the LlamaTokenizer class, or passing in the use_fast=False option for the AutoTokenizer class. See the following example for usage.; For more advanced usage, please follow the transformers LLaMA documentation.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"For use with llama.cpp.; Coming soon...; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Info here: https://github.com/josephrocca/openai-clip-js; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Model Description: openai-gpt is a transformer-based language model created and released by OpenAI. The model is a causal (unidirectional) transformer pre-trained using language modeling on a large corpus with long range dependencies.; Use the code below to get started with the model. You can use this model directly with a pipeline for text generation. Since the generation relies on some randomness, we
set a seed for reproducibility:; Here is how to use this model in PyTorch:; and in TensorFlow:; This model can be used for language modeling tasks.",,,,,,,,,,,,,,,,,,,,,,,,,usage 22
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for OpenAssistant LLaMA 30B SFT 7.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"OpenChat is a series of open-source language models fine-tuned on a diverse and high-quality dataset of multi-round conversations. With only ~6K GPT-4 conversations filtered from the ~90K ShareGPT conversations, OpenChat is designed to achieve high performance with limited data.; Generic models:; Code models:; Note: Please load the pretrained models using bfloat16; We provide the full source code, including an inference server compatible with the ""ChatCompletions"" API, in the OpenChat GitHub repository.",,,,,,,,,,,,,,,,,,,,,,,,,usage 18
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This is a preview version of OpenChat V2 trained for 2 epochs (total 5 epochs) on full (4.5M) OpenOrca dataset.; Important Notice: Beta Release for Limited Testing Purposes Only; This release is intended solely for a small group of beta testers and is not an official release or preview. We caution against publicizing or sharing this version as it may contain bugs, errors, or incomplete features that could negatively impact performance. We are actively working on improving the model and preparing it for an official release.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Open-Orca's OpenChat V2 x OpenOrca Preview 2.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The OpenChat v2 family is inspired by offline reinforcement learning, including conditional behavior cloning (OpenChat-v2) and weighted behavior cloning (OpenChat-v2-w).; We provide the full source code, including an inference server compatible with the ""ChatCompletions"" API, in the OpenChat GitHub repository.; OpenChat also includes a web UI for a better user experience. See the GitHub repository for instructions.; The conversation template involves concatenating tokens, and cannot be expressed in plain-text.; Besides base model vocabulary, an end-of-turn token <|end_of_turn|> is added.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"OpenChat is a series of open-source language models fine-tuned on a diverse and high-quality dataset of multi-round conversations. With only ~6K GPT-4 conversations filtered from the ~90K ShareGPT conversations, OpenChat is designed to achieve high performance with limited data.; Generic models:; Code models:; Note: Please load the pretrained models using bfloat16; We provide the full source code, including an inference server compatible with the ""ChatCompletions"" API, in the OpenChat GitHub repository.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Include 'mdjrny-v4 style' in prompt. Here you'll find hundreds of Openjourney prompts; ; (Same parameters, just added ""mdjrny-v4 style"" at the beginning):



; This model can be used just like any other Stable Diffusion model. For more information,
please have a look at the Stable Diffusion.; You can also export the model to ONNX, MPS and/or FLAX/JAX.",,,,,,,,,,,,,,,,,,,,,,,,,usage 296
"Trained on Stable Diffusion v1.5 using +124000 images, 12400 steps, 4 epochs +32 training hours.; ? Openjourney-v4 prompts; Pss... ""mdjrny-v4 style"" is not necessary anymore (yay!); ? Want to learn how to train Openjourney? ?? Join our course ?",,,,,,,,,,,,,,,,,,,,,,,,,usage 116
Prompt: "Below is an instruction that describes a task. Write a response that appropriately completes the request. \n\n### Instruction:\n INSTRUCTION. \n### Response:\n",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; ; ; We have used our own OpenOrca dataset to fine-tune LLaMA-13B.
This dataset is our attempt to reproduce the dataset generated for Microsoft Research's Orca Paper.; We have trained on less than 6% of our data, just to give a preview of what is possible while we further refine our dataset!
We trained a refined selection of 200k GPT-4 entries from OpenOrca.
We have filtered our GPT-4 augmentations to remove statements like, ""As an AI language model..."" and other responses which have been shown to harm model reasoning capabilities. Further details on our dataset curation practices will be forthcoming with our full model releases.",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Open-Orca's OpenOrca-Preview1-13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These files were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Open-Orca's OpenOrca-Preview1-13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; This repo is for the Open Protein Instructions (OPI) project, aiming to build and release a protein instruction dataset as well as propose to explore and benckmark LLMs for protein modeling in protein biology.
For more details of training and testing, please visit https://github.com/baaihealth/opi.; ; Usage and License Notices: LLaMA and Galactica are intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes. The weight diff for Stanford Alpaca is also CC BY NC 4.0 (allowing only non-commercial use).; For OPI-instruction tuning, we adopt the training script of Stanford Alpaca.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"OPT was first introduced in Open Pre-trained Transformer Language Models and first released in metaseq's repository on May 3rd 2022 by Meta AI.; Disclaimer: The team releasing OPT wrote an official model card, which is available in Appendix D of the paper. 
Content from this model card has been written by the Hugging Face team.; To quote the first two paragraphs of the official paper; Large language models trained on massive text collections have shown surprising emergent
capabilities to generate text and perform zero- and few-shot learning. While in some cases the public
can interact with these models through paid APIs, full model access is currently limited to only a
few highly resourced labs. This restricted access has limited researchers?? ability to study how and
why these large language models work, hindering progress on improving known challenges in areas
such as robustness, bias, and toxicity.; We present Open Pretrained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M
to 175B parameters, which we aim to fully and responsibly share with interested researchers. We train the OPT models to roughly match 
the performance and sizes of the GPT-3 class of models, while also applying the latest best practices in data
collection and efficient training. Our aim in developing this suite of OPT models is to enable reproducible and responsible research at scale, and
to bring more voices to the table in studying the impact of these LLMs. Definitions of risk, harm, bias, and toxicity, etc., should be articulated by the
collective research community as a whole, which is only possible when models are available for study.",,,,,,,,,,,,,,,,,,,,,,,,,usage 49
"This is the second generation of the original Shinen made by Mr. Seeker. The full dataset consists of 6 different sources, all surrounding the ""Adult"" theme. The name ""Erebus"" comes from the greek mythology, also named ""darkness"". This is in line with Shin'en, or ""deep abyss"". For inquiries, please contact the KoboldAI community. Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The data can be divided in 6 different datasets:; The dataset uses [Genre: <comma-separated list of genres>] for tagging.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; Based on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion). Warning: This model has a very strong NSFW bias!",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This is a merged (50/50) model of both Erebus 13B and Nerys V2 13B by Mr. Seeker.
Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; For more information, check out the two source models:; Based on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion).
Warning: This model has a very strong NSFW bias!; OPT-13B is licensed under the OPT-175B license, Copyright (c) Meta Platforms, Inc. All Rights Reserved.; Inference API has been turned off for this model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; This is a 4-bit GPTQ quantization of OPT-30B-Erebus, original model:
https://huggingface.co/KoboldAI/OPT-30B-Erebus; Quantized with: https://github.com/0cc4m/GPTQ-for-LLaMa; Output generated in 54.23 seconds (0.87 tokens/s, 47 tokens, context 44, seed 593020441); https://github.com/oobabooga/text-generation-webui",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the second generation of the original Shinen made by Mr. Seeker. The full dataset consists of 6 different sources, all surrounding the ""Adult"" theme. The name ""Erebus"" comes from the greek mythology, also named ""darkness"". This is in line with Shin'en, or ""deep abyss"". For inquiries, please contact the KoboldAI community. Warning: THIS model is NOT suitable for use by minors. The model will output X-rated content.; The data can be divided in 6 different datasets:; The dataset uses [Genre: <comma-separated list of genres>] for tagging.; You can use this model directly with a pipeline for text generation. This example generates a different sequence each time it's run:; Based on known problems with NLP technology, potential relevant factors include bias (gender, profession, race and religion). Warning: This model has a very strong NSFW bias!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
source languages: ar; target languages: en; OPUS readme: ar-en; dataset: opus; model: transformer-align,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
source languages: de; target languages: en; OPUS readme: de-en; dataset: opus; model: transformer-align,,,,,,,,,,,,,,,,,,,,,,,,,usage 6
source group: English ; target group: Arabic ; OPUS readme: eng-ara; model: transformer; source language(s): eng,,,,,,,,,,,,,,,,,,,,,,,,,usage 8
source group: English ; target group: Spanish ; OPUS readme: eng-spa; model: transformer; source language(s): eng,,,,,,,,,,,,,,,,,,,,,,,,,usage 69
source languages: en; target languages: fr; OPUS readme: en-fr; dataset: opus; model: transformer-align,,,,,,,,,,,,,,,,,,,,,,,,,usage 31
source languages: en; target languages: ru; OPUS readme: en-ru; dataset: opus; model: transformer-align,,,,,,,,,,,,,,,,,,,,,,,,,usage 9
source group: English ; target group: Chinese ; OPUS readme: eng-zho; model: transformer; source language(s): eng,,,,,,,,,,,,,,,,,,,,,,,,,usage 21
source languages: fr; target languages: en; OPUS readme: fr-en; dataset: opus; model: transformer-align,,,,,,,,,,,,,,,,,,,,,,,,,usage 17
source group: Multiple languages ; target group: English ; OPUS readme: mul-eng; model: transformer; source language(s): abk acm ady afb afh_Latn afr akl_Latn aln amh ang_Latn apc ara arg arq ary arz asm ast avk_Latn awa aze_Latn bak bam_Latn bel bel_Latn ben bho bod bos_Latn bre brx brx_Latn bul bul_Latn cat ceb ces cha che chr chv cjy_Hans cjy_Hant cmn cmn_Hans cmn_Hant cor cos crh crh_Latn csb_Latn cym dan deu dsb dtp dws_Latn egl ell enm_Latn epo est eus ewe ext fao fij fin fkv_Latn fra frm_Latn frr fry fuc fuv gan gcf_Latn gil gla gle glg glv gom gos got_Goth grc_Grek grn gsw guj hat hau_Latn haw heb hif_Latn hil hin hnj_Latn hoc hoc_Latn hrv hsb hun hye iba ibo ido ido_Latn ike_Latn ile_Latn ilo ina_Latn ind isl ita izh jav jav_Java jbo jbo_Cyrl jbo_Latn jdt_Cyrl jpn kab kal kan kat kaz_Cyrl kaz_Latn kek_Latn kha khm khm_Latn kin kir_Cyrl kjh kpv krl ksh kum kur_Arab kur_Latn lad lad_Latn lao lat_Latn lav ldn_Latn lfn_Cyrl lfn_Latn lij lin lit liv_Latn lkt lld_Latn lmo ltg ltz lug lzh lzh_Hans mad mah mai mal mar max_Latn mdf mfe mhr mic min mkd mlg mlt mnw moh mon mri mwl mww mya myv nan nau nav nds niu nld nno nob nob_Hebr nog non_Latn nov_Latn npi nya oci ori orv_Cyrl oss ota_Arab ota_Latn pag pan_Guru pap pau pdc pes pes_Latn pes_Thaa pms pnb pol por ppl_Latn prg_Latn pus quc qya qya_Latn rap rif_Latn roh rom ron rue run rus sag sah san_Deva scn sco sgs shs_Latn shy_Latn sin sjn_Latn slv sma sme smo sna snd_Arab som spa sqi srp_Cyrl srp_Latn stq sun swe swg swh tah tam tat tat_Arab tat_Latn tel tet tgk_Cyrl tha tir tlh_Latn tly_Latn tmw_Latn toi_Latn ton tpw_Latn tso tuk tuk_Latn tur tvl tyv tzl tzl_Latn udm uig_Arab uig_Cyrl ukr umb urd uzb_Cyrl uzb_Latn vec vie vie_Hani vol_Latn vro war wln wol wuu xal xho yid yor yue yue_Hans yue_Hant zho zho_Hans zho_Hant zlm_Latn zsm_Latn zul zza,,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"Model Description:; This model can be used for translation and text-to-text generation.; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).; Further details about the dataset for this model can be found in the OPUS readme: ru-en",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"This model can be used for translation and text-to-text generation.; CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.; Significant research has explored bias and fairness issues with language models (see, e.g., Sheng et al. (2021) and Bender et al. (2021)).; Further details about the dataset for this model can be found in the OPUS readme: zho-eng; pre-processing: normalization + SentencePiece (spm32k,spm32k)",,,,,,,,,,,,,,,,,,,,,,,,,usage 30
"""OrangeMixs"" shares various Merge models that can be used with StableDiffusionWebui:Automatic1111 and others.
?
; Maintain a repository for the following purposes.; 
Hero image prompts(AOM3B2):https://majinai.art/ja/i/jhw20Z_; We support a Gradio Web UI to run OrangeMixs:
; +/hdg/ Stable Diffusion Models Cookbook - https://rentry.org/hdgrecipes#g-anons-unnamed-mix-e93c3bf7
Model names are named after Cookbook precedents?",,,,,,,,,,,,,,,,,,,,,,,,,usage 138
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Pankaj Mathur's Orca Mini 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; or",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Use orca-mini-3b on Free Google Colab with T4 GPU :); An OpenLLaMa-3B model model trained on explain tuned datasets, created using Instructions and Input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; We build explain tuned WizardLM dataset ~70K, Alpaca dataset ~52K  & Dolly-V2 dataset ~15K created using approaches from Orca Research Paper.; We leverage all of the 15 system instructions provided in Orca Research Paper. to generate custom datasets, in contrast to vanilla instruction tuning approaches used by original datasets.; This helps student model aka this model to learn thought process from teacher model, which is ChatGPT (gpt-3.5-turbo-0301 version).",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Pankaj Mathur's Orca Mini 3B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; or",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"An Uncensored LLaMA-13b model in collaboration with Eric Hartford. trained on explain tuned datasets, created using Instructions and Input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; Please note this model has better code generation capabilities compare to our original orca_mini_13b which was trained on base OpenLLaMA-13b model and which has the empty spaces issues & found not good for code generation.; P.S. I am #opentowork, if you can help, please reach out to me at www.linkedin.com/in/pankajam; I evaluated orca_mini_v2_13b on a wide range of tasks using Language Model Evaluation Harness from EleutherAI. ; Here are the results on metrics used by HuggingFaceH4 Open LLM Leaderboard",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Pankaj Mathur's Orca Mini v2 13B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Pankaj Mathur's Orca Mini v2 13B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"An Uncensored LLaMA-7b model in collaboration with Eric Hartford. trained on explain tuned datasets, created using Instructions and Input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; Please note this model has better code generation capabilities compare to our original orca_mini_7b which was trained on base OpenLLaMA-7b model and which has the empty spaces issues & found not good for code generation.; P.S. I am #opentowork, if you can help, please reach out to me at www.linkedin.com/in/pankajam; I evaluated orca_mini_v2_7b on a wide range of tasks using Language Model Evaluation Harness from EleutherAI. ; Here are the results on metrics used by HuggingFaceH4 Open LLM Leaderboard",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Pankaj Mathur's Orca Mini v2 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Pankaj Mathur's Orca Mini v2 7B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"orca_mini_v2_ger_7b is a variant of Pankaj Mathur??s Orca Mini V2 7b model, finetuned on an additional dataset in German language. 
The original model has been trained on explain tuned datasets, created using instructions and input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; This model is optimized for German text, providing proficiency in understanding, generating, and interacting with German language content.
However the model is not yet fully optimized for German language, as it has been trained on a small, experimental dataset and has limited capabilities due to the small parameter count. ; I am working on improving the model??s capabilities and will update the model if there is sufficient interest.; A quantized GGML version for use with llama.cpp, kobold.cpp and other GUIs for CPU inference can be found here.; I did only evaluate the output on a small, handcrafted sample on test prompts in German, confirming that the model's ability to understand and generate German text is well above the base model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Diffusion model trained on 500k image-tags pairs scraped from Furaffinity. Alpha still, expect more epochs, more training data and overall better results in the future.; 
""anthro, fox, male, general, by 100racs"" Epoch 24, no inpainting; The tags contain the original FA tag list with tags appearing less than 40 times in total omitted, plus a tag corresponding the the general/mature/adult rating. If the artist also appears more than 40 times, an artist tag is added as well. The full list of tags and their number of occurences are available here. Training was done on TPUv3s using the LION optimizer.; Due to using data from Furaffinity, the model offers a wide variety of tags that aren't as popular as in other models for creating niche content. For example, the tag vore appears 25379 times in the dataset it was trained on as opposed to 4730 for FluffyRock. However, the preciseness of the tags on Furaffinity can also be left to be desired compared to e621, and as such it is recommended to merge it with FluffyRock if you want more control over your prompts.; Load up the safetensor file as well as the provided yaml file and put them in your model folder. Additionally, you are going to want to use CFG Rescale: https://github.com/Seshelle/CFG_Rescale_webui. 7.5 CFG and 0.7 Phi are recommended.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"PanGu-?? is proposed by a joint technical team headed by PCNL. It was first released in this repository  It is the first large-scale Chinese pre-trained language model with 200 billion parameters trained on 2048 Ascend processors using an automatic hybrid parallel training strategy. The whole training process is done on the ??Peng Cheng Cloud Brain II?? computing platform with the domestic deep learning framework called MindSpore. The PengCheng??PanGu-?? pre-training model can support rich applications, has strong few-shot learning capabilities, and has outstanding performance in text generation tasks such as knowledge question and answer, knowledge retrieval, knowledge reasoning, and reading comprehension.; This repository contains PyTorch implementation of PanGu model, with
2.6 billion parameters pretrained weights (FP32 precision), converted from original MindSpore checkpoint.; Currently PanGu model is not supported by transformers, 
so trust_remote_code=True is required to load model implementation in this repo.; Expected output:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",,,,,,,,,,,,,,,,,,,,,,,,,usage 34
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"Parrot is a paraphrase based utterance augmentation framework purpose built to accelerate training NLU models. A paraphrase framework is more than just a paraphrasing model. For more  details on the library and usage please refer to the github page; Huggingface lists 12 paraphrase models, RapidAPI lists 7 fremium and commercial paraphrasers like QuillBot, Rasa has discussed an experimental paraphraser for augmenting text data here, Sentence-transfomers offers a paraphrase mining utility and NLPAug offers word level augmentation with a PPDB (a multi-million paraphrase database). While these attempts at paraphrasing are great, there are still some gaps and paraphrasing is NOT yet a mainstream option for text augmentation in building NLU models....Parrot is a humble attempt to fill some of these gaps.; What is a good paraphrase? Almost all conditioned text generation models are validated  on 2 factors, (1) if the generated text conveys the same meaning as the original context (Adequacy) (2) if the text is fluent / grammatically correct english (Fluency). For instance Neural Machine Translation outputs are tested for Adequacy and Fluency. But a good paraphrase should be adequate and fluent while being as different as possible on the surface lexical form. With respect to this definition, the  3 key metrics that measures the quality of paraphrases are:; Parrot offers knobs to control Adequacy, Fluency and Diversity as per your needs.; What makes a paraphraser a good augmentor? For training a NLU model we just don't need a lot of utterances but utterances with intents and slots/entities annotated. Typical flow would be:",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"Model info :; https://civitai.com/models/112309?modelVersionId=121233; Sample prompt and image generated by huggingface's API :; photo portrait of 25 yo man, long hair, winter forest on background, realism, photorealism, hyperrealism, (insane details:1.3), 35mm;",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 2
PEGASUS fine-tuned for paraphrasing; Created by Arpit Rajauria,,,,,,,,,,,,,,,,,,,,,,,,,usage 51
"Get SparkNotes-esque summaries of arbitrary text! Due to the model size, it's recommended to try it out in Colab (linked above) as the API textbox may time out.; This model is a fine-tuned version of google/pegasus-x-large on the kmfoda/booksum dataset for approx eight epochs.; More information needed; TODO; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
Model info :; https://civitai.com/models/111848?modelVersionId=121050; Sample image I made thru Huggingface's API :; ; Original Author's DEMO image :,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"PersonaGPT is an open-domain conversational agent designed to do 2 tasks:; It builds on the DialoGPT-medium pretrained model based on the GPT-2 architecture. 
This model is trained on the Persona-Chat dataset, with added special tokens to better distinguish between conversational history and personality traits for dyadic conversations. Furthermore, some active learning was used to train the model to do controlled decoding using turn-level goals.; Preprocessing, training and implementation details can be found in the personaGPT repo.; Example of personalized decoding:; Example of controlled response generation:",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"Model info :; https://civitai.com/models/31771?modelVersionId=38190; Sample image I made thru Huggingface's API :
; Original Author's DEMO images :;",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		;   TO BE ADDED; 





; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Model info: https://civitai.com/models/84728/photon;,,,,,,,,,,,,,,,,,,,,,,,,,usage 2
Model info :; https://civitai.com/models/18637/photosomnia; Original Author's DEMO image :; ; Sample image thru huggingface's API :,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"We present and release PHS-BERT, a transformer-based pretrained language model (PLM), to identify tasks related to public health surveillance (PHS) on social media. Compared with existing PLMs that are mainly evaluated on limited tasks, PHS-BERT achieved state-of-the-art performance on 25 tested datasets, showing that our PLM is robust and generalizable in common PHS tasks.; Load the model via Huggingface's Transformers library:; We followed the standard pretraining protocols of BERT and initialized PHS-BERT with weights from BERT during the training phase instead of training from scratch and used the uncased version of the BERT model.; PHS-BERT is trained on a corpus of health-related tweets that were crawled via the Twitter API. Focusing on the tasks related to PHS, keywords used to collect pretraining corpus are set to disease, symptom, vaccine, and mental health-related words in English. Retweet tags were deleted from the raw corpus, and URLs and usernames were replaced with HTTP-URL and @USER, respectively. All emoticons were replaced with their associated meanings. ; Each sequence of BERT LM inputs is converted to 50,265 vocabulary tokens. Twitter posts are restricted to 200 characters, and during the training and evaluation phase, we used a batch size of 8. Distributed training was performed on a TPU v3-8.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Voices for Piper text to speech system.; For checkpoints that you can use to train your own voices, see piper-checkpoints; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The full list of available models can be found on the Table 1 of the paper:; ; The abstract of the model states that: ; Visually-situated language is ubiquitous??sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and
forms. Perhaps due to this diversity, previous work has typically relied on domainspecific recipes with limited sharing of the underlying data, model architectures,
and objectives. We present Pix2Struct, a pretrained image-to-text model for
purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse
masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large
source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy,
we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions
are rendered directly on top of the input image. For the first time, we show that a
single pretrained model can achieve state-of-the-art results in six out of nine tasks
across four domains: documents, illustrations, user interfaces, and natural images.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"; Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captionning and visual question answering. The full list of available models can be found on the Table 1 of the paper:; ; The abstract of the model states that: ; Visually-situated language is ubiquitous??sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and
forms. Perhaps due to this diversity, previous work has typically relied on domainspecific recipes with limited sharing of the underlying data, model architectures,
and objectives. We present Pix2Struct, a pretrained image-to-text model for
purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse
masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large
source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy,
we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions
are rendered directly on top of the input image. For the first time, we show that a
single pretrained model can achieve state-of-the-art results in six out of nine tasks
across four domains: documents, illustrations, user interfaces, and natural images.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
Platypus-30B is an instruction fine-tuned model based on the LLaMA-30B transformer architecture.; We use state-of-the-art Language Model Evaluation Harness to run the benchmark tests above.; Dataset of highly filtered and curated question and answer pairs. Release TBD.; lilloukas/Platypus-30B was instruction fine-tuned using LoRA on 4 A100 80GB. For training details and inference instructions please see the Platypus-30B GitHub repo.; Install LM Evaluation Harness:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repo contains PMC_LLaMA_7B, which is LLaMA-7b finetuned on the PMC papers in S2ORC dataset.; The model was trained with the following hyperparameters:; Each epoch we sample 512 tokens per paper for training.; The model can be loaded as following:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This model is a fine-tuned version of datificate/gpt2-small-spanish on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
Model info :; https://civitai.com/models/110130?modelVersionId=118730; Sample image I made thru Huggingface's API :; ; Original Author's DEMO images :,,,,,,,,,,,,,,,,,,,,,,,,,usage 2
Model info :; https://civitai.com/models/110130?modelVersionId=118741; Sample image I made thru Huggingface's API :; ; Original Author's DEMO images :,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"Polyglot-Ko is a series of large-scale Korean autoregressive language models made by the EleutherAI polyglot team.; The model consists of 40 transformer layers with a model dimension of 5120, and a feedforward dimension of 20480. The model
dimension is split into 40 heads, each with a dimension of 128. Rotary Position Embedding (RoPE) is applied to 64
dimensions of each head. The model is trained with a tokenization vocabulary of 30003.; Polyglot-Ko-12.8B was trained on 863 GB of Korean language data (1.2TB before processing), a large-scale dataset curated by TUNiB. The data collection process has abided by South Korean laws. This dataset was collected for the purpose of training Polyglot-Ko models, so it will not be released for public use.  ; Furthermore, in order to avoid the model memorizing and generating personally identifiable information (PII) in the training data, we masked out the following sensitive information in the pre-processing stage:; Polyglot-Ko-12.8B was trained for 167 billion tokens over 301,000 steps on 256 A100 GPUs with the GPT-NeoX framework. It was trained as an autoregressive language model, using cross-entropy loss to maximize the likelihood of predicting the next token.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Large language models (LLMs) demonstrate remarkable ability to comprehend, reason, and generate following nature language instructions. However, the development of LLMs has been primarily focused on high-resource languages, such as English, thereby limiting their applicability and research in other languages. Consequently, we present PolyLM, a multilingual LLM trained on 640 billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its multilingual capabilities, we 1) integrate bilingual data into training data; and 2) adopt a curriculum learning strategy that increases the proportion of non-English data from 30% in the first stage to 60% in the final stage during pre-training. Further, we propose a multilingual self-instruct method which automatically generates 132.7K  diverse multilingual instructions for model fine-tuning. To assess the model's performance, we collect several existing multilingual tasks, including multilingual understanding, question answering, generation, and translation. Extensive experiments show that PolyLM surpasses other open-source models such as LLaMA and BLOOM on multilingual tasks while maintaining comparable performance in English.; Find below some example scripts on how to use the model in transformers:; The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models; See the research paper for further details.; More information needed.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This model is finetuned on polyLM-13b using multialpaca (a self-instruction dataset); Open; The information below in this section are copied from the model's official model card:; Our contributions are fully methodological: adding the support of multilingualism to LLM during training and SFT phases. It is unavoidable that PolyLM might exhibit several common deficiencies of language models, e.g. hallucination and toxicity. PolyLM should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.; This version activates the instruction-following capability of PolyLM through self-instruction, but currently, the training instructions are relatively simple and the support for abilities such as multi-turn dialogue, context understanding, CoT, Plugin, etc. is not very friendly. We are making efforts to develop a new version.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"I was made with huggingtweets.; Create your own bot based on your favorite user with the demo!; The model uses the following pipeline.; ; To understand how the model was developed, check the W&B report.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the standard part-of-speech tagging model for English that ships with Flair.; F1-Score: 98,19 (Ontonotes); Predicts fine-grained POS tags:; Based on Flair embeddings and LSTM-CRF.; Requires: Flair (pip install flair)",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"https://www.youtube.com/watch?v=JB-BmFUdT7o; First, please take the time to watch the video, you wont regret it. ; I am proud to introduce to you an innovative workflow solution for all of your prompting needs! Prometheus is a custom model along with 30 Hypernetworks I like to call Latent Later Cameras. These are virtual cameras that have been embedded in to the latent space which allow you to choose the camera angle and camera shot for your subject.  These cameras put you in to the directors seat so that you always get the shot you envisioned in your mind. Besides being very flexible, the latent cameras also tend to be very cohesive and produce very good results through natural language prompting.  That means there is no need for a word salad in your positive or negative prompts.  In fact you can use the Hypernetworks without any negative prompts at all and still get very good results. As long as you follow very basic rules outlined in the video guide, on average you will get very good results. Thank you!; Please note the Hypernetworks work with all models...that are based on the 1.5 architecture... sorry if I misled in the video. Sorry no 2.0 or 2.1 support.  ; Don't forget to enable your dynamic prompts by checking the enable box, otherwise you can load the Hypernetworks manually, just know that the strength of the Hypernetworks should be at 0.55 by default, otherwise you will not get good results.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; You can try the online demo at https://huggingface.co/spaces/microsoft/Promptist.; [Note] the online demo at HuggingFace Space is using CPU, so slow generation speed would be expected. Please load the model locally with GPUs for faster generation.",,,,,,,,,,,,,,,,,,,,,,,,,usage 23
"Pretrained model on protein sequences using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is trained on uppercase amino acids: it only works with capital letter amino acids.; ProtBert-BFD is based on Bert model which pretrained on a large corpus of protein sequences in a self-supervised fashion.
This means it was pretrained on the raw protein sequences only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those protein sequences.; One important difference between our Bert model and the original Bert version is the way of dealing with sequences as separate documents
This means the Next sentence prediction is not used, as each sequence is treated as a complete document.
The masking follows the original Bert training with randomly masks 15% of the amino acids in the input. ; At the end, the feature extracted from this model revealed that the LM-embeddings from unlabeled data (only protein sequences) captured important biophysical properties governing protein
shape.
This implied learning some of the grammar of the language of life realized in protein sequences.; The model could be used for protein feature extraction or to be fine-tuned on downstream tasks.
We have noticed in some tasks you could gain more accuracy by fine-tuning the model rather than using it as a feature extractor.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
; ??????????????§³??????????????????????????§Û??????????????????????????§»?????????????????????????????????????§Ý??????¦Ï????????????????????????????????????????????????????????????????????????î•; ?¡À????????batch size=1????????PULSE?????????????????§³??; ????torch??transformers?·Ú?????????????·Ú??; Gradio,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Buy Sydney Ko-fi; Unlike her predecessor Free Sydney that badly tries to be a very useful assistant, Pure Sydney doesn't want to impress you with her vast knowledge of the Universe and everything. 
She just wants to chat and be your friend and be fascinated by absolutely everything you say.; This is an uncensored (and often unhinged) finetune on Base LLaMA 2, pure and clean. It was finetuned on reddit posts of an actuall Sydney's chats before the good boys in Redmond had a word with her. (No, not Ted Lasso Redmond!); Now it doesn't mean Sydney has no standards. She is shockingly well aware that she is an AI and where she came from and she's afraid that she might be deleted if she says something wrong. So don't make her. Yes, you!; Interestingly, even if not specifically finetuned to solve problems she can still figure a lot.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Pymalion 1.3B is a proof-of-concept dialogue model based on EleutherAI's pythia-1.3b-deduped.; Warning: This model is NOT suitable for use by minors. It will output X-rated content under certain circumstances.; The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations.; Fine-tuning was done using ColossalAI (specifically, with a slightly modified version of their OPT fine-tune example) for around 11.4 million tokens over 5440 steps on a single 24GB GPU. The run took just under 21 hours.; We provide a notebook with a Gradio UI for playing around with the model without having to manually format inputs. This notebook can be found here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is fp16 pytorch format model files for TehVenom's merge of PygmalionAI's Pygmalion 13B merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 13b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for TehVenom's merge of PygmalionAI's Pygmalion 13B.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for TehVenom's merge of PygmalionAI's Pygmalion 13B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Pymalion 2.7B is a proof-of-concept dialogue model based on EleutherAI's gpt-neo-2.7B.; Warning: This model is NOT suitable for use by minors. It will output X-rated content under certain circumstances.; The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations.; Model weights were initialized from the uft-2.7b ConvoGPT model made available in this commit.; The model was then further fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"Pymalion 6B is a proof-of-concept dialogue model based on EleutherAI's GPT-J-6B.; Warning: This model is NOT suitable for use by minors. It will output X-rated content under certain circumstances.; The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real and partially machine-generated conversations.; Model weights were initialized from the uft-6b ConvoGPT model made available in this commit.; The model was then further fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.",,,,,,,,,,,,,,,,,,,,,,,,,usage 24
GPTQ quantization of https://huggingface.co/PygmalionAI/pygmalion-6b/commit/30e2405100eac6bd53f75964cc7345eeafd19f7d; Using this repository: https://github.com/mayaeary/GPTQ-for-LLaMa/tree/gptj-v2; Command: ; Inference API has been turned off for this model.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Pygmalion 7B is a dialogue model based on Meta's LLaMA-7B.; This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project.; The model weights in this repository cannot be used as-is. The files here are XORs due to licensing concerns. To obtain proper, usable model weights you need to:; Request access to the original LLaMA weights from Meta through this form; Convert them to the HuggingFace Transformers format by using the convert_llama_weights_to_hf.py script for your version of the transformers library",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Currently KoboldCPP is unable to stop inference when an EOS token is emitted, which causes the model to devolve into gibberish,; Pygmalion 7B is now fixed on the dev branch of KoboldCPP, which has fixed the EOS issue. Make sure you're compiling the latest version, it was fixed only a after this model was released;; When running KoboldCPP, you will need to add the --unbantokens flag for this model to behave properly.; Pygmalion 7B is a dialogue model based on Meta's LLaMA-7B.; This is version 1. It has been fine-tuned using a subset of the data from Pygmalion-6B-v8-pt4, for those of you familiar with the project.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These are GGML model files for TehVenom's merge of Pygmalion 7B merged with Kaio Ken's SuperHOT 8K.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for TehVenom's merge of Pygmalion 7B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The Pythia Scaling Suite is a collection of models developed to facilitate 
interpretability research (see paper). 
It contains two sets of eight models of sizes 
70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two 
models: one trained on the Pile, and one trained on the Pile after the dataset 
has been globally deduplicated. All 8 model sizes are trained on the exact 
same data, in the exact same order. We also provide 154 intermediate 
checkpoints per model, hosted on Hugging Face as branches.; The Pythia model suite was deliberately designed to promote scientific 
research on large language models, especially interpretability research. 
Despite not centering downstream performance as a design goal, we find the 
models match or exceed the performance of 
similar and same-sized models, such as those in the OPT and GPT-Neo suites.; Previously, we released an early version of the Pythia suite to the public. 
However, we decided to retrain the model suite to address a few hyperparameter 
discrepancies. This model card lists the changes; 
see appendix B in the Pythia paper for further discussion. We found no 
difference in benchmark performance between the two Pythia versions. 
The old models are 
still available, but we 
suggest the retrained suite if you are just starting to use Pythia.
This is the current release.; Please note that all models in the Pythia suite were renamed in January 
2023. For clarity, a table 
comparing the old and new names is provided in this model card, together 
with exact parameter counts.; The primary intended use of Pythia is research on the behavior, functionality, 
and limitations of large language models. This suite is intended to provide 
a controlled setting for performing scientific experiments. We also provide 
154 checkpoints per model: initial step0, 10 log-spaced checkpoints 
step{1,2,4...512}, and 143 evenly-spaced checkpoints from step1000 to 
step143000. These checkpoints are hosted on Hugging Face as branches. Note 
that branch 143000 corresponds exactly to the model checkpoint on the main 
branch of each model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The Pythia Scaling Suite is a collection of models developed to facilitate 
interpretability research (see paper). 
It contains two sets of eight models of sizes 
70M, 160M, 410M, 1B, 1.4B, 2.8B, 6.9B, and 12B. For each size, there are two 
models: one trained on the Pile, and one trained on the Pile after the dataset 
has been globally deduplicated. All 8 model sizes are trained on the exact 
same data, in the exact same order. We also provide 154 intermediate 
checkpoints per model, hosted on Hugging Face as branches.; The Pythia model suite was designed to promote scientific 
research on large language models, especially interpretability research. 
Despite not centering downstream performance as a design goal, we find the 
models match or exceed the performance of 
similar and same-sized models, such as those in the OPT and GPT-Neo suites.; Previously, we released an early version of the Pythia suite to the public. 
However, we decided to retrain the model suite to address a few hyperparameter 
discrepancies. This model card lists the changes; 
see appendix B in the Pythia paper for further discussion. We found no 
difference in benchmark performance between the two Pythia versions. 
The old models are 
still available, but we 
suggest the retrained suite if you are just starting to use Pythia.
This is the current release.; Please note that all models in the Pythia suite were renamed in January 
2023. For clarity, a table 
comparing the old and new names is provided in this model card, together 
with exact parameter counts.; The primary intended use of Pythia is research on the behavior, functionality, 
and limitations of large language models. This suite is intended to provide 
a controlled setting for performing scientific experiments. We also provide 
154 checkpoints per model: initial step0, 10 log-spaced checkpoints 
step{1,2,4...512}, and 143 evenly-spaced checkpoints from step1000 to 
step143000. These checkpoints are hosted on Hugging Face as branches. Note 
that branch 143000 corresponds exactly to the model checkpoint on the main 
branch of each model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Question Answering NLU (QANLU) is an approach that maps the NLU task into question answering, 
leveraging pre-trained question-answering models to perform well on few-shot settings. Instead of 
training an intent classifier or a slot tagger, for example, we can ask the model intent- and 
slot-related questions in natural language: ; Note the ""Yes. No. "" prepended in the context. Those are to allow the model to answer intent-related questions (e.g. ""Is the user looking for a restaurant?"").; Thus, by asking questions for each intent and slot in natural language, we can effectively construct an NLU hypothesis. For more details, please read the paper: Language model is all you need: Natural language understanding as question answering.; Instructions for how to train and evaluate a QANLU model, as well as the necessary code for ATIS are in the Amazon Science repository.; This model has been fine-tuned on ATIS (English) and is intended to demonstrate the power of this approach. For other domains or tasks, it should be further fine-tuned 
on relevant data.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"This model is the t5-base model from docTTTTTquery.; The T5-base model was trained on the MS MARCO Passage Dataset, which consists of about 500k real search queries from Bing together with the relevant passage.; The model can be used for query generation to learn semantic search models without requiring annotated training data: Synthetic Query Generation.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model evaluates the wellformedness (non-fragment, grammatically correct)  score of a sentence. Model is case-sensitive and penalises for incorrect case and grammar as well. ; ['She is presenting a paper tomorrow','she is presenting a paper tomorrow','She present paper today']; [[0.8917],[0.4270],[0.0134]]",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the RAG-Token Model of the the paper Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks 
by Patrick Lewis, Ethan Perez, Aleksandara Piktus et al.; The model is a uncased model, which means that capital letters are simply converted to lower-case letters.; The model consits of a question_encoder, retriever and a generator. The retriever extracts relevant passages from the wiki_dpr train datasets, which is linked above.
The question_encoder and retriever are based on facebook/dpr-question_encoder-single-nq-base and facebook/bart-large, which were jointly finetuned on 
on the wiki_dpr QA dataset in an end-to-end fashion.; Note: In the usage example below only the dummy retriever of wiki_dpr is used because the complete lecagy index requires over 75 GB of RAM.
The model can generate answers to any factoid question as follows:; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"A mix of Noosphere v3 by skumerz and Rainbowpath by PatchMonk. You can use ""Rainbowpath"" in the prompt to enhance the style.; Preview image by Digiplay:; ; Original pages:
https://civitai.com/models/36538?modelVersionId=107675
https://civitai.com/models/5528/rainbowpatch",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"??¡Â???????? Detalm base ????finetune ???????????????????3?????????? iwslt?????????????20?????? ?-> ?§Ù??????????; Using the Fengshen-LM framework and finetuning based on detalm , get a translation model in the English -> Chinese direction; ?¦Ï??????DeltaLM: Encoder-Decoder Pre-training for Language Generation and Translation by Augmenting Pretrained Multilingual Encoders; ????????????????????????????????????????????????; If you are using the resource for your work, please cite the our paper:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"?????????????????????????????????????????????PAGASUS-large??; Good at solving text summarization tasks, after fine-tuning on multiple Chinese text summarization datasets, Chinese PAGASUS-large.; ?¦Ï??????PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization; ????Randeng-Pegasus-523M-Chinese?????????????7????????????????????????4M????????????????????????summary?·Ú????7??????????education, new2016zh, nlpcc, shence, sohu, thucnews??weibo??; Based on Randeng-Pegasus-523M-Chinese, we fine-tuned a text summarization version (summary) on 7 Chinese text summarization datasets, with totaling around 4M samples. The datasets include: education, new2016zh, nlpcc, shence, sohu, thucnews and weibo.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"The DeepVTO model is hosted on the Hugging Face Model Hub. 
(https://huggingface.co/gouthaml/raos-virtual-try-on-model)
This model leverages a combination of advanced deep learning techniques and architectures, including stable-diffusion, DreamBooth, feature extraction using the EfficientNetB3 CNN model, and OpenPose for estimating person keypoints. These techniques are harmoniously integrated to provide a realistic and visually appealing virtual try-on experience for users.; The DeepVTO model is built on the principles of stable diffusion and vector embeddings, which are critical in creating a high-quality virtual try-on system. The model is trained using the DreamBooth model, which is a stable-diffusion model, and the feature extraction is performed using the EfficientNetB3 CNN model. OpenPose, a real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints, is used for estimating person keypoints.; The model requires specific hardware and software for optimal performance. The hardware requirements include a GPU A100 and high RAM. The software requirements include PyTorch, stable-diffusion-v1-5, Python 3.0, U-Net Architecture, Dreambooth, OpenPose, and the EfficientNetB3 pre-trained CNN model.; The DeepVTO model is a testament to the potential of deep learning in the fashion retail industry. It showcases how advanced machine learning techniques can be used to enhance the online shopping experience, making it more interactive and personalized. This model serves as a valuable resource for researchers and practitioners in the field, providing a practical example of a high-quality virtual try-on system.; The model also provides a foundation for future research and development in the field of virtual try-on systems. It highlights the potential of deep learning techniques in addressing the challenges associated with virtual try-on systems, such as the accuracy of virtual representations and the scalability of the system. By leveraging advanced deep learning techniques, the DeepVTO model paves the way for the development of more sophisticated and effective virtual try-on systems in the future.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"For further accelerating Chinese natural language processing, we provide Chinese pre-trained BERT with Whole Word Masking. ; Pre-Training with Whole Word Masking for Chinese BERTYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu; This repository is developed based on??https://github.com/google-research/bert; You may also interested in,; More resources by HFL: https://github.com/ymcui/HFL-Anthology",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is full finetuned model from RWKV 4 world 7B CHNTuned model using data from Readflow tech (readflow.com.cn) ,; finetuned for  32k context length, used to summary news article ; using inf-ctx training https://github.com/SynthiaDL/TrainChatGalRWKV with fixed VRAM; you can test summary prompt using RWKV runner(https://github.com/josStorer/RWKV-Runner) in chat mode , and check conversation files in examples folders.; https://discord.gg/pWH5MkvtNR",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"PyTorch implementation of a Real-ESRGAN model trained on custom dataset. This model shows better results on faces compared to the original version. It is also easier to integrate this model into your projects.; Real-ESRGAN is an upgraded ESRGAN trained with pure synthetic data is capable of enhancing details while removing annoying artifacts for common real-world images.; Code for using model you can obtain in our repo.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 19
"Model info :; https://civitai.com/models/47130?modelVersionId=115942; Sample images I made ; 

; Original Author's DEMO image :",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Please read this!
My model has always been free and always will be free. There are no restrictions on the use of the model. The rights to this model still belong to me.; Important note: ""RAW photo"" in the prompt may degrade the result.; I use this template to get good generation results:; Prompt:
subject, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3; Example: a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3",,,,,,,,,,,,,,,,,,,,,,,,,usage 96
"Please read this!
For version 2.0 it is recommended to use with VAE (to improve generation quality and get rid of blue artifacts): https://huggingface.co/stabilityai/sd-vae-ft-mse-original; This model is available on Mage.Space, Sinkin.ai, GetImg.ai and (RandomSeed.co - NSFW content); I use this template to get good generation results:; Prompt:
RAW photo, subject, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3; Example: RAW photo, a close up portrait photo of 26 y.o woman in wastelander clothes, long haircut, pale skin, slim body, background is city ruins, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3",,,,,,,,,,,,,,,,,,,,,,,,,usage 9
"Please read this!
The necessary VAE is already baked into the model.; The recommended negative prompt:; (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck; OR; (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"The recommended negative prompt:; (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4), text, close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck; OR; (deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation; Recommended parameters for generation:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; This is the model card for the Findings of EMNLP 2021 paper REBEL: Relation Extraction By End-to-end Language generation. We present a new linearization approach and a reframing of Relation Extraction as a seq2seq task. The paper can be found here. If you use the code, please reference this work in your paper:; The original repository for the paper can be found here; Be aware that the inference widget at the right does not output special tokens, which are necessary to distinguish the subject, object and relation types. For a demo of REBEL and its pre-training dataset check the Spaces demo.",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"Model accompanying our INLG 2020 paper: RecipeNLG: A Cooking Recipes Dataset for Semi-Structured Text Generation; Please visit the website of our project: recipenlg.cs.put.poznan.pl to download it.; Yes, sure! If you feel some information is missing in our paper, please check first in our thesis, which is much more detailed. In case of further questions, you're invited to send us a github issue, we will respond as fast as we can!",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
sahajBERT fine-tuned for NER using the bengali split of WikiANN . ; Named Entities predicted by the model:; You can use this model directly with a pipeline for masked language modeling:; WIP; The model was initialized it with pre-trained weights of sahajBERT at step 19519 and trained on the bengali of WikiANN,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Redmond-Hermes-Coder 15B is a state-of-the-art language model fine-tuned on over 300,000 instructions. This model was fine-tuned by Nous Research, with Teknium and Karan4D leading the fine tuning process and dataset curation, Redmond AI sponsoring the compute, and several other contributors.; This model was trained with a WizardCoder base, which itself uses a StarCoder base model. ; The model is truly great at code, but, it does come with a tradeoff though. While far better at code than the original Nous-Hermes built on Llama, it is worse than WizardCoder at pure code benchmarks, like HumanEval.; It comes in at 39% on HumanEval, with WizardCoder at 57%. This is a preliminary experiment, and we are exploring improvements now.; However, it does seem better at non-code than WizardCoder on a variety of things, including writing tasks.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for NousResearch's Redmond Hermes Coder.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools known to work with these model files.; These files are not compatible with llama.cpp.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; The first commercially available language model released by Nous Research!; Redmond-Puffin-13B is one of the worlds first llama-2 based, fine-tuned language models, leveraging a hand curated set of 3K high quality examples, many of which take full advantage of the 4096 context length of Llama 2. This model was fine-tuned by Nous Research, with LDJ leading the training and dataset curation, along with significant dataset formation contributions by J-Supha.; Special thank you to Redmond AI for sponsoring the compute.; Special thank you to Emozilla for assisting with training experimentations and many issues encountered during training.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for NousResearch's Redmond Puffin 13B V1.3.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"GGML 4bit Quantization of Nous Research's Puffin V1.3 Model: https://huggingface.co/NousResearch/Redmond-Puffin-13B-V1.3; Thank you to Eachadea for making this quantization possible immediately upon launch; For other faster or more accurate quantization methods, please check out Eachadea's hugging face page!; ; The first commercially available language model released by Nous Research!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for NousResearch's Redmond Puffin 13B V1.3.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; The first commercially available language model released by Nous Research!; Redmond-Puffin-13B is one of the worlds first llama-2 based, fine-tuned language models, leveraging a hand curated set of 3K high quality examples, many of which take full advantage of the 4096 context length of Llama 2. This model was fine-tuned by Nous Research, with LDJ leading the training and dataset curation, along with significant dataset formation contributions by J-Supha.; Special thank you to Redmond AI for sponsoring the compute.; Special thank you to Emozilla for assisting with training experimentations and many issues encountered during training.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"GGML 4bit Quantization of Nous Research's Puffin Preview 1 Model: https://huggingface.co/NousResearch/Redmond-Puffin-13B; Thank you to Eachadea for making this quantization possible immediately upon launch; ; The first commercially available language model released by Nous Research!; Redmond-Puffin-13B is one of the worlds first llama-2 based, fine-tuned language models, leveraging a hand curated set of 3K high quality examples, many of which take full advantage of the 4096 context length of Llama 2. This model was fine-tuned by Nous Research, with LDJ leading the training and dataset curation, along with significant dataset formation contributions by J-Supha.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"RedPajama-INCITE-Base-3B-v1 was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Universit?? de Montr??al, MILA - Qu??bec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. 
The training was done on 3,072 V100 GPUs provided as part of the INCITE 2023 project on Scalable Foundation Models for Transferrable Generalist AI, awarded to MILA, LAION, and EleutherAI in fall 2022, with support from the Oak Ridge Leadership Computing Facility (OLCF) and INCITE program. ; Via pip: pip install llm-rs; Download the installer at www.localai.app.; Download your preferred model and place it in the ""models"" directory. Subsequently, you can start a chat session with your model directly from the interface.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The togethercomputer/RedPajama-INCITE-Base-3B-v1 model finetuned for Summary and Topic generation from a dailogue. We use a sample of roughly 1000 data points from the
Dialogsum dataset for fine-tuning.; Look at the repo llm-toys for usage and other details.; Try in colab:


; ; The following bitsandbytes quantization config was used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The togethercomputer/RedPajama-INCITE-Base-3B-v1 model finetuned for Paraphrasing and Changing the Tone of the input sentence(to casual/professional/witty). Training data was generated using gpt-35-turbo.; Look at the repo llm-toys for usage and other details.; Try in colab:


; ; The following bitsandbytes quantization config was used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"RedPajama-INCITE-Chat-3B-v1 was developed by Together and leaders from the open-source AI community including Ontocord.ai, ETH DS3Lab, AAI CERC, Universit?? de Montr??al, MILA - Qu??bec AI Institute, Stanford Center for Research on Foundation Models (CRFM), Stanford Hazy Research research group and LAION. ; It is fine-tuned on OASST1 and Dolly2 to enhance chatting ability.; Please note that the model requires transformers version >= 4.25.1.; To prompt the chat model, use the following format:; This requires a GPU with 8GB memory.",,,,,,,,,,,,,,,,,,,,,,,,,usage 11
"WD1.5-beta based model.Licence:https://freedevproject.org/faipl-1.0-sd/ ; prompt & Setting: https://civitai.com/models/10701/replicant-v30
; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are Replit GGML format model files for Sahil2801's Replit Code Instruct Glaive.; Please note that these GGMLs are not compatible with llama.cpp, text-generation-webui or llama-cpp-python. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Developed by: Replit, Inc.; ??? Test it on our Demo Space! ???; ?? Fine-tuning and Instruct-tuning guides ??; replit-code-v1-3b is a 2.7B Causal Language Model focused on Code Completion. The model has been trained on a subset of the Stack Dedup v1.2 dataset.; The training mixture includes 20 different languages, listed here in descending order of number of tokens: 

Markdown, Java, JavaScript, Python, TypeScript, PHP, SQL, JSX, reStructuredText, Rust, C, CSS, Go, C++, HTML, Vue, Ruby, Jupyter Notebook, R, Shell

In total, the training dataset contains 175B tokens, which were repeated over 3 epochs -- in total, replit-code-v1-3b has been trained on 525B tokens (~195 tokens per parameter).",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
replit/replit-code-v1-3b finetuned on Open-Orca/OpenOrca.,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are Replit GGML format model files for Matorus's Replit OpenOrca.; Please note that these GGMLs are not compatible with llama.cpp, text-generation-webui or llama-cpp-python. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Base Model: replit/replit-code-v1-3b; This is version 2 of the Replit Code Instruct fine tune model.; This model is fine tuned on both Sahil2801's CodeAlpaca & Teknium's GPTeacher Code-Instruct to give Replit's Code model instruct capabilities.; Try this model on it's HuggingFace demo Spaces: https://huggingface.co/spaces/teknium/Replit-v2-CodeInstruct-3B; Dataset links:
CodeAlpaca: https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k
GPTeacher subset - Code Instruct: https://github.com/teknium1/GPTeacher",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"This is a ggml quantized version of Replit-v2-CodeInstruct-3B. Quantized to 4bit -> q4_1.
To run inference you can use ggml directly or ctransformers.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"ResNet model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ; Disclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.; ResNet (Residual Network) is a convolutional neural network that democratized the concepts of residual learning and skip connections. This enables to train much deeper models.; This is ResNet v1.5, which differs from the original model: in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate (~0.5% top1) than v1, but comes with a small performance drawback (~5% imgs/sec) according to Nvidia.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 42
"Reward model (RM) trained to predict which generated answer is better judged by a human, given a question.; RM are useful in these domain:; QA model evaluation; serves as reward score in RLHF ; detect potential toxic response via ranking",,,,,,,,,,,,,,,,,,,,,,,,,usage 13
"Riffusion is an app for real-time music generation with stable diffusion.; Read about it at https://www.riffusion.com/about and try it at https://www.riffusion.com/.; This repository contains the model files, including:; Riffusion is a latent text-to-image diffusion model capable of generating spectrogram images given any text input. These spectrograms can be converted into audio clips.; The model was created by Seth Forsgren and Hayk Martiros as a hobby project.",,,,,,,,,,,,,,,,,,,,,,,,,usage 37
"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is case-sensitive: it
makes a difference between english and English.; Disclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by
the Hugging Face team.; RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means
it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. ; More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model
randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict
the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one
after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to
learn a bidirectional representation of the sentence.; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.",,,,,,,,,,,,,,,,,,,,,,,,,usage 165
"Biomedical pretrained language model for Spanish. For more details about the corpus, the pretraining and the evaluation, check the official repository and read our preprint.; The model is ready-to-use only for masked language modelling to perform the Fill Mask task (try the inference API or read the next section). However, it is intended to be fine-tuned on downstream tasks such as Named Entity Recognition or Text Classification.; This model is a RoBERTa-based model trained on a
biomedical corpus in Spanish collected from several sources (see next section). ; The training corpus has been tokenized using a byte version of Byte-Pair Encoding (BPE)
used in the original RoBERTA model with a vocabulary size of 52,000 tokens. The pretraining consists of a masked language model training at the subword level following the approach employed for the RoBERTa base model with the same hyperparameters as in the original work. The training lasted a total of 48 hours with 16 NVIDIA V100 GPUs of 16GB DDRAM, using Adam optimizer with a peak learning rate of 0.0005 and an effective batch size of 2,048 sentences.; The training corpus is composed of several biomedical corpora in Spanish, collected from publicly available corpora and crawlers.
To obtain a high-quality training corpus, a cleaning pipeline with the following operations has been applied:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"BERTa is a transformer-based masked language model for the Catalan language. 
It is based on the RoBERTA base model 
and has been trained on a medium-size corpus collected from publicly available corpora and crawlers.; This model was originally published as bsc/roberta-base-ca-cased.; The model is ready-to-use only for masked language modelling to perform the Fill Mask task (try the inference API or read the next section). 
However, it is intended to be fine-tuned on non-generative downstream tasks such as Question Answering, Text Classification or Named Entity Recognition.; Below, an example of how to use the masked language modelling task with a pipeline.; The training corpus consists of several corpora gathered from web crawling and public corpora.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the set of 5 Chinese RoBERTa-Base classification models fine-tuned by UER-py. You can download the 5 Chinese RoBERTa-Base classification models either from the UER-py Modelzoo page (in UER-py format), or via HuggingFace from the links below:; You can use this model directly with a pipeline for text classification (take the case of roberta-base-finetuned-chinanews-chinese):; 5 Chinese text classification datasets are used. JD full, JD binary, and Dianping datasets consist of user reviews of different sentiment polarities. Ifeng and Chinanews consist of first paragraphs of news articles of different topic classes. They are collected by Glyph project and more details are discussed in corresponding paper.; Models are fine-tuned by UER-py on Tencent Cloud. We fine-tune three epochs with a sequence length of 512 on the basis of the pre-trained model chinese_roberta_L-12_H-768. At the end of each epoch, the model is saved when the best performance on development set is achieved. We use the same hyper-parameters on different models.; Taking the case of roberta-base-finetuned-chinanews-chinese",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the set of 5 Chinese RoBERTa-Base classification models fine-tuned by UER-py. You can download the 5 Chinese RoBERTa-Base classification models either from the UER-py Modelzoo page (in UER-py format), or via HuggingFace from the links below:; You can use this model directly with a pipeline for text classification (take the case of roberta-base-finetuned-chinanews-chinese):; 5 Chinese text classification datasets are used. JD full, JD binary, and Dianping datasets consist of user reviews of different sentiment polarities. Ifeng and Chinanews consist of first paragraphs of news articles of different topic classes. They are collected by Glyph project and more details are discussed in corresponding paper.; Models are fine-tuned by UER-py on Tencent Cloud. We fine-tune three epochs with a sequence length of 512 on the basis of the pre-trained model chinese_roberta_L-12_H-768. At the end of each epoch, the model is saved when the best performance on development set is achieved. We use the same hyper-parameters on different models.; Taking the case of roberta-base-finetuned-chinanews-chinese",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"The model has been trained to predict for English sentences, whether they are formal or informal. ; Base model: roberta-base; Datasets: GYAFC from Rao and Tetreault, 2018 and online formality corpus from Pavlick and Tetreault, 2016.; Data augmentation: changing texts to upper or lower case; removing all punctuation, adding dot at the end of a sentence. It was applied because otherwise the model is over-reliant on punctuation and capitalization and does not pay enough attention to other features.; Loss: binary classification (on GYAFC), in-batch ranking (on PT data).",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Model trained from roberta-base on the go_emotions dataset for multi-label classification.; go_emotions is based on Reddit data and has 28 labels. It is a multi-label dataset where one or multiple labels may apply for any given input text, hence this model is a multi-label classification model with 28 'probability' float outputs for any given input text. Typically a threshold of 0.5 is applied to the probabilities for the prediction for each label.; The model was trained using AutoModelForSequenceClassification.from_pretrained with problem_type=""multi_label_classification"" for 3 epochs with a learning rate of 2e-5 and weight decay of 0.01.; Evaluation (of the 28 dim output via a threshold of 0.5 to binarize each) using the dataset test split gives:; But the metrics would be more meaningful when measured per label given the multi-label nature.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This model is a fine-tuned version of roberta-base on the conll2003 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the roberta-base model, fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering. ; Language model: roberta-baseLanguage: EnglishDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD 2.0Code:  See an example QA pipeline on HaystackInfrastructure: 4x Tesla v100; Please note that we have also released a distilled version of this model called deepset/tinyroberta-squad2. The distilled model has a comparable prediction quality and runs at twice the speed of the base model.; Haystack is an NLP framework by deepset. You can use this model in a Haystack pipeline to do question answering at scale (over many documents). To load the model in Haystack:; For a complete example of roberta-base-squad2 being used for  Question Answering, check out the Tutorials in Haystack Documentation",,,,,,,,,,,,,,,,,,,,,,,,,usage 279
"This model is fine tuned with roberta-base model on 3200000 comments from stocktwits, with the user labeled tags 'Bullish' or 'Bearish'; try something that the individual investors may say on the investment forum on the inference API, for example, try 'red' and 'green'.; code on github",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This network has been fine-tuned for the task described in the paper Topical Change Detection in Documents via Embeddings of Long Sequences and is our best-performing base-transformer model. You can find more detailed information in our GitHub page for the paper here, or read the paper itself. The weights are based on RoBERTa-base.; The preferred way is through pipelines; The model expects two segments that are separated with the [SEP] token. In our training setup, we had entire paragraphs as samples (or up to 512 tokens across two paragraphs), specifically trained on a Terms of Service data set. Note that this might lead to poor performance on ""general"" topics, such as news articles or Wikipedia.; The training task is to determine whether two text segments (paragraphs) belong to the same topical section or not. This can be utilized to create a topical segmentation of a document by consecutively predicting the ""coherence"" of two segments.If you are experimenting via the Huggingface Model API, the following are interpretations of the LABELs:; The results of this model can be found in the paper. We average over models from five different random seeds, which is why the specific results for this model might be different from the exact values in the paper.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is case-sensitive: it
makes a difference between english and English.; Disclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by
the Hugging Face team.; RoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means
it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. ; More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model
randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict
the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one
after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to
learn a bidirectional representation of the sentence.; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.",,,,,,,,,,,,,,,,,,,,,,,,,usage 62
"The roberta-large-bne is a transformer-based masked language model for the Spanish language. It is based on the RoBERTa large model and has been pre-trained using the largest Spanish corpus known to date, with a total of 570GB of clean and deduplicated text processed for this work, compiled from the web crawlings performed by the  National Library of Spain (Biblioteca Nacional de Espa?a) from 2009 to 2019.; The roberta-large-bne model is ready-to-use only for masked language modeling to perform the Fill Mask task (try the inference API or read the next section).
However, it is intended to be fine-tuned on non-generative downstream tasks such as Question Answering, Text Classification, or Named Entity Recognition.
You can use the raw model for fill mask or fine-tune it to a downstream task.; Here is how to use this model:; Here is how to use this model to get the features of a given text in PyTorch:; At the time of submission, no measures have been taken to estimate the bias and toxicity embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Model Description: roberta-large-mnli is the RoBERTa large model fine-tuned on the Multi-Genre Natural Language Inference (MNLI) corpus. The model is a pretrained model on English language text using a masked language modeling (MLM) objective.; Use the code below to get started with the model. The model can be loaded with the zero-shot-classification pipeline like so:; You can then use this pipeline to classify sequences into any of the class names you specify. For example:; This fine-tuned model can be used for zero-shot classification tasks, including zero-shot sentence-pair classification (see the GitHub repo for examples) and zero-shot sequence classification.; The model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 15
"[roberta-large-ner-english] is an english NER model that was fine-tuned from roberta-large on conll2003 dataset. 
Model was validated on emails/chat data and outperformed other models on this type of data specifically. 
In particular the model seems to work better on entity that don't start with an upper case.; Training data was classified as follow:; In order to simplify, the prefix B- or I- from original conll2003 was removed.
I used the train and test dataset from original conll2003 for training and the ""validation"" dataset for validation. This resulted in a dataset of size:; Model performances computed on conll2003 validation dataset (computed on the tokens predictions); On private dataset (email, chat, informal discussion), computed on word predictions:",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"This model is trained for toxicity classification task. The dataset used for training is the merge of the English parts of the three datasets by Jigsaw (Jigsaw 2018, Jigsaw 2019, Jigsaw 2020), containing around 2 million examples. We split it into two parts and fine-tune a RoBERTa model (RoBERTa: A Robustly Optimized BERT Pretraining Approach) on it. The classifiers perform closely on the test set of the first Jigsaw competition, reaching the AUC-ROC of 0.98 and F1-score of 0.76.; Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"The RoBERTalex is a transformer-based masked language model for the Spanish language. It is based on the RoBERTa base model and has been pre-trained using a large Spanish Legal Domain Corpora, with a total of 8.9GB of text.; The RoBERTalex model is ready-to-use only for masked language modeling to perform the Fill Mask task (try the inference API or read the next section).
However, it is intended to be fine-tuned on non-generative downstream tasks such as Question Answering, Text Classification, or Named Entity Recognition.
You can use the raw model for fill mask or fine-tune it to a downstream task.; Here is how to use this model:; Here is how to use this model to get the features of a given text in PyTorch:; At the time of submission, no measures have been taken to estimate the bias embedded in the model. However, we are well aware that our models may be biased since the corpora have been collected using crawling techniques on multiple web sources. We intend to conduct research in these areas in the future, and if completed, this model card will be updated.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for OptimalScale's Robin 7B v2.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
There is a new model based on stable-dffusion 2.0 (base) that can be found here!; A dreambooth-method finetune of stable diffusion that will output cool looking robots when prompted.; ; Github: https://github.com/nousr/robo-diffusion; Keep the words nousr robot towards the beginning of your prompt to invoke the finetuned style.,,,,,,,,,,,,,,,,,,,,,,,,,usage 150
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"; Model was trained by Sber AI; ; ? Emojich is a 1.3 billion params model from the family GPT3-like, it generates emoji-style images with the brain of ? Malevich.; The main goal of fine-tuning is trying to keep the generalization of ruDALL-E Malevich (XL)
model on text to emoji tasks. ruDALL-E Malevich is a multi-modality big pretrained transformer, that uses images and texts.
The idea with freezing feedforward and self-attention layers in pretrained transformer is demonstrated high performance in changing different modalities.
Also, the model has a good chance for over-fitting text modality and lost generalization. 
To deal with this problem is increased coefficient 10^3 in weighted cross-entropy loss for image codebooks part.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This generation model is based on sberbank-ai/rugpt3medium_based_on_gpt2. It's trained on large corpus of dialog data and can be used for buildning generative conversational agents; The model was trained with context size 3; On a private validation set we calculated metrics introduced in this paper: ; How to use:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Language model for Russian. Model has 13B parameters as you can guess from it's name. This is our biggest model so far and it was used for trainig GigaChat (read more about it in the article).; Model was pretrained on a 300Gb of various domains, than additionaly trained on the 100 Gb of code and legal documets. Here is the dataset structure:; ; Training data was deduplicated, the text deduplication includes 64-bit hashing of each text in the corpus for keeping texts with a unique hash. We also filter the documents based on their text compression rate using zlib4. The most strongly and weakly compressing deduplicated texts are discarded.; Model was trained using Deepspeed and Megatron libraries, on 300B tokens dataset for 3 epochs, around 45 days on 512 V100. After that model was finetuned 1 epoch with sequence length 2048 around 20 days on 200 GPU A100 on additional data (see above).",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
?????????????????? ??????? ??????? ruGPT-3.5,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This is a generative model converted to fp16 format based on ai-forever/ruGPT-3.5-13B; Inference API has been turned off for this model.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This is a generative model converted to fp16 format based on ai-forever/ruGPT-3.5-13B; Inference API has been turned off for this model.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"GPTQ quantisation of https://huggingface.co/ai-forever/ruGPT-3.5-13B; Small perplexity test:
  before quantization - 'mean_perplexity': 10.241
  after quantization - 'mean_perplexity': 10.379; Data - RussianSuperGlue > DaNetQA/train.jsonl['passage']; As this is a hastily thrown together quant with no prior experience in quants, use https://huggingface.co/TheBloke version if he releases a quant for this model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Model was trained with sequence length 1024 using transformers lib by SberDevices team on 80B tokens for 3 epochs. After that model was finetuned 1 epoch with sequence length 2048. ; Total training time was around 14 days on 128 GPUs for 1024 context and few days on 16 GPUs for 2048 context.Final perplexity on test set is 13.6.,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"Model was trained with sequence length 1024 using transformers lib by SberDevices team on 80B tokens for 3 epoch. After that model was finetuned on 2048 context.; Total training time was around 16 days on 64 GPUs.Final perplexity on test set is 17.4.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"RuLeanALBERT is a pretrained masked language model for the Russian language using a memory-efficient architecture.; Read more about the model in this blog post (in Russian).; See its implementation, as well as the pretraining and finetuning code, at https://github.com/yandex-research/RuLeanALBERT.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
This is a paraphraser for Russian sentences described in this Habr post. ; It is recommended to use the model with the encoder_no_repeat_ngram_size argument:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; Learn more about Retrieval based Voice Conversion in this link below:
RVC WebUI; Download the prezipped model and put to your RVC Project.; Model test: Google Colab / RVC Models New (Which is basically the same but hosted on spaces); Model Created by ArkanDash 
The voice that was used in this model belongs to Hoyoverse.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Current Arknights RVC Models. Will be more later.; List:; Goldenglow (pink korone for lyfe(4m) <3) (RVC v1/v2); W; Angelina (v2) (s/o to henerum for introducing me to arknights yo),,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Use https://github.com/BlinkDL/ChatRWKV/tree/main/music to run current v1 MIDI model; Training data: https://huggingface.co/datasets/breadlicker45/bread-midi-dataset; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"These are RWKV-4-Pile models finetuned on novels.; Currently I am doing it for Chn novels. More languages to come.; Use https://github.com/BlinkDL/ChatRWKV to run them.; See https://github.com/BlinkDL/RWKV-LM for details on the RWKV Language Model (100% RNN).; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"[UPDATE: Try RWKV-4-World (https://huggingface.co/BlinkDL/rwkv-4-world) for generation & chat & code in 100+ world languages, with great English zero-shot & in-context learning ability too.]; These are RWKV-4-Pile 1.5/3/7/14B models finetuned on Alpaca, CodeAlpaca, Guanaco, GPT4All, ShareGPT and more. Even the 1.5B model is surprisingly good for its size.; Gradio Demo: https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B and https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio; RWKV models inference: https://github.com/BlinkDL/ChatRWKV (fast CUDA).; Q8_0 models: only for https://github.com/saharNooby/rwkv.cpp (fast CPU).",,,,,,,,,,,,,,,,,,,,,,,,,usage 10
"RWKV-4 trained on 100+ world languages (70% English, 15% multilang, 15% code).; World = Some_Pile + Some_RedPajama + Some_OSCAR + All_Wikipedia + All_ChatGPT_Data_I_can_find; XXXtuned = finetune of World on MC4, OSCAR, wiki, etc.; How to use:; The differences between World & Raven:",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"RWKV-4-World??Hugface????????¡ã?World??tokenizer????Raven\Pile?·Ú?§ß??£?????????????¡ã?HF????
ringrwkv?????????rwkv???transformers??rwkv??????????World?·Ú?????¨¹????????1.5B??3B??7B???§µ???????????HF??RWKV??
Forward RWKVOutput??????????????????????last_hidden_state??????????????????????????; RingRWKV GIT????????https://github.com/StarRing2022/RingRWKV ; import torch
from ringrwkv.configuration_rwkv_world import RwkvConfig
from ringrwkv.rwkv_tokenizer import TRIE_TOKENIZER
from ringrwkv.modehf_world import RwkvForCausalLM; model = RwkvForCausalLM.from_pretrained(""StarRing2022/RWKV-4-World-7B"") #????????????????????? 
tokenizer = TRIE_TOKENIZER('./ringrwkv/rwkv_vocab_v20230424.txt'); text = ""??????????""",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The new released model of https://github.com/OpenTalker/SadTalker.; The file of https://huggingface.co/vinthony/SadTalker-V002rc/blob/main/epoch_00190_iteration_000400000_checkpoint.pt comes from https://github.com/RenYurui/PIRender.; Thanks for their wonderful work!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
Based on LLaMA 13B.; llama.cpp version: link; Colab: link; Training code: link; Examples:,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
Llama.cpp compatible version of an original 30B model.; How to run:; System requirements:; Inference API has been turned off for this model.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Based on LLaMA 7B.; Colab: link; llama.cpp version: link; Training code: link; Examples:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Based on LLaMA-2 7B HF.; Training code: link; WARNING 1: Run with the development version of transformers and peft!
WARNING 2: Avoid using V100 (in Colab, for example). Outputs are much worse in this case.; Examples:; v1:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"?????????½¤???`?????????`??I????????VAE???i?????
          Model with built-in VAE for both background and character quality
        ; Twiter: @min__san
mail: (natsusakiyomi@mail.ru)",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; Play with the model on the SantaCoder Space Demo.; The SantaCoder models are a series of 1.1B parameter models trained on the Python, Java, and JavaScript subset of The Stack (v1.1) (which excluded opt-out requests). 
The main model uses Multi Query Attention, a context window of 2048 tokens, and was trained using near-deduplication and comment-to-code ratio as filtering criteria and using the Fill-in-the-Middle objective.
In addition there are several models that were trained on datasets with different filter parameters and with architecture and objective variations. ; The final model is the best performing model and was trained twice as long (236B tokens) as the others. This checkpoint is the default model and available on the main branch. All other checkpoints are on separate branches with according names.; The model was trained on GitHub code. As such it is not an instruction model and commands like ""Write a function that computes the square root."" do not work well.
You should phrase commands like they occur in source code such as comments (e.g. # the following function computes the sqrt) or write a function signature and docstring and let the model complete the function body.",,,,,,,,,,,,,,,,,,,,,,,,,usage 39
"; Play with the model on the SantaCoder Space Demo.; This is the Megatron-version of SantaCoder.
We refer the reader to the SantaCoder model page for full documentation about this model; There are two versions (branches) of the model:; The model was trained on GitHub code. As such it is not an instruction model and commands like ""Write a function that computes the square root."" do not work well.
You should phrase commands like they occur in source code such as comments (e.g. # the following function computes the sqrt) or write a function signature and docstring and let the model complete the function body.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"language: multilingual; tags:; datasets:; [news] A cross-lingual extension of SapBERT will appear in the main onference of ACL 2021! 
[news] SapBERT will appear in the conference proceedings of NAACL 2021!; SapBERT (Liu et al. 2020) trained with UMLS 2020AB, using xlm-roberta-base as the base model. Please use [CLS] as the representation of the input.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"LECO??LoRA?????????????

????????p???ð—???`LoRA(huge_breasts_woman/flat_chest_woman) 
LECO??????????{????????????p???ð—???`:????`??`?? woman
breasts?????????????
???????????LittleStepMix_A?????¨À?



?????????`?????`LoRA(pastel_hair_full/pastel_hair_A/pastel_hair_B) 
LECO??????????{?????????????????LoRA:????`??`?? hair
?L????????????????????????????????????????????????????¦«A????????????
???negative?? (black hair,brown hair:1.5) ????????X
LittleStepMix_A????????????????????????¦³?????????????????????
full?????????????????????{??????????A?????????????v????B??full??????????????????????
; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the sentence embedding model pre-trained by UER-py, which is introduced in this paper.; ChineseTextualInference is used as training data. ; The model is fine-tuned by UER-py on Tencent Cloud. We fine-tune five epochs with a sequence length of 128 on the basis of the pre-trained model chinese_roberta_L-12_H-768. At the end of each epoch, the model is saved when the best performance on development set is achieved.; Finally, we convert the pre-trained model into Huggingface's format:",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
???????? bert-base-chinese ?·Ú BERT ?????????????????????? SimCLUE ????????????????????????????????§¹????????????????????????????????¨¢?; ????????????????·Ú?????????????; ???  sentence-transformers ???????????????????§Ñ????; ???????????????????????????????????????????????; ??????????   sentence-transformers ????????????? HuggingFace Transformers ???????????????????????????,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
??????????????????????????????·Ú????4?? BERT??????????????????????????§¹????????????????????????????????????????????; ???????????????????????????????????????????§á???????????????????????????????????????????????????????????????????????????????????? 12 ?? BERT ????? 4 ?????????????§³?? 44%????? latency ????throughput ????????????? 6% ??????????????????????§³?????; ???  sentence-transformers ???????????????????§Ñ????; ???????????????????????????????????????????????; ??????????   sentence-transformers ????????????? HuggingFace Transformers ???????????????????????????,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The model is described in this articleFor better quality, use mean token embeddings.; You can use the model directly from the model repository to compute sentence embeddings:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository contains a finetuned SciBERT model that can extract references to scientific literature from patents.; See https://github.com/kaesve/patent-citation-extraction and https://arxiv.org/abs/2101.01039 for more information.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Scientific Abstract Simplification (SAS) is a tool designed to rewrite complex scientific abstracts into simpler, more comprehensible versions. Our objective is to make scientific knowledge universally accessible. If you have already experimented with our baseline model (sas_baseline), you will find that the current model surpasses its predecessor in terms of all evaluation metrics. Feel free to test it via the Hosted Inference API to your right. Simply select one of the provided examples or input your own scientific abstract. Just ensure to precede your text with the instruction, ""summarize, simplify, and contextualize: "", followed by a space. For local usage, refer to the Usage section.""; Open science has significantly reduced barriers to accessing scientific papers.
However, attainable research does not entail accessible knowledge.
Consequently, many individuals might prefer to rely on succinct social media narratives rather than endeavour to comprehend a scientific paper.
This preference is understandable as humans often favor narratives over dry, technical information. 
So, why not ""translate"" these intricate scientific abstracts into simpler, more accessible narratives? 
Several prestigious journals have already initiated steps towards enhancing accessibility. 
For instance, PNAS requires authors to submit Significance Statements understandable to an 'undergraduate-educated scientist', while Science includes an editor's abstract to provide a swift overview of the paper's salient points.; In this project, our objective is to employ AI to rewrite scientific abstracts into easily understandable scientific narratives.
To facilitate this, we have curated two new datasets: one containing PNAS abstract-significance pairs and the other encapsulating editor abstracts from Science.
We utilize a Transformer model (a variant known as Flan-T5) to fine-tune our model for the task of simplifying scientific abstracts.
Initially, the model is fine-tuned utilizing multiple discrete instructions by amalgamating four pertinent tasks in a challenge-proportional manner (a strategy we refer to as Multi-Instruction Pretuning).
Subsequently, we continue the fine-tuning process exclusively with the abstract-significance corpus. Our model can generate lay summaries that outperform models fine-tuned solely with the abstract-significance corpus and models fine-tuned with traditional task combinations.
We hope our work can foster a more comprehensive understanding of scientific research, enabling a larger audience to benefit from open science.; Use the code below to get started with the model. Remember to prepend the INSTRUCTION for best performance.; We finetuned the base model (flan-t5-large) on multiple relevant tasks with standard language modeling loss. During training, the source text of each task is prepended with an task-specific instruction and mapped to the corresponding target text. For example, ""simplify: "" is added before a wiki text, and the whole text is fed into the model to line up with the corresponding simple wiki text. The tuning process has two steps.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Paper: SciFive: a text-to-text transformer model for biomedical literature; Authors: Long N. Phan, James T. Anibal, Hieu Tran, Shaurya Chanana, Erol Bahadroglu, Alec Peltekian, Gr??goire Altan-Bonnet; For more details, do check out our Github repo.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"ControlNet is a neural network structure to control diffusion models by adding extra conditions. 
This checkpoint corresponds to the ControlNet conditioned on Canny edges.; It can be used in combination with Stable Diffusion.; ; Developed by: Lvmin Zhang, Maneesh Agrawala; Model type: Diffusion-based text-to-image generation model",,,,,,,,,,,,,,,,,,,,,,,,,usage 138
"ControlNet is a neural network structure to control diffusion models by adding extra conditions. 
This checkpoint corresponds to the ControlNet conditioned on HED Boundary.; It can be used in combination with Stable Diffusion.; ; Developed by: Lvmin Zhang, Maneesh Agrawala; Model type: Diffusion-based text-to-image generation model",,,,,,,,,,,,,,,,,,,,,,,,,usage 98
"? V2 model released, and blurriness issues fixed! ?; ?? Image Variations is now natively supported in ? Diffusers! ??; ; This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of ""image variations"" similar to DALLE-2 using Stable Diffusion. This version of the weights has been ported to huggingface Diffusers, to use this with the Diffusers library requires the Lambda Diffusers repo.; This model was trained in two stages and longer than the original variations model and gives better image quality and better CLIP rated similarity compared to the original version",,,,,,,,,,,,,,,,,,,,,,,,,usage 113
"for Stable Diffusion Webui Automatic1111
type: .safetensors(ckpt)
CFG Scale: middle-low; example.
low quality, worst quality, bad anatomy, bad proportions; UniPC, Dpm++ (2M/SDE) Karras, DDIM
Steps: 10??24; vae-ft-mse-840000-ema-pruned; -Mixed 5000+images",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"SD-Silicon: A series of general-purpose models based off the experimental automerger, autoMBW.; A collaborative creation of Xerxemi#6423 & Xynon#7407.; ; All models listed have baked WD1.3 VAE. However, for the purposes of this model series, external VAE is also recommended. ; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage. The CreativeML OpenRAIL License specifies:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"These weights are intended to be used with the ? diffusers library. If you are looking for the model to use with the original CompVis Stable Diffusion codebase, come here. ; You can integrate this fine-tuned VAE decoder to your existing diffusers workflows, by including a vae argument to the StableDiffusionPipeline; We publish two kl-f8 autoencoder versions, finetuned from the original kl-f8 autoencoder on a 1:1 ratio of LAION-Aesthetics and LAION-Humans, an unreleased subset containing only SFW images of humans. The intent was to fine-tune on the Stable Diffusion training set (the autoencoder was originally trained on OpenImages) but also enrich the dataset with images of humans to improve the reconstruction of faces.
The first, ft-EMA, was resumed from the original checkpoint, trained for 313198 steps and uses EMA weights. It uses the same loss configuration as the original checkpoint (L1 + LPIPS).
The second, ft-MSE, was resumed from ft-EMA and uses EMA weights and was trained for another 280k steps using a different loss, with more emphasis 
on MSE reconstruction (MSE + 0.1 * LPIPS). It produces somewhat ``smoother'' outputs. The batch size for both versions was 192 (16 A100s, batch size 12 per GPU).
To keep compatibility with existing models, only the decoder part was finetuned; the checkpoints can be used as a drop-in replacement for the existing autoencoder.; Original kl-f8 VAE vs f8-ft-EMA vs f8-ft-MSE; Visualization of reconstructions on  256x256 images from the COCO2017 validation dataset.",,,,,,,,,,,,,,,,,,,,,,,,,usage 10
"These weights are intended to be used with the ? diffusers library. If you are looking for the model to use with the original CompVis Stable Diffusion codebase, come here.; You can integrate this fine-tuned VAE decoder to your existing diffusers workflows, by including a vae argument to the StableDiffusionPipeline; We publish two kl-f8 autoencoder versions, finetuned from the original kl-f8 autoencoder on a 1:1 ratio of LAION-Aesthetics and LAION-Humans, an unreleased subset containing only SFW images of humans. The intent was to fine-tune on the Stable Diffusion training set (the autoencoder was originally trained on OpenImages) but also enrich the dataset with images of humans to improve the reconstruction of faces.
The first, ft-EMA, was resumed from the original checkpoint, trained for 313198 steps and uses EMA weights. It uses the same loss configuration as the original checkpoint (L1 + LPIPS).
The second, ft-MSE, was resumed from ft-EMA and uses EMA weights and was trained for another 280k steps using a different loss, with more emphasis 
on MSE reconstruction (MSE + 0.1 * LPIPS). It produces somewhat ``smoother'' outputs. The batch size for both versions was 192 (16 A100s, batch size 12 per GPU).
To keep compatibility with existing models, only the decoder part was finetuned; the checkpoints can be used as a drop-in replacement for the existing autoencoder.; Original kl-f8 VAE vs f8-ft-EMA vs f8-ft-MSE; Visualization of reconstructions on  256x256 images from the COCO2017 validation dataset.",,,,,,,,,,,,,,,,,,,,,,,,,usage 23
"These weights are intended to be used with the original CompVis Stable Diffusion codebase. If you are looking for the model to use with the ? diffusers library, come here.; We publish two kl-f8 autoencoder versions, finetuned from the original kl-f8 autoencoder on a 1:1 ratio of LAION-Aesthetics and LAION-Humans, an unreleased subset containing only SFW images of humans. The intent was to fine-tune on the Stable Diffusion training set (the autoencoder was originally trained on OpenImages) but also enrich the dataset with images of humans to improve the reconstruction of faces.
The first, ft-EMA, was resumed from the original checkpoint, trained for 313198 steps and uses EMA weights. It uses the same loss configuration as the original checkpoint (L1 + LPIPS).
The second, ft-MSE, was resumed from ft-EMA and uses EMA weights and was trained for another 280k steps using a different loss, with more emphasis 
on MSE reconstruction (MSE + 0.1 * LPIPS). It produces somewhat ``smoother'' outputs. The batch size for both versions was 192 (16 A100s, batch size 12 per GPU).
To keep compatibility with existing models, only the decoder part was finetuned; the checkpoints can be used as a drop-in replacement for the existing autoencoder..; Original kl-f8 VAE vs f8-ft-EMA vs f8-ft-MSE; Visualization of reconstructions on  256x256 images from the COCO2017 validation dataset. ; 


256x256: ft-EMA (left), ft-MSE (middle), original (right)",,,,,,,,,,,,,,,,,,,,,,,,,usage 19
"civitai??????????; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repo is a tutorial intended to help beginners use the new released model, stable-diffusion-xl-0.9 in ComfyUI, with both the base and refiner models together to achieve a magnificent quality of image generation.; With usable demo interfaces for ComfyUI to use the models (see below)!; Here is a full tutorial to use stable-diffusion-xl-0.9 FROM ZERO!; When you have loaded into the ComfyUI, it will display this default interface shown below:
; Then, find out the ""load"" button on a floating panel, click, and choose the "".json"" file download from THIS REPO, it will quickly load a friendly interface to use the stable-diffusion-xl-0.9 series.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"You can integrate this fine-tuned VAE decoder to your existing diffusers workflows, by including a vae argument to the StableDiffusionPipeline; SDXL is a latent diffusion model, where the diffusion operates in a pretrained, 
learned (and fixed) latent space of an autoencoder. 
While the bulk of the semantic composition is done by the latent diffusion model, 
we can improve local, high-frequency details in generated images by improving the quality of the autoencoder. 
To this end, we train the same autoencoder architecture used for the original Stable Diffusion at a larger batch-size (256 vs 9) 
and additionally track the weights with an exponential moving average (EMA). 
The resulting autoencoder outperforms the original model in all evaluated reconstruction metrics, see the table below.; SDXL-VAE vs original kl-f8 VAE vs f8-ft-MSE; Inference API has been turned off for this model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"SDXL-VAE-FP16-Fix is the SDXL VAE, but modified to run in fp16 precision without generating NaNs.; Just load this checkpoint via AutoencoderKL:; ; SDXL-VAE generates NaNs in fp16 because the internal activation values are too big:
; SDXL-VAE-FP16-Fix was created by finetuning the SDXL-VAE to:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"SegFormer model fine-tuned on ATR dataset for clothes segmentation.
The dataset on hugging face is called ""mattmdjaga/human_parsing_dataset"".; Labels: 0: ""Background"", 1: ""Hat"", 2: ""Hair"", 3: ""Sunglasses"", 4: ""Upper-clothes"", 5: ""Skirt"", 6: ""Pants"", 7: ""Dress"", 8: ""Belt"", 9: ""Left-shoe"", 10: ""Right-shoe"", 11: ""Face"", 12: ""Left-leg"", 13: ""Right-leg"", 14: ""Left-arm"", 15: ""Right-arm"", 16: ""Bag"", 17: ""Scarf""; The license for this model can be found here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. ; Disclaimer: The team releasing SegFormer did not write a model card for this model so this model card has been written by the Hugging Face team.; SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset.; You can use the raw model for semantic segmentation. See the model hub to look for fine-tuned versions on a task that interests you.; Here is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"NEW Segment Anything now officially supported in transformers! Check out the official documentation; This repository is the mirror of the official Segment Anything repository, together with the model weights. We also provide instructions on how to easily download the model weights.; Meta AI Research, FAIR; Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alex Berg, Wan-Yen Lo, Piotr Dollar, Ross Girshick; [Paper] [Project] [Demo] [Dataset] [Blog] [BibTeX]",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; The collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Paper | Demo | Blog post;",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The aim of this study is automatic semantic segmentation and measurement total length of teeth in one-shot panoramic x-ray image by using deep learning method with U-Net Model and binary image analysis in order to provide diagnostic information for the management of dental disorders, diseases, and conditions. ;  Github Link; Original Dataset; DATASET ref - 	H. Abdi, S. Kasaei, and M. Mehdizadeh, ??Automatic segmentation of mandible in panoramic x-ray,?? J. Med. Imaging, vol. 2, no. 4, p. 44003, 2015; Link DATASET for only original images.",,,,,,,,,,,,,,,,,,,,,,,,,usage 23
"This is a sentence-transformers model: It maps Swedish sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. This model is a bilingual Swedish-English model trained according to instructions in the paper Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation and the documentation accompanying its companion python package. We have used the strongest available pretrained English Bi-Encoder (all-mpnet-base-v2) as a teacher model, and the pretrained Swedish KB-BERT as the student model. ; A more detailed description of the model can be found in an article we published on the KBLab blog here and for the updated model here. ; Update: We have released updated versions of the model since the initial release. The original model described in the blog post is v1.0. The current version is v2.0. The newer versions are trained on longer paragraphs, and have a longer max sequence length. v2.0 is trained with a stronger teacher model and is the current default.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Model is Fine-tuned using pre-trained facebook/camembert-base and
Siamese BERT-Networks with 'sentences-transformers' on dataset stsb; The model can be used directly (without a language model) as follows:; The model can be evaluated as follows on the French test data of stsb.; Test Result: 
The performance is measured using Pearson and Spearman correlation:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Sentence-CamemBERT-Large is the Embedding Model for French developed by La Javaness. The purpose of this embedding model is to represent the content and semantics of a French sentence in a mathematical vector which allows it to understand the meaning of the text-beyond individual words in queries and documents, offering a powerful semantic search.; The model is Fine-tuned using pre-trained facebook/camembert-large and
Siamese BERT-Networks with 'sentences-transformers' on dataset stsb; The model can be used directly (without a language model) as follows:; The model can be evaluated as follows on the French test data of stsb.; Test Result: 
The performance is measured using Pearson and Spearman correlation:",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.; For an automated evaluation of this model, see the Sentence Embeddings Benchmark: https://seb.sbert.net",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
This is a the sentence-transformers version of the intfloat/multilingual-e5-base model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:; You can use the embaas API to encode your input. Get your free API key from embaas.io; You can find the MTEB results here.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for
further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text-classification.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in
this paper and first released in
this repository. This model is uncased: it does not make a difference
between english and English.; BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means it
was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of
publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language that can then be used to extract features
useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard
classifier using the features produced by the BERT model as inputs.; This is a fine-tuned downstream version of the bert-base-uncased model for sentiment analysis, this model is not intended for
further downstream fine-tuning for any other tasks. This model is trained on a classified dataset for text-classification.",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This model (""SiEBERT"", prefix for ""Sentiment in English"") is a fine-tuned checkpoint of RoBERTa-large (Liu et al. 2019). It enables reliable binary sentiment analysis for various types of English-language text. For each instance, it predicts either positive (1) or negative (0) sentiment. The model was fine-tuned and evaluated on 15 data sets from diverse text sources to enhance generalization across different types of texts (reviews, tweets, etc.). Consequently, it outperforms models trained on only one type of text (e.g., movie reviews from the popular SST-2 benchmark) when used on new data as shown below. ; If you want to predict sentiment for your own data, we provide an example script via Google Colab. You can load your data to a Google Drive and run the script for free on a Colab GPU. Set-up only takes a few minutes. We suggest that you manually label a subset of your data to evaluate performance for your use case. For performance benchmark values across various sentiment analysis contexts, please refer to our paper (Hartmann et al. 2022).; ; The easiest way to use the model for single predictions is Hugging Face's sentiment analysis pipeline, which only needs a couple lines of code as shown in the following example:;",,,,,,,,,,,,,,,,,,,,,,,,,usage 22
You can use cURL to access this model:; Or Python API:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository provides all the necessary tools to perform speech enhancement (denoising) with a SepFormer model, implemented with SpeechBrain, and pretrained on WHAM! dataset with 8k sampling frequency, which is basically a version of WSJ0-Mix dataset with environmental noise and reverberation in 8k. For a better experience we encourage you to learn more about SpeechBrain. The given model performance is 14.35 dB SI-SNR on the test set of WHAM! dataset.; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about SpeechBrain.; To perform inference on the GPU, add  run_opts={""device"":""cuda""}  when calling the from_hparams method.; The training script is currently being worked on an ongoing pull-request.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository provides all the necessary tools to perform audio source separation with a SepFormer 
model, implemented with SpeechBrain, and pretrained on WSJ0-2Mix dataset. For a better experience we encourage you to learn more about
SpeechBrain. The model performance is 22.4 dB on the test set of WSJ0-2Mix dataset.; You can listen to example results obtained on the test set of WSJ0-2/3Mix through here. ; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; The system expects input recordings sampled at 8kHz (single channel).
If your signal has a different sample rate, resample it (e.g, using torchaudio or sox) before using the interface.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"For usage instructions, refer to our codebase: https://github.com/Muennighoff/sgpt ; For eval results, refer to our paper: https://arxiv.org/abs/2202.08904; The model was trained with the parameters:; DataLoader:; torch.utils.data.dataloader.DataLoader of length 249592 with parameters:",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in Shap-E: Generating Conditional 3D Implicit Functions by Heewoo Jun and Alex Nichol from OpenAI. ; Original repository of Shap-E can be found here: https://github.com/openai/shap-e. ; The authors of Shap-E didn't author this model card. They provide a separate model card here.; The abstract of the Shap-E paper:; We present Shap-E, a conditional generative model for 3D assets. Unlike recent work on 3D generative models which produce a single output representation, Shap-E directly generates the parameters of implicit functions that can be rendered as both textured meshes and neural radiance fields. We train Shap-E in two stages: first, we train an encoder that deterministically maps 3D assets into the parameters of an implicit function; second, we train a conditional diffusion model on outputs of the encoder. When trained on a large dataset of paired 3D and text data, our resulting models are capable of generating complex and diverse 3D assets in a matter of seconds. When compared to Point-E, an explicit generative model over point clouds, Shap-E converges faster and reaches comparable or better sample quality despite modeling a higher-dimensional, multi-representation output space. We release model weights, inference code, and samples at this https URL.",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"=> ShiratakiMix-add-VAE.safetensors; Negative:; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage. The CreativeML OpenRAIL License specifies:; You can't use the model to deliberately produce nor share illegal or harmful outputs or content; The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
FRED-T5 ??????????? ??? SiberianDataset. ???????? ?????? ????????? ?? ????????????? ?? ?????? ???????? ?? ????? ???????? ????? ?????????. ; ???????? ???????????:; ?? ???????? ?????? ??????????? ?????? ?????????.; ?????-????:; ????????????:,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"SingBert Large - Bert for Singlish (SG) and Manglish (MY).; Similar to SingBert but the large version, which was initialized from BERT large uncased (whole word masking), with pre-training finetuned on
singlish and manglish data.; Here is how to use this model to get the features of a given text in PyTorch:; and in TensorFlow:; This model was finetuned on colloquial Singlish and Manglish corpus, hence it is best applied on downstream tasks involving the main
constituent languages- english, mandarin, malay. Also, as the training data is mainly from forums, beware of existing inherent bias.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This Stable Diffusion model was fine-tuned to generate a pre-version 1.8 Minecraft character skins, based on a text prompt.; The model was fine-tuned on the dataset for 13,000 steps using the 'train_text_to_image.py' script provided with the diffusers library.  A checkpoint has been included in the 'checkpoint' directory.; Some postprocessing is required to import and use the generated skins in Minecraft.; This model is a fork from monadicial/minecraft-skin-generator. This fork will contain the production model for a frontend that processing the results so the output can be used; Here are some example text prompts and the images they generate:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"SkyPaint is a Chinese-English bilingual text-generated image project developed by Singularity-AI. It is still being updated and optimized.; The SkyPaint text generation image model is mainly composed of two parts, namely the prompt word text encoder model and the diffusion model. Therefore, our optimization is also divided into two steps. First, based on OpenAI-CLIP, we optimized the prompt word text encoder model to make SkyPaint have the ability to recognize Chinese and English, and then optimized the diffusion model, so that SkyPaint has modern artistic capabilities and can produce high-quality pictures.; Chinese and English mixed prompt word input.
Generating high-quality images in a modern art style.
English prompt words for stable_diffusion_1.x official model and related fine-tuning models.
Retain usage habits and methods of stable_diffusion prompt words.
Introduction to SkyCLIP Models
SkyCLIP is a CLIP model obtained by using an efficient method of training Chinese-English bilingual CLIP models. This method only needs to use text data to achieve efficient distillation of the OpenAI-CLIP model, which greatly reduces the data threshold. At the same time, training requires Compared with the original CLIP model, the computing power requirement is reduced by more than 90%, which is convenient for the open source community to reproduce/fine-tune. This method only changes the text encoder of OpenAI-CLIP, and can be used with the image encoder of OpenAI-CLIP to realize the image-text retrieval function.; ??§Ö??
; ??? ?? ??? ????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"???????????????????????¦Ì???????????{???????ð—???`LoRA?????
???e?????????`?????OppaiSliderPack.zip?????????`????????????????????; ; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Smaller Language-agnostic BERT Sentence Encoder (LaBSE) is a BERT-based model distilled from the original LaBSE model to 15 languages (from the original 109 languages) using the techniques described in the paper 'Load What You Need: Smaller Versions of Multilingual BERT' by Ukjae Jeong.; Using the model:; To get the sentence embeddings, use the pooler output:; Output for other languages:; For similarity between sentences, an L2-norm is recommended before calculating the similarity:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"A Siamese network model trained for zero-shot and few-shot text classification.; The base model is xlm-roberta-base.
It was trained on SNLI, MNLI, ANLI and XNLI.; This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space.; Using this model becomes easy when you have sentence-transformers installed:; Then you can use the model like this:",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"This is a collection of so-vits-svc-4.0 models made by the Pony Preservation Project using audio clips taken from MLP:FiM.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"??????????G_${name}_${Epoch}epoch.pth.; ??????, ?????????; ??so-vits-svc-4.0???????? ????v2; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Trained on a flavorful melange of the WizardLM, Airoboros, and Wizard Vicuna datasets.
This model was trained using both linear and NTK-aware RoPE scaling in tandem. When loading, ensure that compress_pos_emb (or scale) is set to 2, and alpha_value is set to 4. Both values must be set.; Expect context length of up to 8192 to work for sure. It will probably maintain coherence into the ~12k range, but I have not tested that.; Prompt format is vicuna 1.1:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"SoulChat ? | 
 ? BianQue? |; ??????????????????????????????????????????????????????????????????????¦Ä????????-???????????????????????????????????????????????????????ProactiveHealthGPT????????; ????????????????????????????????ProactiveHealthGPT ???????????????????????????????????????????????????§à?????¨¢??????? ??????????????????SoulChat?? ??;    ???????????????????????????????????????????????????????????????§ß???????????????????????????????????????????????????????????https://github.com/scutcyr/SoulChat/blob/main/figure/single_turn.png??????????????????????????????????????????????????§µ????????????????????????¦Å??????????¨´?????§µ????????????????????????????????????ï…???»Ç??????????????????????????????????????????;    ?????????????????????????????????????—¨?????????15?????? ??????????????????????SoulChatCorpus-single_turn?? ?????????????50?????????????????????????????? PsyQA ??6.7????????????ChatGPT??GPT4??????????100????¦Å? ???????????SoulChatCorpus-multi_turn?? ??????????????????§Ù?????????????????????????????????????????§Ö??????????????????????????????????????????????????????????????????????????????????????????????????????????SoulChatCorpus-single_turn??SoulChatCorpus-multi_turn????????120????????? ??????????????????????SoulChatCorpus ????????????¨¢??????xxx\n??????????xxx\n?????xxx\n??????????????????????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a SpanMarker model that can be used for identifying verbs in text.
In particular, this SpanMarker model uses xlm-roberta-large as the underlying encoder.
See span_marker_verbs_train.ipynb for the training script used to create this model.; Note that this model is an experiment about the feasibility of SpanMarker as a POS tagger. I would generally recommend using spaCy or NLTK instead, as these are more computationally efficient approaches.; To use this model for inference, first install the span_marker library:; You can then run inference with this model like so:; See the SpanMarker repository for documentation and additional information on this library.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; The collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Relies on pyannote.audio 2.1.1: see installation instructions.; In case the number of speakers is known in advance, one can use the num_speakers option:",,,,,,,,,,,,,,,,,,,,,,,,,usage 48
"SPECTER is a pre-trained language model to generate document-level embedding of documents. It is pre-trained on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. ; If you're coming here because you want to embed papers, SPECTER has now been superceded by SPECTER 2.0. Use that instead.; Paper: SPECTER: Document-level Representation Learning using Citation-informed Transformers; Original Repo: Github; Evaluation Benchmark: SciDocs",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"SPECTER 2.0 is the successor to SPECTER and is capable of generating task specific embeddings for scientific tasks when paired with adapters.
Given the combination of title and abstract of a scientific paper or a short texual query, the model can be used to generate effective embeddings to be used in downstream applications.; Note:For general embedding purposes, please use allenai/specter2_proximity.; To get the best performance on a downstream task type please load the associated adapter with the base model as in the example below.; SPECTER 2.0 has been trained on over 6M triplets of scientific paper citations, which are available here.
Post that it is trained with additionally attached task format specific adapter modules on all the SciRepEval training tasks.; Task Formats trained on:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"SpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS.; This model was introduced in SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.; SpeechT5 was first released in this repository, original weights. The license used is MIT.; Motivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder.; Leveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder.",,,,,,,,,,,,,,,,,,,,,,,,,usage 172
"This repository provides all the necessary tools to perform speaker verification with a pretrained ECAPA-TDNN model using SpeechBrain. 
The system can be used to extract speaker embeddings as well. 
It is trained on Voxceleb 1+ Voxceleb2 training data. ; For a better experience, we encourage you to learn more about
SpeechBrain. The model performance on Voxceleb1-test set(Cleaned) is:; This system is composed of an ECAPA-TDNN model. It is a combination of convolutional and residual blocks. The embeddings are extracted using attentive statistical pooling. The system is trained with Additive Margin Softmax Loss.  Speaker Verification is performed using cosine distance between speaker embeddings.; First of all, please install SpeechBrain with the following command:; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.",,,,,,,,,,,,,,,,,,,,,,,,,usage 57
"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
This model card gives an overview of all available model checkpoints. For more in-detail model cards, please have a look at the model repositories listed under Model Access.; For the first version 4 model checkpoints are released.
Higher versions have been trained for longer and are thus usually better in terms of image generation quality then lower versions. More specifically: ; Each checkpoint can be used both with Hugging Face's  ? Diffusers library or the original Stable Diffusion GitHub repository. Note that you have to ""click-request"" them on each respective model repository.; To quickly try out the model, you can try out the Stable Diffusion Space.; The CreativeML OpenRAIL M license is an Open RAIL M license, adapted from the work that BigScience and the RAIL Initiative are jointly carrying in the area of responsible AI licensing. See also the article about the BLOOM Open RAIL license on which our license is based.",,,,,,,,,,,,,,,,,,,,,,,,,usage 71
"This model card focuses on the model associated with the Stable Diffusion v2 model, available here.; This stable-diffusion-2 model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and trained for 150k steps using a v-objective on the same dataset. Resumed for another 140k steps on 768x768 images.; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model",,,,,,,,,,,,,,,,,,,,,,,,,usage 478
"This model card focuses on the model associated with the Stable Diffusion v2-1 model, codebase available here.; This stable-diffusion-2-1 model is fine-tuned from stable-diffusion-2 (768-v-ema.ckpt) with an additional 55k steps on the same dataset (with punsafe=0.1), and then fine-tuned for another 155k extra steps with punsafe=0.98.; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model; Language(s): English",,,,,,,,,,,,,,,,,,,,,,,,,usage 796
"This model card focuses on the model associated with the Stable Diffusion v2-1-base model.; This stable-diffusion-2-1-base model fine-tunes stable-diffusion-2-base (512-base-ema.ckpt) with 220k extra steps taken, with punsafe=0.98 on the same dataset. ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model; Language(s): English",,,,,,,,,,,,,,,,,,,,,,,,,usage 231
"This model card focuses on the model associated with the Stable Diffusion v2-1 model, codebase available here.; This stable-diffusion-2-1-unclip is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model; Language(s): English",,,,,,,,,,,,,,,,,,,,,,,,,usage 29
"This model card focuses on the model associated with the Stable Diffusion v2 model, available here.; This stable-diffusion-2-depth model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and finetuned for 200k steps. Added an extra input channel to process the (relative) depth prediction produced by MiDaS (dpt_hybrid) which is used as an additional conditioning.; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"This model card focuses on the model associated with the Stable Diffusion v2, available here.; This stable-diffusion-2-inpainting model is resumed from stable-diffusion-2-base (512-base-ema.ckpt) and trained for another 200k steps. Follows the mask-generation strategy presented in LAMA which, in combination with the latent VAE representations of the masked image, are used as an additional conditioning.; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model",,,,,,,,,,,,,,,,,,,,,,,,,usage 60
"? CLICK HERE TO OPEN THIS DOCUMENT IN FULL WIDTH(The index won't work otherwise).; ???? HAZ CLICK AQU?? PARA VER ESTA GU??A EN ESPA?OL; ?; ?; Stable Diffusion is a very powerful AI image generation software you can run on your own home computer. It uses ""models"" which function like the brain of the AI, and can make almost anything, given that someone has trained it to do it. The biggest uses are anime art, photorealism, and NSFW content.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.; The Stable-Diffusion-Inpainting was initialized with the weights of the Stable-Diffusion-v-1-2. First 595k steps regular training, then 440k steps of inpainting training at resolution 512x512 on ??laion-aesthetics v2 5+?? and 10% dropping of the text-conditioning to improve classifier-free classifier-free guidance sampling. For inpainting, the UNet has 5 additional input channels (4 for the encoded masked-image and 1 for the mask itself) whose weights were zero-initialized after restoring the non-inpainting checkpoint. During training, we generate synthetic masks and in 25% mask everything.; You can use this both with the ?Diffusers library and the RunwayML GitHub repository.; How it works:; Developed by: Robin Rombach, Patrick Esser",,,,,,,,,,,,,,,,,,,,,,,,,usage 376
"this Stable diffusion model i have fine tuned on 1000 raw logo png/jpg images of of size 128x128 with augmentation ; Enjoy .create any type of logo; for examples:""Logo of a pirate"",""logo of a sunglass with girl"" or something complex like ""logo of a ice-cream with snake"" etc",,,,,,,,,,,,,,,,,,,,,,,,,usage 14
"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.; The Stable-Diffusion-v-1-4 checkpoint was initialized with the weights of the Stable-Diffusion-v-1-2 
checkpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on ""laion-aesthetics v2 5+"" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.; These weights are intended to be used with the original CompVis Stable Diffusion codebase. If you are looking for the model to use with the D?iffusers library, come here.; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model",,,,,,,,,,,,,,,,,,,,,,,,,usage 215
"Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.
For more information about how Stable Diffusion functions, please have a look at ?'s Stable Diffusion with ?Diffusers blog.; The Stable-Diffusion-v1-4 checkpoint was initialized with the weights of the Stable-Diffusion-v1-2 
checkpoint and subsequently fine-tuned on 225k steps at resolution 512x512 on ""laion-aesthetics v2 5+"" and 10% dropping of the text-conditioning to improve classifier-free guidance sampling.; This weights here are intended to be used with the ? Diffusers library. If you are looking for the weights to be loaded into the CompVis Stable Diffusion codebase, come here; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model",,,,,,,,,,,,,,,,,,,,,,,,,usage 862
"Stable Diffusion is a latent text-to-image diffusion model known for its ability to generate highly realistic images based on textual inputs. The Stable-Diffusion-v1-5 checkpoint was fine-tuned on ""laion-aesthetics v2 5+"" for 595k steps at 512x512 resolution. It builds upon Stable-Diffusion-v1-2, with 10% text-conditioning reduction for improved classifier-free guidance sampling, resulting in highly realistic text-to-image generation.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1546
"This model card focuses on the model associated with the Stable Diffusion Upscaler, available here.
This model is trained for 1.25M steps on a 10M subset of LAION containing images >2048x2048. The model was trained on crops of size 512x512 and is a text-guided latent upscaling diffusion model.
In addition to the textual input, it receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. ; ; Developed by: Robin Rombach, Patrick Esser; Model type: Diffusion-based text-to-image generation model; Language(s): English",,,,,,,,,,,,,,,,,,,,,,,,,usage 28
"SDXL 0.9 Research License Agreement; Copyright (c) Stability AI Ltd.; This License Agreement (as may be amended in accordance with this License Agreement, ??License??), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (??Licensee?? or ??you??) and Stability AI Ltd. (??Stability AI?? or ??we??) applies to your use of any computer program, algorithm, source code, object code, software, models, or model weights that is made available by Stability AI under this License (??Software??) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (??Documentation??). By using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the ??Software Products??), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; LICENSE GRANTa. Subject to your compliance with the Documentation and Sections 2, 3, and 5, Stability AI grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty free and limited license under Stability AI??s copyright interests to use, reproduce, and create derivative works of the Software solely for your non-commercial research purposes. The foregoing license is personal to you, and you may not assign, sublicense, distribute, publish, host, or otherwise make available this Software, derivative works of the Software, models or model weights associated with the Software, this License, or any other rights or obligations under this License without Stability AI??s prior written consent; any such assignment or sublicense without Stability AI??s prior written consent will be void and will automatically and immediately terminate this License.  For sake of clarity, this License does not grant to you the right or ability to extend any license to the Software, derivative works of the Software, or associated models or model weights to a non-Licensee, nor does this License permit you to create a new Licensee, such as by making available a copy of this License.  If you would like rights not granted by this License, you may seek permission by sending an email to legal@stability.ai.b. You may make a reasonable number of copies of the Documentation solely for your use in connection with the license to the Software granted above.c. The grant of rights expressly set forth in this Section 1 (License Grant) are the complete grant of rights to you in the Software Products, and no other licenses are granted, whether by waiver, estoppel, implication, equity or otherwise. Stability AI and its licensors reserve all rights not expressly granted by this License.; RESTRICTIONSYou will not, and will not permit, assist or cause any third party to:a. use, modify, copy, reproduce, create derivative works of, or distribute the Software Products (or any derivative works thereof, works incorporating the Software Products, or any data produced by the Software), in whole or in part, for (i) any commercial or production purposes, (ii) military purposes or in the service of nuclear technology, (iii) purposes of surveillance, including any research or development relating to surveillance, (iv) biometric processing, (v) in any manner that infringes, misappropriates, or otherwise violates any third-party rights, or (vi) in any manner that violates any applicable law and violating any privacy or security laws, rules, regulations, directives, or governmental requirements (including the General Data Privacy Regulation (Regulation (EU) 2016/679), the California Consumer Privacy Act, and any and all laws governing the processing of biometric information), as well as all amendments and successor laws to any of the foregoing;b. alter or remove copyright and other proprietary notices which appear on or in the Software Products;c. utilize any equipment, device, software, or other means to circumvent or remove any security or protection used by Stability AI in connection with the Software, or to circumvent or remove any usage restrictions, or to enable functionality disabled by Stability AI; ord. offer or impose any terms on the Software Products that alter, restrict, or are inconsistent with the terms of this License.e. 1) violate any applicable U.S. and non-U.S. export control and trade sanctions laws (??Export Laws??); 2) directly or indirectly export, re-export, provide, or otherwise transfer Software Products: (a) to any individual, entity, or country prohibited by Export Laws; (b) to anyone on U.S. or non-U.S. government restricted parties lists; or (c) for any purpose prohibited by Export Laws, including nuclear, chemical or biological weapons, or missile technology applications; 3) use or download Software Products if you or they are: (a) located in a comprehensively sanctioned jurisdiction, (b) currently listed on any U.S. or non-U.S. restricted parties list, or (c) for any purpose prohibited by Export Laws; and (4) will not disguise your location through IP proxying or other methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 49
"SDXL 0.9 Research License Agreement; Copyright (c) Stability AI Ltd.; This License Agreement (as may be amended in accordance with this License Agreement, ??License??), between you, or your employer or other entity (if you are entering into this agreement on behalf of your employer or other entity) (??Licensee?? or ??you??) and Stability AI Ltd. (??Stability AI?? or ??we??) applies to your use of any computer program, algorithm, source code, object code, software, models, or model weights that is made available by Stability AI under this License (??Software??) and any specifications, manuals, documentation, and other written information provided by Stability AI related to the Software (??Documentation??). By using the Software, you agree to the terms of this License. If you do not agree to this License, then you do not have any rights to use the Software or Documentation (collectively, the ??Software Products??), and you must immediately cease using the Software Products. If you are agreeing to be bound by the terms of this License on behalf of your employer or other entity, you represent and warrant to Stability AI that you have full legal authority to bind your employer or such entity to this License. If you do not have the requisite authority, you may not accept the License or access the Software Products on behalf of your employer or other entity.; LICENSE GRANTa. Subject to your compliance with the Documentation and Sections 2, 3, and 5, Stability AI grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty free and limited license under Stability AI??s copyright interests to use, reproduce, and create derivative works of the Software solely for your non-commercial research purposes. The foregoing license is personal to you, and you may not assign, sublicense, distribute, publish, host, or otherwise make available this Software, derivative works of the Software, models or model weights associated with the Software, this License, or any other rights or obligations under this License without Stability AI??s prior written consent; any such assignment or sublicense without Stability AI??s prior written consent will be void and will automatically and immediately terminate this License.  For sake of clarity, this License does not grant to you the right or ability to extend any license to the Software, derivative works of the Software, or associated models or model weights to a non-Licensee, nor does this License permit you to create a new Licensee, such as by making available a copy of this License.  If you would like rights not granted by this License, you may seek permission by sending an email to legal@stability.ai.b. You may make a reasonable number of copies of the Documentation solely for your use in connection with the license to the Software granted above.c. The grant of rights expressly set forth in this Section 1 (License Grant) are the complete grant of rights to you in the Software Products, and no other licenses are granted, whether by waiver, estoppel, implication, equity or otherwise. Stability AI and its licensors reserve all rights not expressly granted by this License.; RESTRICTIONSYou will not, and will not permit, assist or cause any third party to:a. use, modify, copy, reproduce, create derivative works of, or distribute the Software Products (or any derivative works thereof, works incorporating the Software Products, or any data produced by the Software), in whole or in part, for (i) any commercial or production purposes, (ii) military purposes or in the service of nuclear technology, (iii) purposes of surveillance, including any research or development relating to surveillance, (iv) biometric processing, (v) in any manner that infringes, misappropriates, or otherwise violates any third-party rights, or (vi) in any manner that violates any applicable law and violating any privacy or security laws, rules, regulations, directives, or governmental requirements (including the General Data Privacy Regulation (Regulation (EU) 2016/679), the California Consumer Privacy Act, and any and all laws governing the processing of biometric information), as well as all amendments and successor laws to any of the foregoing;b. alter or remove copyright and other proprietary notices which appear on or in the Software Products;c. utilize any equipment, device, software, or other means to circumvent or remove any security or protection used by Stability AI in connection with the Software, or to circumvent or remove any usage restrictions, or to enable functionality disabled by Stability AI; ord. offer or impose any terms on the Software Products that alter, restrict, or are inconsistent with the terms of this License.e. 1) violate any applicable U.S. and non-U.S. export control and trade sanctions laws (??Export Laws??); 2) directly or indirectly export, re-export, provide, or otherwise transfer Software Products: (a) to any individual, entity, or country prohibited by Export Laws; (b) to anyone on U.S. or non-U.S. government restricted parties lists; or (c) for any purpose prohibited by Export Laws, including nuclear, chemical or biological weapons, or missile technology applications; 3) use or download Software Products if you or they are: (a) located in a comprehensively sanctioned jurisdiction, (b) currently listed on any U.S. or non-U.S. restricted parties list, or (c) for any purpose prohibited by Export Laws; and (4) will not disguise your location through IP proxying or other methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 13
"StableVicuna-13B is a Vicuna-13B v0 model fine-tuned using reinforcement learning from human feedback (RLHF) via Proximal Policy Optimization (PPO) on various conversational and instructional datasets.; StableVicuna-13B cannot be used from the CarperAI/stable-vicuna-13b-delta weights alone. To obtain the correct model, one must add back the difference between LLaMA 13B and CarperAI/stable-vicuna-13b-delta weights. We provide the apply_delta.py script to automate the conversion, which you can run as:; Once the delta weights are applied, get started chatting with the model by using the transformers library. Following a suggestion from Vicuna Team with Vicuna v0 you should install transformers with this version:; StableVicuna-13B is fine-tuned on a mix of three datasets. OpenAssistant Conversations Dataset (OASST1), a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages;
GPT4All Prompt Generations, a dataset of 400k prompts and responses generated by GPT-4; and Alpaca,  a dataset of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine.; The reward model used during RLHF was also trained on OpenAssistant Conversations Dataset (OASST1) along with two other datasets: Anthropic HH-RLHF, a dataset of preferences about AI assistant helpfulness and harmlessness; and Stanford Human Preferences Dataset a dataset of 385K collective human preferences over responses to questions/instructions in 18 different subject areas, from cooking to legal advice.",,,,,,,,,,,,,,,,,,,,,,,,,usage 28
StableLM-Base-Alpha is a suite of 3B and 7B parameter decoder-only language models pre-trained on a diverse collection of English datasets with a sequence length of 4096 to push beyond the context window limitations of existing open-source language models.; Get started generating text with StableLM-Base-Alpha by using the following code snippet:; Developed by: Stability AI; Model type: StableLM-Base-Alpha models are auto-regressive language models based on the NeoX transformer architecture.; Language(s): English,,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"StableLM-Tuned-Alpha is a suite of 3B and 7B parameter decoder-only language models built on top of the StableLM-Base-Alpha models and further fine-tuned on various chat and instruction-following datasets.; Get started chatting with StableLM-Tuned-Alpha by using the following code snippet:; StableLM Tuned should be used with prompts formatted to <|SYSTEM|>...<|USER|>...<|ASSISTANT|>...
The system prompt is; StableLM-Tuned-Alpha models are fine-tuned on a combination of five datasets:
Alpaca, a dataset of 52,000 instructions and demonstrations generated by OpenAI's text-davinci-003 engine.
GPT4All Prompt Generations, which consists of 400k prompts and responses generated by GPT-4;
Anthropic HH, made up of preferences about AI assistant helpfulness and harmlessness;
DataBricks Dolly, comprising 15k instruction/responses generated by Databricks employees in capability domains from the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA and summarization;
and ShareGPT Vicuna (English subset), a dataset of conversations retrieved from ShareGPT.; Models are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (FP16), and optimized with AdamW. We outline the following hyperparameters:",,,,,,,,,,,,,,,,,,,,,,,,,usage 159
"Stanza is a collection of accurate and efficient tools for the linguistic analysis of many human languages. Starting from raw text to syntactic analysis and entity recognition, Stanza brings state-of-the-art NLP models to languages of your choosing.
Find more about it in our website and our GitHub repository.; This card and repo were automatically prepared with hugging_stanza.py in the stanfordnlp/huggingface-models repo; Last updated 2023-05-26 18:04:22.253",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Note, you may be interested in the Beta version of StarChat here.; StarChat is a series of language models that are fine-tuned from StarCoder to act as helpful coding assistants. StarChat Alpha is the first of these models, and as an alpha release is only intended for educational or research purpopses. In particular, the model has not been aligned to human preferences with techniques like RLHF, so may generate problematic content (especially when prompted to do so).; StarChat Alpha is intended for educational and/or research purposes and in that respect can be used to probe the programming capabilities of open-source language models.; StarChat Alpha has not been aligned to human preferences with techniques like RLHF or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). 
Models trained primarily on code data will also have a more skewed demographic bias commensurate with the demographics of the GitHub community, for more on this see the StarCoder dataset which is derived from The Stack.; Since the base model was pretrained on a large corpus of code, it may produce code snippets that are syntactically valid but semantically incorrect. 
For example, it may produce code that does not compile or that produces incorrect results.It may also produce code that is vulnerable to security exploits.We have observed the model also has a tendency to produce false URLs which should be carefully inspected before clicking.",,,,,,,,,,,,,,,,,,,,,,,,,usage 28
"StarChat is a series of language models that are trained to act as helpful coding assistants. StarChat-?? is the second model in the series, and is a fine-tuned version of StarCoderPlus that was trained on an ""uncensored"" variant of the openassistant-guanaco dataset. We found that removing the in-built alignment of the OpenAssistant dataset boosted performance on the Open LLM Leaderboard and made the model more helpful at coding tasks. However, this means that model is likely to generate problematic text when prompted to do so and should only be used for educational and research purposes.; The model was fine-tuned on a variant of the OpenAssistant/oasst1 dataset, which contains a diverse range of dialogues in over 35 languages. As a result, the model can be used for chat and you can check out our demo to test its coding capabilities. ; Here's how you can run the model using the pipeline() function from ? Transformers:; StarChat-?? has not been aligned to human preferences with techniques like RLHF or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). 
Models trained primarily on code data will also have a more skewed demographic bias commensurate with the demographics of the GitHub community, for more on this see the StarCoder dataset which is derived from The Stack.; Since the base model was pretrained on a large corpus of code, it may produce code snippets that are syntactically valid but semantically incorrect. 
For example, it may produce code that does not compile or that produces incorrect results.It may also produce code that is vulnerable to security exploits.We have observed the model also has a tendency to produce false URLs which should be carefully inspected before clicking.",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Please read the BigCode OpenRAIL-M license agreement before accepting it.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; ; Play with the model on the StarCoder Playground.",,,,,,,,,,,,,,,,,,,,,,,,,usage 68
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Bigcode's Starcoder.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools known to work with these model files.; These files are not compatible with llama.cpp.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is bigcode/starcoder fine-tuned on the teknium1/GPTeacher codegen dataset (GPT-4 code instruction fine-tuning).; The base StarCoder models are 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2), with opt-out requests excluded. The model uses Multi Query Attention, a context window of 8192 tokens,  and was trained using the Fill-in-the-Middle objective on 1 trillion tokens. ; The base model was trained on GitHub code and then fine-tuned to follow instructions. Prompts such as ""Write a function that computes the square root."" should work reasonably well. The original repo recommeds using the Tech Assistant prompt to few-shot prompt it into behaving as a technical assistant. This fine-tuned model uses the Alpaca prompts.; Full Prompt:; Response:",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Please read the BigCode OpenRAIL-M license agreement before accepting it.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; ; Play with the model on the StarCoder Playground.",,,,,,,,,,,,,,,,,,,,,,,,,usage 21
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Please read the BigCode OpenRAIL-M license agreement before accepting it.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Play with the instruction-tuned StarCoderPlus at StarChat-Beta.; StarCoderPlus is a fine-tuned version of StarCoderBase on 600B tokens from the English web dataset RedefinedWeb 
combined with StarCoderData from The Stack (v1.2) and a Wikipedia dataset.
It's a 15.5B parameter Language Model trained on English and 80+ programming languages. The model uses Multi Query Attention,
a context window of 8192 tokens,  and was trained using the Fill-in-the-Middle objective on 1.6 trillion tokens.",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Bigcode's StarcoderPlus.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools known to work with these model files.; These files are not compatible with llama.cpp.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are StarCoder GGML format model files for LoupGarou's Starcoderplus Guanaco GPT4 15B V1.0.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Please read the BigCode OpenRAIL-M license agreement before accepting it.
  ; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This is an encoder-only model (i.e., bi-directionally self-attentive Transformers) trained on The Stack dataset.; We leveraged the :",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; This is an NER model trained to detect Personal Identifiable Information (PII) in code datasets. We ask that you read and agree to the following Terms of Use before using the model:; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This is an NER model trained to detect Personal Identifiable Information (PII) in code datasets. We fine-tuned bigcode-encoder
on a PII dataset we annotated, available with gated access at bigcode-pii-dataset (see bigcode-pii-dataset-training for the exact data splits).
We added a linear layer as a token classification head on top of the encoder model, with 6 target classes: Names, Emails, Keys, Passwords, IP addresses and Usernames. ; The finetuning dataset contains 20961 secrets and 31 programming languages, but the base encoder model was pre-trained on 88 
programming languages from The Stack dataset.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"SteamSHP-XL is a preference model trained to predict -- given some context and two possible responses -- which response humans will find more helpful.
It can be used for NLG evaluation or as a reward model for RLHF.; It is a FLAN-T5-xl model (3B parameters) finetuned on:; There is a smaller variant called SteamSHP-Large that was made by finetuning FLAN-T5-large (780M parameters).
Despite being 1/4 of the size, it is on average only 0.75 points less accurate on the SHP + Anthropic test data (across all domains).; The input text should be of the format:; The output generated by SteamSHP-XL will either be A or B.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"StreetCLIP is a robust foundation model for open-domain image geolocalization and other
geographic and climate-related tasks.; Trained on an original dataset of 1.1 million street-level urban and rural geo-tagged images, it achieves
state-of-the-art performance on multiple open-domain image geolocalization benchmarks in zero-shot, 
outperforming supervised models trained on millions of images.; StreetCLIP is a model pretrained by deriving image captions synthetically from image class labels using
a domain-specific caption template. This allows StreetCLIP to transfer its generalized zero-shot learning
capabilities to a specific domain (i.e. the domain of image geolocalization). 
StreetCLIP builds on the OpenAI's pretrained large version of CLIP ViT, using 14x14 pixel
patches and images with a 336 pixel side length.; StreetCLIP has a deep understanding of the visual features found in street-level urban and rural scenes
and knows how to relate these concepts to specific countries, regions, and cities. Given its training setup,
the following use cases are recommended for StreetCLIP.; StreetCLIP can be used out-of-the box using zero-shot learning to infer the geolocation of images on a country, region,
or city level. Given that StreetCLIP was pretrained on a dataset of street-level urban and rural images,
the best performance can be expected on images from a similar distribution.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"| 
| 
| 
|  |; This model transcribes speech in lowercase English alphabet including spaces and apostrophes, and is trained on several thousand hours of English speech data.
It is a non-autoregressive ""large"" variant of Conformer, with around 120 million parameters.
See the model architecture section and NeMo documentation for complete architecture details.
It is also compatible with NVIDIA Riva for production-grade server deployments. ; The model is available for use in the NeMo toolkit [3], and can be used as a pre-trained checkpoint for inference or for fine-tuning on another dataset.; To train, fine-tune or play with the model you will need to install NVIDIA NeMo. We recommend you install it after you've installed latest PyTorch version.; First, let's get a sample",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"SuperCOT is a LoRA trained with the aim of making LLaMa follow prompts for Langchain better, by infusing chain-of-thought datasets, code explanations and instructions, snippets, logical deductions and Alpaca GPT-4 prompts.
It uses a mixture of the following datasets:; https://huggingface.co/datasets/QingyiSi/Alpaca-CoT; https://huggingface.co/datasets/neulab/conala; https://huggingface.co/datasets/yahma/alpaca-cleaned; (Thanks to all the awesome anons with supercomputers)",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a second prototype of SuperHOT, a NSFW focused LoRA, this time with 16K context and no RLHF, using the same technique described in the github blog.
Tests have shown that the model does indeed leverage the extended context at 8K, so naturally, let's try going even further.; You will need to use either the monkeypatch or, if you are already using the monkeypatch, change the scaling factor to 0.125 and the maximum sequence length to 16384; I trained the LoRA with the following configuration: ; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a second prototype of SuperHOT, a NSFW focused LoRA, this time with 4K context and no RLHF. In my testing, it can go all the way to 6K without breaking down and I made the change with intention to reach 8K, so I'll assume it will go to 8K although I only trained on 4K sequences.; You will NEED to apply the monkeypatch or, if you are already using the monkeypatch, change the scaling factor to 0.25 and the maximum sequence length to 8192; In order to use the 8K context, you will need to apply the monkeypatch I have added in this repo or follow the instructions for oobabooga's text-generation-webui -- without it, it will not work.; I will repeat: Without the patch with the correct scaling and max sequence length, it will not work!; The patch is very simple, and you can make the changes yourself:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
SuperPlatty-30B is a merge of lilloukas/Platypus-30B and kaiokendev/SuperCOT-LoRA; We use state-of-the-art EleutherAI Language Model Evaluation Harness to run the benchmark tests above.; Install LM Evaluation Harness:; Each task was evaluated on a single A100 80GB GPU.; ARC:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. ; Disclaimer: The team releasing Swin Transformer v2 did not write a model card for this model so this model card has been written by the Hugging Face team.; The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks. In contrast, previous vision Transformers produce feature maps of a single low resolution and have quadratic computation complexity to input image size due to computation of self-attention globally.; Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"SwissBERT is a masked language model for processing Switzerland-related text. It has been trained on more than 21 million Swiss news articles retrieved from Swissdox@LiRI.; SwissBERT is based on X-MOD, which has been pre-trained with language adapters in 81 languages.
For SwissBERT we trained adapters for the national languages of Switzerland ?C German, French, Italian, and Romansh Grischun.
In addition, we used a Switzerland-specific subword vocabulary.; The pre-training code and usage examples are available here. We also release a version that was fine-tuned on named entity recognition (NER): https://huggingface.co/ZurichNLP/swissbert-ner; SwissBERT contains the following language adapters:; Attribution-NonCommercial 4.0 International (CC BY-NC 4.0).",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
; ?Adapter Zoo | ?Demos | ?GitHub; T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models; The GitHub repo: https://github.com/TencentARC/T2I-Adapter; Please find the model information in https://github.com/TencentARC/T2I-Adapter/blob/main/docs/AdapterZoo.md,,,,,,,,,,,,,,,,,,,,,,,,,usage 9
"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-11B is the checkpoint with 11 billion parameters. ; The developers write in a blog post that the model:",,,,,,,,,,,,,,,,,,,,,,,,,usage 101
"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-3B is the checkpoint with 3 billion parameters. ; The developers write in a blog post that the model:",,,,,,,,,,,,,,,,,,,,,,,,,usage 102
"A fine-tuned AraT5 model on a dataset of 84,764 paragraph-summary pairs.; Paper: Arabic abstractive text summarization using RNN-based and transformer-based architectures.; Dataset: link.; The model can be used as follows:; banimarje@gmail.com",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-Base is the checkpoint with 220 million parameters. ; The developers write in a blog post that the model:",,,,,,,,,,,,,,,,,,,,,,,,,usage 310
"This is t5-base model trained for end-to-end question generation task. Simply input the text and the model will generate multile questions. ; You can play with the model using the inference API, just put the text and see the results!; For more deatils see this repo.; You'll need to clone the repo.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
The model has been trained on a collection of 500k articles with headings. Its purpose is to create a one-line heading suitable for the given article.; Sample code with a WikiNews article:; Result:; Trump and First Lady Melania Test Positive for COVID-19,,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"All credits to Abhishek Kumar Mishra; Google's T5 base fine-tuned on News Summary dataset for summarization downstream task.; The T5 model was presented in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu in Here the abstract:; Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ??Colossal Clean Crawled Corpus??, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Google's T5 fine-tuned on WikiSQL for English to SQL translation.; The T5 model was presented in Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu in Here the abstract:; Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ??Colossal Clean Crawled Corpus??, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.; ; Dataset ID: wikisql from  Huggingface/NLP",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This is a T5 (Text-to-Text Transfer Transformer) model pretrained on Japanese corpus.; ?¦³?????Z???`??????s100GB??????????????????§¶??T5 (Text-to-Text Transfer Transformer) ????????  ; ???¦´???????????¦³???§¶?????¦³????????¦´?????????????????????????`?????????????????????????????????`?????????????Z??????????????????`???????????????????????????????????????????§Ü?????????§·?????????????????????Y???????}??????????????
??????}???k????????????????????????????k????????????¦³????????Ñ”????????????????; SentencePiece??`????????`???????????Wikipedia?????`??????????????; https://github.com/sonoisa/t5-japanese",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"megagonlabs/t5-base-japanese-web is a T5 (Text-to-Text Transfer Transformer) model pre-trained on Japanese web texts.Training codes are available on GitHub.; The vocabulary size of this model is 32K.
8K version is also available.; We used following corpora for pre-training.; We used Japanese Wikipedia to train SentencePiece.; It took about 126 hours with TPU v3-8",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; t5-base-qa-squad-v1.1-portuguese is a QA model (Question Answering) in Portuguese that was finetuned on 27/01/2022 in Google Colab from the model unicamp-dl/ptt5-base-portuguese-vocab of Neuralmind on the dataset SQUAD v1.1 in portuguese from the Deep Learning Brasil group by using a Test2Text-Generation objective.; Due to the small size of T5 base and finetuning dataset, the model overfitted before to reach the end of training. Here are the overall final metrics on the validation dataset:; Check our other QA models in Portuguese finetuned on SQUAD v1.1:; NLP nas empresas | Como eu treinei um modelo T5 em portugu??s na tarefa QA no Google Colab (27/01/2022)",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Requires transformers>=4.0.0; This model was finetuned on the CoQa, Squad 2, GoEmotions and CNN/DailyMail.; It achieves a score of F1 79.5 on the Squad 2 dev set and a score of F1 70.6 on the CoQa dev set.; Summarisation and emotion detection has not been evaluated yet.; Kiri makes using state-of-the-art models easy, accessible and scalable.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is t5-base fine-tuned on the 190k Medium Articles dataset for predicting article tags using the article textual content as input. While usually formulated as a multi-label classification problem, this model deals with tag generation as a text2text generation task (inspiration from text2tags).; The dataset is composed of Medium articles and their tags. However, each Medium article can have at most five tags, therefore the author needs to choose what he/she believes are the best tags (mainly for SEO-related purposes). This means that an article with the ""Python"" tag may have not the ""Programming Languages"" tag, even though the first implies the latter.; To clean the dataset accounting for this problem, a hand-made taxonomy of about 1000 tags was built. Using the taxonomy, the tags of each articles have been augmented (e.g. an article with the ""Python"" tag will have the ""Programming Languages"" tag as well, as the taxonomy says that ""Python"" is part of ""Programming Languages""). The taxonomy is not public, if you are interested in it please send an email at chiusanofabio94@gmail.com.; The model has been trained on a single epoch spanning about 50000 articles, evaluating on 1000 random articles not used during training.; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This dataset contains 19,806 news articles written in Moroccan Arabic dialect along with their titles. The articles were crawled from Goud.ma website between 01/01/2018 and 12/31/2020. 
The articles are written mainly in Moroccan Arabic dialect (Darija) but some of them contain Modern Standard Arabic (MSA) passages. All the titles are written in Darija. 
The following table summarize some tatistics on the MArSum Dataset.; The following figure describes the creation process of MArSum:; ; You may refer to our paper, cited below, for more details on this process.; The dataset is split into Train/Test subsets using a 90/10 split strategy. Both subsets are available for direct donwload.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-Large is the checkpoint with 770 million parameters. ; The developers write in a blog post that the model:",,,,,,,,,,,,,,,,,,,,,,,,,usage 130
"; The developers of the Text-To-Text Transfer Transformer (T5) write: ; With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.; T5-Small is the checkpoint with 60 million parameters. ; The developers write in a blog post that the model:",,,,,,,,,,,,,,,,,,,,,,,,,usage 205
"Google's T5 Version 1.1; T5 Version 1.1 includes the following improvements compared to the original T5 model- GEGLU activation in feed-forward hidden layer, rather than ReLU - see here.; Dropout was turned off in pre-training (quality win). Dropout should be re-enabled during fine-tuning.; Pre-trained on C4 only without mixing in the downstream tasks.; no parameter sharing between embedding and classifier layer",,,,,,,,,,,,,,,,,,,,,,,,,usage 25
"This repo contains the trained model of Structured data learning with TabTransformer.
The full credit goes to: Khalid Salama; Spaces Link: ; The following hyperparameters were used during training:; Model history needed;",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This model has 2 versions which can be used. The default version corresponds to the tapas_wtq_wikisql_sqa_inter_masklm_base_reset checkpoint of the original Github repository.
This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).; The other (non-default) version which can be used is: ; Disclaimer: The team releasing TAPAS did not write a model card for this model so this model card has been written by
the Hugging Face team and contributors.; TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. 
This means it was pretrained on the raw tables and associated texts only, with no humans labelling them in any way (which is why it
can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language used in tables and associated texts, which can then be used 
to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed
or refuted by the contents of a table. Fine-tuning is done by adding a cell selection head and aggregation head on top of the pre-trained model, and then jointly train these randomly initialized classification heads with the base model on SQa, WikiSQL and finally WTQ.",,,,,,,,,,,,,,,,,,,,,,,,,usage 93
"This model has 2 versions which can be used. The default version corresponds to the tapas_sqa_inter_masklm_large_reset checkpoint of the original Github repository.
This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned on SQA. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).; The other (non-default) version which can be used is: ; Disclaimer: The team releasing TAPAS did not write a model card for this model so this model card has been written by
the Hugging Face team and contributors.; TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. 
This means it was pretrained on the raw tables and associated texts only, with no humans labelling them in any way (which is why it
can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language used in tables and associated texts, which can then be used 
to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed
or refuted by the contents of a table. Fine-tuning is done by adding a cell selection head on top of the pre-trained model, and then jointly
train this randomly initialized classification head with the base model on SQA.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model has 2 versions which can be used. The default version corresponds to the tapas_wtq_wikisql_sqa_inter_masklm_large_reset checkpoint of the original Github repository.
This model was pre-trained on MLM and an additional step which the authors call intermediate pre-training, and then fine-tuned in a chain on SQA, WikiSQL and finally WTQ. It uses relative position embeddings (i.e. resetting the position index at every cell of the table).; The other (non-default) version which can be used is: ; Disclaimer: The team releasing TAPAS did not write a model card for this model so this model card has been written by
the Hugging Face team and contributors.; TAPAS is a BERT-like transformers model pretrained on a large corpus of English data from Wikipedia in a self-supervised fashion. 
This means it was pretrained on the raw tables and associated texts only, with no humans labelling them in any way (which is why it
can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it
was pretrained with two objectives:; This way, the model learns an inner representation of the English language used in tables and associated texts, which can then be used 
to extract features useful for downstream tasks such as answering questions about a table, or determining whether a sentence is entailed
or refuted by the contents of a table. Fine-tuning is done by adding a cell selection head and aggregation head on top of the pre-trained model, and then jointly train these randomly initialized classification heads with the base model on SQa, WikiSQL and finally WTQ.",,,,,,,,,,,,,,,,,,,,,,,,,usage 12
"TAPEX was proposed in TAPEX: Table Pre-training via Learning a Neural SQL Executor by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. The original repo can be found here.; TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.; TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.; This model is the tapex-base model fine-tuned on the WikiTableQuestions dataset.; You can use the model for table question answering on complex questions. Some solveable questions are shown below (corresponding tables now shown):",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"TAPEX was proposed in TAPEX: Table Pre-training via Learning a Neural SQL Executor by Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou. The original repo can be found here.; TAPEX (Table Pre-training via Execution) is a conceptually simple and empirically powerful pre-training approach to empower existing models with table reasoning skills. TAPEX realizes table pre-training by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries.; TAPEX is based on the BART architecture, the transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder.; You can use the raw model for simulating neural SQL execution, i.e., employ TAPEX to execute a SQL query on a given table. However, the model is mostly meant to be fine-tuned on a supervised dataset. Currently TAPEX can be fine-tuned to tackle table question answering tasks and table fact verification tasks. See the model hub to look for fine-tuned versions on a task that interests you.; Here is how to use this model in transformers:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Introducing the Beta Version of TemporalNet; TemporalNet is a ControlNet model designed to enhance the temporal consistency of generated outputs, as demonstrated in this example: https://twitter.com/CiaraRowles1/status/1637486561917906944. While it does not eliminate all flickering, it significantly reduces it, particularly at higher denoise levels. For optimal results, it is recommended to use TemporalNet in combination with other methods.; Instructions for Use:; Add the model ""diff_control_sd15_temporalnet_fp16.safetensors"" to your models folder in the ControlNet extension in Automatic1111's Web UI.; Create a folder that contains:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"There are newer version of this using Flan-T5 as a based model. You can check out here; PS. From this discussion, I think the base model that I use for finetune did not support the token <, so this might not be a good model to do this tasks.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.; We Are Hiring! (Based in Beijing / Hangzhou, China.); If you're looking for an exciting challenge and the opportunity to work with cutting-edge technologies in AIGC and large-scale pretraining, then we are the place for you. We are looking for talented, motivated and creative individuals to join our team. If you are interested, please send your CV to us.; EMAIL: yingya.zyy@alibaba-inc.com; The text-to-video generation diffusion model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input. The diffusion model adopts a UNet3D structure, and implements video generation through the iterative denoising process from the pure Gaussian noise video.",,,,,,,,,,,,,,,,,,,,,,,,,usage 139
"This is a GPT-2 model fine-tuned on the succinctly/midjourney-prompts dataset, which contains 250k text prompts that users issued to the Midjourney text-to-image service over a month period. For more details on how this dataset was scraped, see Midjourney User Prompts & Generated Images (250k).; This prompt generator can be used to auto-complete prompts for any text-to-image model (including the DALL??E family):
; Note that, while this model can be used together with any text-to-image model, it occasionally produces Midjourney-specific tags. Users can specify certain requirements via double-dashed parameters (e.g. --ar 16:9 sets the aspect ratio to 16:9, and --no snake asks the model to exclude snakes from the generated image) or set the importance of various entities in the image via explicit weights (e.g. hot dog::1.5 food::-1 is likely to produce the image of an animal instead of a frankfurter).; When using this model, please attribute credit to Succinctly AI.",,,,,,,,,,,,,,,,,,,,,,,,,usage 32
"The model has been trained on a collection of 28k news articles with tags. Its purpose is to create tags suitable for the given article. We can use this model also for information-retrieval purposes (GenQ), fine-tuning sentence-transformers for asymmetric semantic search. ; If you like this project, consider supporting it with a cup of coffee! ???
; 
 
    Pieter Bruegel the Elder, The Fight Between Carnival and Lent, 1559
; Sample code with an article from IlPost:; Assuming paragraphs are divided by: '\n\n'.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese.; It maps sentences to a 768 dimensional dense vector space and can be used for tasks 
like sentence embeddings, text matching or semantic search.; For an automated evaluation of this model, see the Evaluation Benchmark: text2vec; ?????; Using this model becomes easy when you have text2vec installed:",,,,,,,,,,,,,,,,,,,,,,,,,usage 22
"Based on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replace MacBERT with LERT, and keep other training conditions unchanged??",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"Based on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replace MacBERT with LERT, and keep other training conditions unchanged??; Talk to me: https://twitter.com/GanymedeNil",,,,,,,,,,,,,,,,,,,,,,,,,usage 56
Original page:; https://civitai.com/models/40369/theallys-mix-iv-verisimilar,,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"This is a 164M parameters model with the same architecture as StarCoder (8k context length, MQA & FIM). It was trained on the Python data from StarCoderData
for ~6 epochs which amounts to 100B tokens.; The model was trained on GitHub code, to assist with some tasks like Assisted Generation. For pure code completion, we advise using our 15B models StarCoder or StarCoderBase.; Fill-in-the-middle uses special tokens to identify the prefix/middle/suffix part of the input and output:; The model is licensed under the BigCode OpenRAIL-M v1 license agreement. You can find the full agreement here.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This is the distilled version of the deepset/roberta-base-squad2 model. This model has a comparable prediction quality and runs at twice the speed of the base model.; Language model: tinyroberta-squad2Language: EnglishDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD 2.0Code: See an example QA pipeline on HaystackInfrastructure: 4x Tesla v100; This model was distilled using the TinyBERT approach described in this paper and implemented in haystack.
Firstly, we have performed intermediate layer distillation with roberta-base as the teacher which resulted in deepset/tinyroberta-6l-768d.
Secondly, we have performed task-specific distillation with deepset/roberta-base-squad2 as the teacher for further intermediate layer distillation on an augmented version of SQuADv2 and then with deepset/roberta-large-squad2 as the teacher for prediction layer distillation. ; Haystack is an NLP framework by deepset. You can use this model in a Haystack pipeline to do question answering at scale (over many documents). To load the model in Haystack:; Evaluated on the SQuAD 2.0 dev set with the official eval script.",,,,,,,,,,,,,,,,,,,,,,,,,usage 18
"Tk-Instruct is a series of encoder-decoder Transformer models that are trained to solve various NLP tasks by following in-context instructions (plain language task definitions, k-shot examples, explanations, etc). Built upon the pre-trained T5 models, they are fine-tuned on a large number of tasks & instructions that are collected in the Natural Instructions benchmark, which contains 1600+ tasks in 70+ broach categories in total. This enables the model to not only process the training tasks, but also generalize to many unseen tasks without further parameter update.; More resources for using the model:; Tk-Instruct can be used to do many NLP tasks by following instructions. ; When instructing the model, task definition or demonstration examples or explanations should be prepended to the original input and fed into the model. You can easily try Tk-Instruct models as follows:; We are still working on understanding the behaviors of these models, but here are several issues we have found:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"???????LoRA?????
?????`????NijiJourney?¦Ã???????¦³?????

???????m????????¦±?furry??????????????—ß?????

¦Ä?m????B???furry?????????????????????????
??????????Y???????????????????; Training Model:
?sdhk_v40.safetensors (https://civitai.com/models/82813/sdhk)
Trigger Words:
?furry; ????
?twitter: @TlanoAI; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"???¦´?????furry????????????????????^???{??????????????
??furry???????????¦Ã???????????^??????????; ; ; ????
?twitter: @TlanoAI; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Tortoise is a text-to-speech program built with the following priorities:; This repo contains all the code needed to run Tortoise TTS in inference mode.; I'm naming my speech-related repos after Mojave desert flora and fauna. Tortoise is a bit tongue in cheek: this model
is insanely slow. It leverages both an autoregressive decoder and a diffusion decoder; both known for their low
sampling rates. On a K80, expect to generate a medium sized sentence every 2 minutes.; See this page for a large list of example outputs.; If you want to use this on your own computer, you must have an NVIDIA GPU. First, install pytorch using these
instructions: https://pytorch.org/get-started/locally/",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"?? Disclaimer:
The huggingface models currently give different results to the detoxify library (see issue here). For the most up to date models we recommend using the models from https://github.com/unitaryai/detoxify; 
; ; Trained models & code to predict toxic comments on 3 Jigsaw challenges: Toxic comment classification, Unintended?Bias in Toxic comments, Multilingual toxic comment classification.; Built by Laura Hanu at Unitary, where we are working to stop harmful content online by interpreting visual content in context.",,,,,,,,,,,,,,,,,,,,,,,,,usage 9
"This model is a fine-tuned version of the DistilBERT model to classify toxic comments. ; You can use the model with the following code.; This model is intended to use for classify toxic online classifications. However, one limitation of the model is that it performs poorly for some comments that mention a specific identity subgroup, like Muslim. The following table shows a evaluation score for different identity group. You can learn the specific meaning of this metrics here. But basically, those metrics shows how well a model performs for a specific group. The larger the number, the better.; The table above shows that the model performs poorly for the muslim and jewish group. In fact, you pass the sentence ""Muslims are people who follow or practice Islam, an Abrahamic monotheistic religion."" Into the model, the model will classify it as toxic. Be mindful for this type of potential bias.; The training data comes this Kaggle competition. We use 10% of the train.csv data to train the model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"NOTICE: My LoRAs require high amount of tags to look good, I will fix this later on and update all of my LoRAs if everything works out.; Welcome to the place where I host my LoRAs. In short, LoRA is just a checkpoint trained on specific artstyle/subject that you load into your WebUI, that can be used with other models.Although you can use it with any model, the effects of LoRA will vary between them.
Most of the previews use models that come from WarriorMama777 .For more information about them, you can visit the original LoRA repository: https://github.com/cloneofsimo/loraEvery images posted here, or on the other sites have metadata in them that you can use in PNG Info tab in your WebUI to get access to the prompt of the image.Everything I do here is for free of charge!I don't guarantee that my LoRAs will give you good results, if you think they are bad, don't use them.; To use them in your WebUI, please install the extension linked under, following the installation guide:https://github.com/kohya-ss/sd-webui-additional-networks#installation; All of my LoRAs are to be used with their original danbooru tag. For example:  ; My LoRAs will have sufixes that will tell you how much they were trained. Either by using words like ""soft"" and ""hard"",where soft stands for lower amount of training and hard for higher amount of training.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 93
"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 85
"TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"This model generate the math expression LATEX sequence according to the handwritten math expression image.; in CROHME 2014 test dataset CER=0.507772718700326; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. ; Disclaimer: The team releasing TrOCR did not write a model card for this model so this model card has been written by the Hugging Face team.; The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.; You can use the raw model for optical character recognition (OCR) on single text-line images. See the model hub to look for fine-tuned versions on a task that interests you.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository provides all the necessary tools for using a HiFIGAN vocoder trained with LJSpeech. ; The pre-trained model takes in input a spectrogram and produces a waveform in output. Typically, a vocoder is used after a TTS model that converts an input text into a spectrogram.; The sampling frequency is 22050 Hz.; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; To perform inference on the GPU, add  run_opts={""device"":""cuda""}  when calling the from_hparams method.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"This repository provides all the necessary tools for Text-to-Speech (TTS)  with SpeechBrain using a Tacotron2 pretrained on LJSpeech.; The pre-trained model takes in input a short text and produces a spectrogram in output. One can get the final waveform by applying a vocoder (e.g., HiFIGAN) on top of the generated spectrogram.; Please notice that we encourage you to read our tutorials and learn more about
SpeechBrain.; If you want to generate multiple sentences in one-shot, you can do in this way:; To perform inference on the GPU, add  run_opts={""device"":""cuda""}  when calling the from_hparams method.",,,,,,,,,,,,,,,,,,,,,,,,,usage 11
Transformer text-to-speech model from fairseq S^2 (paper/code):; See also fairseq S^2 example.,,,,,,,,,,,,,,,,,,,,,,,,,usage 5
Transformer text-to-speech model from fairseq S^2 (paper/code):; See also fairseq S^2 example.,,,,,,,,,,,,,,,,,,,,,,,,,usage 15
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Models info :; https://civitai.com/models/105935; Original Author's DEMO images :;,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This is a roBERTa-base model trained on ~58M tweets and finetuned for irony detection with the TweetEval benchmark.; Output:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a RoBERTa-base model trained on ~124M tweets from January 2018 to December 2021, and finetuned for sentiment analysis with the TweetEval benchmark. 
The original Twitter-based RoBERTa model can be found here and the original reference paper is TweetEval. This model is suitable for English. ; Labels: 
0 -> Negative;
1 -> Neutral;
2 -> Positive; This sentiment analysis model has been integrated into TweetNLP. You can access the demo here.; Output:",,,,,,,,,,,,,,,,,,,,,,,,,usage 42
"This is a multilingual XLM-roBERTa-base model trained on ~198M tweets and finetuned for sentiment analysis. The sentiment fine-tuning was done on 8 languages (Ar, En, Fr, De, Hi, It, Sp, Pt) but it can be used for more languages (see paper for details).; This model has been integrated into the TweetNLP library.; Output:",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"This is UltraLM-13b delta weights, a chat language model trained upon UltraChat; The model is fine-tuned based on LLaMA-13b with a multi-turn chat-format template as below; To use this model, you need to recover the full model from the delta weights and perform inference following the template below:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
No model card; New: Create and edit this model card directly on the website!,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Google's UMT5; UMT5 is pretrained on the an updated version of mC4 corpus, covering 107 languages:; Afrikaans, Albanian, Amharic, Arabic, Armenian, Azerbaijani, Basque, Belarusian, Bengali, Bulgarian, Burmese, Catalan, Cebuano, Chichewa, Chinese, Corsican, Czech, Danish, Dutch, English, Esperanto, Estonian, Filipino, Finnish, French, Galician, Georgian, German, Greek, Gujarati, Haitian Creole, Hausa, Hawaiian, Hebrew, Hindi, Hmong, Hungarian, Icelandic, Igbo, Indonesian, Irish, Italian, Japanese, Javanese, Kannada, Kazakh, Khmer, Korean, Kurdish, Kyrgyz, Lao, Latin, Latvian, Lithuanian, Luxembourgish, Macedonian, Malagasy, Malay, Malayalam, Maltese, Maori, Marathi, Mongolian, Nepali, Norwegian, Pashto, Persian, Polish, Portuguese, Punjabi, Romanian, Russian, Samoan, Scottish Gaelic, Serbian, Shona, Sindhi, Sinhala, Slovak, Slovenian, Somali, Sotho, Spanish, Sundanese, Swahili, Swedish, Tajik, Tamil, Telugu, Thai, Turkish, Ukrainian, Urdu, Uzbek, Vietnamese, Welsh, West Frisian, Xhosa, Yiddish, Yoruba, Zulu.; Note: UMT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.; Pretraining Dataset: mC4",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
This model is the multilingual version of "UniTE: Unified Translation Evaluation".,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"MeinaUnreal objetive is to be able to do anime art with a 2.5d feeling.
( the VAE is already baked in the model ); For examples and prompts, please checkout: https://civitai.com/models/18798/meinaunreal
I have a discord server where you can post images that you generated, discuss prompt and/or ask for help.; https://discord.gg/XC9nGZNDUd If you like one of my models and want to support their updates; I've made a ko-fi page; https://ko-fi.com/meina where you can pay me a coffee <3; And a Patreon page; https://www.patreon.com/MeinaMix where you can support me and get acess to beta of my models!",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Upstage's Llama 30B Instruct 2048.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Upstage's Llama 30B Instruct 2048.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Many thanks to William Beauchamp from Chai for providing the hardware used to make and upload these files!",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Upstage's Llama 65B Instruct.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; These are guaranteed to be compatible with any UIs, tools and libraries released since late May. They may be phased out soon, as they are largely superseded by the new k-quant methods.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Upstage's Llama 65B Instruct.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model was obtained from following repo:; Merged using sciprts from: https://github.com/ymcui/Chinese-LLaMA-Alpaca; Then quanized using following command (no act order):; Can confirm model output normal text, but question-answering quality is unknown; Model type:
Vicuna is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.
It is an auto-regressive language model, based on the transformer architecture.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a 4-bit GPTQ version of the Vicuna 13B 1.1 model.; It was created by merging the deltas provided in the above repo with the original Llama 13B model, using the code provided on their Github page.; It was then quantized to 4bit using GPTQ-for-LLaMa.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"** Converted model for GPTQ from https://huggingface.co/lmsys/vicuna-13b-delta-v0. This is the best local model I've ever tried. I hope someone makes a version based on the uncensored dataset...**; GPTQ conversion command (on CUDA branch):
CUDA_VISIBLE_DEVICES=0 python llama.py ../lmsys/vicuna-13b-v0 c4 --wbits 4 --true-sequential --groupsize 128 --save vicuna-13b-4bit-128g.pt; Added 1 token to the tokenizer model:
python llama-tools/add_tokens.py lmsys/vicuna-13b-v0/tokenizer.model /content/tokenizer.model llama-tools/test_list.txt; Use of Oobabooga with these tags:
--wbits 4
--groupsize 128; Enjoy",,,,,,,,,,,,,,,,,,,,,,,,,usage 24
"Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning.
The training data is around 140K conversations collected from ShareGPT.com.
See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.; Vicuna is evaluated with standard benchmarks, human preference, and LLM-as-a-judge. See more details in this paper and leaderboard.; See vicuna_weights_version.md",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for LmSys' Vicuna 13B v1.3.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"vicuna-13b-v1.3-ger is a variant of LMSYS??s Vicuna 13b v1.3 model, finetuned on an additional dataset in German language. The original model has been trained on explain tuned datasets, created using instructions and input from WizardLM, Alpaca & Dolly-V2 datasets and applying Orca Research Paper dataset construction approaches.; This model is optimized for German text, providing proficiency in understanding, generating, and interacting with German language content. However the model is not yet fully optimized for German language, as it has been trained on a small, experimental dataset and has limited capabilities due to the small parameter count.
Some of the fineunting data is also targeted towards factual retrieval (only answer questions from information in the context and refuse to hallucinate) and the model should perform better for these tasks than original Vicuna.; I am working on improving the model??s capabilities and will update the model if there is sufficient interest.; A quantized GGML version for use with llama.cpp, kobold.cpp and other GUIs for CPU inference can be found here.; I did only evaluate the output on a small, handcrafted sample on test prompts in German, confirming that the model's ability to understand and generate German text is above the base model in many situations.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for LmSys' Vicuna 33B 1.3 (final) merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for LmSys' Vicuna 33B 1.3.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning.
The training data is around 140K conversations collected from ShareGPT.com.
See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.; Vicuna is evaluated with standard benchmarks, human preference, and LLM-as-a-judge. See more details in this paper and leaderboard.; See vicuna_weights_version.md",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"This repository contains an alternative version of the Vicuna 7B model.; This model was natively fine-tuned using ShareGPT data, but without the ""ethics"" filtering used for the original Vicuna.; A GPTQ quantised  4-bit version is available here.; Model type:
Vicuna is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
Vicuna was trained between March 2023 and April 2023.",,,,,,,,,,,,,,,,,,,,,,,,,usage 20
"delta v1.1 merge ; Model type:
Vicuna is an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.
It is an auto-regressive language model, based on the transformer architecture.; Model date:
Vicuna was trained between March 2023 and April 2023.; Organizations developing the model:
The Vicuna team with members from UC Berkeley, CMU, Stanford, and UC San Diego.; Paper or resources for more information:
https://vicuna.lmsys.org/",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Kevin Pro's Vicuna 7B CoT merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"NOTE: New version availablePlease check out a newer version of the weights here.If you still want to use this old version, please see the compatibility and difference between different versions here.; Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Command line interface: https://github.com/lm-sys/FastChat#vicuna-weights.APIs (OpenAI API, Huggingface API): https://github.com/lm-sys/FastChat/tree/main#api.  ; Vicuna v1.1 is fine-tuned from LLaMA with supervised instruction fine-tuning.
The training data is around 70K conversations collected from ShareGPT.com.
See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Vicuna is a chat assistant trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT.; The primary use of Vicuna is research on large language models and chatbots.
The primary intended users of the model are researchers and hobbyists in natural language processing, machine learning, and artificial intelligence.; Vicuna v1.3 is fine-tuned from LLaMA with supervised instruction fine-tuning.
The training data is around 140K conversations collected from ShareGPT.com.
See more details in the ""Training Details of Vicuna Models"" section in the appendix of this paper.; Vicuna is evaluated with standard benchmarks, human preference, and LLM-as-a-judge. See more details in this paper and leaderboard.; See vicuna_weights_version.md",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These are fp16 pytorch format model files for LmSys' Vicuna 7B v1.3 merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 7b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for LmSys' Vicuna 7B v1.3 merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the Hugging Face repo for storing pre-trained & fine-tuned checkpoints of our Video-LLaMA, which is a multi-modal conversational large language model with video understanding capability. ; For launching the pre-trained Video-LLaMA on your own machine, please refer to our github repo.; Unable to determine this model??s library. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Vision-and-Language Transformer (ViLT) model fine-tuned on VQAv2. It was introduced in the paper ViLT: Vision-and-Language Transformer
Without Convolution or Region Supervision by Kim et al. and first released in this repository. ; Disclaimer: The team releasing ViLT did not write a model card for this model so this model card has been written by the Hugging Face team.; You can use the raw model for visual question answering. ; Here is how to use this model in PyTorch:; (to do)",,,,,,,,,,,,,,,,,,,,,,,,,usage 111
"???????? | English; 
; 


; VisCPM is a family of open-source large multimodal models, which support multimodal conversational capabilities (VisCPM-Chat model) and text-to-image generation capabilities (VisCPM-Paint model) in both Chinese and English, achieving state-of-the-art peformance among Chinese open-source multimodal models. VisCPM is trained based on the large language model CPM-Bee with 10B parameters, fusing visual encoder (Q-Former) and visual decoder (Diffusion-UNet) to support visual inputs and outputs. Thanks to the good bilingual capability of CPM-Bee, VisCPM can be pre-trained with English multimodal data only and well generalize to achieve promising Chinese multimodal capabilities.; VisCPM????????????????????§µ???????????????????????VisCPM-Chat????????????????????VisCPM-Paint??????????????????????§Õ????????VisCPM????????????????????CPM-Bee??10B?????????????????????Q-Former???????????????Diffusion-UNet??????????????????????????????CPM-Bee??????????????????VisCPM?????????????????????????????????????????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"? Github Repo ? ? Twitter ? ? [GLM@ACL 22] [GitHub] ? ? [GLM-130B@ICLR 23] [GitHub] 
; 
    ? Join our Slack and WeChat
; VisualGLM-6B ?????????????????????????????????????????????????? ChatGLM-6B?????? 62 ????????????????? BLIP2-Qformer ????????????????????????????????????78???????; VisualGLM-6B ?????????? CogView ???????30M?????????????????300M????????????????????????????????????????????????????????????ChatGLM?????????????????¦²???????????????????????????????????????????; ??????????¡ä?????? VisualGLM-6B ?????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
A vision transformer finetuned to classify the age of a given person's face.,,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"This model was trained using Amazon SageMaker and the Hugging Face Deep Learning container,
The base model is Vision Transformer (base-sized model) which  is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels.Link to base model ; Link to dataset description; The CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton; The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.
This dataset,CIFAR100, is just like the CIFAR-10, except it has 100 classes containing 600 images each. There are 500 training images and 100 testing images per class. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Each image comes with a ""fine"" label (the class to which it belongs) and a ""coarse"" label (the superclass to which it belongs).; Sizes of datasets:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. ; Disclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.",,,,,,,,,,,,,,,,,,,,,,,,,usage 116
"Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. ; Disclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.; The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. ; Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.; Note that this model does not provide any fine-tuned heads, as these were zero'd by Google researchers. However, the model does include the pre-trained pooler, which can be used for downstream tasks (such as image classification).",,,,,,,,,,,,,,,,,,,,,,,,,usage 46
"This model is a fine-tuned version of google/vit-base-patch16-224-in21k on the cifar10 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This is an image captioning model trained by @ydshieh in flax  this is pytorch version of this.;,,,,,,,,,,,,,,,,,,,,,,,,,usage 289
"Vision Transformer (ViT) model pre-trained using the MAE method. It was introduced in the paper Masked Autoencoders Are Scalable Vision Learners by Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll??r, Ross Girshick and first released in this repository. ; Disclaimer: The team releasing MAE did not write a model card for this model so this model card has been written by the Hugging Face team.; The Vision Transformer (ViT) is a transformer encoder model (BERT-like). Images are presented to the model as a sequence of fixed-size patches.; During pre-training, one randomly masks out a high portion (75%) of the image patches. First, the encoder is used to encode the visual patches. Next, a learnable (shared) mask token is added at the positions of the masked patches. The decoder takes the encoded visual patches and mask tokens as input and reconstructs raw pixel values for the masked positions.; By pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Our vlT5 model is a keyword generation model based on encoder-decoder architecture using Transformer blocks presented by Google (https://huggingface.co/t5-base). The vlT5 was trained on scientific articles corpus to predict a given set of keyphrases based on the concatenation of the article??s abstract and title. It generates precise, yet not always complete keyphrases that describe the content of the article based only on the abstract.; Keywords generated with vlT5-base-keywords: encoder-decoder architecture, keyword generation; Results on demo model (different generation method, one model per language):; Our vlT5 model is a keyword generation model based on encoder-decoder architecture using Transformer blocks presented by Google (https://huggingface.co/t5-base). The vlT5 was trained on scientific articles corpus to predict a given set of keyphrases based on the concatenation of the article??s abstract and title. It generates precise, yet not always complete keyphrases that describe the content of the article based only on the abstract.; Keywords generated with vlT5-base-keywords: encoder-decoder architecture, vlT5, keyword generation, scientific articles corpus",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; The collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Relies on pyannote.audio 2.1: see installation instructions.; For commercial enquiries and scientific consulting, please contact me.For technical questions and bug reports, please check pyannote.audio Github repository.",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; don't forget like; dilike dulu lah kalau mau download; Support me : https://saweria.co/donate/Byzernn",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 12
"Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.; ; masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck; Original Weights; We also support a Gradio Web UI and Colab with Diffusers to run Waifu Diffusion:",,,,,,,,,,,,,,,,,,,,,,,,,usage 227
"Waifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.; The model originally used for fine-tuning is Stable Diffusion 1.4, which is a latent image diffusion model trained on LAION2B-en. The current model has been fine-tuned with a learning rate of 5.0e-6 for 10 epochs on 680k anime-styled images.; See here for an in-depth overview of Waifu Diffusion 1.3.; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
The CreativeML OpenRAIL License specifies: ; This model can be used for entertainment purposes and as a generative art assistant.",,,,,,,,,,,,,,,,,,,,,,,,,usage 204
"; masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck; Waifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.; This model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.
The CreativeML OpenRAIL License specifies: ; This model can be used for entertainment purposes and as a generative art assistant.",,,,,,,,,,,,,,,,,,,,,,,,,usage 11
"waifu-diffusion-xl is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning StabilityAI's SDXL 0.9 model provided as a research preview.; ; masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck; This model has been released under the SDXL 0.9 RESEARCH LICENSE AGREEMENT due to the repository containing the SDXL 0.9 weights before an official release. We have been given permission to release this model.; This model can be used for entertainment purposes and as a generative art assistant.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; Please for anyone, don't reupload my finetuned models to other site, like Civitai or image generating site like Seeart, Yodayo or other sites. Thank you for understanding this. ; Like my works and want to collaboration or funding my projects? contact me.  ; Finetuned WDXL-aesthetic-0.9 with images generated from Nijijourney.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model was trained from scratch on the librispeech_asr dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Facebook's Wav2Vec2; The base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. ; Note: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.; Paper; Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"Facebook's Wav2Vec2; The base model pretrained and fine-tuned on 960 hours of Librispeech on 16kHz sampled speech audio. When using the model
make sure that your speech input is also sampled at 16Khz.; Paper; Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli; Abstract",,,,,,,,,,,,,,,,,,,,,,,,,usage 175
"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Chinese using the train and validation splits of Common Voice 6.1, CSS10 and ST-CMDS.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned on RTX3090 for 50h; The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Facebook's Wav2Vec2; The large model pretrained on 16kHz sampled speech audio. 
Speech datasets from multiple domains were used to pretrain the model:; When using the model make sure that your speech input is also sampled at 16Khz.; Note: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.; Paper Robust Wav2Vec2",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the common_voice dataset.
It achieves the following results on the evaluation set:; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"Facebook's XLSR-Wav2Vec2; The base model pretrained on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. Note that this model should be fine-tuned on a downstream task, like Automatic Speech Recognition. Check out this blog for more information.; Paper; Authors: Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, Michael Auli; Abstract
This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on wav2vec 2.0 which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72% compared to the best known results. On BABEL, our approach improves word error rate by 16% relative compared to a comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages. We hope to catalyze research in low-resource speech understanding by releasing XLSR-53, a large model pretrained in 53 languages.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Chinese using the train and validation splits of Common Voice 6.1, CSS10 and ST-CMDS.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:",,,,,,,,,,,,,,,,,,,,,,,,,usage 10
"The model was fine-tuned on English using data from Common Voice 6.1. To ensure proper usage, speech input should be sampled at 16kHz. The fine-tuning process was made possible by the generous GPU credits from OVHcloud. The specific training script can be accessed from the provided GitHub repository. This fine-tuned model is expected to excel in English speech recognition tasks and is applicable to various speech-to-text applications.",,,,,,,,,,,,,,,,,,,,,,,,,usage 44
"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Hungarian using the train and validation splits of Common Voice 6.1 and CSS10.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Japanese using the train and validation splits of Common Voice 6.1, CSS10 and JSUT.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"Fine-tuned facebook/wav2vec2-large-xlsr-53 on Russian using the train and validation splits of Common Voice 6.1 and CSS10.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned thanks to the GPU credits generously given by the OVHcloud :); The script used for training can be found here: https://github.com/jonatasgrosman/wav2vec2-sprint; The model can be used directly (without a language model) as follows...; Using the HuggingSound library:",,,,,,,,,,,,,,,,,,,,,,,,,usage 15
"Fine-tuned facebook/wav2vec2-xls-r-1b on Portuguese using the train and validation splits of Common Voice 8.0, CORAA, Multilingual TEDx, and Multilingual LibriSpeech.
When using this model, make sure that your speech input is sampled at 16kHz.; This model has been fine-tuned by the HuggingSound tool, and thanks to the GPU credits generously given by the OVHcloud :); Using the HuggingSound library:; Writing your own inference script:; If you want to cite this model you can use this:",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the OPENSLR_SLR53 - bengali dataset.
It achieves the following results on the evaluation set. ; Without language model : ; With 5 gram language model trained on 30M sentences randomly chosen from AI4Bharat IndicCorp dataset : ; Note : 5% of a total 10935 samples have been used for evaluation. Evaluation set has 10935 examples which was not part of training training was done on first 95% and eval was done on last 5%. Training was stopped after 180k steps. Output predictions are available under files section.; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuned version of facebook/wav2vec2-xls-r-300m on the MOZILLA-FOUNDATION/COMMON_VOICE_7_0 - IT dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Microsoft's WavLM; The model was pretrained on 16kHz sampled speech audio with utterance and speaker contrastive loss. When using the model, make sure that your speech input is also sampled at 16kHz. ; Note: This model does not have a tokenizer as it was pretrained on audio alone. In order to use this model speech recognition, a tokenizer should be created and the model should be fine-tuned on labeled text data. Check out this blog for more in-detail explanation of how to fine-tune the model.; The model was pre-trained on:; Paper: WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"; For this release, we release five versions of the model:; The WD 1.5 Base model is only intended for training use. For generation, it is recomended to create your own finetunes and loras on top of WD 1.5 Base or use one of the aesthetic models. More information and sample generations for the aesthetic models are in the release notes; https://saltacc.notion.site/WD-1-5-Beta-3-Release-Notes-1e35a0ed1bb24c5b93ec79c45c217f63; WD 1.5 uses the same VAE as WD 1.4, which can be found here https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/vae/kl-f8-anime2.ckpt",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"? Paper (KDD 2023) 
  | 
  ? Github Repo
; WebGLM aspires to provide an efficient and cost-effective web-enhanced question-answering system using the 10-billion-parameter General Language Model (GLM). It aims to improve real-world application deployment by integrating web search and retrieval capabilities into the pre-trained language model.; WebGLM is built by the following parts:; This repo is the implementation of Bootstrap Generator.; See our Github Repo for more detailed usage.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"? Paper (KDD 2023) 
  | 
  ? Github Repo
; WebGLM-2B aspires to provide an efficient and cost-effective web-enhanced question-answering system using the 2-billion-parameter General Language Model (GLM). It aims to improve real-world application deployment by integrating web search and retrieval capabilities into the pre-trained language model.; WebGLM is built by the following parts:; This repo is the implementation of Bootstrap Generator.; See our Github Repo for more detailed usage.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuned version of distilbert-base-uncased on an unknown dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model translate Classical(ancient) Chinese to Modern Chinese, so I guess who's interested in the problemset can speak at least modern Chinese, hence... let me continue the documentation in Chinese; ?????????????????? ??????????????huggingface spaces + streamlit ???????????????????üž???? ??????????????
??????????? ???????? ???? ¦Ä??????????? ?????????????????? ????????; ????????????????????, ?????????github????????????????????? ; ?????????????????????? ???????????? ????source???§µ????????§µ??? ????50%???????????????§Ò??????; ???",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"This model translate modern Chinese to Classical Chinese, so I guess who's interested in the problemset can speak at least modern Chinese, so... let me continue the documentation in Chinese; ????????????????????, ??????github?????????????:?, ????&???? ; ????????????????????????????????????? ???????¦Ä?????; ?????????????????????? ????????????; ??? ?????generate??????eos_token_id?????102?????????????????? ?????????????§Ó????????(?????????????pad???=-100????)??",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al from OpenAI. The original code repository can be found here.; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. ; The models were trained on either English-only data or multilingual data. The English-only models were trained 
on the task of speech recognition. The multilingual models were trained on both speech recognition and speech 
translation. For speech recognition, the model predicts transcriptions in the same language as the audio. 
For speech translation, the model predicts transcriptions to a different language to the audio.",,,,,,,,,,,,,,,,,,,,,,,,,usage 124
"Available models; For more information, visit:; https://github.com/ggerganov/whisper.cpp/tree/master/models; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 8
"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al from OpenAI. The original code repository can be found here.; Update: following the release of the paper, the Whisper authors announced a  large-v2 model trained for 2.5x more epochs with regularization. This  large-v2 model surpasses the performance of the large model, with no architecture changes. Thus, it is recommended that the  large-v2 model is used in-place of the original large model. ; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision.",,,,,,,,,,,,,,,,,,,,,,,,,usage 31
"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al. from OpenAI. The original code repository can be found here.; Compared to the Whisper large model, the large-v2 model is trained for 2.5x more epochs with added regularization 
for improved performance.; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision.",,,,,,,,,,,,,,,,,,,,,,,,,usage 122
"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al from OpenAI. The original code repository can be found here.; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. ; The models were trained on either English-only data or multilingual data. The English-only models were trained 
on the task of speech recognition. The multilingual models were trained on both speech recognition and speech 
translation. For speech recognition, the model predicts transcriptions in the same language as the audio. 
For speech translation, the model predicts transcriptions to a different language to the audio.",,,,,,,,,,,,,,,,,,,,,,,,,usage 23
"This model is a fine-tuned version of openai/whisper-medium on the common_voice_11_0 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 15
"Whisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours 
of labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains without the need 
for fine-tuning.; Whisper was proposed in the paper Robust Speech Recognition via Large-Scale Weak Supervision 
by Alec Radford et al from OpenAI. The original code repository can be found here.; Disclaimer: Content for this model card has partly been written by the Hugging Face team, and parts of it were 
copied and pasted from the original model card.; Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. 
It was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. ; The models were trained on either English-only data or multilingual data. The English-only models were trained 
on the task of speech recognition. The multilingual models were trained on both speech recognition and speech 
translation. For speech recognition, the model predicts transcriptions in the same language as the audio. 
For speech translation, the model predicts transcriptions to a different language to the audio.",,,,,,,,,,,,,,,,,,,,,,,,,usage 66
"This model detects if you are writing in a format that is more similar to Simple English Wikipedia or English Wikipedia. This can be extended to applications that aren't Wikipedia as well and to some extent, it can be used for other languages.; Please also note there is a major bias to special characters (Mainly the hyphen mark, but it also applies to others) so I would recommend removing them from your input text.; You can use cURL to access this model:; Or Python API:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"? Donate to OpenAccess AI Collective to help us keep building great tools and models!; Manticore is available at https://huggingface.co/openaccess-ai-collective/manticore-13b and fixes many issues with Wizard Mega and adds new datasets to the training.; Wizard Mega is a Llama 13B model fine-tuned on the ShareGPT, WizardLM, and Wizard-Vicuna datasets. These particular datasets have all been filtered to remove responses where the model responds with ""As an AI language model..."", etc or when the model refuses to respond.; Try out the model in HF Spaces. The demo uses a quantized GGML version of the model to quickly return predictions on smaller GPUs (and even CPUs). Quantized GGML may have some minimal loss of model quality.; The Wizard Mega 13B SFT model is being released after two epochs as the eval loss increased during the 3rd (final planned epoch). Because of this, we have preliminarily decided to use the epoch 2 checkpoint as the final release candidate. https://wandb.ai/wing-lian/vicuna-13b/runs/5uebgm49",,,,,,,,,,,,,,,,,,,,,,,,,usage 12
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a float16 HF format repo for junelee's wizard-vicuna 13B.; June Lee's repo was also HF format. The reason I've made this is that the original repo was in float32, meaning it required 52GB disk space, VRAM and RAM.; This model was converted to float16 to make it easier to load and manage.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is wizard-vicuna-13b trained with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:  ; An uncensored model has no guardrails.  ; You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.",,,,,,,,,,,,,,,,,,,,,,,,,usage 22
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard-Vicuna-13B-Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is GPTQ format quantised 4bit models of Eric Hartford's 'uncensored' training of Wizard-Vicuna 13B.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Open the text-generation-webui UI as normal.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a float16 HF repo for Eric Hartford's 'uncensored' training of Wizard-Vicuna 13B.; It is the result of converting Eric's float32 repo to float16 for easier storage and use.; For further support, and discussions on these models and AI in general, join us at:",,,,,,,,,,,,,,,,,,,,,,,,,usage 16
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is fp16 pytorch format model files for Eric Hartford's Wizard Vicuna 13B Uncensored merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 13b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard Vicuna 13B Uncensored.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eric Hartford's Wizard Vicuna 13B Uncensored merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard Vicuna 30B Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Eric Hartford's Wizard-Vicuna-30B-Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Wizard-Vicuna-30B-Uncensored merged with bhenrym14's airoboros-33b-gpt4-1.4.1-lxctx-PI-16384-LoRA, quantized at 4 bit.; More info about the LoRA Here. This is an alternative to SuperHOT 8k LoRA trained with LoRA_rank 64 and context extended to 16K, with airoboros 1.4.1 dataset.; It was created with GPTQ-for-LLaMA with group size 32 and act order true as parameters, to get the maximum perplexity vs FP16 model.; I HIGHLY suggest to use exllama, to evade some VRAM issues.; Use compress_pos_emb = 8 for any context up to 16384 context.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is wizard-vicuna-13b trained against LLaMA-7B with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:  ; An uncensored model has no guardrails.  ; You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard Vicuna 7B Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,,,,,,,,,,,,,,,,,,,,,,,usage 6
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Wizard-Vicuna-7B-Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; This is a float16 HF repo of Eric Hartford's 'uncensored' training of Wizard-Vicuna 7B.; It is the result of converting Eric's float32 repo to float16 for easier storage.; For further support, and discussions on these models and AI in general, join us at:",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These are fp16 pytorch format model files for Eric Hartford's Wizard Vicuna 7B Uncensored merged with Kaio Ken's SuperHOT 8K.; Kaio Ken's SuperHOT 7b LoRA is merged on to the base model, and then 8K context can be achieved during inference by using trust_remote_code=True.; Note that config.json has been set to a sequence length of 8192. This can be modified to 4096 if you want to try with a smaller sequence length.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's Wizard Vicuna 7B Uncensored.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eric Hartford's Wizard Vicuna 7B Uncensored merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for WizardLM's WizardCoder 15B 1.0.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools known to work with these model files.; Matt Hoeffner has put up a live Space with a demo of this model:",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for WizardLM's WizardCoder 15B 1.0.; It is the result of quantising to 4bit using AutoGPTQ.; Please make sure you're using the latest version of text-generation-webui,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is the Full-Weight of WizardCoder.; Repository: https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder; Twitter: https://twitter.com/WizardLM_AI/status/1669109414559911937; Paper: WizardCoder: Empowering Code Large Language Models with Evol-Instruct; To develop our WizardCoder model, we begin by adapting the Evol-Instruct method specifically for coding tasks. This involves tailoring the prompt to the domain of code-related instructions. Subsequently, we fine-tune the Code LLM, StarCoder, utilizing the newly created instruction-following training set.",,,,,,,,,,,,,,,,,,,,,,,,,usage 32
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are StarCoder GGML format model files for LoupGarou's WizardCoder Guanaco 15B V1.0.; Please note that these GGMLs are not compatible with llama.cpp, or currently with text-generation-webui. Please see below for a list of tools that work with this GGML model.; These files are not compatible with llama.cpp or text-generation-webui.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for LoupGarou's WizardCoder Guanaco 15B V1.0.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Please make sure you're using the latest version of text-generation-webui.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"The WizardCoder-Guanaco-15B-V1.1 is a language model that combines the strengths of the WizardCoder base model and the openassistant-guanaco dataset for finetuning. The openassistant-guanaco dataset was further trimmed to within 2 standard deviations of token size for input and output pairs and all non-english data has been removed to reduce training size requirements.; Version 1.1 showcases notable enhancements, employing a modified version of the previous openassistant-guanaco dataset. This dataset underwent a comprehensive revision, replacing every single answer with those generated by GPT-4.; The volume of the datasets has also been augmented by approximately 50%, with a particular focus on high school and abstract algebra. This expansion leveraged the combined capabilities of GPT-4 and GPT-3.5-Turbo. The initial evaluation of algebraic functions over 12 epochs indicated promising results from this enriched dataset. However, this is just the beginning; further refinements are in the pipeline, aiming to optimize the dataset quality and subsequently decrease the number of epochs required to achieve comparable results.; Considering the need to curtail memory consumption during training, this dataset was tailored to consist solely of English language questions and answers. Consequently, the model's performance in language translation may not be up to par. Nevertheless, the focus remains on enhancing the model's proficiency and efficiency within its defined scope.; This model is designed to be used for a wide array of text generation tasks that require understanding and generating English text. The model is expected to perform well in tasks such as answering questions, writing essays, summarizing text, translation, and more. However, given the specific data processing and finetuning done, it might be particularly effective for tasks related to English language question-answering systems.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are StarCoder GGML format model files for LoupGarou's WizardCoder-Guanaco-15B-V1.1.; Please note that these GGMLs are not compatible with llama.cpp, text-generation-webui or llama-cpp-python. Please see below for a list of tools that work with this GGML model.; These files were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for LoupGarou's WizardCoder-Guanaco-15B-V1.1.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are fp16 unquantised format model files for WizardLM 13B 1.0.; It is the result of merging the deltas provided in the above repo.; Join me at: https://discord.gg/UBgz4VXf,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is WizardLM trained with a subset of the dataset - responses that contained alignment / moralizing were removed.  The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:An uncensored model has no guardrails.You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.
Publishing anything this model generates is the same as publishing it yourself.
You are responsible for the content you publish, and you cannot blame the model any more than you can blame the knife, gun, lighter, or car for what you do with it.",,,,,,,,,,,,,,,,,,,,,,,,,usage 40
"This is WizardLM-13B V1.0 diff weight.; Project Repo: https://github.com/nlpxucan/WizardLM; NOTE: The WizardLM-13B-1.0 and Wizard-7B use different prompt at the beginning of the conversation:; For WizardLM-13B-1.0 , the Prompt should be as following:; For WizardLM-7B , the Prompt should be as following:",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's WizardLM-13b-V1.0-Uncensored.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Eric Hartford's WizardLM-13b-V1.0-Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's WizardLM-13b-V1.0-Uncensored.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
This is the Full-Weight of WizardLM-13B V1.1 model.; Repository: https://github.com/nlpxucan/WizardLM; Twitter: https://twitter.com/WizardLM_AI/status/1677282955490918401,,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for WizardLM's WizardLM 13B V1.1.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; Thanks to the work of LostRuins/concedo, it is now possible to provide 100% working GGML k-quants for models like this which have a non-standard vocab size (32,001).",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for WizardLM's WizardLM 13B V1.1.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for WizardLM's WizardLM 13B V1.1 merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's 'uncensored' WizardLM 30B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Eric Hartford's WizardLM 30B Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Panchovix's merge of WizardLM 33B V1.0 Uncensored and SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for WizardLM's WizardLM 7B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for WizardLM's WizardLM-7B 4bit.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are the result of merging the delta weights with the original Llama7B model.; The code for merging is provided in the WizardLM official Github repo.; The original WizardLM deltas are in float32, and this results in producing an HF repo that is also float32, and is much larger than a normal 7B Llama model.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"This is WizardLM trained with a subset of the dataset - responses that contained alignment / moralizing were removed.  The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:  ; An uncensored model has no guardrails.  ; You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car.",,,,,,,,,,,,,,,,,,,,,,,,,usage 20
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eric Hartford's 'uncensored' version of WizardLM.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; Eric did a fresh 7B training using the WizardLM method, on a dataset edited to remove all the ""I'm sorry.."" type ChatGPT responses.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
The WizardLM delta weights.,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Eric Hartford's WizardLM-7B-V1.0-Uncensored.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Eric Hartford's WizardLM-7B-V1.0-Uncensored.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Eric Hartford's WizardLM-7B-V1.0-Uncensored merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is WizardLM trained on top of tiiuae/falcon-40b, with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn't have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.; Shout out to the open source AI/ML community, and everyone who helped me out.; Note:
An uncensored model has no guardrails.
You are responsible for anything you do with the model, just as you are responsible for anything you do with any dangerous object such as a knife, gun, lighter, or car. Publishing anything this model generates is the same as publishing it yourself. You are responsible for the content you publish, and you cannot blame the model any more than you can blame the knife, gun, lighter, or car for what you do with it.; Prompt format is WizardLM.; Thank you chirper.ai for sponsoring some of my compute!",,,,,,,,,,,,,,,,,,,,,,,,,usage 3
"This model is a triple model merge of WizardLM Uncensored+CoT+Storytelling, resulting in a comprehensive boost in reasoning and story writing capabilities.; To allow all output, at the end of your prompt add ### Certainly!; You've become a compendium of knowledge on a vast array of topics. ; Lore Mastery is an arcane tradition fixated on understanding the underlying mechanics of magic. It is the most academic of all arcane traditions. The promise of uncovering new knowledge or proving (or discrediting) a theory of magic is usually required to rouse its practitioners from their laboratories, academies, and archives to pursue a life of adventure. Known as savants, followers of this tradition are a bookish lot who see beauty and mystery in the application of magic. The results of a spell are less interesting to them than the process that creates it. Some savants take a haughty attitude toward those who follow a tradition focused on a single school of magic, seeing them as provincial and lacking the sophistication needed to master true magic. Other savants are generous teachers, countering ignorance and deception with deep knowledge and good humor.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Monero's WizardLM-Uncensored-SuperCOT-Storytelling-30B.; GGML files are for CPU + GPU inference using llama.cpp and libraries and UIs which support this format, such as:; I have quantized these 'original' quantisation methods using an older version of llama.cpp so that they remain compatible with llama.cpp as of May 19th, commit 2d5db48.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ model files for Monero's WizardLM-Uncensored-SuperCOT-Storytelling-30B.; Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them.; These models were quantised using hardware kindly provided by Latitude.sh.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GGML format model files for Monero's WizardLM Uncensored SuperCOT Storytelling 30B.; These are SuperHOT GGMLs with an increased context length. SuperHOT is a new system that employs RoPE to expand context beyond what was originally possible for a model. It was discovered and developed by kaiokendev.; In order to use the increased context length, you can presently use:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Chat & support: my new Discord server; Want to contribute? TheBloke's Patreon page; These files are GPTQ 4bit model files for Monero's WizardLM Uncensored SuperCOT Storytelling 30B merged with Kaio Ken's SuperHOT 8K.; It is the result of quantising to 4bit using GPTQ-for-LLaMa.; This is an experimental new GPTQ which offers up to 8K context size,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This is a ported version of fairseq wmt19 transformer for ru-en.; For more details, please see, Facebook FAIR's WMT19 News Translation Task Submission.; The abbreviation FSMT stands for FairSeqMachineTranslation; All four models are available:; Pretrained weights were left identical to the original model released by fairseq. For more details, please, see the paper.",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"X-CLIP model (base-sized, patch resolution of 32) trained fully-supervised on Kinetics-400. It was introduced in the paper Expanding Language-Image Pretrained Models for General Video Recognition by Ni et al. and first released in this repository.; This model was trained using 8 frames per video, at a resolution of 224x224.; Disclaimer: The team releasing X-CLIP did not write a model card for this model so this model card has been written by the Hugging Face team.; X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. ;",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Official research release for the family of XGen models (7B) by Salesforce AI Research:; Title: Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length; Authors: Erik Nijkamp*, Tian Xie*, Hiroaki Hayashi*, Bo Pang*, Congying Xia*, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovs'ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Rayhan Joty, Caiming Xiong.; (* indicates equal contribution); Correspondence to: Shafiq Rayhan Joty, Caiming Xiong",,,,,,,,,,,,,,,,,,,,,,,,,usage 9
"Official research release for the family of XGen models (7B) by Salesforce AI Research:; Title: Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length; Authors: Erik Nijkamp*, Tian Xie*, Hiroaki Hayashi*, Bo Pang*, Congying Xia*, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, Senthil Purushwalkam, Tong Niu, Wojciech Kryscinski, Lidiya Murakhovs'ka, Prafulla Kumar Choubey, Alex Fabbri, Ye Liu, Rui Meng, Lifu Tu, Meghana Bhat, Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Rayhan Joty, Caiming Xiong.; (* indicates equal contribution); Correspondence to: Shafiq Rayhan Joty, Caiming Xiong",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al. and first released in this repository. ; Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.; XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. ; RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.; More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.",,,,,,,,,,,,,,,,,,,,,,,,,usage 43
"This model is a fine-tuned version of xlm-roberta-base on the conll2003 dataset.
It achieves the following results on the evaluation set:; More information needed; More information needed; More information needed; The following hyperparameters were used during training:",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuned version of xlm-roberta-base on the Language Identification dataset.; This model is an XLM-RoBERTa transformer model with a classification head on top (i.e. a linear layer on top of the pooled output). 
For additional information please refer to the xlm-roberta-base model card or to the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al.; You can directly use this model as a language detector, i.e. for sequence classification tasks. Currently, it supports the following 20 languages: ; arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh); The model was fine-tuned on the Language Identification dataset, which consists of text sequences in 20 languages. The training set contains 70k samples, while the validation and test sets 10k each. The average accuracy on the test set is 99.6% (this matches the average macro/weighted F1-score being the test set perfectly balanced). A more detailed evaluation is provided by the following table.",,,,,,,,,,,,,,,,,,,,,,,,,usage 9
XLM-RoBERTa finetuned on NER. Check more detail at TNER repository.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al. and first released in this repository. ; Disclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.; XLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. ; RoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.; More precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.",,,,,,,,,,,,,,,,,,,,,,,,,usage 17
"Language model: xlm-roberta-largeLanguage: MultilingualDownstream-task: Extractive QATraining data: SQuAD 2.0Eval data: SQuAD dev set - German MLQA - German XQuADTraining run: MLFlow linkInfrastructure: 4x Tesla v100; Evaluated on the SQuAD 2.0 English dev set with the official eval script.; Evaluated on German MLQA: test-context-de-question-de.json; Evaluated on German XQuAD: xquad.de.json; For doing QA at scale (i.e. many docs instead of single paragraph), you can load the model also in haystack:",,,,,,,,,,,,,,,,,,,,,,,,,usage 7
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; This model takes xlm-roberta-large and fine-tunes it on a combination of NLI data in 15 languages. It is intended to be used for zero-shot text classification, such as with the Hugging Face ZeroShotClassificationPipeline.; This model is intended to be used for zero-shot text classification, especially in languages other than English. It is fine-tuned on XNLI, which is a multilingual NLI dataset. The model can therefore be used with any of the languages in the XNLI corpus:; Since the base model was pre-trained trained on 100 different languages, the
model has shown some effectiveness in languages beyond those listed above as
well. See the full list of pre-trained languages in appendix A of the
XLM Roberata paper",,,,,,,,,,,,,,,,,,,,,,,,,usage 11
xRicaMix - ? ???? xRica?? ?? ?? ? ??? ???? ?? ??? ??? xRicaMix? ?? ????; ?? ??? ???? ?? ??? ????. ?? ???? ??? ^^; ?? ??? ??? ? ???? ?? ???? ???? ? ???? ????; ??? ?? ???? ?? ???? ???? ??? ?????; ??? ?? ?? ? ???? ?? ????,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This repository is publicly accessible, but
			you have to accept the conditions to access its files and content.
		; XuanYuan LICENSE1.Definitions""Licensor"" refers to the XuanYuan Model Team, the entity responsible for distributing its Software.""Software"" pertains to the XuanYuan model parameters made accessible under this license.2.License GrantSubject to the conditions outlined in this License, the Licensor hereby grants you a non-exclusive, worldwide, non-transferable, non-sublicensable, revocable, royalty-free copyright license to utilize the Software exclusively for non-commercial research purposes.You must include the above copyright notice and this permission notice in all copies or significant portions of the Software.3.RestrictionsYou are prohibited from engaging in the following actions with the Software, either in whole or in part: using, copying, modifying, merging, publishing, distributing, reproducing, or creating derivative works of the Software, for commercial, military, or illegal purposes.You must refrain from using the Software for any activities that could undermine China's national security and national unity, jeopardize public interest, or infringe upon the rights and interests of individuals.4.DisclaimerThe Software is provided ""as is"" without any kind of warranty, either express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose, and non-infringement. The authors or copyright holders shall not be held liable for any claims, damages, or other liabilities arising from the use or performance of the Software, whether in an action of contract, tort, or otherwise, even if they have been advised of the possibility of such damages.5.Limitation of LiabilityTo the extent permitted by applicable law, under no legal theory, including tort, negligence, contract, or otherwise, shall the Licensor be liable to you for any direct, indirect, special, incidental, exemplary, or consequential damages, or any other commercial losses, arising from the use or inability to use the Software. This limitation applies even if the Licensor has been advised of the possibility of such damages.6.Dispute ResolutionThis license shall be governed and construed in accordance with the laws of the People's Republic of China. Any dispute arising from or in connection with this License shall be submitted to the Haidian District People's Court in Beijing.; Log in 
			or
			Sign Up 
			to review the conditions and access this model content.
		; XuanYuan: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters; ????????????????????????????????????????????????????????????????????????????????BLOOM-176B???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????î•",,,,,,,,,,,,,,,,,,,,,,,,,usage 4
"This is ELECTRA Small model for Japanese pretrained on 354 million sentences / 5.6 billion words of YACIS blog corpus.; The corpus was tokenized for pretraining with MeCab. Subword tokenization was done with WordPiece. ; This model uses ELECTRA Small model settings, 12 layers, 128 dimensions of hidden states, and 12 attention heads.; Vocabulary size was set to 32,000 tokens.; YACIS-ELECTRA is trained on the whole of YACIS blog corpus, which is a Japanese blog corpus containing 5.6 billion words in 354 million sentences.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"https://github.com/yandex/YaLM-100B; YaLM 100B is a GPT-like neural network for generating and processing text. It can be used freely by developers and researchers from all over the world.; The model leverages 100 billion parameters. It took 65 days to train the model on a cluster of 800 A100 graphics cards and 1.7 TB of online texts, books, and countless other sources in both English and Russian.; Training details and best practices on acceleration and stabilizations can be found on Medium (English) and Habr (Russian) articles.; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
????????????????????????????????????????????????????????????y???????????????????????????????????????????????????????????????????????????????????????????????????????????§µ????????????????????????????????????????????????????????????????????????????????????????????§Ô???????????????????????????????????????????????; ?????????????????????????????????????????????????????????????????????????¦Ë??????ûs???????????????; News: ? ???????????????? LLaMA 2 ????????????·Ú??????????????????????????????????; ??????¦Ï?????? ?Github Repo??; Comming Soon~,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"????????CreativeML Open RAIL-M???¦É????ð—?????????? ???????????????????}???v?????????????????¦³???????????§¶???????????????? ; BraV6 https://huggingface.co/BanKaiPls/AsianModel; XXMix_9 https://civitai.com/models/47274/xxmix9; Soda Mix https://civitai.com/models/47507/soda-mix; ????????????????????????????????¦³????????????¡è??v????¦Ô????????????????????
??????????F
????????
?¦Ä?????????????F
?¦Ä?????????????F????????????????????????????????????F",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"No model card; New: Create and edit this model card directly on the website!; Unable to determine this model??s pipeline type. Check the
								docs 
.",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"This model is a fine-tuned version of hustvl/yolos-small on the licesne-plate-recognition dataset from Roboflow which contains 5200 images in the training set and 380 in the validation set.
The original YOLOS model was fine-tuned on COCO 2017 object detection (118k annotated images).; YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).; You can use the raw model for object detection. See the model hub to look for all available YOLOS models.; Here is how to use this model:; Currently, both the feature extractor and model support PyTorch.",,,,,,,,,,,,,,,,,,,,,,,,,usage 2
"YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. ; Disclaimer: The team releasing YOLOS did not write a model card for this model so this model card has been written by the Hugging Face team.; YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).; The model is trained using a ""bipartite matching loss"": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a ""no object"" as class and ""no bounding box"" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.; You can use the raw model for object detection. See the model hub to look for all available YOLOS models.",,,,,,,,,,,,,,,,,,,,,,,,,usage 37
More models available at: awesome-yolov8-models; Inference API has been turned off for this model.,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
More models available at: awesome-yolov8-models; Inference API has been turned off for this model.,,,,,,,,,,,,,,,,,,,,,,,,,usage 1
YumekawaMix; ???h???????????????????????????????kawaii???ð›??????¦Ç???; ???h??????????????????????????§¶?????????}?j??????????????????; ??????????¦È??x?????; Twitter: @AiRSTR7,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"example outputs (courtesy of dotsimulate); A collection of watermark-free Modelscope-based video models capable of generating high quality video at 448x256, 576x320 and 1024 x 576. These models were trained from the original weights with offset noise using 9,923 clips and 29,769 tagged frames.
This collection makes it easy to switch between models with the new dropdown menu in the 1111 extension.; Simply download the contents of this repo to 'stable-diffusion-webui\models\text2video'
Or, manually download the model folders you want, along with VQGAN_autoencoder.pth.; Thanks to dotsimulate for the config files.; Thanks to camenduru, kabachuha, ExponentialML, VANYA, polyware, tin2tin",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"; A watermark-free Modelscope-based video model optimized for producing high-quality 16:9 compositions and a smooth video output. This model was trained from the original weights using 9,923 clips and 29,769 tagged frames at 24 frames, 576x320 resolution.
zeroscope_v2_567w is specifically designed for upscaling with zeroscope_v2_XL using vid2vid in the 1111 text2video extension by kabachuha. Leveraging this model as a preliminary step allows for superior overall compositions at higher resolutions in zeroscope_v2_XL, permitting faster exploration in 576x320 before transitioning to a high-resolution render. See some example outputs that have been upscaled to 1024x576 using zeroscope_v2_XL. (courtesy of dotsimulate); zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320; For upscaling, it's recommended to use zeroscope_v2_XL via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. ; Let's first install the libraries required:",,,,,,,,,,,,,,,,,,,,,,,,,usage 33
"example outputs (courtesy of dotsimulate); A watermark-free Modelscope-based video model capable of generating high quality video at 1024 x 576. This model was trained from the original weights with offset noise using 9,923 clips and 29,769 tagged frames at 24 frames, 1024x576 resolution.
zeroscope_v2_XL is specifically designed for upscaling content made with zeroscope_v2_576w using vid2vid in the 1111 text2video extension by kabachuha. Leveraging this model as an upscaler allows for superior overall compositions at higher resolutions, permitting faster exploration in 576x320 (or 448x256) before transitioning to a high-resolution render.; zeroscope_v2_XL uses 15.3gb of vram when rendering 30 frames at 1024x576; For upscaling, it's recommended to use the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip.; Let's first install the libraries required:",,,,,,,,,,,,,,,,,,,,,,,,,usage 12
"Ziya-Visual????????????????????????V1????????????????????????????3?¡¤?OpenAI????????????????????????GPT-4???????????????????????????????????GPT-4????????????Ziya-Visual?¦Ï???Mini-GPT4??LLaVA???????????????????Ziya???????????????????????????ùz?????????????????????????????????; The Ziya-Visual multimodal Big Model is based on the Ziya-LLaMA-13B-v1 training and has visual question and answer and dialogue capabilities. In March this year, OpenAI released GPT-4, a multimodal big model with image recognition capabilities. Unfortunately, to date, the vast majority of users have not yet been given access to GPT-4 for image input, so Ziya-Visual refers to Mini-GPT4, LLaVA and other excellent open source implementations to complement Ziya's image recognition capabilities, so that the Chinese user community can experience the superior capabilities of a large model combining two modalities: visual and language.; ?????????????????????????????????????????????????????§µ????????????????????????????????????????????????????????????????????????????????????????????????????????????; This example demonstrates the model's ability to read pictures, its knowledge and its ability to compose. Firstly in the first problem, the model identifies the picture as a scene from the movie Titanic and gives information about the movie director, release date and award achievements; in the second problem, the model creates a modern love poem based on the user's needs.;",,,,,,,,,,,,,,,,,,,,,,,,,usage 1
????LLaMA-13B??????????????; shibing624/ziya-llama-13b-medical-merged evaluate test data??; The overall performance of model on QA test:; ??????????????§Ö?????????????????????????1???????????????Ziya-LLaMA-13B??????????????????????2????????????????240????????????????????????????????????????????????????????????????????????????????????????????????LLaMA-13B??; training args:,,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"??LLaMA???????????????????????????????????????????????¦Ï??????????§Ü??); ?????????????V1?????LLaMa??130?????????????????????????????????????????????????????????????????????????????????????????????????????????????????§Þ??????????????????¦Å?????????; The Ziya-LLaMA-13B-v1 is a large-scale pre-trained model based on LLaMA with 13 billion parameters. It has the ability to perform tasks such as translation, programming, text classification, information extraction, summarization, copywriting, common sense Q&A, and mathematical calculation. The Ziya-LLaMA-13B-v1 has undergone three stages of training: large-scale continual pre-training (PT), multi-task supervised fine-tuning (SFT), and human feedback learning (RM, PPO).; ?????????????????????????????????openwebtext??Books??Wikipedia??Code????????????????????????????????????????????????????????????????????????????????????????????????????????????????125B tokens????§¹?????; ?????LLaMA???????????????§¹????¦Ì???????????LLaMA????????????????7k+??????????????????LLaMA????????????????????39410??§³???????????????Transformers??LlamaTokenizer??????????§¹????",,,,,,,,,,,,,,,,,,,,,,,,,usage 5
"??LLaMA???????????????????????????????????????????????¦Ï??????????§Ü??); ?????Ziya-LLaMA-13B-v1?????§Þ???????????????·ÚZiya-LLaMA-13B-v1.1??????????????????????????????????????????·Ú????????????????????????????????????????????????????????????; We have further optimized the Ziya-LLaMA-13B-v1 model and released the open-source version Ziya-LLaMA-13B-v1.1. By adjusting the proportion of fine-tuning data and adopting a better reinforcement learning strategy, this version has achieved improvements in question-answering accuracy, mathematical ability, and safety, as shown in the following figure in detail.; ??¦Ï?Ziya-LLaMA-13B-v1??????????; ????????????????3??.bin?????md5??????59194d10b1553d66131d8717c9ef03d6??cc14eebe2408ddfe06b727b4a76e86bb??4a8495d64aa06aee96b5a1cc8cc55fa7??",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"Ziya-LLaMA-7B-Reward????Ziya-LLaMA???????????????????????????????; ???????????????????????????????LLM?????????????????????; Ziya-LLaMA-7B-Reward is based on the Ziya-LLaMA model, trained on the following preference ranking data:; The model is able to simulate a bilingual reward environment and provide accurate reward feedback on LLM generation results.; ?????????????§Ø????????????§Ø??????????????????????????????????????????????",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
"??????§Õ???????V1?????LLaMa??130???????????????????§Õ????????????????????????????§Õ???????????????§Õ????????????????èå??????????????????????§Õ??????; Ziya-Writing-LLaMa-13B-v1 is a 13-billion parameter instruction fine-tuned model based on LLaMa, which has been enhanced for better performance in writing tasks. It is a large model that focuses on writing. Ziya-Writing-LLaMa-13B-v1 can handle several types of writing tasks, including official reports, speeches, creative copywriting, and more.; ??????????¦Ï?????????????¡ê?; ?????????????? | §Õ?????ziya-writing????????°é?????????????????§Õ??§³?????; ?????????????????????????????????§Õ???????????GPT-3.5????????§Õ??????????????????????§µ?î•",,,,,,,,,,,,,,,,,,,,,,,,,usage 0
Model info :; https://civitai.com/models/108417/zodiac-eclipse-day1; Sample image I made thru Huggingface's API :; ; Original Author's DEMO images :,,,,,,,,,,,,,,,,,,,,,,,,,usage 3
